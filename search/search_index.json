{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RAG Blueprint Documentation Overview The RAG blueprint project is a Retrieval-Augmented Generation system that integrates with several datasources to provide intelligent document search and analysis. The system combines the power of different large language models with knowledge bases to deliver accurate, context-aware responses through a chat interface. Codebase provides out-of-the-box integration with UI and observability services. Data Sources Data Source Description Confluence Enterprise wiki and knowledge base integration Notion Workspace and document management integration PDF PDF document processing and text extraction Bundestag Data source fetching speeches from BundestagMine Check how to configure datasources here . Embeddding Models Models Provider Description * HuggingFace Open-sourced, run locally embedding models provided by HuggingFace * OpenAI Embedding models provided by OpenAI * VoyageAI Embedding models provided by VoyageAI Check how to configure embedding model here . Language Models Model Provider Description * LiteLLM Availability of many LLMs via providers like OpenAI , Google or Anthropic as well as local LLMs Check how to configure LLM here . Vector Databases Vector Store Description Qdrant High-performance vector similarity search engine Chroma Lightweight embedded vector database PGVector Postgres extension for embedding data support Check how to configure vector store here . Key Features Multiple Knowledge Base Integration : Seamless extraction from several Data Sources (Confluence, Notion, PDF) Wide Models Support : Availability of numerous embedding and language models Vector Search : Efficient similarity search using vector stores Interactive Chat : User-friendly interface for querying knowledge on Chainlit Performance Monitoring : Query and response tracking with Langfuse Evaluation : Comprehensive evaluation metrics using RAGAS Setup flexibility : Easy and flexible setup process of the pipeline Quick Start QuickStart Setup Dveloper Setup","title":"Home"},{"location":"#rag-blueprint-documentation","text":"","title":"RAG Blueprint Documentation"},{"location":"#overview","text":"The RAG blueprint project is a Retrieval-Augmented Generation system that integrates with several datasources to provide intelligent document search and analysis. The system combines the power of different large language models with knowledge bases to deliver accurate, context-aware responses through a chat interface. Codebase provides out-of-the-box integration with UI and observability services.","title":"Overview"},{"location":"#data-sources","text":"Data Source Description Confluence Enterprise wiki and knowledge base integration Notion Workspace and document management integration PDF PDF document processing and text extraction Bundestag Data source fetching speeches from BundestagMine Check how to configure datasources here .","title":"Data Sources"},{"location":"#embeddding-models","text":"Models Provider Description * HuggingFace Open-sourced, run locally embedding models provided by HuggingFace * OpenAI Embedding models provided by OpenAI * VoyageAI Embedding models provided by VoyageAI Check how to configure embedding model here .","title":"Embeddding Models"},{"location":"#language-models","text":"Model Provider Description * LiteLLM Availability of many LLMs via providers like OpenAI , Google or Anthropic as well as local LLMs Check how to configure LLM here .","title":"Language Models"},{"location":"#vector-databases","text":"Vector Store Description Qdrant High-performance vector similarity search engine Chroma Lightweight embedded vector database PGVector Postgres extension for embedding data support Check how to configure vector store here .","title":"Vector Databases"},{"location":"#key-features","text":"Multiple Knowledge Base Integration : Seamless extraction from several Data Sources (Confluence, Notion, PDF) Wide Models Support : Availability of numerous embedding and language models Vector Search : Efficient similarity search using vector stores Interactive Chat : User-friendly interface for querying knowledge on Chainlit Performance Monitoring : Query and response tracking with Langfuse Evaluation : Comprehensive evaluation metrics using RAGAS Setup flexibility : Easy and flexible setup process of the pipeline","title":"Key Features"},{"location":"#quick-start","text":"QuickStart Setup Dveloper Setup","title":"Quick Start"},{"location":"datasources/bundesapi_deutschland_package/","text":"bundesAPI \"deutschland\" Python Package Documentation Overview The deutschland Python package provides unified, easy-to-use access to Germany's most important public datasets and APIs. Instead of dealing with multiple APIs individually, this package consolidates them into a single, consistent Python library with auto-generated clients for numerous German government and institutional data sources. Key Value Proposition: One package, one installation, many APIs - all with consistent Python interfaces. Official Resources: - GitHub: https://github.com/bundesAPI/deutschland - PyPI: https://pypi.org/project/deutschland/ - Version: 0.4.2 (released June 18, 2024) - License: Apache-2.0 - Maintainer: Lilith Wittmann (bundesAPI) Why Use This Package? Advantages Over Direct API Access \u2705 Unified Interface: Consistent Python API across all German government data sources \u2705 Type Safety: Auto-generated Pydantic models for all data structures \u2705 No Manual HTTP: Handle authentication, pagination, and errors automatically \u2705 IDE Support: Full autocomplete and type hints in modern IDEs \u2705 Maintained: Regular updates when APIs change \u2705 Tested: Runs on Linux, macOS, and Windows (Python 3.9-3.12) \u2705 Well-Documented: Generated documentation for all models and methods Comparison with Direct REST API Access Feature deutschland Package Direct REST API Installation pip install deutschland Manual HTTP library setup Authentication Configured once in code Manual headers every request Data Models Pydantic models with validation Manual dict parsing Type Hints Full IDE support None Error Handling Structured exceptions Manual HTTP error handling Pagination Often handled automatically Manual cursor management Updates pip install --upgrade Rewrite code when API changes Learning Curve Python-native, intuitive Learn each API's quirks Installation Basic Installation pip install deutschland This installs the core package with essential APIs (bundestag, bundesrat, autobahn, dwd, nina, and more). With Optional APIs Many APIs are optional extras to keep the base package lightweight: # Install with DIP Bundestag API pip install deutschland[dip_bundestag] # Install with Bundestag Lobby Register pip install deutschland[bundestag_lobbyregister] # Install with Bundestag Tagesordnung (agenda) pip install deutschland[bundestag_tagesordnung] # Install multiple extras pip install deutschland[dip_bundestag,bundestag_lobbyregister,bundestag_tagesordnung] # Install ALL extras (comprehensive installation) pip install deutschland[all] With Poetry # Basic installation poetry add deutschland # With specific extras poetry add deutschland -E dip_bundestag poetry add deutschland -E bundestag_lobbyregister # With multiple extras poetry add deutschland -E dip_bundestag -E bundestag_lobbyregister Supported Python Versions Python 3.9 - 3.12 (tested on Linux, macOS, Windows) Dependencies (automatically installed): - requests - dateparser - beautifulsoup4 - more-itertools - numpy - pandas - pillow - lxml - onnxruntime - And API-specific sub-packages Available APIs Core APIs (Included by Default) These are installed automatically with the base package: bundestag - Basic Bundestag information API bundesrat - Federal Council information autobahn - German highway data, charging stations dwd - German Weather Service (Deutscher Wetterdienst) interpol - International police data jobsuche - Job search data ladestationen - Charging station locations mudab - Market transparency unit for electricity and gas nina - Disaster and emergency alerts polizei_brandenburg - Brandenburg police data risikogebiete - COVID-19 and other risk area classifications smard - Energy market data strahlenschutz - Radiation protection information travelwarning - Travel warnings and advisories zoll - Customs information Optional Bundestag APIs Install with specific extras for comprehensive parliamentary data: dip_bundestag \u2b50 Most Comprehensive Extra: deutschland[dip_bundestag] Package: de-dip-bundestag What it provides: Complete DIP API access (proceedings, documents, protocols, full text) Use case: Building comprehensive RAG systems with all parliamentary data bundestag_lobbyregister Extra: deutschland[bundestag_lobbyregister] Package: de-bundestag-lobbyregister What it provides: Lobby register data Use case: Transparency research, lobbying analysis bundestag_tagesordnung Extra: deutschland[bundestag_tagesordnung] Package: de-bundestag-tagesordnung What it provides: Parliamentary agendas and schedules Use case: Tracking upcoming sessions and topics Other Optional APIs Over 25 additional APIs available as extras: destatis - Federal Statistical Office feiertage - Public holidays bundeshaushalt - Federal budget data dashboarddeutschland - Government dashboard data deutschlandatlas - Germany atlas data diga - Digital health applications entgeltatlas - Wage atlas hilfsmittel - Medical aids registry hochwasserzentralen - Flood warning centers klinikatlas - Hospital atlas marktstammdaten - Master data register for energy market pegel-online - Water level data pflanzenschutzmittelzulassung - Plant protection product approvals regionalatlas - Regional atlas studiensuche - Study program search weiterbildungssuche - Further education search And more... Complete list: See pyproject.toml extras section Basic Usage Pattern All auto-generated APIs follow the same pattern: from deutschland import <api_name> from deutschland.<api_name>.api import <api_class> # Configure authentication (if needed) configuration = <api_name>.Configuration( host = \"https://api.example.de\", api_key = {'ApiKeyHeader': 'YOUR_KEY'} # or ApiKeyQuery ) # Create API client with <api_name>.ApiClient(configuration) as api_client: api_instance = <api_class>(api_client) # Make requests response = api_instance.some_method(param1, param2) print(response) Bundestag Data Access 1. Basic Bundestag API (Included by Default) Access basic Bundestag information without extra installation: from deutschland import bundestag from deutschland.bundestag.api import default_api configuration = bundestag.Configuration( host = \"https://www.bundestag.de\" ) with bundestag.ApiClient(configuration) as api_client: api_instance = default_api.DefaultApi(api_client) # Get article details article_id = 849630 response = api_instance.blueprint_servlet_content_articleidas_app_v2_newsarticle_xml_get(article_id) print(response) # Get plenary sessions overview response = api_instance.static_appdata_plenum_v2_conferences_xml_get() print(response) # Get current speaker response = api_instance.static_appdata_plenum_v2_speaker_xml_get() print(response) Available Endpoints: - News articles - Plenary session overview - Current speaker information - Committee information - MP biographies - Video on demand 2. DIP Bundestag API (Most Comprehensive) \u2b50 Installation: pip install deutschland[dip_bundestag] Access complete parliamentary data: from deutschland import dip_bundestag from deutschland.dip_bundestag.api import ( vorgnge_api, drucksachen_api, plenarprotokolle_api, personenstammdaten_api ) # Configure with API key configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) # API key can be set as header or query param configuration.api_key['ApiKeyQuery'] = 'OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw' with dip_bundestag.ApiClient(configuration) as api_client: # ============================================ # PROCEEDINGS (Vorg\u00e4nge) # ============================================ vorgang_api = vorgnge_api.VorgngeApi(api_client) # List all proceedings for Wahlperiode 20 vorgaenge = vorgang_api.get_vorgang_list( f_wahlperiode=20, format='json' ) print(f\"Total proceedings: {vorgaenge.num_found}\") # Get specific proceeding vorgang = vorgang_api.get_vorgang(id=320244, format='json') print(f\"Title: {vorgang.titel}\") print(f\"Status: {vorgang.beratungsstand}\") # ============================================ # PRINTED MATERIALS (Drucksachen) # ============================================ drucksache_api = drucksachen_api.DrucksachenApi(api_client) # List documents drucksachen = drucksache_api.get_drucksache_list( f_wahlperiode=20, f_drucksachetyp='Gesetzentwurf', # Bills only format='json' ) # Get document with full text drucksache_text = drucksache_api.get_drucksache_text( id=279131, format='json' ) print(f\"Full text: {drucksache_text.text[:500]}...\") # ============================================ # PLENARY PROTOCOLS (Plenarprotokolle) # ============================================ protokoll_api = plenarprotokolle_api.PlenarprotokolleApi(api_client) # List protocols protokolle = protokoll_api.get_plenarprotokoll_list( f_wahlperiode=20, format='json' ) # Get full transcript protokoll_text = protokoll_api.get_plenarprotokoll_text( id=5701, format='json' ) print(f\"Session: {protokoll_text.dokumentnummer}\") print(f\"Transcript length: {len(protokoll_text.text)} chars\") # ============================================ # PERSONS (MPs, Ministers) # ============================================ person_api = personenstammdaten_api.PersonenstammdatenApi(api_client) # List persons personen = person_api.get_person_list(format='json') # Get specific person person = person_api.get_person(id=40, format='json') print(f\"Name: {person.vorname} {person.nachname}\") print(f\"Party: {person.fraktion}\") print(f\"Served in periods: {person.wahlperiode}\") Available API Classes: - VorgngeApi - Proceedings - DrucksachenApi - Printed materials (with full text) - PlenarprotokolleApi - Plenary protocols (with full text) - PersonenstammdatenApi - Person data - VorgangspositionenApi - Proceeding positions - AktivittenApi - Activities All Data Models (with type hints): - Vorgang - Proceeding model - Drucksache , DrucksacheText - Document models - Plenarprotokoll , PlenarprotokollText - Protocol models - Person , PersonRole - Person models - Vorgangsposition - Position model - Aktivitaet - Activity model - And many more supporting models 3. Bundestag Lobby Register Installation: pip install deutschland[bundestag_lobbyregister] Usage: from deutschland import bundestag_lobbyregister # Similar pattern to DIP API # See de-bundestag-lobbyregister documentation for specifics 4. Bundestag Tagesordnung (Agenda) Installation: pip install deutschland[bundestag_tagesordnung] Usage: from deutschland import bundestag_tagesordnung # Access parliamentary agenda and schedules # See de-bundestag-tagesordnung documentation for specifics Built-in Helper Modules Geographic Data (Geo) Fetch detailed location information including streets, buildings, addresses: from deutschland.geo import Geo geo = Geo() # Define bounding box (top-right and bottom-left coordinates) top_right = [52.530116236589244, 13.426532801586827] bottom_left = [52.50876180448243, 13.359631043007212] data = geo.fetch(top_right, bottom_left) print(data.keys()) # dict_keys(['Adresse', 'Barrierenlinie', 'Bauwerksflaeche', ...]) print(data[\"Adresse\"][0]) # {'geometry': {'type': 'Point', 'coordinates': (13.422, 52.515)}, # 'properties': {'postleitzahl': '10179', 'ort': 'Berlin', # 'strasse': 'Holzmarktstra\u00dfe', 'hausnummer': '55'}} Data provided by: AdV SmartMapping (German state surveying offices) Company Data (Bundesanzeiger) Get financial reports for German companies: from deutschland.bundesanzeiger import Bundesanzeiger ba = Bundesanzeiger() # Search for company data = ba.get_reports(\"Deutsche Bahn AG\") print(data.keys()) # dict_keys(['Jahresabschluss zum Gesch\u00e4ftsjahr ...', ...]) # Returns fulltext financial reports extracted via ML Note: Uses machine learning to extract structured data from fulltext documents. Consumer Protection (Lebensmittelwarnung) Get product warnings from the federal food safety portal: from deutschland.lebensmittelwarnung import Lebensmittelwarnung lw = Lebensmittelwarnung() # Search by content type and region data = lw.get(\"lebensmittel\", \"berlin\") print(data[0]) # {'id': 19601, 'title': 'Product Name', # 'manufacturer': 'Lebensmittel', 'warning': 'Pyrrolizidinalkaloide', # 'affectedStates': ['Baden-W\u00fcrttemberg', ...]} Autobahn Data Access highway information and charging stations: from deutschland import autobahn from deutschland.autobahn.api import default_api api_instance = default_api.DefaultApi() # List all Autobahns autobahnen = api_instance.list_autobahnen() print(autobahnen) # Get charging station details station_id = \"RUxFQ1RSSUNfQ0hBUkdJTkdfU1RBVElPTl9fMTczMzM=\" station = api_instance.get_charging_station(station_id) print(station) Working with Pagination Many APIs return paginated results with a cursor: from deutschland import dip_bundestag from deutschland.dip_bundestag.api import vorgnge_api configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) configuration.api_key['ApiKeyQuery'] = 'YOUR_API_KEY' with dip_bundestag.ApiClient(configuration) as api_client: vorgang_api = vorgnge_api.VorgngeApi(api_client) cursor = None all_vorgaenge = [] while True: # Request with cursor response = vorgang_api.get_vorgang_list( f_wahlperiode=20, cursor=cursor if cursor else \"\", format='json' ) all_vorgaenge.extend(response.documents) # Check if more pages new_cursor = getattr(response, 'cursor', None) if not new_cursor or new_cursor == cursor: break cursor = new_cursor print(f\"Fetched {len(all_vorgaenge)} total proceedings...\") print(f\"Complete! Total: {len(all_vorgaenge)}\") Error Handling from deutschland import dip_bundestag from deutschland.dip_bundestag.api import vorgnge_api configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) configuration.api_key['ApiKeyQuery'] = 'YOUR_API_KEY' with dip_bundestag.ApiClient(configuration) as api_client: vorgang_api = vorgnge_api.VorgngeApi(api_client) try: vorgang = vorgang_api.get_vorgang(id=999999999, format='json') except dip_bundestag.ApiException as e: print(f\"API Error: {e.status}\") print(f\"Reason: {e.reason}\") print(f\"Body: {e.body}\") Common exceptions: - ApiException - Base exception for API errors - Status codes: 400 (Bad Request), 401 (Unauthorized), 404 (Not Found) Type Safety and IDE Support All models are auto-generated with full type hints: from deutschland.dip_bundestag.model.vorgang import Vorgang from deutschland.dip_bundestag.model.drucksache import Drucksache # IDE will provide autocomplete for all fields def process_vorgang(vorgang: Vorgang) -> None: print(vorgang.id) # IDE knows this is an int/str print(vorgang.titel) # IDE knows this is a string print(vorgang.wahlperiode) # IDE knows this is an int # Type checking at development time if vorgang.deskriptor: for descriptor in vorgang.deskriptor: print(descriptor.name) # Full autocomplete! Advanced Configuration Custom Timeouts configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) # Create client with custom timeout import urllib3 http_client = urllib3.PoolManager(timeout=30.0) with dip_bundestag.ApiClient(configuration) as api_client: api_client.rest_client.pool_manager = http_client # ... use api_client Proxy Support configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) # Configure proxy configuration.proxy = \"http://proxy.example.com:8080\" with dip_bundestag.ApiClient(configuration) as api_client: # ... use api_client through proxy Debug Logging import logging # Enable debug logging for all HTTP requests logging.basicConfig(level=logging.DEBUG) # Now all API calls will log detailed information Comparison: deutschland Package vs Direct DIP API Aspect deutschland Package Direct DIP API Installation pip install deutschland[dip_bundestag] No installation needed Import from deutschland import dip_bundestag import requests Auth Setup Configure once in Configuration object Pass in every request Making Requests api.get_vorgang_list(f_wahlperiode=20) requests.get(url, params={...}) Response Parsing Auto-parsed to Pydantic models Manual response.json() Type Hints Full type safety None (dict) IDE Support Complete autocomplete Minimal Error Handling Structured exceptions Manual status code checking Pagination Helper methods available Manual cursor management Documentation Generated docs + docstrings Read API specs API Changes Update package Rewrite code Learning Curve Pythonic, intuitive Learn API quirks Best Practices 1. Use Context Managers Always use with statement for proper resource cleanup: with dip_bundestag.ApiClient(configuration) as api_client: # API operations pass # Connections automatically closed 2. Configure Authentication Once # Don't repeat API key in every call configuration.api_key['ApiKeyQuery'] = 'YOUR_KEY' # Now all API instances use this configuration with dip_bundestag.ApiClient(configuration) as api_client: vorgang_api = vorgnge_api.VorgngeApi(api_client) drucksache_api = drucksachen_api.DrucksachenApi(api_client) # Both use the same auth 3. Handle Large Responses # For endpoints with large responses, process in batches cursor = None while True: response = api.get_vorgang_list(f_wahlperiode=20, cursor=cursor) # Process batch for vorgang in response.documents: process_vorgang(vorgang) # Process one at a time cursor = response.cursor if not cursor: break 4. Cache Results Locally import pickle import os CACHE_FILE = 'vorgaenge_cache.pkl' if os.path.exists(CACHE_FILE): with open(CACHE_FILE, 'rb') as f: vorgaenge = pickle.load(f) else: vorgaenge = api.get_vorgang_list(f_wahlperiode=20) with open(CACHE_FILE, 'wb') as f: pickle.dump(vorgaenge, f) 5. Use Specific Imports for Large APIs If you encounter recursion errors: # Instead of: from deutschland import dip_bundestag # Use specific imports: from deutschland.dip_bundestag.api.vorgnge_api import VorgngeApi from deutschland.dip_bundestag.model.vorgang import Vorgang Or increase recursion limit: import sys sys.setrecursionlimit(1500) from deutschland import dip_bundestag Use Cases for RAG Applications 1. Extract All Parliamentary Proceedings proceedings = [] cursor = None while True: response = vorgang_api.get_vorgang_list( f_wahlperiode=20, cursor=cursor, format='json' ) proceedings.extend(response.documents) if not response.cursor or response.cursor == cursor: break cursor = response.cursor # Now embed and index proceedings 2. Get Full-Text Documents # Get document metadata drucksachen = drucksache_api.get_drucksache_list(f_wahlperiode=20) # Fetch full text for each for doc_meta in drucksachen.documents: full_doc = drucksache_api.get_drucksache_text( id=doc_meta.id, format='json' ) # Extract text and metadata for RAG text = full_doc.text metadata = { 'id': full_doc.id, 'nummer': full_doc.dokumentnummer, 'typ': full_doc.drucksachetyp, 'datum': full_doc.datum, 'pdf_url': full_doc.fundstelle.pdf_url } # Add to vector database add_to_rag(text, metadata) 3. Incremental Updates from datetime import datetime, timedelta # Get only recent updates yesterday = (datetime.now() - timedelta(days=1)).isoformat() updated_vorgaenge = vorgang_api.get_vorgang_list( f_aktualisiert_start=yesterday, format='json' ) # Update only changed documents in RAG Limitations and Considerations Package Limitations Size: Full installation with all extras is large (~100+ MB) Recursion errors: Some APIs may need setrecursionlimit() increase Update lag: Package updates may lag behind API changes Python only: Not available for other languages API-Specific Limitations DIP API: Same limitations as direct API access (rate limits, data completeness) Authentication: API keys still required for protected endpoints No caching: Package doesn't cache responses (implement yourself) When to Use Direct API Instead Consider direct REST API when: - Non-Python environment - Extremely lightweight deployment needed - Want maximum control over HTTP behavior - API not yet supported by deutschland package Troubleshooting Installation Issues # If installation fails, try upgrading pip pip install --upgrade pip # Or use specific Python version python3.11 -m pip install deutschland[dip_bundestag] Import Errors # If imports fail with RecursionError: import sys sys.setrecursionlimit(1500) # Then import from deutschland import dip_bundestag API Key Issues # Check if API key is set correctly print(configuration.api_key) # Try both auth methods: # Method 1: Header configuration.api_key['ApiKeyHeader'] = 'YOUR_KEY' # Method 2: Query param (more reliable for DIP) configuration.api_key['ApiKeyQuery'] = 'YOUR_KEY' Timeout Issues # Increase timeout for slow endpoints configuration.timeout = 60 # seconds Development and Contributing Repository Structure deutschland/ \u251c\u2500\u2500 src/deutschland/ \u2502 \u251c\u2500\u2500 bundestag/ # Basic Bundestag API \u2502 \u251c\u2500\u2500 bundesanzeiger/ # Company financial data \u2502 \u251c\u2500\u2500 geo/ # Geographic data \u2502 \u251c\u2500\u2500 lebensmittelwarnung/ # Food warnings \u2502 \u2514\u2500\u2500 ... # Other modules \u251c\u2500\u2500 docs/ # API documentation \u2502 \u251c\u2500\u2500 bundestag/ \u2502 \u251c\u2500\u2500 dip_bundestag/ # Installed as extra \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 tests/ # Unit tests Running Tests # Install with development dependencies poetry install # Run tests poetry run pytest Contributing Contributions welcome! See GitHub repository for: - Issue tracker - Contributing guidelines - Development setup Related Resources Main Repo: https://github.com/bundesAPI/deutschland PyPI: https://pypi.org/project/deutschland/ DIP API Docs: https://dip.bundestag.api.bund.dev/ bundesAPI Organization: https://github.com/bundesAPI Summary The deutschland Python package is the easiest way for Python developers to access German government APIs, including comprehensive Bundestag data through the DIP API. Quick Decision Guide Use deutschland package if: \u2705 You're developing in Python \u2705 You want type safety and IDE support \u2705 You prefer Pythonic interfaces over HTTP calls \u2705 You're building production applications \u2705 You want automatic pagination and error handling Use direct REST API if: \u2705 You're not using Python \u2705 You need ultra-lightweight deployment \u2705 You want maximum control over HTTP \u2705 The API isn't supported by deutschland yet Installation Quick Reference # Minimal installation pip install deutschland # With DIP Bundestag (most comprehensive parliamentary data) pip install deutschland[dip_bundestag] # With lobby register pip install deutschland[bundestag_lobbyregister] # With everything pip install deutschland[all] Import Quick Reference # DIP Bundestag API (comprehensive) from deutschland import dip_bundestag from deutschland.dip_bundestag.api import vorgnge_api, drucksachen_api # Basic Bundestag API from deutschland import bundestag # Geographic data from deutschland.geo import Geo # Company data from deutschland.bundesanzeiger import Bundesanzeiger For RAG/LLM Applications: Use deutschland[dip_bundestag] for easiest access to complete parliamentary data with type-safe, Pythonic interfaces.","title":"bundesAPI \"deutschland\" Python Package Documentation"},{"location":"datasources/bundesapi_deutschland_package/#bundesapi-deutschland-python-package-documentation","text":"","title":"bundesAPI \"deutschland\" Python Package Documentation"},{"location":"datasources/bundesapi_deutschland_package/#overview","text":"The deutschland Python package provides unified, easy-to-use access to Germany's most important public datasets and APIs. Instead of dealing with multiple APIs individually, this package consolidates them into a single, consistent Python library with auto-generated clients for numerous German government and institutional data sources. Key Value Proposition: One package, one installation, many APIs - all with consistent Python interfaces. Official Resources: - GitHub: https://github.com/bundesAPI/deutschland - PyPI: https://pypi.org/project/deutschland/ - Version: 0.4.2 (released June 18, 2024) - License: Apache-2.0 - Maintainer: Lilith Wittmann (bundesAPI)","title":"Overview"},{"location":"datasources/bundesapi_deutschland_package/#why-use-this-package","text":"","title":"Why Use This Package?"},{"location":"datasources/bundesapi_deutschland_package/#advantages-over-direct-api-access","text":"\u2705 Unified Interface: Consistent Python API across all German government data sources \u2705 Type Safety: Auto-generated Pydantic models for all data structures \u2705 No Manual HTTP: Handle authentication, pagination, and errors automatically \u2705 IDE Support: Full autocomplete and type hints in modern IDEs \u2705 Maintained: Regular updates when APIs change \u2705 Tested: Runs on Linux, macOS, and Windows (Python 3.9-3.12) \u2705 Well-Documented: Generated documentation for all models and methods","title":"Advantages Over Direct API Access"},{"location":"datasources/bundesapi_deutschland_package/#comparison-with-direct-rest-api-access","text":"Feature deutschland Package Direct REST API Installation pip install deutschland Manual HTTP library setup Authentication Configured once in code Manual headers every request Data Models Pydantic models with validation Manual dict parsing Type Hints Full IDE support None Error Handling Structured exceptions Manual HTTP error handling Pagination Often handled automatically Manual cursor management Updates pip install --upgrade Rewrite code when API changes Learning Curve Python-native, intuitive Learn each API's quirks","title":"Comparison with Direct REST API Access"},{"location":"datasources/bundesapi_deutschland_package/#installation","text":"","title":"Installation"},{"location":"datasources/bundesapi_deutschland_package/#basic-installation","text":"pip install deutschland This installs the core package with essential APIs (bundestag, bundesrat, autobahn, dwd, nina, and more).","title":"Basic Installation"},{"location":"datasources/bundesapi_deutschland_package/#with-optional-apis","text":"Many APIs are optional extras to keep the base package lightweight: # Install with DIP Bundestag API pip install deutschland[dip_bundestag] # Install with Bundestag Lobby Register pip install deutschland[bundestag_lobbyregister] # Install with Bundestag Tagesordnung (agenda) pip install deutschland[bundestag_tagesordnung] # Install multiple extras pip install deutschland[dip_bundestag,bundestag_lobbyregister,bundestag_tagesordnung] # Install ALL extras (comprehensive installation) pip install deutschland[all]","title":"With Optional APIs"},{"location":"datasources/bundesapi_deutschland_package/#with-poetry","text":"# Basic installation poetry add deutschland # With specific extras poetry add deutschland -E dip_bundestag poetry add deutschland -E bundestag_lobbyregister # With multiple extras poetry add deutschland -E dip_bundestag -E bundestag_lobbyregister","title":"With Poetry"},{"location":"datasources/bundesapi_deutschland_package/#supported-python-versions","text":"Python 3.9 - 3.12 (tested on Linux, macOS, Windows) Dependencies (automatically installed): - requests - dateparser - beautifulsoup4 - more-itertools - numpy - pandas - pillow - lxml - onnxruntime - And API-specific sub-packages","title":"Supported Python Versions"},{"location":"datasources/bundesapi_deutschland_package/#available-apis","text":"","title":"Available APIs"},{"location":"datasources/bundesapi_deutschland_package/#core-apis-included-by-default","text":"These are installed automatically with the base package: bundestag - Basic Bundestag information API bundesrat - Federal Council information autobahn - German highway data, charging stations dwd - German Weather Service (Deutscher Wetterdienst) interpol - International police data jobsuche - Job search data ladestationen - Charging station locations mudab - Market transparency unit for electricity and gas nina - Disaster and emergency alerts polizei_brandenburg - Brandenburg police data risikogebiete - COVID-19 and other risk area classifications smard - Energy market data strahlenschutz - Radiation protection information travelwarning - Travel warnings and advisories zoll - Customs information","title":"Core APIs (Included by Default)"},{"location":"datasources/bundesapi_deutschland_package/#optional-bundestag-apis","text":"Install with specific extras for comprehensive parliamentary data:","title":"Optional Bundestag APIs"},{"location":"datasources/bundesapi_deutschland_package/#dip_bundestag-most-comprehensive","text":"Extra: deutschland[dip_bundestag] Package: de-dip-bundestag What it provides: Complete DIP API access (proceedings, documents, protocols, full text) Use case: Building comprehensive RAG systems with all parliamentary data","title":"dip_bundestag \u2b50 Most Comprehensive"},{"location":"datasources/bundesapi_deutschland_package/#bundestag_lobbyregister","text":"Extra: deutschland[bundestag_lobbyregister] Package: de-bundestag-lobbyregister What it provides: Lobby register data Use case: Transparency research, lobbying analysis","title":"bundestag_lobbyregister"},{"location":"datasources/bundesapi_deutschland_package/#bundestag_tagesordnung","text":"Extra: deutschland[bundestag_tagesordnung] Package: de-bundestag-tagesordnung What it provides: Parliamentary agendas and schedules Use case: Tracking upcoming sessions and topics","title":"bundestag_tagesordnung"},{"location":"datasources/bundesapi_deutschland_package/#other-optional-apis","text":"Over 25 additional APIs available as extras: destatis - Federal Statistical Office feiertage - Public holidays bundeshaushalt - Federal budget data dashboarddeutschland - Government dashboard data deutschlandatlas - Germany atlas data diga - Digital health applications entgeltatlas - Wage atlas hilfsmittel - Medical aids registry hochwasserzentralen - Flood warning centers klinikatlas - Hospital atlas marktstammdaten - Master data register for energy market pegel-online - Water level data pflanzenschutzmittelzulassung - Plant protection product approvals regionalatlas - Regional atlas studiensuche - Study program search weiterbildungssuche - Further education search And more... Complete list: See pyproject.toml extras section","title":"Other Optional APIs"},{"location":"datasources/bundesapi_deutschland_package/#basic-usage-pattern","text":"All auto-generated APIs follow the same pattern: from deutschland import <api_name> from deutschland.<api_name>.api import <api_class> # Configure authentication (if needed) configuration = <api_name>.Configuration( host = \"https://api.example.de\", api_key = {'ApiKeyHeader': 'YOUR_KEY'} # or ApiKeyQuery ) # Create API client with <api_name>.ApiClient(configuration) as api_client: api_instance = <api_class>(api_client) # Make requests response = api_instance.some_method(param1, param2) print(response)","title":"Basic Usage Pattern"},{"location":"datasources/bundesapi_deutschland_package/#bundestag-data-access","text":"","title":"Bundestag Data Access"},{"location":"datasources/bundesapi_deutschland_package/#1-basic-bundestag-api-included-by-default","text":"Access basic Bundestag information without extra installation: from deutschland import bundestag from deutschland.bundestag.api import default_api configuration = bundestag.Configuration( host = \"https://www.bundestag.de\" ) with bundestag.ApiClient(configuration) as api_client: api_instance = default_api.DefaultApi(api_client) # Get article details article_id = 849630 response = api_instance.blueprint_servlet_content_articleidas_app_v2_newsarticle_xml_get(article_id) print(response) # Get plenary sessions overview response = api_instance.static_appdata_plenum_v2_conferences_xml_get() print(response) # Get current speaker response = api_instance.static_appdata_plenum_v2_speaker_xml_get() print(response) Available Endpoints: - News articles - Plenary session overview - Current speaker information - Committee information - MP biographies - Video on demand","title":"1. Basic Bundestag API (Included by Default)"},{"location":"datasources/bundesapi_deutschland_package/#2-dip-bundestag-api-most-comprehensive","text":"Installation: pip install deutschland[dip_bundestag] Access complete parliamentary data: from deutschland import dip_bundestag from deutschland.dip_bundestag.api import ( vorgnge_api, drucksachen_api, plenarprotokolle_api, personenstammdaten_api ) # Configure with API key configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) # API key can be set as header or query param configuration.api_key['ApiKeyQuery'] = 'OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw' with dip_bundestag.ApiClient(configuration) as api_client: # ============================================ # PROCEEDINGS (Vorg\u00e4nge) # ============================================ vorgang_api = vorgnge_api.VorgngeApi(api_client) # List all proceedings for Wahlperiode 20 vorgaenge = vorgang_api.get_vorgang_list( f_wahlperiode=20, format='json' ) print(f\"Total proceedings: {vorgaenge.num_found}\") # Get specific proceeding vorgang = vorgang_api.get_vorgang(id=320244, format='json') print(f\"Title: {vorgang.titel}\") print(f\"Status: {vorgang.beratungsstand}\") # ============================================ # PRINTED MATERIALS (Drucksachen) # ============================================ drucksache_api = drucksachen_api.DrucksachenApi(api_client) # List documents drucksachen = drucksache_api.get_drucksache_list( f_wahlperiode=20, f_drucksachetyp='Gesetzentwurf', # Bills only format='json' ) # Get document with full text drucksache_text = drucksache_api.get_drucksache_text( id=279131, format='json' ) print(f\"Full text: {drucksache_text.text[:500]}...\") # ============================================ # PLENARY PROTOCOLS (Plenarprotokolle) # ============================================ protokoll_api = plenarprotokolle_api.PlenarprotokolleApi(api_client) # List protocols protokolle = protokoll_api.get_plenarprotokoll_list( f_wahlperiode=20, format='json' ) # Get full transcript protokoll_text = protokoll_api.get_plenarprotokoll_text( id=5701, format='json' ) print(f\"Session: {protokoll_text.dokumentnummer}\") print(f\"Transcript length: {len(protokoll_text.text)} chars\") # ============================================ # PERSONS (MPs, Ministers) # ============================================ person_api = personenstammdaten_api.PersonenstammdatenApi(api_client) # List persons personen = person_api.get_person_list(format='json') # Get specific person person = person_api.get_person(id=40, format='json') print(f\"Name: {person.vorname} {person.nachname}\") print(f\"Party: {person.fraktion}\") print(f\"Served in periods: {person.wahlperiode}\") Available API Classes: - VorgngeApi - Proceedings - DrucksachenApi - Printed materials (with full text) - PlenarprotokolleApi - Plenary protocols (with full text) - PersonenstammdatenApi - Person data - VorgangspositionenApi - Proceeding positions - AktivittenApi - Activities All Data Models (with type hints): - Vorgang - Proceeding model - Drucksache , DrucksacheText - Document models - Plenarprotokoll , PlenarprotokollText - Protocol models - Person , PersonRole - Person models - Vorgangsposition - Position model - Aktivitaet - Activity model - And many more supporting models","title":"2. DIP Bundestag API (Most Comprehensive) \u2b50"},{"location":"datasources/bundesapi_deutschland_package/#3-bundestag-lobby-register","text":"Installation: pip install deutschland[bundestag_lobbyregister] Usage: from deutschland import bundestag_lobbyregister # Similar pattern to DIP API # See de-bundestag-lobbyregister documentation for specifics","title":"3. Bundestag Lobby Register"},{"location":"datasources/bundesapi_deutschland_package/#4-bundestag-tagesordnung-agenda","text":"Installation: pip install deutschland[bundestag_tagesordnung] Usage: from deutschland import bundestag_tagesordnung # Access parliamentary agenda and schedules # See de-bundestag-tagesordnung documentation for specifics","title":"4. Bundestag Tagesordnung (Agenda)"},{"location":"datasources/bundesapi_deutschland_package/#built-in-helper-modules","text":"","title":"Built-in Helper Modules"},{"location":"datasources/bundesapi_deutschland_package/#geographic-data-geo","text":"Fetch detailed location information including streets, buildings, addresses: from deutschland.geo import Geo geo = Geo() # Define bounding box (top-right and bottom-left coordinates) top_right = [52.530116236589244, 13.426532801586827] bottom_left = [52.50876180448243, 13.359631043007212] data = geo.fetch(top_right, bottom_left) print(data.keys()) # dict_keys(['Adresse', 'Barrierenlinie', 'Bauwerksflaeche', ...]) print(data[\"Adresse\"][0]) # {'geometry': {'type': 'Point', 'coordinates': (13.422, 52.515)}, # 'properties': {'postleitzahl': '10179', 'ort': 'Berlin', # 'strasse': 'Holzmarktstra\u00dfe', 'hausnummer': '55'}} Data provided by: AdV SmartMapping (German state surveying offices)","title":"Geographic Data (Geo)"},{"location":"datasources/bundesapi_deutschland_package/#company-data-bundesanzeiger","text":"Get financial reports for German companies: from deutschland.bundesanzeiger import Bundesanzeiger ba = Bundesanzeiger() # Search for company data = ba.get_reports(\"Deutsche Bahn AG\") print(data.keys()) # dict_keys(['Jahresabschluss zum Gesch\u00e4ftsjahr ...', ...]) # Returns fulltext financial reports extracted via ML Note: Uses machine learning to extract structured data from fulltext documents.","title":"Company Data (Bundesanzeiger)"},{"location":"datasources/bundesapi_deutschland_package/#consumer-protection-lebensmittelwarnung","text":"Get product warnings from the federal food safety portal: from deutschland.lebensmittelwarnung import Lebensmittelwarnung lw = Lebensmittelwarnung() # Search by content type and region data = lw.get(\"lebensmittel\", \"berlin\") print(data[0]) # {'id': 19601, 'title': 'Product Name', # 'manufacturer': 'Lebensmittel', 'warning': 'Pyrrolizidinalkaloide', # 'affectedStates': ['Baden-W\u00fcrttemberg', ...]}","title":"Consumer Protection (Lebensmittelwarnung)"},{"location":"datasources/bundesapi_deutschland_package/#autobahn-data","text":"Access highway information and charging stations: from deutschland import autobahn from deutschland.autobahn.api import default_api api_instance = default_api.DefaultApi() # List all Autobahns autobahnen = api_instance.list_autobahnen() print(autobahnen) # Get charging station details station_id = \"RUxFQ1RSSUNfQ0hBUkdJTkdfU1RBVElPTl9fMTczMzM=\" station = api_instance.get_charging_station(station_id) print(station)","title":"Autobahn Data"},{"location":"datasources/bundesapi_deutschland_package/#working-with-pagination","text":"Many APIs return paginated results with a cursor: from deutschland import dip_bundestag from deutschland.dip_bundestag.api import vorgnge_api configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) configuration.api_key['ApiKeyQuery'] = 'YOUR_API_KEY' with dip_bundestag.ApiClient(configuration) as api_client: vorgang_api = vorgnge_api.VorgngeApi(api_client) cursor = None all_vorgaenge = [] while True: # Request with cursor response = vorgang_api.get_vorgang_list( f_wahlperiode=20, cursor=cursor if cursor else \"\", format='json' ) all_vorgaenge.extend(response.documents) # Check if more pages new_cursor = getattr(response, 'cursor', None) if not new_cursor or new_cursor == cursor: break cursor = new_cursor print(f\"Fetched {len(all_vorgaenge)} total proceedings...\") print(f\"Complete! Total: {len(all_vorgaenge)}\")","title":"Working with Pagination"},{"location":"datasources/bundesapi_deutschland_package/#error-handling","text":"from deutschland import dip_bundestag from deutschland.dip_bundestag.api import vorgnge_api configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) configuration.api_key['ApiKeyQuery'] = 'YOUR_API_KEY' with dip_bundestag.ApiClient(configuration) as api_client: vorgang_api = vorgnge_api.VorgngeApi(api_client) try: vorgang = vorgang_api.get_vorgang(id=999999999, format='json') except dip_bundestag.ApiException as e: print(f\"API Error: {e.status}\") print(f\"Reason: {e.reason}\") print(f\"Body: {e.body}\") Common exceptions: - ApiException - Base exception for API errors - Status codes: 400 (Bad Request), 401 (Unauthorized), 404 (Not Found)","title":"Error Handling"},{"location":"datasources/bundesapi_deutschland_package/#type-safety-and-ide-support","text":"All models are auto-generated with full type hints: from deutschland.dip_bundestag.model.vorgang import Vorgang from deutschland.dip_bundestag.model.drucksache import Drucksache # IDE will provide autocomplete for all fields def process_vorgang(vorgang: Vorgang) -> None: print(vorgang.id) # IDE knows this is an int/str print(vorgang.titel) # IDE knows this is a string print(vorgang.wahlperiode) # IDE knows this is an int # Type checking at development time if vorgang.deskriptor: for descriptor in vorgang.deskriptor: print(descriptor.name) # Full autocomplete!","title":"Type Safety and IDE Support"},{"location":"datasources/bundesapi_deutschland_package/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"datasources/bundesapi_deutschland_package/#custom-timeouts","text":"configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) # Create client with custom timeout import urllib3 http_client = urllib3.PoolManager(timeout=30.0) with dip_bundestag.ApiClient(configuration) as api_client: api_client.rest_client.pool_manager = http_client # ... use api_client","title":"Custom Timeouts"},{"location":"datasources/bundesapi_deutschland_package/#proxy-support","text":"configuration = dip_bundestag.Configuration( host = \"https://search.dip.bundestag.de/api/v1\" ) # Configure proxy configuration.proxy = \"http://proxy.example.com:8080\" with dip_bundestag.ApiClient(configuration) as api_client: # ... use api_client through proxy","title":"Proxy Support"},{"location":"datasources/bundesapi_deutschland_package/#debug-logging","text":"import logging # Enable debug logging for all HTTP requests logging.basicConfig(level=logging.DEBUG) # Now all API calls will log detailed information","title":"Debug Logging"},{"location":"datasources/bundesapi_deutschland_package/#comparison-deutschland-package-vs-direct-dip-api","text":"Aspect deutschland Package Direct DIP API Installation pip install deutschland[dip_bundestag] No installation needed Import from deutschland import dip_bundestag import requests Auth Setup Configure once in Configuration object Pass in every request Making Requests api.get_vorgang_list(f_wahlperiode=20) requests.get(url, params={...}) Response Parsing Auto-parsed to Pydantic models Manual response.json() Type Hints Full type safety None (dict) IDE Support Complete autocomplete Minimal Error Handling Structured exceptions Manual status code checking Pagination Helper methods available Manual cursor management Documentation Generated docs + docstrings Read API specs API Changes Update package Rewrite code Learning Curve Pythonic, intuitive Learn API quirks","title":"Comparison: deutschland Package vs Direct DIP API"},{"location":"datasources/bundesapi_deutschland_package/#best-practices","text":"","title":"Best Practices"},{"location":"datasources/bundesapi_deutschland_package/#1-use-context-managers","text":"Always use with statement for proper resource cleanup: with dip_bundestag.ApiClient(configuration) as api_client: # API operations pass # Connections automatically closed","title":"1. Use Context Managers"},{"location":"datasources/bundesapi_deutschland_package/#2-configure-authentication-once","text":"# Don't repeat API key in every call configuration.api_key['ApiKeyQuery'] = 'YOUR_KEY' # Now all API instances use this configuration with dip_bundestag.ApiClient(configuration) as api_client: vorgang_api = vorgnge_api.VorgngeApi(api_client) drucksache_api = drucksachen_api.DrucksachenApi(api_client) # Both use the same auth","title":"2. Configure Authentication Once"},{"location":"datasources/bundesapi_deutschland_package/#3-handle-large-responses","text":"# For endpoints with large responses, process in batches cursor = None while True: response = api.get_vorgang_list(f_wahlperiode=20, cursor=cursor) # Process batch for vorgang in response.documents: process_vorgang(vorgang) # Process one at a time cursor = response.cursor if not cursor: break","title":"3. Handle Large Responses"},{"location":"datasources/bundesapi_deutschland_package/#4-cache-results-locally","text":"import pickle import os CACHE_FILE = 'vorgaenge_cache.pkl' if os.path.exists(CACHE_FILE): with open(CACHE_FILE, 'rb') as f: vorgaenge = pickle.load(f) else: vorgaenge = api.get_vorgang_list(f_wahlperiode=20) with open(CACHE_FILE, 'wb') as f: pickle.dump(vorgaenge, f)","title":"4. Cache Results Locally"},{"location":"datasources/bundesapi_deutschland_package/#5-use-specific-imports-for-large-apis","text":"If you encounter recursion errors: # Instead of: from deutschland import dip_bundestag # Use specific imports: from deutschland.dip_bundestag.api.vorgnge_api import VorgngeApi from deutschland.dip_bundestag.model.vorgang import Vorgang Or increase recursion limit: import sys sys.setrecursionlimit(1500) from deutschland import dip_bundestag","title":"5. Use Specific Imports for Large APIs"},{"location":"datasources/bundesapi_deutschland_package/#use-cases-for-rag-applications","text":"","title":"Use Cases for RAG Applications"},{"location":"datasources/bundesapi_deutschland_package/#1-extract-all-parliamentary-proceedings","text":"proceedings = [] cursor = None while True: response = vorgang_api.get_vorgang_list( f_wahlperiode=20, cursor=cursor, format='json' ) proceedings.extend(response.documents) if not response.cursor or response.cursor == cursor: break cursor = response.cursor # Now embed and index proceedings","title":"1. Extract All Parliamentary Proceedings"},{"location":"datasources/bundesapi_deutschland_package/#2-get-full-text-documents","text":"# Get document metadata drucksachen = drucksache_api.get_drucksache_list(f_wahlperiode=20) # Fetch full text for each for doc_meta in drucksachen.documents: full_doc = drucksache_api.get_drucksache_text( id=doc_meta.id, format='json' ) # Extract text and metadata for RAG text = full_doc.text metadata = { 'id': full_doc.id, 'nummer': full_doc.dokumentnummer, 'typ': full_doc.drucksachetyp, 'datum': full_doc.datum, 'pdf_url': full_doc.fundstelle.pdf_url } # Add to vector database add_to_rag(text, metadata)","title":"2. Get Full-Text Documents"},{"location":"datasources/bundesapi_deutschland_package/#3-incremental-updates","text":"from datetime import datetime, timedelta # Get only recent updates yesterday = (datetime.now() - timedelta(days=1)).isoformat() updated_vorgaenge = vorgang_api.get_vorgang_list( f_aktualisiert_start=yesterday, format='json' ) # Update only changed documents in RAG","title":"3. Incremental Updates"},{"location":"datasources/bundesapi_deutschland_package/#limitations-and-considerations","text":"","title":"Limitations and Considerations"},{"location":"datasources/bundesapi_deutschland_package/#package-limitations","text":"Size: Full installation with all extras is large (~100+ MB) Recursion errors: Some APIs may need setrecursionlimit() increase Update lag: Package updates may lag behind API changes Python only: Not available for other languages","title":"Package Limitations"},{"location":"datasources/bundesapi_deutschland_package/#api-specific-limitations","text":"DIP API: Same limitations as direct API access (rate limits, data completeness) Authentication: API keys still required for protected endpoints No caching: Package doesn't cache responses (implement yourself)","title":"API-Specific Limitations"},{"location":"datasources/bundesapi_deutschland_package/#when-to-use-direct-api-instead","text":"Consider direct REST API when: - Non-Python environment - Extremely lightweight deployment needed - Want maximum control over HTTP behavior - API not yet supported by deutschland package","title":"When to Use Direct API Instead"},{"location":"datasources/bundesapi_deutschland_package/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"datasources/bundesapi_deutschland_package/#installation-issues","text":"# If installation fails, try upgrading pip pip install --upgrade pip # Or use specific Python version python3.11 -m pip install deutschland[dip_bundestag]","title":"Installation Issues"},{"location":"datasources/bundesapi_deutschland_package/#import-errors","text":"# If imports fail with RecursionError: import sys sys.setrecursionlimit(1500) # Then import from deutschland import dip_bundestag","title":"Import Errors"},{"location":"datasources/bundesapi_deutschland_package/#api-key-issues","text":"# Check if API key is set correctly print(configuration.api_key) # Try both auth methods: # Method 1: Header configuration.api_key['ApiKeyHeader'] = 'YOUR_KEY' # Method 2: Query param (more reliable for DIP) configuration.api_key['ApiKeyQuery'] = 'YOUR_KEY'","title":"API Key Issues"},{"location":"datasources/bundesapi_deutschland_package/#timeout-issues","text":"# Increase timeout for slow endpoints configuration.timeout = 60 # seconds","title":"Timeout Issues"},{"location":"datasources/bundesapi_deutschland_package/#development-and-contributing","text":"","title":"Development and Contributing"},{"location":"datasources/bundesapi_deutschland_package/#repository-structure","text":"deutschland/ \u251c\u2500\u2500 src/deutschland/ \u2502 \u251c\u2500\u2500 bundestag/ # Basic Bundestag API \u2502 \u251c\u2500\u2500 bundesanzeiger/ # Company financial data \u2502 \u251c\u2500\u2500 geo/ # Geographic data \u2502 \u251c\u2500\u2500 lebensmittelwarnung/ # Food warnings \u2502 \u2514\u2500\u2500 ... # Other modules \u251c\u2500\u2500 docs/ # API documentation \u2502 \u251c\u2500\u2500 bundestag/ \u2502 \u251c\u2500\u2500 dip_bundestag/ # Installed as extra \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 tests/ # Unit tests","title":"Repository Structure"},{"location":"datasources/bundesapi_deutschland_package/#running-tests","text":"# Install with development dependencies poetry install # Run tests poetry run pytest","title":"Running Tests"},{"location":"datasources/bundesapi_deutschland_package/#contributing","text":"Contributions welcome! See GitHub repository for: - Issue tracker - Contributing guidelines - Development setup","title":"Contributing"},{"location":"datasources/bundesapi_deutschland_package/#related-resources","text":"Main Repo: https://github.com/bundesAPI/deutschland PyPI: https://pypi.org/project/deutschland/ DIP API Docs: https://dip.bundestag.api.bund.dev/ bundesAPI Organization: https://github.com/bundesAPI","title":"Related Resources"},{"location":"datasources/bundesapi_deutschland_package/#summary","text":"The deutschland Python package is the easiest way for Python developers to access German government APIs, including comprehensive Bundestag data through the DIP API.","title":"Summary"},{"location":"datasources/bundesapi_deutschland_package/#quick-decision-guide","text":"Use deutschland package if: \u2705 You're developing in Python \u2705 You want type safety and IDE support \u2705 You prefer Pythonic interfaces over HTTP calls \u2705 You're building production applications \u2705 You want automatic pagination and error handling Use direct REST API if: \u2705 You're not using Python \u2705 You need ultra-lightweight deployment \u2705 You want maximum control over HTTP \u2705 The API isn't supported by deutschland yet","title":"Quick Decision Guide"},{"location":"datasources/bundesapi_deutschland_package/#installation-quick-reference","text":"# Minimal installation pip install deutschland # With DIP Bundestag (most comprehensive parliamentary data) pip install deutschland[dip_bundestag] # With lobby register pip install deutschland[bundestag_lobbyregister] # With everything pip install deutschland[all]","title":"Installation Quick Reference"},{"location":"datasources/bundesapi_deutschland_package/#import-quick-reference","text":"# DIP Bundestag API (comprehensive) from deutschland import dip_bundestag from deutschland.dip_bundestag.api import vorgnge_api, drucksachen_api # Basic Bundestag API from deutschland import bundestag # Geographic data from deutschland.geo import Geo # Company data from deutschland.bundesanzeiger import Bundesanzeiger For RAG/LLM Applications: Use deutschland[dip_bundestag] for easiest access to complete parliamentary data with type-safe, Pythonic interfaces.","title":"Import Quick Reference"},{"location":"datasources/bundestag_dip_api/","text":"Bundestag DIP API Documentation Overview The DIP (Dokumentations- und Informationssystem f\u00fcr Parlamentsmaterialien) API is the official German Bundestag API providing comprehensive access to parliamentary materials, activities, and information. This is the most complete and authoritative source for Bundestag data. Official Name: Documentation and Information System for Parliamentary Materials Provider: Deutscher Bundestag (German Federal Parliament) API Version: 1.2 OpenAPI Specification: https://dip.bundestag.api.bund.dev/openapi.yaml Why Use This API? The DIP API is the most comprehensive Bundestag data source available, providing: Full Parliamentary Proceedings (Vorg\u00e4nge): Complete legislative processes from start to finish Printed Materials (Drucksachen): Bills, motions, government responses, reports Full-Text Content: Complete text of plenary protocols and printed materials Person Data: MPs, ministers, and other parliamentary actors Activity Tracking: All parliamentary activities and positions Structured Metadata: Rich relational data connecting all resources This is significantly more comprehensive than specialized APIs like: - Bundestag Lobby Register (only lobbying data) - Bundestag Mine (only speech data) API Specifications Base Information Base URL: https://search.dip.bundestag.de/api/v1 Data Formats: JSON (primary), XML (alternative) OpenAPI Docs: https://dip.bundestag.api.bund.dev/ Terms of Service: https://dip.bundestag.de/\u00fcber-dip/nutzungsbedingungen Support Contact: parlamentsdokumentation@bundestag.de Authentication Required: API key for all requests Public Test Key (valid until May 2026): OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw Request Permanent Key: Email: parlamentsdokumentation@bundestag.de Implementation Methods: HTTP Header (recommended): Authorization: ApiKey OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw Query Parameter: ?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw Rate Limiting No explicit rate limiting documented. Best practices: - Implement reasonable request delays - Use cursor-based pagination efficiently - Cache results appropriately - Be respectful of API resources Available Resources and Endpoints The API provides 8 main resource types with 16 endpoints: 1. Vorg\u00e4nge (Proceedings/Processes) Legislative and parliamentary processes from initiation to completion. Endpoints: - GET /vorgang - List all proceedings (with filters) - GET /vorgang/{id} - Get specific proceeding by ID Use Cases: - Track legislation through the parliamentary process - Monitor bills, motions, and resolutions - Analyze legislative activity by topic or party Example Data: - Legislative type (Gesetzgebung, Antrag, etc.) - Status (in debate, completed, rejected) - Subject areas (Sachgebiet) - Initiators (Bundesregierung, parties, Bundesrat) - Associated documents and protocols 2. Vorgangspositionen (Proceeding Positions) Individual positions/steps within a larger proceeding. Endpoints: - GET /vorgangsposition - List all positions (with filters) - GET /vorgangsposition/{id} - Get specific position by ID Use Cases: - Track individual steps in legislative process - Monitor committee activities - Analyze procedural timeline 3. Drucksachen (Printed Materials) Official printed documents (bills, motions, reports, responses). Endpoints: - GET /drucksache - List all printed materials (with filters) - GET /drucksache/{id} - Get specific material by ID Document Types (Drucksachetyp): - Gesetzentwurf (Bill) - Antrag (Motion) - Kleine Anfrage (Minor Inquiry) - Gro\u00dfe Anfrage (Major Inquiry) - Antwort (Response) - Beschlussempfehlung und Bericht (Recommendation and Report) - Unterrichtung (Information/Notification) - And many more... Use Cases: - Access official government documents - Retrieve bills and legislative proposals - Get responses to parliamentary inquiries 4. Drucksache-Text (Printed Material Full Text) Full-text content of printed materials. Endpoints: - GET /drucksache-text - List materials with full text (with filters) - GET /drucksache-text/{id} - Get specific material with full text Use Cases: - Full-text search in parliamentary documents - Extract complete document content for RAG - Analyze actual legislative language 5. Plenarprotokolle (Plenary Protocols) Official transcripts of parliamentary sessions. Endpoints: - GET /plenarprotokoll - List all protocols (with filters) - GET /plenarprotokoll/{id} - Get specific protocol by ID Publishers (Herausgeber): - BT (Bundestag) - BR (Bundesrat) Use Cases: - Access official session transcripts - Track debates on specific topics - Analyze parliamentary discourse Key Fields: - Session number (dokumentnummer) - Date of session - PDF URL and XML URL (for Bundestag) - Associated proceedings (vorgangsbezug) - PDF and XML hashes for integrity 6. Plenarprotokoll-Text (Plenary Protocol Full Text) Complete stenographic records with full text. Endpoints: - GET /plenarprotokoll-text - List protocols with full text (with filters) - GET /plenarprotokoll-text/{id} - Get protocol with complete text Content Includes: - Stenographic transcript - Speaker identification - Procedural notes - Interruptions and remarks - Voting results Use Cases: - Full-text search in parliamentary debates - Speech analysis and attribution - RAG applications with complete debate context 7. Aktivit\u00e4ten (Activities) Parliamentary activities and actions. Endpoints: - GET /aktivitaet - List all activities (with filters) - GET /aktivitaet/{id} - Get specific activity by ID Use Cases: - Track specific parliamentary actions - Monitor committee activities - Analyze procedural steps 8. Personen (Persons) Members of Parliament, ministers, and other parliamentary actors. Endpoints: - GET /person - List all persons (with filters) - GET /person/{id} - Get specific person by ID Data Includes: - Name and function (MdB - Member of Bundestag, etc.) - Party/Fraction (Fraktion) - Electoral periods served (Wahlperiode) - Historical role information Use Cases: - Link documents to specific MPs - Track individual parliamentary activity - Analyze party membership and roles Query Parameters and Filtering All list endpoints support filtering parameters (prefixed with f. ): Common Filters Parameter Type Description Example f.id string Filter by ID f.id=320244 f.wahlperiode integer Electoral period number f.wahlperiode=20 f.datum.start date Start date (YYYY-MM-DD) f.datum.start=2024-01-01 f.datum.end date End date (YYYY-MM-DD) f.datum.end=2024-12-31 f.aktualisiert.start datetime Updated after f.aktualisiert.start=2025-03-01 f.aktualisiert.end datetime Updated before f.aktualisiert.end=2025-03-31 f.drucksache string Document number f.drucksache=20/15096 f.plenarprotokoll string Protocol number f.plenarprotokoll=20/214 f.dokumentnummer string Document number f.dokumentnummer=20/214 f.dokumentart string Document type f.dokumentart=Plenarprotokoll f.drucksachetyp string Print type f.drucksachetyp=Gesetzentwurf f.zuordnung string Assignment/classification varies cursor string Pagination cursor (see Pagination) format string Response format json or xml Resource-Specific Filters Vorgang: - f.gesta - GESTA procedure number Vorgangsposition: - f.aktivitaet - Activity ID - f.vorgang - Proceeding ID Drucksache: - No additional specific filters beyond common ones Person: - No common filters apply (use ID-based lookup) Pagination The API uses cursor-based pagination for efficient traversal of large result sets. How It Works Initial Request: Make request without cursor parameter Response: API returns results + cursor field Next Page: Include returned cursor in next request End Detection: When no more results, cursor may be empty or unchanged Important Notes No page size control: API determines result count per page Cursor format: Opaque Base64-encoded string Stateful: Cursor maintains query state Expiration: Cursors may expire; unclear documentation on timeout Pagination Pattern GET /vorgang?f.wahlperiode=20&apikey={key} \u2192 Returns {numFound: 37666, cursor: \"ABC123...\", documents: [...]} GET /vorgang?f.wahlperiode=20&cursor=ABC123...&apikey={key} \u2192 Returns next batch with new cursor ... repeat until all results fetched Response Structure List Response Format All list endpoints return the same structure: { \"numFound\": 37666, \"cursor\": \"AoQIQ4zX03n+rouj/w...\", \"documents\": [ { /* document object */ }, { /* document object */ }, ... ] } Fields: - numFound : Total number of matching documents (may be null for some queries) - cursor : Pagination token for next page - documents : Array of document objects Detail Response Format Single-item endpoints return the document object directly: { \"id\": \"5701\", \"typ\": \"Dokument\", \"dokumentart\": \"Plenarprotokoll\", \"dokumentnummer\": \"20/214\", /* ... other fields ... */ } Data Models Common Fields Most resources share these fields: id : Unique identifier (string or integer) typ : Type (e.g., \"Vorgang\", \"Dokument\", \"Person\") wahlperiode : Electoral period number(s) datum : Date (YYYY-MM-DD) aktualisiert : Last update timestamp (ISO 8601) titel : Title or name Vorgang (Proceeding) Schema { \"id\": \"320244\", \"typ\": \"Vorgang\", \"vorgangstyp\": \"Selbst\u00e4ndiger Antrag von L\u00e4ndern auf Entschlie\u00dfung\", \"titel\": \"Title of the proceeding\", \"abstract\": \"Summary description\", \"datum\": \"2025-09-26\", \"wahlperiode\": 20, \"initiative\": [\"Brandenburg\"], // Who initiated \"beratungsstand\": \"In der Beratung...\", // Status \"sachgebiet\": [\"\u00d6ffentliche Finanzen, Steuern und Abgaben\"], // Subject areas \"deskriptor\": [ // Keywords/descriptors { \"name\": \"Steuerbefreiung\", \"typ\": \"Sachbegriffe\", \"fundstelle\": true } ], \"aktualisiert\": \"2025-10-15T11:06:54+02:00\" } Key Fields: - vorgangstyp : Type of proceeding (legislation, motion, report, etc.) - beratungsstand : Current status in parliamentary process - initiative : Initiating body (government, party, Bundesrat) - sachgebiet : Policy areas - deskriptor : Controlled vocabulary keywords - mitteilung : Additional notes/messages Drucksache (Printed Material) Schema { \"id\": \"279131\", \"typ\": \"Dokument\", \"dokumentart\": \"Drucksache\", \"drucksachetyp\": \"Antwort\", \"dokumentnummer\": \"20/15151\", \"wahlperiode\": 20, \"herausgeber\": \"BT\", \"titel\": \"Document title\", \"datum\": \"2025-03-26\", \"pdf_hash\": \"8321f919a809d05581eccb4ba5bdb8b3\", \"vorgangsbezug_anzahl\": 1, \"vorgangsbezug\": [ { \"id\": \"320358\", \"titel\": \"Related proceeding title\", \"vorgangstyp\": \"Kleine Anfrage\" } ], \"urheber\": [ { \"einbringer\": false, \"bezeichnung\": \"BRg\", \"titel\": \"Bundesregierung\" } ], \"ressort\": [ { \"federfuehrend\": true, \"titel\": \"Ausw\u00e4rtiges Amt\" } ], \"fundstelle\": { \"pdf_url\": \"https://dserver.bundestag.de/btd/20/151/2015151.pdf\", \"id\": \"279131\", \"dokumentnummer\": \"20/15151\", \"datum\": \"2025-03-26\", \"verteildatum\": \"2025-04-01\" }, \"autoren_anzahl\": 0, \"aktualisiert\": \"2025-04-01T08:01:10+02:00\" } Key Fields: - drucksachetyp : Type (Bill, Motion, Inquiry, Response, Report, etc.) - herausgeber : Publisher (BT=Bundestag, BR=Bundesrat) - urheber : Authors/originators - ressort : Responsible ministry/department - fundstelle : Document location with PDF URL - vorgangsbezug : Related proceedings - pdf_hash : PDF file hash for integrity checking Plenarprotokoll (Plenary Protocol) Schema { \"id\": \"5701\", \"typ\": \"Dokument\", \"dokumentart\": \"Plenarprotokoll\", \"dokumentnummer\": \"20/214\", \"wahlperiode\": 20, \"herausgeber\": \"BT\", \"titel\": \"Protokoll der 214. Sitzung des 20. Deutschen Bundestages\", \"datum\": \"2025-03-18\", \"pdf_hash\": \"adbabad803cbf3ca32661d5336d8281e\", \"xml_hash\": \"6e1e62918ae23f99e075c58803b1b183\", \"vorgangsbezug_anzahl\": 12, \"vorgangsbezug\": [ { \"id\": \"320785\", \"titel\": \"Related proceeding\", \"vorgangstyp\": \"Gesetzgebung\" } ], \"fundstelle\": { \"id\": \"5701\", \"dokumentnummer\": \"20/214\", \"datum\": \"2025-03-18\", \"verteildatum\": \"2025-03-19\", \"pdf_url\": \"https://dserver.bundestag.de/btp/20/20214.pdf\", \"xml_url\": \"https://dserver.bundestag.de/btp/20/20214.xml\", \"dokumentart\": \"Plenarprotokoll\", \"herausgeber\": \"BT\" }, \"aktualisiert\": \"2025-04-29T10:20:33+02:00\" } Key Fields: - xml_url : Structured XML version (Bundestag only) - pdf_url : PDF version - xml_hash , pdf_hash : File integrity hashes - vorgangsbezug : Proceedings discussed in session - verteildatum : Distribution date With Full Text ( /plenarprotokoll-text/{id} ): - Adds text field with complete stenographic transcript - Includes speaker names, speeches, votes, procedures - Plain text format with section markers Person Schema { \"id\": \"40\", \"typ\": \"Person\", \"nachname\": \"Str\u00f6bele\", \"vorname\": \"Hans-Christian\", \"funktion\": [\"MdB\"], \"fraktion\": [\"B\u00dcNDNIS 90/DIE GR\u00dcNEN\"], \"wahlperiode\": [10, 14, 15, 16, 17, 18], \"titel\": \"Hans-Christian Str\u00f6bele, MdB, B\u00dcNDNIS 90/DIE GR\u00dcNEN\", \"datum\": \"2017-10-23\", \"basisdatum\": \"1985-04-15\", \"person_roles\": [ { \"funktion\": \"MdB\", \"fraktion\": \"Die Gr\u00fcnen\", \"nachname\": \"Str\u00f6bele\", \"vorname\": \"Hans-Christian\", \"wahlperiode_nummer\": [10] } ], \"aktualisiert\": \"2022-07-26T19:57:10+02:00\" } Key Fields: - funktion : Role (MdB = Member of Bundestag, Minister, etc.) - fraktion : Party/parliamentary group - wahlperiode : Electoral periods served (array) - person_roles : Historical role breakdown - basisdatum : Base date (first appearance) Vorgangsposition Schema Similar to Vorgang but represents a single step/position within a larger proceeding. Contains references to parent proceeding. Aktivit\u00e4t Schema Represents specific parliamentary activities. Structure varies by activity type. Document Numbering System Understanding document numbers is crucial for working with the API: Drucksache Numbers Format: {wahlperiode}/{number} Examples: - 20/15151 - Wahlperiode 20, Document 15151 - 19/12345 - Wahlperiode 19, Document 12345 Plenarprotokoll Numbers Bundestag: {wahlperiode}/{session} - Example: 20/214 - 214th session of 20th Bundestag Bundesrat: {session} - Example: 1052 - 1052nd session of Bundesrat Wahlperiode (Electoral Period) The Bundestag operates in electoral periods (typically 4 years): - 20th Wahlperiode: 2021-2025 (current as of 2025) - 19th Wahlperiode: 2017-2021 - 18th Wahlperiode: 2013-2017 Each period resets document numbering. Data Relationships The API provides rich relational data: Vorgang (Proceeding) \u251c\u2500> Vorgangsposition (Positions) \u251c\u2500> Drucksache (Printed Materials) \u251c\u2500> Plenarprotokoll (Protocols) \u2514\u2500> Aktivit\u00e4t (Activities) Drucksache \u251c\u2500> Vorgang (Parent Proceeding) \u2514\u2500> PDF/Text Content Plenarprotokoll \u251c\u2500> Multiple Vorg\u00e4nge (Discussed Proceedings) \u251c\u2500> XML/PDF Files \u2514\u2500> Full Text Transcript Person \u2514\u2500> Associated Documents (via authorship, speeches) Use vorgangsbezug to traverse relationships: - Documents link to their parent proceedings - Protocols link to all discussed proceedings - Positions link to parent proceedings and documents Use Cases for RAG/LLM Applications The DIP API is ideal for building comprehensive parliamentary knowledge systems: 1. Legislative Tracking and Analysis Track bills from introduction to passage Analyze legislative timelines and bottlenecks Monitor specific policy areas 2. Full-Text Parliamentary Search Search across all speeches, documents, inquiries Semantic search on debate transcripts Answer questions about parliamentary positions 3. MP Activity and Behavior Analysis Track individual MP activities and votes Analyze party positions on issues Study parliamentary discourse patterns 4. Government Accountability Access all government responses to inquiries Track ministry activities and communications Monitor implementation of legislation 5. Historical Research Access complete parliamentary records Track policy evolution across electoral periods Analyze long-term political trends 6. Multi-Document Question Answering Connect debates to bills to votes to outcomes Provide comprehensive answers with full context Trace arguments across multiple sessions Metadata Recommendations for RAG Core Identification document_id : Unique identifier document_type : Vorgang, Drucksache, Plenarprotokoll, etc. document_number : Official number (e.g., 20/214) wahlperiode : Electoral period Content Context title : Document title abstract : Summary (if available) sachgebiet : Subject areas (for filtering) deskriptor : Keywords (for semantic grouping) Temporal Context date : Official date updated : Last modification date distribution_date : When distributed (verteildatum) Relational Context related_proceeding_ids : Linked proceedings proceeding_type : Type of legislative process status : Current state in process Authority Context publisher : BT or BR authors : Who created/submitted responsible_ministry : Ministry (Ressort) initiator : Who initiated (party, government, etc.) Document Location pdf_url : Link to PDF xml_url : Link to XML (if available) pdf_hash : For integrity checking For Speeches (from Plenarprotokoll-Text) speaker : Person who spoke party : Speaker's party session_date : When spoken topic : Related proceeding/agenda item Data Quality Considerations Completeness Full text available: Yes, for most documents via -text endpoints Historical coverage: Complete for recent Wahlperioden (18+), partial for earlier Update frequency: Real-time to daily updates Missing data: Some older documents may lack full text or XML Accuracy Official source: Direct from Bundestag; authoritative Verification: PDF/XML hashes provided for integrity Corrections: Updates reflected in aktualisiert timestamp Consistency Structured format: Well-defined schemas Controlled vocabulary: deskriptor uses standardized terms ID stability: IDs appear stable across requests Language Primary language: German Translation: Not provided; consider external translation for non-German users Field names: Mix of German (data) and some English (structure) Best Practices for Data Extraction 1. Incremental Updates Use f.aktualisiert.start to fetch only recent changes: GET /vorgang?f.aktualisiert.start=2025-03-01&apikey={key} 2. Filter by Electoral Period Limit scope with f.wahlperiode for focused extraction: GET /drucksache?f.wahlperiode=20&apikey={key} 3. Leverage Relationships Use vorgangsbezug IDs to fetch related documents efficiently. 4. Full Text Strategy Start with metadata endpoints (faster) Fetch full text only for relevant documents Cache full text locally to avoid re-fetching 5. Pagination Pattern cursor = \"\" while True: response = fetch_data(cursor) process_documents(response['documents']) new_cursor = response.get('cursor') if not new_cursor or new_cursor == cursor: break cursor = new_cursor 6. Error Handling Handle missing fields gracefully (not all documents have all fields) Retry on network errors Respect API rate limits (implement backoff) 7. Data Storage Store pdf_hash / xml_hash to detect changes Track aktualisiert for incremental updates Maintain relationships (vorgangsbezug IDs) Comparison: DIP vs. Other Bundestag APIs Feature DIP API Lobby Register Bundestag Mine Scope Complete parliamentary data Lobbying only Speeches only Documents All types Lobby entries None Full Text Yes N/A Speech text Proceedings Complete No No Persons All MPs/Ministers Lobbyists Speakers Historical Full archives From 2022 Limited Relationships Rich Minimal Minimal Best For Comprehensive RAG Transparency research Speech analysis Recommendation: Use DIP as primary source; supplement with specialized APIs if needed. Technical Specifications HTTP Methods GET only (read-only API) Response Codes 200 - Success 400 - Bad Request (invalid parameters) 401 - Unauthorized (invalid API key) 404 - Not Found (invalid ID) Headers Accept: application/json Authorization: ApiKey {your-key} Content-Type Request: Not applicable (GET only) Response: application/json or application/xml Character Encoding UTF-8 Date/Time Format Dates: YYYY-MM-DD Timestamps: ISO 8601 ( YYYY-MM-DDTHH:mm:ss+02:00 ) Limitations and Considerations API Limitations No full-text search: API filters on metadata only; implement search in your application No batch endpoints: Must fetch documents individually or paginate lists Cursor expiration: Unclear; may need to restart pagination if cursor becomes invalid No rate limit documentation: Implement conservative rate limiting Data Limitations German language: All content in German; translation needed for multilingual applications Historical gaps: Very old documents may have limited metadata or missing full text No sentiment/analysis: Raw data only; NLP/analysis must be done client-side Large full-text responses: Protocol transcripts can be very long (100KB+) Practical Considerations Storage requirements: Full corpus is large (GB to TB range) Processing time: Complete extraction takes hours/days Update frequency: Check aktualisiert regularly for changes PDF processing: For documents without text endpoints, may need OCR Example Extraction Strategy For building a comprehensive RAG system: Phase 1: Core Data (Metadata) Extract all vorgang for recent Wahlperiode Extract associated drucksache metadata Extract plenarprotokoll metadata Extract person data Build relationship graph Phase 2: Full Text Fetch drucksache-text for all documents Fetch plenarprotokoll-text for all protocols Parse and chunk text appropriately Extract embedded metadata (speakers, votes, etc.) Phase 3: Incremental Updates Query with f.aktualisiert.start daily Update changed documents Add new documents Refresh relationships Phase 4: Enhancement Add embeddings for semantic search Link to external resources (PDF URLs) Implement cross-document search Build knowledge graph Contact and Support Email: parlamentsdokumentation@bundestag.de Purpose: API keys, technical support, usage questions Terms of Service: https://dip.bundestag.de/\u00fcber-dip/nutzungsbedingungen Related Resources DIP Web Interface: https://dip.bundestag.de OpenAPI Spec: https://dip.bundestag.api.bund.dev/openapi.yaml Interactive Docs: https://dip.bundestag.api.bund.dev/ GitHub: https://github.com/bundesAPI/dip-bundestag-api Bundestag Document Server: https://dserver.bundestag.de Summary The DIP API is the most comprehensive and authoritative source for German Bundestag data. It provides: \u2705 Complete Coverage: All parliamentary documents, proceedings, and activities \u2705 Full Text: Complete transcripts and document text \u2705 Rich Metadata: Structured, relational data \u2705 Official Source: Direct from Bundestag \u2705 Well-Documented: OpenAPI specification available \u2705 Free Access: Public API key available Ideal for: Comprehensive RAG systems, legislative tracking, parliamentary research, policy analysis, government accountability tools.","title":"Bundestag DIP API Documentation"},{"location":"datasources/bundestag_dip_api/#bundestag-dip-api-documentation","text":"","title":"Bundestag DIP API Documentation"},{"location":"datasources/bundestag_dip_api/#overview","text":"The DIP (Dokumentations- und Informationssystem f\u00fcr Parlamentsmaterialien) API is the official German Bundestag API providing comprehensive access to parliamentary materials, activities, and information. This is the most complete and authoritative source for Bundestag data. Official Name: Documentation and Information System for Parliamentary Materials Provider: Deutscher Bundestag (German Federal Parliament) API Version: 1.2 OpenAPI Specification: https://dip.bundestag.api.bund.dev/openapi.yaml","title":"Overview"},{"location":"datasources/bundestag_dip_api/#why-use-this-api","text":"The DIP API is the most comprehensive Bundestag data source available, providing: Full Parliamentary Proceedings (Vorg\u00e4nge): Complete legislative processes from start to finish Printed Materials (Drucksachen): Bills, motions, government responses, reports Full-Text Content: Complete text of plenary protocols and printed materials Person Data: MPs, ministers, and other parliamentary actors Activity Tracking: All parliamentary activities and positions Structured Metadata: Rich relational data connecting all resources This is significantly more comprehensive than specialized APIs like: - Bundestag Lobby Register (only lobbying data) - Bundestag Mine (only speech data)","title":"Why Use This API?"},{"location":"datasources/bundestag_dip_api/#api-specifications","text":"","title":"API Specifications"},{"location":"datasources/bundestag_dip_api/#base-information","text":"Base URL: https://search.dip.bundestag.de/api/v1 Data Formats: JSON (primary), XML (alternative) OpenAPI Docs: https://dip.bundestag.api.bund.dev/ Terms of Service: https://dip.bundestag.de/\u00fcber-dip/nutzungsbedingungen Support Contact: parlamentsdokumentation@bundestag.de","title":"Base Information"},{"location":"datasources/bundestag_dip_api/#authentication","text":"Required: API key for all requests Public Test Key (valid until May 2026): OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw Request Permanent Key: Email: parlamentsdokumentation@bundestag.de Implementation Methods: HTTP Header (recommended): Authorization: ApiKey OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw Query Parameter: ?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw","title":"Authentication"},{"location":"datasources/bundestag_dip_api/#rate-limiting","text":"No explicit rate limiting documented. Best practices: - Implement reasonable request delays - Use cursor-based pagination efficiently - Cache results appropriately - Be respectful of API resources","title":"Rate Limiting"},{"location":"datasources/bundestag_dip_api/#available-resources-and-endpoints","text":"The API provides 8 main resource types with 16 endpoints:","title":"Available Resources and Endpoints"},{"location":"datasources/bundestag_dip_api/#1-vorgange-proceedingsprocesses","text":"Legislative and parliamentary processes from initiation to completion. Endpoints: - GET /vorgang - List all proceedings (with filters) - GET /vorgang/{id} - Get specific proceeding by ID Use Cases: - Track legislation through the parliamentary process - Monitor bills, motions, and resolutions - Analyze legislative activity by topic or party Example Data: - Legislative type (Gesetzgebung, Antrag, etc.) - Status (in debate, completed, rejected) - Subject areas (Sachgebiet) - Initiators (Bundesregierung, parties, Bundesrat) - Associated documents and protocols","title":"1. Vorg\u00e4nge (Proceedings/Processes)"},{"location":"datasources/bundestag_dip_api/#2-vorgangspositionen-proceeding-positions","text":"Individual positions/steps within a larger proceeding. Endpoints: - GET /vorgangsposition - List all positions (with filters) - GET /vorgangsposition/{id} - Get specific position by ID Use Cases: - Track individual steps in legislative process - Monitor committee activities - Analyze procedural timeline","title":"2. Vorgangspositionen (Proceeding Positions)"},{"location":"datasources/bundestag_dip_api/#3-drucksachen-printed-materials","text":"Official printed documents (bills, motions, reports, responses). Endpoints: - GET /drucksache - List all printed materials (with filters) - GET /drucksache/{id} - Get specific material by ID Document Types (Drucksachetyp): - Gesetzentwurf (Bill) - Antrag (Motion) - Kleine Anfrage (Minor Inquiry) - Gro\u00dfe Anfrage (Major Inquiry) - Antwort (Response) - Beschlussempfehlung und Bericht (Recommendation and Report) - Unterrichtung (Information/Notification) - And many more... Use Cases: - Access official government documents - Retrieve bills and legislative proposals - Get responses to parliamentary inquiries","title":"3. Drucksachen (Printed Materials)"},{"location":"datasources/bundestag_dip_api/#4-drucksache-text-printed-material-full-text","text":"Full-text content of printed materials. Endpoints: - GET /drucksache-text - List materials with full text (with filters) - GET /drucksache-text/{id} - Get specific material with full text Use Cases: - Full-text search in parliamentary documents - Extract complete document content for RAG - Analyze actual legislative language","title":"4. Drucksache-Text (Printed Material Full Text)"},{"location":"datasources/bundestag_dip_api/#5-plenarprotokolle-plenary-protocols","text":"Official transcripts of parliamentary sessions. Endpoints: - GET /plenarprotokoll - List all protocols (with filters) - GET /plenarprotokoll/{id} - Get specific protocol by ID Publishers (Herausgeber): - BT (Bundestag) - BR (Bundesrat) Use Cases: - Access official session transcripts - Track debates on specific topics - Analyze parliamentary discourse Key Fields: - Session number (dokumentnummer) - Date of session - PDF URL and XML URL (for Bundestag) - Associated proceedings (vorgangsbezug) - PDF and XML hashes for integrity","title":"5. Plenarprotokolle (Plenary Protocols)"},{"location":"datasources/bundestag_dip_api/#6-plenarprotokoll-text-plenary-protocol-full-text","text":"Complete stenographic records with full text. Endpoints: - GET /plenarprotokoll-text - List protocols with full text (with filters) - GET /plenarprotokoll-text/{id} - Get protocol with complete text Content Includes: - Stenographic transcript - Speaker identification - Procedural notes - Interruptions and remarks - Voting results Use Cases: - Full-text search in parliamentary debates - Speech analysis and attribution - RAG applications with complete debate context","title":"6. Plenarprotokoll-Text (Plenary Protocol Full Text)"},{"location":"datasources/bundestag_dip_api/#7-aktivitaten-activities","text":"Parliamentary activities and actions. Endpoints: - GET /aktivitaet - List all activities (with filters) - GET /aktivitaet/{id} - Get specific activity by ID Use Cases: - Track specific parliamentary actions - Monitor committee activities - Analyze procedural steps","title":"7. Aktivit\u00e4ten (Activities)"},{"location":"datasources/bundestag_dip_api/#8-personen-persons","text":"Members of Parliament, ministers, and other parliamentary actors. Endpoints: - GET /person - List all persons (with filters) - GET /person/{id} - Get specific person by ID Data Includes: - Name and function (MdB - Member of Bundestag, etc.) - Party/Fraction (Fraktion) - Electoral periods served (Wahlperiode) - Historical role information Use Cases: - Link documents to specific MPs - Track individual parliamentary activity - Analyze party membership and roles","title":"8. Personen (Persons)"},{"location":"datasources/bundestag_dip_api/#query-parameters-and-filtering","text":"All list endpoints support filtering parameters (prefixed with f. ):","title":"Query Parameters and Filtering"},{"location":"datasources/bundestag_dip_api/#common-filters","text":"Parameter Type Description Example f.id string Filter by ID f.id=320244 f.wahlperiode integer Electoral period number f.wahlperiode=20 f.datum.start date Start date (YYYY-MM-DD) f.datum.start=2024-01-01 f.datum.end date End date (YYYY-MM-DD) f.datum.end=2024-12-31 f.aktualisiert.start datetime Updated after f.aktualisiert.start=2025-03-01 f.aktualisiert.end datetime Updated before f.aktualisiert.end=2025-03-31 f.drucksache string Document number f.drucksache=20/15096 f.plenarprotokoll string Protocol number f.plenarprotokoll=20/214 f.dokumentnummer string Document number f.dokumentnummer=20/214 f.dokumentart string Document type f.dokumentart=Plenarprotokoll f.drucksachetyp string Print type f.drucksachetyp=Gesetzentwurf f.zuordnung string Assignment/classification varies cursor string Pagination cursor (see Pagination) format string Response format json or xml","title":"Common Filters"},{"location":"datasources/bundestag_dip_api/#resource-specific-filters","text":"Vorgang: - f.gesta - GESTA procedure number Vorgangsposition: - f.aktivitaet - Activity ID - f.vorgang - Proceeding ID Drucksache: - No additional specific filters beyond common ones Person: - No common filters apply (use ID-based lookup)","title":"Resource-Specific Filters"},{"location":"datasources/bundestag_dip_api/#pagination","text":"The API uses cursor-based pagination for efficient traversal of large result sets.","title":"Pagination"},{"location":"datasources/bundestag_dip_api/#how-it-works","text":"Initial Request: Make request without cursor parameter Response: API returns results + cursor field Next Page: Include returned cursor in next request End Detection: When no more results, cursor may be empty or unchanged","title":"How It Works"},{"location":"datasources/bundestag_dip_api/#important-notes","text":"No page size control: API determines result count per page Cursor format: Opaque Base64-encoded string Stateful: Cursor maintains query state Expiration: Cursors may expire; unclear documentation on timeout","title":"Important Notes"},{"location":"datasources/bundestag_dip_api/#pagination-pattern","text":"GET /vorgang?f.wahlperiode=20&apikey={key} \u2192 Returns {numFound: 37666, cursor: \"ABC123...\", documents: [...]} GET /vorgang?f.wahlperiode=20&cursor=ABC123...&apikey={key} \u2192 Returns next batch with new cursor ... repeat until all results fetched","title":"Pagination Pattern"},{"location":"datasources/bundestag_dip_api/#response-structure","text":"","title":"Response Structure"},{"location":"datasources/bundestag_dip_api/#list-response-format","text":"All list endpoints return the same structure: { \"numFound\": 37666, \"cursor\": \"AoQIQ4zX03n+rouj/w...\", \"documents\": [ { /* document object */ }, { /* document object */ }, ... ] } Fields: - numFound : Total number of matching documents (may be null for some queries) - cursor : Pagination token for next page - documents : Array of document objects","title":"List Response Format"},{"location":"datasources/bundestag_dip_api/#detail-response-format","text":"Single-item endpoints return the document object directly: { \"id\": \"5701\", \"typ\": \"Dokument\", \"dokumentart\": \"Plenarprotokoll\", \"dokumentnummer\": \"20/214\", /* ... other fields ... */ }","title":"Detail Response Format"},{"location":"datasources/bundestag_dip_api/#data-models","text":"","title":"Data Models"},{"location":"datasources/bundestag_dip_api/#common-fields","text":"Most resources share these fields: id : Unique identifier (string or integer) typ : Type (e.g., \"Vorgang\", \"Dokument\", \"Person\") wahlperiode : Electoral period number(s) datum : Date (YYYY-MM-DD) aktualisiert : Last update timestamp (ISO 8601) titel : Title or name","title":"Common Fields"},{"location":"datasources/bundestag_dip_api/#vorgang-proceeding-schema","text":"{ \"id\": \"320244\", \"typ\": \"Vorgang\", \"vorgangstyp\": \"Selbst\u00e4ndiger Antrag von L\u00e4ndern auf Entschlie\u00dfung\", \"titel\": \"Title of the proceeding\", \"abstract\": \"Summary description\", \"datum\": \"2025-09-26\", \"wahlperiode\": 20, \"initiative\": [\"Brandenburg\"], // Who initiated \"beratungsstand\": \"In der Beratung...\", // Status \"sachgebiet\": [\"\u00d6ffentliche Finanzen, Steuern und Abgaben\"], // Subject areas \"deskriptor\": [ // Keywords/descriptors { \"name\": \"Steuerbefreiung\", \"typ\": \"Sachbegriffe\", \"fundstelle\": true } ], \"aktualisiert\": \"2025-10-15T11:06:54+02:00\" } Key Fields: - vorgangstyp : Type of proceeding (legislation, motion, report, etc.) - beratungsstand : Current status in parliamentary process - initiative : Initiating body (government, party, Bundesrat) - sachgebiet : Policy areas - deskriptor : Controlled vocabulary keywords - mitteilung : Additional notes/messages","title":"Vorgang (Proceeding) Schema"},{"location":"datasources/bundestag_dip_api/#drucksache-printed-material-schema","text":"{ \"id\": \"279131\", \"typ\": \"Dokument\", \"dokumentart\": \"Drucksache\", \"drucksachetyp\": \"Antwort\", \"dokumentnummer\": \"20/15151\", \"wahlperiode\": 20, \"herausgeber\": \"BT\", \"titel\": \"Document title\", \"datum\": \"2025-03-26\", \"pdf_hash\": \"8321f919a809d05581eccb4ba5bdb8b3\", \"vorgangsbezug_anzahl\": 1, \"vorgangsbezug\": [ { \"id\": \"320358\", \"titel\": \"Related proceeding title\", \"vorgangstyp\": \"Kleine Anfrage\" } ], \"urheber\": [ { \"einbringer\": false, \"bezeichnung\": \"BRg\", \"titel\": \"Bundesregierung\" } ], \"ressort\": [ { \"federfuehrend\": true, \"titel\": \"Ausw\u00e4rtiges Amt\" } ], \"fundstelle\": { \"pdf_url\": \"https://dserver.bundestag.de/btd/20/151/2015151.pdf\", \"id\": \"279131\", \"dokumentnummer\": \"20/15151\", \"datum\": \"2025-03-26\", \"verteildatum\": \"2025-04-01\" }, \"autoren_anzahl\": 0, \"aktualisiert\": \"2025-04-01T08:01:10+02:00\" } Key Fields: - drucksachetyp : Type (Bill, Motion, Inquiry, Response, Report, etc.) - herausgeber : Publisher (BT=Bundestag, BR=Bundesrat) - urheber : Authors/originators - ressort : Responsible ministry/department - fundstelle : Document location with PDF URL - vorgangsbezug : Related proceedings - pdf_hash : PDF file hash for integrity checking","title":"Drucksache (Printed Material) Schema"},{"location":"datasources/bundestag_dip_api/#plenarprotokoll-plenary-protocol-schema","text":"{ \"id\": \"5701\", \"typ\": \"Dokument\", \"dokumentart\": \"Plenarprotokoll\", \"dokumentnummer\": \"20/214\", \"wahlperiode\": 20, \"herausgeber\": \"BT\", \"titel\": \"Protokoll der 214. Sitzung des 20. Deutschen Bundestages\", \"datum\": \"2025-03-18\", \"pdf_hash\": \"adbabad803cbf3ca32661d5336d8281e\", \"xml_hash\": \"6e1e62918ae23f99e075c58803b1b183\", \"vorgangsbezug_anzahl\": 12, \"vorgangsbezug\": [ { \"id\": \"320785\", \"titel\": \"Related proceeding\", \"vorgangstyp\": \"Gesetzgebung\" } ], \"fundstelle\": { \"id\": \"5701\", \"dokumentnummer\": \"20/214\", \"datum\": \"2025-03-18\", \"verteildatum\": \"2025-03-19\", \"pdf_url\": \"https://dserver.bundestag.de/btp/20/20214.pdf\", \"xml_url\": \"https://dserver.bundestag.de/btp/20/20214.xml\", \"dokumentart\": \"Plenarprotokoll\", \"herausgeber\": \"BT\" }, \"aktualisiert\": \"2025-04-29T10:20:33+02:00\" } Key Fields: - xml_url : Structured XML version (Bundestag only) - pdf_url : PDF version - xml_hash , pdf_hash : File integrity hashes - vorgangsbezug : Proceedings discussed in session - verteildatum : Distribution date With Full Text ( /plenarprotokoll-text/{id} ): - Adds text field with complete stenographic transcript - Includes speaker names, speeches, votes, procedures - Plain text format with section markers","title":"Plenarprotokoll (Plenary Protocol) Schema"},{"location":"datasources/bundestag_dip_api/#person-schema","text":"{ \"id\": \"40\", \"typ\": \"Person\", \"nachname\": \"Str\u00f6bele\", \"vorname\": \"Hans-Christian\", \"funktion\": [\"MdB\"], \"fraktion\": [\"B\u00dcNDNIS 90/DIE GR\u00dcNEN\"], \"wahlperiode\": [10, 14, 15, 16, 17, 18], \"titel\": \"Hans-Christian Str\u00f6bele, MdB, B\u00dcNDNIS 90/DIE GR\u00dcNEN\", \"datum\": \"2017-10-23\", \"basisdatum\": \"1985-04-15\", \"person_roles\": [ { \"funktion\": \"MdB\", \"fraktion\": \"Die Gr\u00fcnen\", \"nachname\": \"Str\u00f6bele\", \"vorname\": \"Hans-Christian\", \"wahlperiode_nummer\": [10] } ], \"aktualisiert\": \"2022-07-26T19:57:10+02:00\" } Key Fields: - funktion : Role (MdB = Member of Bundestag, Minister, etc.) - fraktion : Party/parliamentary group - wahlperiode : Electoral periods served (array) - person_roles : Historical role breakdown - basisdatum : Base date (first appearance)","title":"Person Schema"},{"location":"datasources/bundestag_dip_api/#vorgangsposition-schema","text":"Similar to Vorgang but represents a single step/position within a larger proceeding. Contains references to parent proceeding.","title":"Vorgangsposition Schema"},{"location":"datasources/bundestag_dip_api/#aktivitat-schema","text":"Represents specific parliamentary activities. Structure varies by activity type.","title":"Aktivit\u00e4t Schema"},{"location":"datasources/bundestag_dip_api/#document-numbering-system","text":"Understanding document numbers is crucial for working with the API:","title":"Document Numbering System"},{"location":"datasources/bundestag_dip_api/#drucksache-numbers","text":"Format: {wahlperiode}/{number} Examples: - 20/15151 - Wahlperiode 20, Document 15151 - 19/12345 - Wahlperiode 19, Document 12345","title":"Drucksache Numbers"},{"location":"datasources/bundestag_dip_api/#plenarprotokoll-numbers","text":"Bundestag: {wahlperiode}/{session} - Example: 20/214 - 214th session of 20th Bundestag Bundesrat: {session} - Example: 1052 - 1052nd session of Bundesrat","title":"Plenarprotokoll Numbers"},{"location":"datasources/bundestag_dip_api/#wahlperiode-electoral-period","text":"The Bundestag operates in electoral periods (typically 4 years): - 20th Wahlperiode: 2021-2025 (current as of 2025) - 19th Wahlperiode: 2017-2021 - 18th Wahlperiode: 2013-2017 Each period resets document numbering.","title":"Wahlperiode (Electoral Period)"},{"location":"datasources/bundestag_dip_api/#data-relationships","text":"The API provides rich relational data: Vorgang (Proceeding) \u251c\u2500> Vorgangsposition (Positions) \u251c\u2500> Drucksache (Printed Materials) \u251c\u2500> Plenarprotokoll (Protocols) \u2514\u2500> Aktivit\u00e4t (Activities) Drucksache \u251c\u2500> Vorgang (Parent Proceeding) \u2514\u2500> PDF/Text Content Plenarprotokoll \u251c\u2500> Multiple Vorg\u00e4nge (Discussed Proceedings) \u251c\u2500> XML/PDF Files \u2514\u2500> Full Text Transcript Person \u2514\u2500> Associated Documents (via authorship, speeches) Use vorgangsbezug to traverse relationships: - Documents link to their parent proceedings - Protocols link to all discussed proceedings - Positions link to parent proceedings and documents","title":"Data Relationships"},{"location":"datasources/bundestag_dip_api/#use-cases-for-ragllm-applications","text":"The DIP API is ideal for building comprehensive parliamentary knowledge systems:","title":"Use Cases for RAG/LLM Applications"},{"location":"datasources/bundestag_dip_api/#1-legislative-tracking-and-analysis","text":"Track bills from introduction to passage Analyze legislative timelines and bottlenecks Monitor specific policy areas","title":"1. Legislative Tracking and Analysis"},{"location":"datasources/bundestag_dip_api/#2-full-text-parliamentary-search","text":"Search across all speeches, documents, inquiries Semantic search on debate transcripts Answer questions about parliamentary positions","title":"2. Full-Text Parliamentary Search"},{"location":"datasources/bundestag_dip_api/#3-mp-activity-and-behavior-analysis","text":"Track individual MP activities and votes Analyze party positions on issues Study parliamentary discourse patterns","title":"3. MP Activity and Behavior Analysis"},{"location":"datasources/bundestag_dip_api/#4-government-accountability","text":"Access all government responses to inquiries Track ministry activities and communications Monitor implementation of legislation","title":"4. Government Accountability"},{"location":"datasources/bundestag_dip_api/#5-historical-research","text":"Access complete parliamentary records Track policy evolution across electoral periods Analyze long-term political trends","title":"5. Historical Research"},{"location":"datasources/bundestag_dip_api/#6-multi-document-question-answering","text":"Connect debates to bills to votes to outcomes Provide comprehensive answers with full context Trace arguments across multiple sessions","title":"6. Multi-Document Question Answering"},{"location":"datasources/bundestag_dip_api/#metadata-recommendations-for-rag","text":"","title":"Metadata Recommendations for RAG"},{"location":"datasources/bundestag_dip_api/#core-identification","text":"document_id : Unique identifier document_type : Vorgang, Drucksache, Plenarprotokoll, etc. document_number : Official number (e.g., 20/214) wahlperiode : Electoral period","title":"Core Identification"},{"location":"datasources/bundestag_dip_api/#content-context","text":"title : Document title abstract : Summary (if available) sachgebiet : Subject areas (for filtering) deskriptor : Keywords (for semantic grouping)","title":"Content Context"},{"location":"datasources/bundestag_dip_api/#temporal-context","text":"date : Official date updated : Last modification date distribution_date : When distributed (verteildatum)","title":"Temporal Context"},{"location":"datasources/bundestag_dip_api/#relational-context","text":"related_proceeding_ids : Linked proceedings proceeding_type : Type of legislative process status : Current state in process","title":"Relational Context"},{"location":"datasources/bundestag_dip_api/#authority-context","text":"publisher : BT or BR authors : Who created/submitted responsible_ministry : Ministry (Ressort) initiator : Who initiated (party, government, etc.)","title":"Authority Context"},{"location":"datasources/bundestag_dip_api/#document-location","text":"pdf_url : Link to PDF xml_url : Link to XML (if available) pdf_hash : For integrity checking","title":"Document Location"},{"location":"datasources/bundestag_dip_api/#for-speeches-from-plenarprotokoll-text","text":"speaker : Person who spoke party : Speaker's party session_date : When spoken topic : Related proceeding/agenda item","title":"For Speeches (from Plenarprotokoll-Text)"},{"location":"datasources/bundestag_dip_api/#data-quality-considerations","text":"","title":"Data Quality Considerations"},{"location":"datasources/bundestag_dip_api/#completeness","text":"Full text available: Yes, for most documents via -text endpoints Historical coverage: Complete for recent Wahlperioden (18+), partial for earlier Update frequency: Real-time to daily updates Missing data: Some older documents may lack full text or XML","title":"Completeness"},{"location":"datasources/bundestag_dip_api/#accuracy","text":"Official source: Direct from Bundestag; authoritative Verification: PDF/XML hashes provided for integrity Corrections: Updates reflected in aktualisiert timestamp","title":"Accuracy"},{"location":"datasources/bundestag_dip_api/#consistency","text":"Structured format: Well-defined schemas Controlled vocabulary: deskriptor uses standardized terms ID stability: IDs appear stable across requests","title":"Consistency"},{"location":"datasources/bundestag_dip_api/#language","text":"Primary language: German Translation: Not provided; consider external translation for non-German users Field names: Mix of German (data) and some English (structure)","title":"Language"},{"location":"datasources/bundestag_dip_api/#best-practices-for-data-extraction","text":"","title":"Best Practices for Data Extraction"},{"location":"datasources/bundestag_dip_api/#1-incremental-updates","text":"Use f.aktualisiert.start to fetch only recent changes: GET /vorgang?f.aktualisiert.start=2025-03-01&apikey={key}","title":"1. Incremental Updates"},{"location":"datasources/bundestag_dip_api/#2-filter-by-electoral-period","text":"Limit scope with f.wahlperiode for focused extraction: GET /drucksache?f.wahlperiode=20&apikey={key}","title":"2. Filter by Electoral Period"},{"location":"datasources/bundestag_dip_api/#3-leverage-relationships","text":"Use vorgangsbezug IDs to fetch related documents efficiently.","title":"3. Leverage Relationships"},{"location":"datasources/bundestag_dip_api/#4-full-text-strategy","text":"Start with metadata endpoints (faster) Fetch full text only for relevant documents Cache full text locally to avoid re-fetching","title":"4. Full Text Strategy"},{"location":"datasources/bundestag_dip_api/#5-pagination-pattern","text":"cursor = \"\" while True: response = fetch_data(cursor) process_documents(response['documents']) new_cursor = response.get('cursor') if not new_cursor or new_cursor == cursor: break cursor = new_cursor","title":"5. Pagination Pattern"},{"location":"datasources/bundestag_dip_api/#6-error-handling","text":"Handle missing fields gracefully (not all documents have all fields) Retry on network errors Respect API rate limits (implement backoff)","title":"6. Error Handling"},{"location":"datasources/bundestag_dip_api/#7-data-storage","text":"Store pdf_hash / xml_hash to detect changes Track aktualisiert for incremental updates Maintain relationships (vorgangsbezug IDs)","title":"7. Data Storage"},{"location":"datasources/bundestag_dip_api/#comparison-dip-vs-other-bundestag-apis","text":"Feature DIP API Lobby Register Bundestag Mine Scope Complete parliamentary data Lobbying only Speeches only Documents All types Lobby entries None Full Text Yes N/A Speech text Proceedings Complete No No Persons All MPs/Ministers Lobbyists Speakers Historical Full archives From 2022 Limited Relationships Rich Minimal Minimal Best For Comprehensive RAG Transparency research Speech analysis Recommendation: Use DIP as primary source; supplement with specialized APIs if needed.","title":"Comparison: DIP vs. Other Bundestag APIs"},{"location":"datasources/bundestag_dip_api/#technical-specifications","text":"","title":"Technical Specifications"},{"location":"datasources/bundestag_dip_api/#http-methods","text":"GET only (read-only API)","title":"HTTP Methods"},{"location":"datasources/bundestag_dip_api/#response-codes","text":"200 - Success 400 - Bad Request (invalid parameters) 401 - Unauthorized (invalid API key) 404 - Not Found (invalid ID)","title":"Response Codes"},{"location":"datasources/bundestag_dip_api/#headers","text":"Accept: application/json Authorization: ApiKey {your-key}","title":"Headers"},{"location":"datasources/bundestag_dip_api/#content-type","text":"Request: Not applicable (GET only) Response: application/json or application/xml","title":"Content-Type"},{"location":"datasources/bundestag_dip_api/#character-encoding","text":"UTF-8","title":"Character Encoding"},{"location":"datasources/bundestag_dip_api/#datetime-format","text":"Dates: YYYY-MM-DD Timestamps: ISO 8601 ( YYYY-MM-DDTHH:mm:ss+02:00 )","title":"Date/Time Format"},{"location":"datasources/bundestag_dip_api/#limitations-and-considerations","text":"","title":"Limitations and Considerations"},{"location":"datasources/bundestag_dip_api/#api-limitations","text":"No full-text search: API filters on metadata only; implement search in your application No batch endpoints: Must fetch documents individually or paginate lists Cursor expiration: Unclear; may need to restart pagination if cursor becomes invalid No rate limit documentation: Implement conservative rate limiting","title":"API Limitations"},{"location":"datasources/bundestag_dip_api/#data-limitations","text":"German language: All content in German; translation needed for multilingual applications Historical gaps: Very old documents may have limited metadata or missing full text No sentiment/analysis: Raw data only; NLP/analysis must be done client-side Large full-text responses: Protocol transcripts can be very long (100KB+)","title":"Data Limitations"},{"location":"datasources/bundestag_dip_api/#practical-considerations","text":"Storage requirements: Full corpus is large (GB to TB range) Processing time: Complete extraction takes hours/days Update frequency: Check aktualisiert regularly for changes PDF processing: For documents without text endpoints, may need OCR","title":"Practical Considerations"},{"location":"datasources/bundestag_dip_api/#example-extraction-strategy","text":"For building a comprehensive RAG system:","title":"Example Extraction Strategy"},{"location":"datasources/bundestag_dip_api/#phase-1-core-data-metadata","text":"Extract all vorgang for recent Wahlperiode Extract associated drucksache metadata Extract plenarprotokoll metadata Extract person data Build relationship graph","title":"Phase 1: Core Data (Metadata)"},{"location":"datasources/bundestag_dip_api/#phase-2-full-text","text":"Fetch drucksache-text for all documents Fetch plenarprotokoll-text for all protocols Parse and chunk text appropriately Extract embedded metadata (speakers, votes, etc.)","title":"Phase 2: Full Text"},{"location":"datasources/bundestag_dip_api/#phase-3-incremental-updates","text":"Query with f.aktualisiert.start daily Update changed documents Add new documents Refresh relationships","title":"Phase 3: Incremental Updates"},{"location":"datasources/bundestag_dip_api/#phase-4-enhancement","text":"Add embeddings for semantic search Link to external resources (PDF URLs) Implement cross-document search Build knowledge graph","title":"Phase 4: Enhancement"},{"location":"datasources/bundestag_dip_api/#contact-and-support","text":"Email: parlamentsdokumentation@bundestag.de Purpose: API keys, technical support, usage questions Terms of Service: https://dip.bundestag.de/\u00fcber-dip/nutzungsbedingungen","title":"Contact and Support"},{"location":"datasources/bundestag_dip_api/#related-resources","text":"DIP Web Interface: https://dip.bundestag.de OpenAPI Spec: https://dip.bundestag.api.bund.dev/openapi.yaml Interactive Docs: https://dip.bundestag.api.bund.dev/ GitHub: https://github.com/bundesAPI/dip-bundestag-api Bundestag Document Server: https://dserver.bundestag.de","title":"Related Resources"},{"location":"datasources/bundestag_dip_api/#summary","text":"The DIP API is the most comprehensive and authoritative source for German Bundestag data. It provides: \u2705 Complete Coverage: All parliamentary documents, proceedings, and activities \u2705 Full Text: Complete transcripts and document text \u2705 Rich Metadata: Structured, relational data \u2705 Official Source: Direct from Bundestag \u2705 Well-Documented: OpenAPI specification available \u2705 Free Access: Public API key available Ideal for: Comprehensive RAG systems, legislative tracking, parliamentary research, policy analysis, government accountability tools.","title":"Summary"},{"location":"datasources/bundestag_dip_api_examples/","text":"Bundestag DIP API - Terminal Examples This document provides ready-to-use terminal commands for exploring the comprehensive DIP (Documentation and Information System) Bundestag API. Prerequisites These examples use: - curl : HTTP client - jq : JSON processor (install with brew install jq on macOS) API Key All examples use the public test API key (valid until May 2026): OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw Quick Start Examples 1. Get Recent Proceedings (Vorg\u00e4nge) List proceedings from the 20th Wahlperiode: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' 2. Get Plenary Protocols List recent parliamentary session transcripts: curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' 3. Get Printed Materials (Drucksachen) List official documents from current period: curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' 4. Get Full Text of Protocol Retrieve complete stenographic transcript: curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' 5. Search for Specific Person Find information about a parliament member: curl -s \"https://search.dip.bundestag.de/api/v1/person?f.id=40&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Resource-Specific Examples Vorg\u00e4nge (Proceedings) List All Proceedings with Count curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{total: .numFound, count: (.documents | length), first_id: .documents[0].id}' Get Specific Proceeding by ID curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/320244?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Filter by Date Range Get proceedings from specific time period: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&f.datum.start=2025-01-01&f.datum.end=2025-03-31&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Extract Proceeding Summaries curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | {id, titel, vorgangstyp, datum, sachgebiet}]' | head -100 Find Legislation (Gesetzgebung) curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.vorgangstyp == \"Gesetzgebung\") | {id, titel, datum}]' | head -50 Drucksachen (Printed Materials) List All Document Types curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[].drucksachetyp] | unique' Get Bills (Gesetzentw\u00fcrfe) curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&f.drucksachetyp=Gesetzentwurf&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Get Government Responses (Antworten) curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&f.drucksachetyp=Antwort&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Get Specific Document by Number curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.dokumentnummer=20/15151&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Extract PDF URLs curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.documents[] | {number: .dokumentnummer, pdf: .fundstelle.pdf_url}' | head -20 Get Documents by Ministry (Ressort) curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.ressort[]?.titel == \"Ausw\u00e4rtiges Amt\") | {nummer: .dokumentnummer, titel}]' | head -50 Plenarprotokolle (Plenary Protocols) List Recent Sessions curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.documents[0:10] | [.[] | {session: .dokumentnummer, date: .datum, topics: .vorgangsbezug_anzahl}]' Get Bundestag vs Bundesrat Protocols Bundestag only: curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.herausgeber == \"BT\") | {session: .dokumentnummer, date: .datum}]' | head -50 Bundesrat only: curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.herausgeber == \"BR\") | {session: .dokumentnummer, date: .datum}]' | head -50 Get Protocol with Related Proceedings curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{session: .dokumentnummer, date: .datum, topics: [.vorgangsbezug[] | {id, titel, typ: .vorgangstyp}]}' Extract Protocol Download Links curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.documents[] | select(.herausgeber == \"BT\") | {session: .dokumentnummer, pdf: .fundstelle.pdf_url, xml: .fundstelle.xml_url}' | head -40 Plenarprotokoll-Text (Full Transcript) Get Complete Transcript curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Extract Just the Text curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.text' | head -100 Save Full Transcript to File curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.text' > protokoll_20_214.txt Get Transcript Metadata curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq 'del(.text) | .' Personen (Persons) Search for Person by ID curl -s \"https://search.dip.bundestag.de/api/v1/person?f.id=40&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' List All Persons (First Page) curl -s \"https://search.dip.bundestag.de/api/v1/person?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.documents[0:20] | [.[] | {id, name: \"\\(.vorname) \\(.nachname)\", fraktion, funktion}]' Extract Person Roles curl -s \"https://search.dip.bundestag.de/api/v1/person?f.id=40&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.documents[0].person_roles' Find MPs from Specific Party curl -s \"https://search.dip.bundestag.de/api/v1/person?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.fraktion[]? == \"B\u00dcNDNIS 90/DIE GR\u00dcNEN\") | {name: \"\\(.vorname) \\(.nachname)\", wahlperiode}]' | head -50 Vorgangspositionen (Proceeding Positions) List Positions for Electoral Period curl -s \"https://search.dip.bundestag.de/api/v1/vorgangsposition?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Get Specific Position curl -s \"https://search.dip.bundestag.de/api/v1/vorgangsposition/[ID]?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Aktivit\u00e4ten (Activities) List Recent Activities curl -s \"https://search.dip.bundestag.de/api/v1/aktivitaet?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Pagination Examples Basic Pagination Pattern # Get first page curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{cursor, numFound, count: (.documents | length)}' Extract and Use Cursor # Save cursor to variable CURSOR=$(curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.cursor') echo \"Cursor: $CURSOR\" # Use cursor for next page curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&cursor=$CURSOR&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{cursor, count: (.documents | length)}' Paginate Through All Results (Shell Script) #!/bin/bash APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" WAHLPERIODE=20 ENDPOINT=\"vorgang\" CURSOR=\"\" PAGE=1 echo \"Fetching all ${ENDPOINT} for Wahlperiode ${WAHLPERIODE}...\" while true; do echo \"Fetching page $PAGE...\" if [ -z \"$CURSOR\" ]; then URL=\"https://search.dip.bundestag.de/api/v1/${ENDPOINT}?f.wahlperiode=${WAHLPERIODE}&apikey=${APIKEY}\" else URL=\"https://search.dip.bundestag.de/api/v1/${ENDPOINT}?f.wahlperiode=${WAHLPERIODE}&cursor=${CURSOR}&apikey=${APIKEY}\" fi RESPONSE=$(curl -s \"$URL\") NEW_CURSOR=$(echo \"$RESPONSE\" | jq -r '.cursor // empty') COUNT=$(echo \"$RESPONSE\" | jq '.documents | length') echo \"Got $COUNT documents\" echo \"$RESPONSE\" > \"${ENDPOINT}_page_${PAGE}.json\" # Check if cursor is empty or unchanged if [ -z \"$NEW_CURSOR\" ] || [ \"$CURSOR\" = \"$NEW_CURSOR\" ]; then echo \"Reached end of results\" break fi CURSOR=\"$NEW_CURSOR\" PAGE=$((PAGE + 1)) # Be respectful to the API sleep 1 done echo \"Downloaded $PAGE pages total\" Filtering Examples By Date Range Recent proceedings: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&f.datum.start=2025-03-01&f.datum.end=2025-03-31&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.numFound' By Update Date (Incremental Sync) Get documents updated since specific date: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.aktualisiert.start=2025-03-15&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{total: .numFound, recent: [.documents[0:5] | .[] | {id, titel, aktualisiert}]}' By Document Number Specific document: curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.dokumentnummer=20/15096&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' By Proceeding Reference Find documents related to specific proceeding: curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.vorgang=320244&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' Analysis Examples Count Documents by Type curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[].drucksachetyp] | group_by(.) | map({type: .[0], count: length}) | sort_by(.count) | reverse' Extract Subject Areas (Sachgebiete) curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[].sachgebiet[]?] | unique | sort' | head -50 Find Government Initiatives curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.initiative[]? == \"Bundesregierung\") | {id, titel, datum}]' | head -50 Track Proceeding Status curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[].beratungsstand] | group_by(.) | map({status: .[0], count: length})' Find Proceedings with Keywords curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.deskriptor[]?.name | test(\"Klima|Energie\")) | {id, titel, keywords: [.deskriptor[].name]}]' | head -100 Data Export Examples Export to CSV Extract proceedings to CSV: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.documents[] | [.id, .datum, .vorgangstyp, .titel] | @csv' > vorgaenge.csv Export documents with PDF links: curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.documents[] | [.dokumentnummer, .datum, .drucksachetyp, .fundstelle.pdf_url] | @csv' > drucksachen.csv Save Complete JSON curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/320244?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' > vorgang_320244.json Create Document Inventory #!/bin/bash APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" echo \"Resource,Total Count\" > dip_inventory.csv for resource in vorgang vorgangsposition drucksache plenarprotokoll person aktivitaet; do count=$(curl -s \"https://search.dip.bundestag.de/api/v1/${resource}?f.wahlperiode=20&apikey=${APIKEY}\" | jq -r '.numFound // \"N/A\"') echo \"${resource},${count}\" >> dip_inventory.csv echo \"${resource}: ${count}\" sleep 1 done cat dip_inventory.csv Relationship Traversal Examples Get Proceeding and All Related Documents # Get proceeding VORGANG_ID=320244 curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/${VORGANG_ID}?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" > vorgang.json # Find related drucksachen curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.vorgang=${VORGANG_ID}&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" > related_drucksachen.json # Display summary echo \"Proceeding:\" jq '.titel' vorgang.json echo -e \"\\nRelated Documents:\" jq '.documents[] | {nummer: .dokumentnummer, typ: .drucksachetyp, datum}' related_drucksachen.json Find All Protocols Discussing a Topic # Get protocols mentioning specific proceeding curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.vorgang=320785&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.documents[] | {session: .dokumentnummer, date: .datum, pdf: .fundstelle.pdf_url}' Advanced Examples Monitor Recent Changes Check for updates in last 24 hours: YESTERDAY=$(date -u -v-1d +\"%Y-%m-%dT%H:%M:%S\") curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.aktualisiert.start=${YESTERDAY}&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{total: .numFound, changes: [.documents[] | {id, titel, aktualisiert}]}' Download All PDFs for a Proceeding #!/bin/bash VORGANG_ID=320244 APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" mkdir -p pdfs_${VORGANG_ID} # Get related documents curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.vorgang=${VORGANG_ID}&apikey=${APIKEY}\" | \\ jq -r '.documents[] | .fundstelle.pdf_url' | \\ while read url; do filename=$(basename \"$url\") echo \"Downloading $filename...\" curl -s \"$url\" -o \"pdfs_${VORGANG_ID}/${filename}\" sleep 1 done echo \"PDFs saved to pdfs_${VORGANG_ID}/\" Build Proceeding Timeline VORGANG_ID=320244 APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" echo \"=== Proceeding Timeline ===\" echo \"\" # Get proceeding info curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/${VORGANG_ID}?apikey=${APIKEY}\" | \\ jq -r '\"\\(.datum): \\(.titel)\\nType: \\(.vorgangstyp)\\nStatus: \\(.beratungsstand)\\n\"' echo \"=== Related Documents ===\" curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.vorgang=${VORGANG_ID}&apikey=${APIKEY}\" | \\ jq -r '.documents | sort_by(.datum) | .[] | \"\\(.datum): \\(.drucksachetyp) \\(.dokumentnummer)\"' Compare Two Electoral Periods APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" echo \"Wahlperiode,Total Vorg\u00e4nge,Total Drucksachen\" for wp in 19 20; do vorgaenge=$(curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=${wp}&apikey=${APIKEY}\" | jq -r '.numFound') drucksachen=$(curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=${wp}&apikey=${APIKEY}\" | jq -r '.numFound') echo \"${wp},${vorgaenge},${drucksachen}\" sleep 1 done Troubleshooting Examples Test API Connectivity curl -I \"https://search.dip.bundestag.de/api/v1/\" Validate API Key curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=invalid_key\" | jq '.' # Should return 401 error Check Response Time time curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" > /dev/null Verify JSON Structure curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/320244?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq empty && echo \"Valid JSON\" || echo \"Invalid JSON\" Debug with Verbose Output curl -v \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" 2>&1 | head -50 Performance Tips Use Specific Filters: Reduce result sets with filters ```bash # Better: filtered query curl -s \"...?f.wahlperiode=20&f.datum.start=2025-03-01\" # Avoid: unfiltered query curl -s \"...?apikey=...\" ``` Fetch Metadata First, Full Text Later: ```bash # Step 1: Get list curl -s \".../plenarprotokoll?...\" | jq '.documents[].id' > ids.txt # Step 2: Fetch full text for relevant IDs only cat ids.txt | while read id; do curl -s \".../plenarprotokoll-text/${id}?...\" done ``` Cache Results Locally: bash # Check if cached if [ ! -f \"cache_vorgang_20.json\" ]; then curl -s \"...\" > cache_vorgang_20.json fi cat cache_vorgang_20.json | jq '.' Implement Rate Limiting: bash for id in $(seq 1 100); do curl -s \".../${id}?...\" sleep 0.5 # 2 requests/second done Quick Reference Task Command Template List proceedings curl -s \".../vorgang?f.wahlperiode=20&apikey=...\" Get by ID curl -s \".../vorgang/{id}?apikey=...\" Filter by date ...?f.datum.start=YYYY-MM-DD&f.datum.end=YYYY-MM-DD Get updates ...?f.aktualisiert.start=YYYY-MM-DD Paginate ...?cursor={cursor} Get full text curl -s \".../plenarprotokoll-text/{id}?apikey=...\" Export to CSV jq -r '[...] \\| @csv' Count results jq '.numFound' Extract field jq '.documents[].titel' Next Steps After exploring with these examples: Review the full API documentation: bundestag_dip_api.md Plan your data extraction strategy Identify which resources are most valuable for your use case Design your document schema and metadata fields Implement pagination for bulk extraction Consider incremental updates using f.aktualisiert.start","title":"Bundestag DIP API - Terminal Examples"},{"location":"datasources/bundestag_dip_api_examples/#bundestag-dip-api-terminal-examples","text":"This document provides ready-to-use terminal commands for exploring the comprehensive DIP (Documentation and Information System) Bundestag API.","title":"Bundestag DIP API - Terminal Examples"},{"location":"datasources/bundestag_dip_api_examples/#prerequisites","text":"These examples use: - curl : HTTP client - jq : JSON processor (install with brew install jq on macOS)","title":"Prerequisites"},{"location":"datasources/bundestag_dip_api_examples/#api-key","text":"All examples use the public test API key (valid until May 2026): OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw","title":"API Key"},{"location":"datasources/bundestag_dip_api_examples/#quick-start-examples","text":"","title":"Quick Start Examples"},{"location":"datasources/bundestag_dip_api_examples/#1-get-recent-proceedings-vorgange","text":"List proceedings from the 20th Wahlperiode: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"1. Get Recent Proceedings (Vorg\u00e4nge)"},{"location":"datasources/bundestag_dip_api_examples/#2-get-plenary-protocols","text":"List recent parliamentary session transcripts: curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"2. Get Plenary Protocols"},{"location":"datasources/bundestag_dip_api_examples/#3-get-printed-materials-drucksachen","text":"List official documents from current period: curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"3. Get Printed Materials (Drucksachen)"},{"location":"datasources/bundestag_dip_api_examples/#4-get-full-text-of-protocol","text":"Retrieve complete stenographic transcript: curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"4. Get Full Text of Protocol"},{"location":"datasources/bundestag_dip_api_examples/#5-search-for-specific-person","text":"Find information about a parliament member: curl -s \"https://search.dip.bundestag.de/api/v1/person?f.id=40&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"5. Search for Specific Person"},{"location":"datasources/bundestag_dip_api_examples/#resource-specific-examples","text":"","title":"Resource-Specific Examples"},{"location":"datasources/bundestag_dip_api_examples/#vorgange-proceedings","text":"","title":"Vorg\u00e4nge (Proceedings)"},{"location":"datasources/bundestag_dip_api_examples/#list-all-proceedings-with-count","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{total: .numFound, count: (.documents | length), first_id: .documents[0].id}'","title":"List All Proceedings with Count"},{"location":"datasources/bundestag_dip_api_examples/#get-specific-proceeding-by-id","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/320244?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"Get Specific Proceeding by ID"},{"location":"datasources/bundestag_dip_api_examples/#filter-by-date-range","text":"Get proceedings from specific time period: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&f.datum.start=2025-01-01&f.datum.end=2025-03-31&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"Filter by Date Range"},{"location":"datasources/bundestag_dip_api_examples/#extract-proceeding-summaries","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | {id, titel, vorgangstyp, datum, sachgebiet}]' | head -100","title":"Extract Proceeding Summaries"},{"location":"datasources/bundestag_dip_api_examples/#find-legislation-gesetzgebung","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.vorgangstyp == \"Gesetzgebung\") | {id, titel, datum}]' | head -50","title":"Find Legislation (Gesetzgebung)"},{"location":"datasources/bundestag_dip_api_examples/#drucksachen-printed-materials","text":"","title":"Drucksachen (Printed Materials)"},{"location":"datasources/bundestag_dip_api_examples/#list-all-document-types","text":"curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[].drucksachetyp] | unique'","title":"List All Document Types"},{"location":"datasources/bundestag_dip_api_examples/#get-bills-gesetzentwurfe","text":"curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&f.drucksachetyp=Gesetzentwurf&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"Get Bills (Gesetzentw\u00fcrfe)"},{"location":"datasources/bundestag_dip_api_examples/#get-government-responses-antworten","text":"curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&f.drucksachetyp=Antwort&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"Get Government Responses (Antworten)"},{"location":"datasources/bundestag_dip_api_examples/#get-specific-document-by-number","text":"curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.dokumentnummer=20/15151&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"Get Specific Document by Number"},{"location":"datasources/bundestag_dip_api_examples/#extract-pdf-urls","text":"curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.documents[] | {number: .dokumentnummer, pdf: .fundstelle.pdf_url}' | head -20","title":"Extract PDF URLs"},{"location":"datasources/bundestag_dip_api_examples/#get-documents-by-ministry-ressort","text":"curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.ressort[]?.titel == \"Ausw\u00e4rtiges Amt\") | {nummer: .dokumentnummer, titel}]' | head -50","title":"Get Documents by Ministry (Ressort)"},{"location":"datasources/bundestag_dip_api_examples/#plenarprotokolle-plenary-protocols","text":"","title":"Plenarprotokolle (Plenary Protocols)"},{"location":"datasources/bundestag_dip_api_examples/#list-recent-sessions","text":"curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.documents[0:10] | [.[] | {session: .dokumentnummer, date: .datum, topics: .vorgangsbezug_anzahl}]'","title":"List Recent Sessions"},{"location":"datasources/bundestag_dip_api_examples/#get-bundestag-vs-bundesrat-protocols","text":"Bundestag only: curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.herausgeber == \"BT\") | {session: .dokumentnummer, date: .datum}]' | head -50 Bundesrat only: curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.herausgeber == \"BR\") | {session: .dokumentnummer, date: .datum}]' | head -50","title":"Get Bundestag vs Bundesrat Protocols"},{"location":"datasources/bundestag_dip_api_examples/#get-protocol-with-related-proceedings","text":"curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{session: .dokumentnummer, date: .datum, topics: [.vorgangsbezug[] | {id, titel, typ: .vorgangstyp}]}'","title":"Get Protocol with Related Proceedings"},{"location":"datasources/bundestag_dip_api_examples/#extract-protocol-download-links","text":"curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.documents[] | select(.herausgeber == \"BT\") | {session: .dokumentnummer, pdf: .fundstelle.pdf_url, xml: .fundstelle.xml_url}' | head -40","title":"Extract Protocol Download Links"},{"location":"datasources/bundestag_dip_api_examples/#plenarprotokoll-text-full-transcript","text":"","title":"Plenarprotokoll-Text (Full Transcript)"},{"location":"datasources/bundestag_dip_api_examples/#get-complete-transcript","text":"curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"Get Complete Transcript"},{"location":"datasources/bundestag_dip_api_examples/#extract-just-the-text","text":"curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.text' | head -100","title":"Extract Just the Text"},{"location":"datasources/bundestag_dip_api_examples/#save-full-transcript-to-file","text":"curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.text' > protokoll_20_214.txt","title":"Save Full Transcript to File"},{"location":"datasources/bundestag_dip_api_examples/#get-transcript-metadata","text":"curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll-text/5701?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq 'del(.text) | .'","title":"Get Transcript Metadata"},{"location":"datasources/bundestag_dip_api_examples/#personen-persons","text":"","title":"Personen (Persons)"},{"location":"datasources/bundestag_dip_api_examples/#search-for-person-by-id","text":"curl -s \"https://search.dip.bundestag.de/api/v1/person?f.id=40&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"Search for Person by ID"},{"location":"datasources/bundestag_dip_api_examples/#list-all-persons-first-page","text":"curl -s \"https://search.dip.bundestag.de/api/v1/person?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.documents[0:20] | [.[] | {id, name: \"\\(.vorname) \\(.nachname)\", fraktion, funktion}]'","title":"List All Persons (First Page)"},{"location":"datasources/bundestag_dip_api_examples/#extract-person-roles","text":"curl -s \"https://search.dip.bundestag.de/api/v1/person?f.id=40&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.documents[0].person_roles'","title":"Extract Person Roles"},{"location":"datasources/bundestag_dip_api_examples/#find-mps-from-specific-party","text":"curl -s \"https://search.dip.bundestag.de/api/v1/person?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.fraktion[]? == \"B\u00dcNDNIS 90/DIE GR\u00dcNEN\") | {name: \"\\(.vorname) \\(.nachname)\", wahlperiode}]' | head -50","title":"Find MPs from Specific Party"},{"location":"datasources/bundestag_dip_api_examples/#vorgangspositionen-proceeding-positions","text":"","title":"Vorgangspositionen (Proceeding Positions)"},{"location":"datasources/bundestag_dip_api_examples/#list-positions-for-electoral-period","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgangsposition?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"List Positions for Electoral Period"},{"location":"datasources/bundestag_dip_api_examples/#get-specific-position","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgangsposition/[ID]?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"Get Specific Position"},{"location":"datasources/bundestag_dip_api_examples/#aktivitaten-activities","text":"","title":"Aktivit\u00e4ten (Activities)"},{"location":"datasources/bundestag_dip_api_examples/#list-recent-activities","text":"curl -s \"https://search.dip.bundestag.de/api/v1/aktivitaet?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"List Recent Activities"},{"location":"datasources/bundestag_dip_api_examples/#pagination-examples","text":"","title":"Pagination Examples"},{"location":"datasources/bundestag_dip_api_examples/#basic-pagination-pattern","text":"# Get first page curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{cursor, numFound, count: (.documents | length)}'","title":"Basic Pagination Pattern"},{"location":"datasources/bundestag_dip_api_examples/#extract-and-use-cursor","text":"# Save cursor to variable CURSOR=$(curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.cursor') echo \"Cursor: $CURSOR\" # Use cursor for next page curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&cursor=$CURSOR&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{cursor, count: (.documents | length)}'","title":"Extract and Use Cursor"},{"location":"datasources/bundestag_dip_api_examples/#paginate-through-all-results-shell-script","text":"#!/bin/bash APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" WAHLPERIODE=20 ENDPOINT=\"vorgang\" CURSOR=\"\" PAGE=1 echo \"Fetching all ${ENDPOINT} for Wahlperiode ${WAHLPERIODE}...\" while true; do echo \"Fetching page $PAGE...\" if [ -z \"$CURSOR\" ]; then URL=\"https://search.dip.bundestag.de/api/v1/${ENDPOINT}?f.wahlperiode=${WAHLPERIODE}&apikey=${APIKEY}\" else URL=\"https://search.dip.bundestag.de/api/v1/${ENDPOINT}?f.wahlperiode=${WAHLPERIODE}&cursor=${CURSOR}&apikey=${APIKEY}\" fi RESPONSE=$(curl -s \"$URL\") NEW_CURSOR=$(echo \"$RESPONSE\" | jq -r '.cursor // empty') COUNT=$(echo \"$RESPONSE\" | jq '.documents | length') echo \"Got $COUNT documents\" echo \"$RESPONSE\" > \"${ENDPOINT}_page_${PAGE}.json\" # Check if cursor is empty or unchanged if [ -z \"$NEW_CURSOR\" ] || [ \"$CURSOR\" = \"$NEW_CURSOR\" ]; then echo \"Reached end of results\" break fi CURSOR=\"$NEW_CURSOR\" PAGE=$((PAGE + 1)) # Be respectful to the API sleep 1 done echo \"Downloaded $PAGE pages total\"","title":"Paginate Through All Results (Shell Script)"},{"location":"datasources/bundestag_dip_api_examples/#filtering-examples","text":"","title":"Filtering Examples"},{"location":"datasources/bundestag_dip_api_examples/#by-date-range","text":"Recent proceedings: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&f.datum.start=2025-03-01&f.datum.end=2025-03-31&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.numFound'","title":"By Date Range"},{"location":"datasources/bundestag_dip_api_examples/#by-update-date-incremental-sync","text":"Get documents updated since specific date: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.aktualisiert.start=2025-03-15&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{total: .numFound, recent: [.documents[0:5] | .[] | {id, titel, aktualisiert}]}'","title":"By Update Date (Incremental Sync)"},{"location":"datasources/bundestag_dip_api_examples/#by-document-number","text":"Specific document: curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.dokumentnummer=20/15096&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"By Document Number"},{"location":"datasources/bundestag_dip_api_examples/#by-proceeding-reference","text":"Find documents related to specific proceeding: curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.vorgang=320244&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.'","title":"By Proceeding Reference"},{"location":"datasources/bundestag_dip_api_examples/#analysis-examples","text":"","title":"Analysis Examples"},{"location":"datasources/bundestag_dip_api_examples/#count-documents-by-type","text":"curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[].drucksachetyp] | group_by(.) | map({type: .[0], count: length}) | sort_by(.count) | reverse'","title":"Count Documents by Type"},{"location":"datasources/bundestag_dip_api_examples/#extract-subject-areas-sachgebiete","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[].sachgebiet[]?] | unique | sort' | head -50","title":"Extract Subject Areas (Sachgebiete)"},{"location":"datasources/bundestag_dip_api_examples/#find-government-initiatives","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.initiative[]? == \"Bundesregierung\") | {id, titel, datum}]' | head -50","title":"Find Government Initiatives"},{"location":"datasources/bundestag_dip_api_examples/#track-proceeding-status","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[].beratungsstand] | group_by(.) | map({status: .[0], count: length})'","title":"Track Proceeding Status"},{"location":"datasources/bundestag_dip_api_examples/#find-proceedings-with-keywords","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '[.documents[] | select(.deskriptor[]?.name | test(\"Klima|Energie\")) | {id, titel, keywords: [.deskriptor[].name]}]' | head -100","title":"Find Proceedings with Keywords"},{"location":"datasources/bundestag_dip_api_examples/#data-export-examples","text":"","title":"Data Export Examples"},{"location":"datasources/bundestag_dip_api_examples/#export-to-csv","text":"Extract proceedings to CSV: curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.documents[] | [.id, .datum, .vorgangstyp, .titel] | @csv' > vorgaenge.csv Export documents with PDF links: curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq -r '.documents[] | [.dokumentnummer, .datum, .drucksachetyp, .fundstelle.pdf_url] | @csv' > drucksachen.csv","title":"Export to CSV"},{"location":"datasources/bundestag_dip_api_examples/#save-complete-json","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/320244?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.' > vorgang_320244.json","title":"Save Complete JSON"},{"location":"datasources/bundestag_dip_api_examples/#create-document-inventory","text":"#!/bin/bash APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" echo \"Resource,Total Count\" > dip_inventory.csv for resource in vorgang vorgangsposition drucksache plenarprotokoll person aktivitaet; do count=$(curl -s \"https://search.dip.bundestag.de/api/v1/${resource}?f.wahlperiode=20&apikey=${APIKEY}\" | jq -r '.numFound // \"N/A\"') echo \"${resource},${count}\" >> dip_inventory.csv echo \"${resource}: ${count}\" sleep 1 done cat dip_inventory.csv","title":"Create Document Inventory"},{"location":"datasources/bundestag_dip_api_examples/#relationship-traversal-examples","text":"","title":"Relationship Traversal Examples"},{"location":"datasources/bundestag_dip_api_examples/#get-proceeding-and-all-related-documents","text":"# Get proceeding VORGANG_ID=320244 curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/${VORGANG_ID}?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" > vorgang.json # Find related drucksachen curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.vorgang=${VORGANG_ID}&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" > related_drucksachen.json # Display summary echo \"Proceeding:\" jq '.titel' vorgang.json echo -e \"\\nRelated Documents:\" jq '.documents[] | {nummer: .dokumentnummer, typ: .drucksachetyp, datum}' related_drucksachen.json","title":"Get Proceeding and All Related Documents"},{"location":"datasources/bundestag_dip_api_examples/#find-all-protocols-discussing-a-topic","text":"# Get protocols mentioning specific proceeding curl -s \"https://search.dip.bundestag.de/api/v1/plenarprotokoll?f.vorgang=320785&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '.documents[] | {session: .dokumentnummer, date: .datum, pdf: .fundstelle.pdf_url}'","title":"Find All Protocols Discussing a Topic"},{"location":"datasources/bundestag_dip_api_examples/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"datasources/bundestag_dip_api_examples/#monitor-recent-changes","text":"Check for updates in last 24 hours: YESTERDAY=$(date -u -v-1d +\"%Y-%m-%dT%H:%M:%S\") curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.aktualisiert.start=${YESTERDAY}&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq '{total: .numFound, changes: [.documents[] | {id, titel, aktualisiert}]}'","title":"Monitor Recent Changes"},{"location":"datasources/bundestag_dip_api_examples/#download-all-pdfs-for-a-proceeding","text":"#!/bin/bash VORGANG_ID=320244 APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" mkdir -p pdfs_${VORGANG_ID} # Get related documents curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.vorgang=${VORGANG_ID}&apikey=${APIKEY}\" | \\ jq -r '.documents[] | .fundstelle.pdf_url' | \\ while read url; do filename=$(basename \"$url\") echo \"Downloading $filename...\" curl -s \"$url\" -o \"pdfs_${VORGANG_ID}/${filename}\" sleep 1 done echo \"PDFs saved to pdfs_${VORGANG_ID}/\"","title":"Download All PDFs for a Proceeding"},{"location":"datasources/bundestag_dip_api_examples/#build-proceeding-timeline","text":"VORGANG_ID=320244 APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" echo \"=== Proceeding Timeline ===\" echo \"\" # Get proceeding info curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/${VORGANG_ID}?apikey=${APIKEY}\" | \\ jq -r '\"\\(.datum): \\(.titel)\\nType: \\(.vorgangstyp)\\nStatus: \\(.beratungsstand)\\n\"' echo \"=== Related Documents ===\" curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.vorgang=${VORGANG_ID}&apikey=${APIKEY}\" | \\ jq -r '.documents | sort_by(.datum) | .[] | \"\\(.datum): \\(.drucksachetyp) \\(.dokumentnummer)\"'","title":"Build Proceeding Timeline"},{"location":"datasources/bundestag_dip_api_examples/#compare-two-electoral-periods","text":"APIKEY=\"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" echo \"Wahlperiode,Total Vorg\u00e4nge,Total Drucksachen\" for wp in 19 20; do vorgaenge=$(curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=${wp}&apikey=${APIKEY}\" | jq -r '.numFound') drucksachen=$(curl -s \"https://search.dip.bundestag.de/api/v1/drucksache?f.wahlperiode=${wp}&apikey=${APIKEY}\" | jq -r '.numFound') echo \"${wp},${vorgaenge},${drucksachen}\" sleep 1 done","title":"Compare Two Electoral Periods"},{"location":"datasources/bundestag_dip_api_examples/#troubleshooting-examples","text":"","title":"Troubleshooting Examples"},{"location":"datasources/bundestag_dip_api_examples/#test-api-connectivity","text":"curl -I \"https://search.dip.bundestag.de/api/v1/\"","title":"Test API Connectivity"},{"location":"datasources/bundestag_dip_api_examples/#validate-api-key","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=invalid_key\" | jq '.' # Should return 401 error","title":"Validate API Key"},{"location":"datasources/bundestag_dip_api_examples/#check-response-time","text":"time curl -s \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" > /dev/null","title":"Check Response Time"},{"location":"datasources/bundestag_dip_api_examples/#verify-json-structure","text":"curl -s \"https://search.dip.bundestag.de/api/v1/vorgang/320244?apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" | jq empty && echo \"Valid JSON\" || echo \"Invalid JSON\"","title":"Verify JSON Structure"},{"location":"datasources/bundestag_dip_api_examples/#debug-with-verbose-output","text":"curl -v \"https://search.dip.bundestag.de/api/v1/vorgang?f.wahlperiode=20&apikey=OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" 2>&1 | head -50","title":"Debug with Verbose Output"},{"location":"datasources/bundestag_dip_api_examples/#performance-tips","text":"Use Specific Filters: Reduce result sets with filters ```bash # Better: filtered query curl -s \"...?f.wahlperiode=20&f.datum.start=2025-03-01\" # Avoid: unfiltered query curl -s \"...?apikey=...\" ``` Fetch Metadata First, Full Text Later: ```bash # Step 1: Get list curl -s \".../plenarprotokoll?...\" | jq '.documents[].id' > ids.txt # Step 2: Fetch full text for relevant IDs only cat ids.txt | while read id; do curl -s \".../plenarprotokoll-text/${id}?...\" done ``` Cache Results Locally: bash # Check if cached if [ ! -f \"cache_vorgang_20.json\" ]; then curl -s \"...\" > cache_vorgang_20.json fi cat cache_vorgang_20.json | jq '.' Implement Rate Limiting: bash for id in $(seq 1 100); do curl -s \".../${id}?...\" sleep 0.5 # 2 requests/second done","title":"Performance Tips"},{"location":"datasources/bundestag_dip_api_examples/#quick-reference","text":"Task Command Template List proceedings curl -s \".../vorgang?f.wahlperiode=20&apikey=...\" Get by ID curl -s \".../vorgang/{id}?apikey=...\" Filter by date ...?f.datum.start=YYYY-MM-DD&f.datum.end=YYYY-MM-DD Get updates ...?f.aktualisiert.start=YYYY-MM-DD Paginate ...?cursor={cursor} Get full text curl -s \".../plenarprotokoll-text/{id}?apikey=...\" Export to CSV jq -r '[...] \\| @csv' Count results jq '.numFound' Extract field jq '.documents[].titel'","title":"Quick Reference"},{"location":"datasources/bundestag_dip_api_examples/#next-steps","text":"After exploring with these examples: Review the full API documentation: bundestag_dip_api.md Plan your data extraction strategy Identify which resources are most valuable for your use case Design your document schema and metadata fields Implement pagination for bulk extraction Consider incremental updates using f.aktualisiert.start","title":"Next Steps"},{"location":"evaluation/in_progress/","text":"In progress...","title":"In progress"},{"location":"how_to/how_to_add_new_datasource/","text":"How to Add a New Datasource Implementation This guide demonstrates how to add support for a new datasource implementation, using Confluence as an example. Architecture Datasources are managed by the DatasourceManager , which aggregates required components and orchestrates them to retrieve documents, clean them, and parse them to markdown format - which is strictly required by the embedding process. The general datasource manager flow is: Reader -> Parser (Optional) -> Cleaner (Optional) -> Splitter (Optional). Therefore, adding support for a new datasource requires implementing these components and their respective manager. Implementation Step 1: Dependencies Add the required packages to pyproject.toml under the following section: [project.optional-dependencies] extraction = [ \"atlassian-python-api>=3.41.19\", ... ] Step 2: Datasource Enum In datasources.py , add the new datasource to the DatasourceName enumeration: class DatasourceName(str, Enum): ... CONFLUENCE = \"confluence\" Step 3: Datasource Secrets Create a new directory src/extraction/datasources/confluence and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal, Union from pydantic import ConfigDict, Field, SecretStr from core.base_configuration import BaseSecrets from extraction.bootstrap.configuration.datasources import ( DatasourceConfiguration, DatasourceName, ) class ConfluenceDatasourceConfiguration(DatasourceConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAG__DATASOURCES__CONFLUENCE__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) username: SecretStr = Field( ..., description=\"Username credential used to authenticate with the Confluence instance\", ) password: SecretStr = Field( ..., description=\"Password credential used to authenticate with the Confluence instance\", ) secrets: Secrets = Field( None, description=\"Authentication credentials required to access the Confluence instance\", ) The first part is to create a configuration that extends DatasourceConfiguration . The Secrets inner class defines secret fields that will be present in the environment secret file under the RAG__DATASOURCES__CONFLUENCE__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAG__DATASOURCES__CONFLUENCE__USERNAME=<confluence_username> RAG__DATASOURCES__CONFLUENCE__PASSWORD=<confluence_password> Note : If your datasource doesn't require secrets, you can skip this step. Step 4: Datasource Configuration Finish up ConfluenceDatasourceConfiguration implementation and add the rest of the configuration required for the datasource: ... class ConfluenceDatasourceConfiguration(DatasourceConfiguration): ... host: str = Field( \"127.0.0.1\", description=\"Hostname or IP address of the Confluence server instance\", ) protocol: Union[Literal[\"http\"], Literal[\"https\"]] = Field( \"http\", description=\"Communication protocol used to connect to the Confluence server\", ) name: Literal[DatasourceName.CONFLUENCE] = Field( ..., description=\"Identifier specifying this configuration is for a Confluence datasource\", ) @property def base_url(self) -> str: return f\"{self.protocol}://{self.host}\" provider field constraints the value to DatasourceName.CONFLUENCE , which serves as an indicator for pydantic validator. Step 5: Confluence Document The next step is to create a Confluence document data class in document.py : from extraction.datasources.core.document import BaseDocument class ConfluenceDocument(BaseDocument): \"\"\"Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. \"\"\" pass In our case, we don't need anything beyond the BaseDocument implementation. Step 6: Confluence Client To create a Confluence client, we implement ConfluenceClientFactory in client.py . It extends SingletonFactory , which provides an interface for initializing a single instance for the duration of the application runtime. from typing import Type from atlassian import Confluence from core import SingletonFactory from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) class ConfluenceClientFactory(SingletonFactory): _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> Confluence: return Confluence( url=configuration.base_url, username=configuration.secrets.username.get_secret_value(), password=configuration.secrets.password.get_secret_value(), ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding client initialization. Step 7: Datasource Reader Create a Confluence reader in reader.py that implements the BaseReader interface: from extraction.datasources.core.reader import BaseReader ... class ConfluenceDatasourceReader(BaseReader): async def read_all_async( self, ) -> AsyncIterator[dict]: # read Confluence pages implementation This method returns an iterator, which improves runtime memory management. Next, implement a factory that defines how the ConfluenceDatasourceReader is initialized: from core import Factory ... class ConfluenceDatasourceReaderFactory(Factory): _configuration_class = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceReader: client = ConfluenceClientFactory.create(configuration) return ConfluenceDatasourceReader( configuration=configuration, client=client, ) Note that instead of initializing the Confluence client directly, the factory uses ConfluenceClientFactory to handle this task. Step 8: Datasource Parser In parser.py implement a parser responsible for converting the raw Confluence page to markdown format: from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) from extraction.datasources.confluence.document import ConfluenceDocument from extraction.datasources.core.parser import BaseParser class ConfluenceDatasourceParser(BaseParser[ConfluenceDocument]): def parse(self, page: str) -> ConfluenceDocument: # parse Confluence page implementation As before, define a factory for the parser: class ConfluenceDatasourceParserFactory(Factory): _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceParser: return ConfluenceDatasourceParser(configuration) Step 9: Datasource Manager To orchestrate all the previous components, we will reuse BasicDatasourceManager and implement a factory for it in manager.py : class ConfluenceDatasourceManagerFactory(Factory): \"\"\"Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class: Configuration class used for validating and processing Confluence-specific settings. \"\"\" _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> BasicDatasourceManager: \"\"\"Create a configured Confluence datasource manager. Sets up the necessary reader and parser components based on the provided configuration and assembles them into a functional manager. Args: configuration: Configuration object containing Confluence-specific parameters including authentication details, spaces to extract, and other extraction options. Returns: A fully initialized datasource manager that can extract and process data from Confluence. \"\"\" reader = ConfluenceDatasourceReaderFactory.create(configuration) parser = ConfluenceDatasourceParserFactory.create(configuration) return BasicDatasourceManager(configuration, reader, parser) Following the design pattern, ConfluenceDatasourceManagerFactory uses reader and parser factories to obtain the instances needed for the manager. Step 10: Datasource Integration Create an __init__.py file as follows: from extraction.bootstrap.configuration.datasources import ( DatasourceConfigurationRegistry, DatasourceName, ) from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) from extraction.datasources.confluence.manager import ( ConfluenceDatasourceManagerFactory, ) from extraction.datasources.registry import DatasourceManagerRegistry def register() -> None: DatasourceManagerRegistry.register( DatasourceName.CONFLUENCE, ConfluenceDatasourceManagerFactory ) DatasourceConfigurationRegistry.register( DatasourceName.CONFLUENCE, ConfluenceDatasourceConfiguration ) The initialization file includes a register() method responsible for registering our configuration and manager factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Confluence configuration in configurations/configuration.{environment}.json file: \"extraction\": { \"datasources\": [ { \"name\": \"confluence\", \"host\": \"wissen.feld-m.de\", \"protocol\": \"https\" } ] ... } ... We can dynamically retrieve the corresponding manager implementation by using the name specified in the configuration: datasource_config = read_datasource_from_config() datasource_manager = DatasourceManagerRegistry.get(datasource_config.name).create(datasource_config) This mechanism is later used by DatasourceOrchestrator to initialize datasources defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 extraction/ \u2514\u2500\u2500 datasources/ \u2514\u2500\u2500 confluence/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 client.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 document.py \u251c\u2500\u2500 manager.py \u251c\u2500\u2500 parser.py \u2514\u2500\u2500 reader.py Notes Below is the __init__ method of BasicDatasourceManager used in our tutorial: class BasicDatasourceManager(BaseDatasourceManager, Generic[DocType]): def __init__( self, configuration: ExtractionConfiguration, reader: BaseReader, parser: BaseParser = BasicMarkdownParser(), cleaner: BaseCleaner = BasicMarkdownCleaner(), splitter: BaseSplitter = BasicMarkdownSplitter(), ): Note that in this guide we skipped the implementation of custom cleaner and splitter components, instead using the default ones. When building a new datasource integration, you might need to implement custom versions of these components based on your specific requirements.","title":"Add a New Datasource"},{"location":"how_to/how_to_add_new_datasource/#how-to-add-a-new-datasource-implementation","text":"This guide demonstrates how to add support for a new datasource implementation, using Confluence as an example.","title":"How to Add a New Datasource Implementation"},{"location":"how_to/how_to_add_new_datasource/#architecture","text":"Datasources are managed by the DatasourceManager , which aggregates required components and orchestrates them to retrieve documents, clean them, and parse them to markdown format - which is strictly required by the embedding process. The general datasource manager flow is: Reader -> Parser (Optional) -> Cleaner (Optional) -> Splitter (Optional). Therefore, adding support for a new datasource requires implementing these components and their respective manager.","title":"Architecture"},{"location":"how_to/how_to_add_new_datasource/#implementation","text":"","title":"Implementation"},{"location":"how_to/how_to_add_new_datasource/#step-1-dependencies","text":"Add the required packages to pyproject.toml under the following section: [project.optional-dependencies] extraction = [ \"atlassian-python-api>=3.41.19\", ... ]","title":"Step 1: Dependencies"},{"location":"how_to/how_to_add_new_datasource/#step-2-datasource-enum","text":"In datasources.py , add the new datasource to the DatasourceName enumeration: class DatasourceName(str, Enum): ... CONFLUENCE = \"confluence\"","title":"Step 2: Datasource Enum"},{"location":"how_to/how_to_add_new_datasource/#step-3-datasource-secrets","text":"Create a new directory src/extraction/datasources/confluence and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal, Union from pydantic import ConfigDict, Field, SecretStr from core.base_configuration import BaseSecrets from extraction.bootstrap.configuration.datasources import ( DatasourceConfiguration, DatasourceName, ) class ConfluenceDatasourceConfiguration(DatasourceConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAG__DATASOURCES__CONFLUENCE__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) username: SecretStr = Field( ..., description=\"Username credential used to authenticate with the Confluence instance\", ) password: SecretStr = Field( ..., description=\"Password credential used to authenticate with the Confluence instance\", ) secrets: Secrets = Field( None, description=\"Authentication credentials required to access the Confluence instance\", ) The first part is to create a configuration that extends DatasourceConfiguration . The Secrets inner class defines secret fields that will be present in the environment secret file under the RAG__DATASOURCES__CONFLUENCE__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAG__DATASOURCES__CONFLUENCE__USERNAME=<confluence_username> RAG__DATASOURCES__CONFLUENCE__PASSWORD=<confluence_password> Note : If your datasource doesn't require secrets, you can skip this step.","title":"Step 3: Datasource Secrets"},{"location":"how_to/how_to_add_new_datasource/#step-4-datasource-configuration","text":"Finish up ConfluenceDatasourceConfiguration implementation and add the rest of the configuration required for the datasource: ... class ConfluenceDatasourceConfiguration(DatasourceConfiguration): ... host: str = Field( \"127.0.0.1\", description=\"Hostname or IP address of the Confluence server instance\", ) protocol: Union[Literal[\"http\"], Literal[\"https\"]] = Field( \"http\", description=\"Communication protocol used to connect to the Confluence server\", ) name: Literal[DatasourceName.CONFLUENCE] = Field( ..., description=\"Identifier specifying this configuration is for a Confluence datasource\", ) @property def base_url(self) -> str: return f\"{self.protocol}://{self.host}\" provider field constraints the value to DatasourceName.CONFLUENCE , which serves as an indicator for pydantic validator.","title":"Step 4: Datasource Configuration"},{"location":"how_to/how_to_add_new_datasource/#step-5-confluence-document","text":"The next step is to create a Confluence document data class in document.py : from extraction.datasources.core.document import BaseDocument class ConfluenceDocument(BaseDocument): \"\"\"Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. \"\"\" pass In our case, we don't need anything beyond the BaseDocument implementation.","title":"Step 5: Confluence Document"},{"location":"how_to/how_to_add_new_datasource/#step-6-confluence-client","text":"To create a Confluence client, we implement ConfluenceClientFactory in client.py . It extends SingletonFactory , which provides an interface for initializing a single instance for the duration of the application runtime. from typing import Type from atlassian import Confluence from core import SingletonFactory from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) class ConfluenceClientFactory(SingletonFactory): _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> Confluence: return Confluence( url=configuration.base_url, username=configuration.secrets.username.get_secret_value(), password=configuration.secrets.password.get_secret_value(), ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding client initialization.","title":"Step 6: Confluence Client"},{"location":"how_to/how_to_add_new_datasource/#step-7-datasource-reader","text":"Create a Confluence reader in reader.py that implements the BaseReader interface: from extraction.datasources.core.reader import BaseReader ... class ConfluenceDatasourceReader(BaseReader): async def read_all_async( self, ) -> AsyncIterator[dict]: # read Confluence pages implementation This method returns an iterator, which improves runtime memory management. Next, implement a factory that defines how the ConfluenceDatasourceReader is initialized: from core import Factory ... class ConfluenceDatasourceReaderFactory(Factory): _configuration_class = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceReader: client = ConfluenceClientFactory.create(configuration) return ConfluenceDatasourceReader( configuration=configuration, client=client, ) Note that instead of initializing the Confluence client directly, the factory uses ConfluenceClientFactory to handle this task.","title":"Step 7: Datasource Reader"},{"location":"how_to/how_to_add_new_datasource/#step-8-datasource-parser","text":"In parser.py implement a parser responsible for converting the raw Confluence page to markdown format: from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) from extraction.datasources.confluence.document import ConfluenceDocument from extraction.datasources.core.parser import BaseParser class ConfluenceDatasourceParser(BaseParser[ConfluenceDocument]): def parse(self, page: str) -> ConfluenceDocument: # parse Confluence page implementation As before, define a factory for the parser: class ConfluenceDatasourceParserFactory(Factory): _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceParser: return ConfluenceDatasourceParser(configuration)","title":"Step 8: Datasource Parser"},{"location":"how_to/how_to_add_new_datasource/#step-9-datasource-manager","text":"To orchestrate all the previous components, we will reuse BasicDatasourceManager and implement a factory for it in manager.py : class ConfluenceDatasourceManagerFactory(Factory): \"\"\"Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class: Configuration class used for validating and processing Confluence-specific settings. \"\"\" _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> BasicDatasourceManager: \"\"\"Create a configured Confluence datasource manager. Sets up the necessary reader and parser components based on the provided configuration and assembles them into a functional manager. Args: configuration: Configuration object containing Confluence-specific parameters including authentication details, spaces to extract, and other extraction options. Returns: A fully initialized datasource manager that can extract and process data from Confluence. \"\"\" reader = ConfluenceDatasourceReaderFactory.create(configuration) parser = ConfluenceDatasourceParserFactory.create(configuration) return BasicDatasourceManager(configuration, reader, parser) Following the design pattern, ConfluenceDatasourceManagerFactory uses reader and parser factories to obtain the instances needed for the manager.","title":"Step 9: Datasource Manager"},{"location":"how_to/how_to_add_new_datasource/#step-10-datasource-integration","text":"Create an __init__.py file as follows: from extraction.bootstrap.configuration.datasources import ( DatasourceConfigurationRegistry, DatasourceName, ) from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) from extraction.datasources.confluence.manager import ( ConfluenceDatasourceManagerFactory, ) from extraction.datasources.registry import DatasourceManagerRegistry def register() -> None: DatasourceManagerRegistry.register( DatasourceName.CONFLUENCE, ConfluenceDatasourceManagerFactory ) DatasourceConfigurationRegistry.register( DatasourceName.CONFLUENCE, ConfluenceDatasourceConfiguration ) The initialization file includes a register() method responsible for registering our configuration and manager factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Confluence configuration in configurations/configuration.{environment}.json file: \"extraction\": { \"datasources\": [ { \"name\": \"confluence\", \"host\": \"wissen.feld-m.de\", \"protocol\": \"https\" } ] ... } ... We can dynamically retrieve the corresponding manager implementation by using the name specified in the configuration: datasource_config = read_datasource_from_config() datasource_manager = DatasourceManagerRegistry.get(datasource_config.name).create(datasource_config) This mechanism is later used by DatasourceOrchestrator to initialize datasources defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 extraction/ \u2514\u2500\u2500 datasources/ \u2514\u2500\u2500 confluence/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 client.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 document.py \u251c\u2500\u2500 manager.py \u251c\u2500\u2500 parser.py \u2514\u2500\u2500 reader.py","title":"Step 10: Datasource Integration"},{"location":"how_to/how_to_add_new_datasource/#notes","text":"Below is the __init__ method of BasicDatasourceManager used in our tutorial: class BasicDatasourceManager(BaseDatasourceManager, Generic[DocType]): def __init__( self, configuration: ExtractionConfiguration, reader: BaseReader, parser: BaseParser = BasicMarkdownParser(), cleaner: BaseCleaner = BasicMarkdownCleaner(), splitter: BaseSplitter = BasicMarkdownSplitter(), ): Note that in this guide we skipped the implementation of custom cleaner and splitter components, instead using the default ones. When building a new datasource integration, you might need to implement custom versions of these components based on your specific requirements.","title":"Notes"},{"location":"how_to/how_to_add_new_embedding_model/","text":"How to Add a New Embedding Model Implementation This guide demonstrates how to add support for a new embedding model implementation, using VoyageAI provider as an example. Architecture Embedding models are used to generate the user query and datasource embeddings. These embeddings are used for semantic search and retrieval in the RAG pipeline. Therefore, adding support for a new embedding model requires implementing the configuration and Llamaindex integration. Implementation Step 1: Dependencies Add the required packages to pyproject.toml under the following section: [project.optional-dependencies] embedding = [ \"llama-index-embeddings-voyageai>=0.3.5\", ... ] Step 2: Embedding Model Enum Embedding model configuration is scoped by provider. Each provider, such as Voyage , requires its own Pydantic configuration class. Begin by assigning a meaningful name to the new provider in the LLMProviderName enumeration in embedding_model_configuration.py : class EmbeddingModelProviderName(str, Enum): ... VOYAGE = \"voyage\" Step 3: Embedding Model Secrets And Configuration Create a new directory src/embedding/embedding_models/voyage and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal from pydantic import ConfigDict, Field, SecretStr from core.base_configuration import BaseSecrets from embedding.bootstrap.configuration.embedding_model_configuration import ( EmbeddingModelConfiguration, EmbeddingModelProviderName, ) class VoyageEmbeddingModelConfiguration(EmbeddingModelConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAGKB__EMBEDDING_MODELS__VOYAGE__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) api_key: SecretStr = Field(..., description=\"API key for the model\") provider: Literal[EmbeddingModelProviderName.VOYAGE] = Field( ..., description=\"The provider of the embedding model.\" ) secrets: Secrets = Field( None, description=\"The secrets for the language model.\" ) The first part is to create a configuration that extends EmbeddingModelConfiguration . provider field constraints the value to EmbeddingModelProviderName.VOYAGE , which serves as an indicator for pydantic validator. The Secrets inner class defines secret fields that will be present in the environment secret file under the RAGKB__EMBEDDING_MODELS__VOYAGE__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAGKB__EMBEDDING_MODELS__VOYAGE__API_KEY=<voyage_api_key> Note : If your embedding model doesn't require secrets, you can skip this step. Step 4: Embedding Model Implementation In the embedding_model.py file, create singleton embedding model factory. It provides a framework, where embedding model can be retrieved through VoyageEmbeddingModelFactory and is initialized only once per runtime, saving up the memory (e.g. in cases of small in-memory embedding models). To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Callable, Type from llama_index.embeddings.voyageai import VoyageEmbedding from transformers import AutoTokenizer from core import SingletonFactory from embedding.embedding_models.voyage.configuration import ( VoyageEmbeddingModelConfiguration, ) class VoyageEmbeddingModelFactory(SingletonFactory): _configuration_class: Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance( cls, configuration: VoyageEmbeddingModelConfiguration ) -> VoyageEmbedding: return VoyageEmbedding( voyage_api_key=configuration.secrets.api_key.get_secret_value(), model_name=configuration.name, embed_batch_size=configuration.batch_size, ) In the same file implement a factory that defines the tokenizer used along with this embedding model. For the same reasons as previously we use singleton factory. class VoyageEmbeddingModelTokenizerFactory(SingletonFactory): _configuration_class: Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance( cls, configuration: VoyageEmbeddingModelConfiguration ) -> Callable: return AutoTokenizer.from_pretrained( configuration.tokenizer_name ).tokenize Step 7: Embedding Model Integration Create an __init__.py file as follows: from embedding.bootstrap.configuration.embedding_model_configuration import ( EmbeddingModelConfigurationRegistry, EmbeddingModelProviderName, ) from embedding.embedding_models.registry import ( EmbeddingModelRegistry, EmbeddingModelTokenizerRegistry, ) from embedding.embedding_models.voyage.configuration import ( VoyageEmbeddingModelConfiguration, ) from embedding.embedding_models.voyage.embedding_model import ( VoyageEmbeddingModelFactory, VoyageEmbeddingModelTokenizerFactory, ) def register(): EmbeddingModelConfigurationRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelConfiguration ) EmbeddingModelRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelFactory ) EmbeddingModelTokenizerRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelTokenizerFactory ) The initialization file includes a register() method responsible for registering our configuration, embedding model and its tokenizer factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Voyage configuration in configurations/configuration.{environment}.json file: \"embedding\": { \"embedding_model\": { \"provider\": \"voyage\", \"name\": \"voyage-3\", // any model name compatible with VoyageAI API \"tokenizer_name\": \"voyageai/voyage-3\", // any tokenizer name compatible with VoyageAI and AutoTokenizer \"splitter\": { \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } ... } ... Note : You can use any model_name and tokenizer_name exposed by VoyageAI We can dynamically retrieve the corresponding embedding model implementation by using the name specified in the configuration: embedding_model_config = read_embedding_model_from_config() embedding_model = EmbeddingModelRegistry.get(embedding_model_config.name).create(embedding_model_config) This mechanism is later used by the embedding manager to initialize the embedding model defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 embedding/ \u2514\u2500\u2500 embedding_models/ \u2514\u2500\u2500 voyage/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 configuration.py \u2514\u2500\u2500 embedding_model.py","title":"Add a New Embedding Model"},{"location":"how_to/how_to_add_new_embedding_model/#how-to-add-a-new-embedding-model-implementation","text":"This guide demonstrates how to add support for a new embedding model implementation, using VoyageAI provider as an example.","title":"How to Add a New Embedding Model Implementation"},{"location":"how_to/how_to_add_new_embedding_model/#architecture","text":"Embedding models are used to generate the user query and datasource embeddings. These embeddings are used for semantic search and retrieval in the RAG pipeline. Therefore, adding support for a new embedding model requires implementing the configuration and Llamaindex integration.","title":"Architecture"},{"location":"how_to/how_to_add_new_embedding_model/#implementation","text":"","title":"Implementation"},{"location":"how_to/how_to_add_new_embedding_model/#step-1-dependencies","text":"Add the required packages to pyproject.toml under the following section: [project.optional-dependencies] embedding = [ \"llama-index-embeddings-voyageai>=0.3.5\", ... ]","title":"Step 1: Dependencies"},{"location":"how_to/how_to_add_new_embedding_model/#step-2-embedding-model-enum","text":"Embedding model configuration is scoped by provider. Each provider, such as Voyage , requires its own Pydantic configuration class. Begin by assigning a meaningful name to the new provider in the LLMProviderName enumeration in embedding_model_configuration.py : class EmbeddingModelProviderName(str, Enum): ... VOYAGE = \"voyage\"","title":"Step 2: Embedding Model Enum"},{"location":"how_to/how_to_add_new_embedding_model/#step-3-embedding-model-secrets-and-configuration","text":"Create a new directory src/embedding/embedding_models/voyage and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal from pydantic import ConfigDict, Field, SecretStr from core.base_configuration import BaseSecrets from embedding.bootstrap.configuration.embedding_model_configuration import ( EmbeddingModelConfiguration, EmbeddingModelProviderName, ) class VoyageEmbeddingModelConfiguration(EmbeddingModelConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAGKB__EMBEDDING_MODELS__VOYAGE__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) api_key: SecretStr = Field(..., description=\"API key for the model\") provider: Literal[EmbeddingModelProviderName.VOYAGE] = Field( ..., description=\"The provider of the embedding model.\" ) secrets: Secrets = Field( None, description=\"The secrets for the language model.\" ) The first part is to create a configuration that extends EmbeddingModelConfiguration . provider field constraints the value to EmbeddingModelProviderName.VOYAGE , which serves as an indicator for pydantic validator. The Secrets inner class defines secret fields that will be present in the environment secret file under the RAGKB__EMBEDDING_MODELS__VOYAGE__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAGKB__EMBEDDING_MODELS__VOYAGE__API_KEY=<voyage_api_key> Note : If your embedding model doesn't require secrets, you can skip this step.","title":"Step 3: Embedding Model Secrets And Configuration"},{"location":"how_to/how_to_add_new_embedding_model/#step-4-embedding-model-implementation","text":"In the embedding_model.py file, create singleton embedding model factory. It provides a framework, where embedding model can be retrieved through VoyageEmbeddingModelFactory and is initialized only once per runtime, saving up the memory (e.g. in cases of small in-memory embedding models). To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Callable, Type from llama_index.embeddings.voyageai import VoyageEmbedding from transformers import AutoTokenizer from core import SingletonFactory from embedding.embedding_models.voyage.configuration import ( VoyageEmbeddingModelConfiguration, ) class VoyageEmbeddingModelFactory(SingletonFactory): _configuration_class: Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance( cls, configuration: VoyageEmbeddingModelConfiguration ) -> VoyageEmbedding: return VoyageEmbedding( voyage_api_key=configuration.secrets.api_key.get_secret_value(), model_name=configuration.name, embed_batch_size=configuration.batch_size, ) In the same file implement a factory that defines the tokenizer used along with this embedding model. For the same reasons as previously we use singleton factory. class VoyageEmbeddingModelTokenizerFactory(SingletonFactory): _configuration_class: Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance( cls, configuration: VoyageEmbeddingModelConfiguration ) -> Callable: return AutoTokenizer.from_pretrained( configuration.tokenizer_name ).tokenize","title":"Step 4: Embedding Model Implementation"},{"location":"how_to/how_to_add_new_embedding_model/#step-7-embedding-model-integration","text":"Create an __init__.py file as follows: from embedding.bootstrap.configuration.embedding_model_configuration import ( EmbeddingModelConfigurationRegistry, EmbeddingModelProviderName, ) from embedding.embedding_models.registry import ( EmbeddingModelRegistry, EmbeddingModelTokenizerRegistry, ) from embedding.embedding_models.voyage.configuration import ( VoyageEmbeddingModelConfiguration, ) from embedding.embedding_models.voyage.embedding_model import ( VoyageEmbeddingModelFactory, VoyageEmbeddingModelTokenizerFactory, ) def register(): EmbeddingModelConfigurationRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelConfiguration ) EmbeddingModelRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelFactory ) EmbeddingModelTokenizerRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelTokenizerFactory ) The initialization file includes a register() method responsible for registering our configuration, embedding model and its tokenizer factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Voyage configuration in configurations/configuration.{environment}.json file: \"embedding\": { \"embedding_model\": { \"provider\": \"voyage\", \"name\": \"voyage-3\", // any model name compatible with VoyageAI API \"tokenizer_name\": \"voyageai/voyage-3\", // any tokenizer name compatible with VoyageAI and AutoTokenizer \"splitter\": { \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } ... } ... Note : You can use any model_name and tokenizer_name exposed by VoyageAI We can dynamically retrieve the corresponding embedding model implementation by using the name specified in the configuration: embedding_model_config = read_embedding_model_from_config() embedding_model = EmbeddingModelRegistry.get(embedding_model_config.name).create(embedding_model_config) This mechanism is later used by the embedding manager to initialize the embedding model defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 embedding/ \u2514\u2500\u2500 embedding_models/ \u2514\u2500\u2500 voyage/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 configuration.py \u2514\u2500\u2500 embedding_model.py","title":"Step 7: Embedding Model Integration"},{"location":"how_to/how_to_add_new_llm/","text":"How to Add a New LLM Implementation This guide demonstrates how to add support for a new Language Model (LLM) implementation, using OpenAI as an example. Architecture Large Language Models are mainly responsible for generating the answers based on the user query and retrieved nodes, injected as a context. They are also used in the evaluation process. Additionally, they can be used in various components e.g. AutoRetriever . Implementation Step 1: Dependencies Add the required packages to pyproject.toml : [project.optional-dependencies] augmentation = [ \"llama-index-llms-openai>=0.3.25\", ... ] Step 2: LLM Enum LLM configuration is scoped by provider. Each provider, such as OpenAI , requires its own Pydantic configuration class. Begin by assigning a meaningful name to the new provider in the LLMProviderName enumeration in llm_configuration.py : class LLMProviderName(str, Enum): ... OPENAI = \"openai\" Step 3: LLM Configuration And Secrets Create a new directory src/augmentation/components/llms/openai and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal from pydantic import ConfigDict, Field, SecretStr from augmentation.bootstrap.configuration.components.llm_configuration import ( LLMConfiguration, LLMProviderName, ) from core.base_configuration import BaseSecrets class OpenAILLMConfiguration(LLMConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAG__LLMS__OPENAI__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) api_key: SecretStr = Field( ..., description=\"API key for the model provider.\" ) provider: Literal[LLMProviderName.OPENAI] = Field( ..., description=\"The name of the language model provider.\" ) secrets: Secrets = Field( None, description=\"The secrets for the language model.\" ) The first part is to create a configuration that extends LLMConfiguration . provider field constraints the value to LLMProviderName.OPENAI , which serves as an indicator for pydantic validator. The Secrets inner class defines secret fields that will be present in the environment secret file under the RAG__LLMS__OPENAI__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAG__LLMS__OPENAI__API_KEY=<openai_api_key> Step 4: LLM Implementation In the llm.py file, create singleton LLM factory. It provides a framework, where LLM can be retrieved through OpenaAILLMFactory and is initialized only once per runtime, saving up the memory (e.g. in cases of small in-memory LLMs). To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Type from llama_index.llms.openai import OpenAI from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from core import SingletonFactory class OpenaAILLMFactory(SingletonFactory): _configuration_class: Type = OpenAILLMConfiguration @classmethod def _create_instance(cls, configuration: OpenAILLMConfiguration) -> OpenAI: return OpenAI( api_key=configuration.secrets.api_key.get_secret_value(), model=configuration.name, max_tokens=configuration.max_tokens, max_retries=configuration.max_retries, ) Step 5: LLM Output Extractor Human feedback feature between Chainlit and Langfuse require extraction of information about LLM response. Each provider returns the differently structured output dictionary. Therefore, we need to implement an extractor of required fields. Create output_extractor.py : from typing import Type from langfuse.api.resources.commons.types.trace_with_details import ( TraceWithDetails, ) from augmentation.components.llms.core.base_output_extractor import ( BaseLlamaindexLLMOutputExtractor, ) from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from core.base_factory import Factory class OpenAILlamaindexLLMOutputExtractor(BaseLlamaindexLLMOutputExtractor): def get_text(self, trace: TraceWithDetails) -> str: return trace.output[\"blocks\"][0][\"text\"] def get_generated_by_model(self, trace: TraceWithDetails) -> str: return self.configuration.name Implemented interface BaseLlamaindexLLMOutputExtractor , provide sufficient extractor for ChainlitFeedbackService purposes. Now just add correspodning factory: class OpenAILlamaindexLLMOutputExtractorFactory(Factory): _configuration_class: Type = OpenAILLMConfiguration @classmethod def _create_instance( cls, configuration: OpenAILLMConfiguration ) -> OpenAILlamaindexLLMOutputExtractor: return OpenAILlamaindexLLMOutputExtractor(configuration) Step 6: LLM Integration Create an __init__.py file as follows: from augmentation.bootstrap.configuration.components.llm_configuration import ( LLMConfigurationRegistry, LLMProviderName, ) from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from augmentation.components.llms.openai.llm import OpenaAILLMFactory from augmentation.components.llms.openai.output_extractor import ( OpenAILlamaindexLLMOutputExtractorFactory, ) from augmentation.components.llms.registry import ( LlamaindexLLMOutputExtractorRegistry, LLMRegistry, ) def register() -> None: LLMRegistry.register(LLMProviderName.OPENAI, OpenaAILLMFactory) LLMConfigurationRegistry.register( LLMProviderName.OPENAI, OpenAILLMConfiguration ) LlamaindexLLMOutputExtractorRegistry.register( LLMProviderName.OPENAI, OpenAILlamaindexLLMOutputExtractorFactory ) The initialization file includes a register() method responsible for registering our configuration, output extractor and LLM factories. Registries are used to dynamically inform the system about available implementations. This way, with the following OpenAI configuration in configurations/configuration.{environment}.json file: \"augmentation\": { \"chat_engine\": { \"llm\": { \"provider\": \"openai\", \"name\": \"gpt-4o\", // any model name compatible with OpenAI API } } ... } Note : You can use any name exposed by OpenAI We can dynamically retrieve the corresponding LLM implementation by using the name specified in the configuration: llm_config = read_llm_from_config() llm_model = LLMRegistry.get(llm_config.name).create(llm_config) This mechanism is later used by the chat engine to initialize the llm defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 augmentation/ \u2514\u2500\u2500 components/ \u2514\u2500\u2500 llms/ \u2514\u2500\u2500 openai/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 llm.py \u2514\u2500\u2500 output_extractor.py","title":"Add a New LLM"},{"location":"how_to/how_to_add_new_llm/#how-to-add-a-new-llm-implementation","text":"This guide demonstrates how to add support for a new Language Model (LLM) implementation, using OpenAI as an example.","title":"How to Add a New LLM Implementation"},{"location":"how_to/how_to_add_new_llm/#architecture","text":"Large Language Models are mainly responsible for generating the answers based on the user query and retrieved nodes, injected as a context. They are also used in the evaluation process. Additionally, they can be used in various components e.g. AutoRetriever .","title":"Architecture"},{"location":"how_to/how_to_add_new_llm/#implementation","text":"","title":"Implementation"},{"location":"how_to/how_to_add_new_llm/#step-1-dependencies","text":"Add the required packages to pyproject.toml : [project.optional-dependencies] augmentation = [ \"llama-index-llms-openai>=0.3.25\", ... ]","title":"Step 1: Dependencies"},{"location":"how_to/how_to_add_new_llm/#step-2-llm-enum","text":"LLM configuration is scoped by provider. Each provider, such as OpenAI , requires its own Pydantic configuration class. Begin by assigning a meaningful name to the new provider in the LLMProviderName enumeration in llm_configuration.py : class LLMProviderName(str, Enum): ... OPENAI = \"openai\"","title":"Step 2: LLM Enum"},{"location":"how_to/how_to_add_new_llm/#step-3-llm-configuration-and-secrets","text":"Create a new directory src/augmentation/components/llms/openai and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal from pydantic import ConfigDict, Field, SecretStr from augmentation.bootstrap.configuration.components.llm_configuration import ( LLMConfiguration, LLMProviderName, ) from core.base_configuration import BaseSecrets class OpenAILLMConfiguration(LLMConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAG__LLMS__OPENAI__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) api_key: SecretStr = Field( ..., description=\"API key for the model provider.\" ) provider: Literal[LLMProviderName.OPENAI] = Field( ..., description=\"The name of the language model provider.\" ) secrets: Secrets = Field( None, description=\"The secrets for the language model.\" ) The first part is to create a configuration that extends LLMConfiguration . provider field constraints the value to LLMProviderName.OPENAI , which serves as an indicator for pydantic validator. The Secrets inner class defines secret fields that will be present in the environment secret file under the RAG__LLMS__OPENAI__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAG__LLMS__OPENAI__API_KEY=<openai_api_key>","title":"Step 3: LLM Configuration And Secrets"},{"location":"how_to/how_to_add_new_llm/#step-4-llm-implementation","text":"In the llm.py file, create singleton LLM factory. It provides a framework, where LLM can be retrieved through OpenaAILLMFactory and is initialized only once per runtime, saving up the memory (e.g. in cases of small in-memory LLMs). To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Type from llama_index.llms.openai import OpenAI from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from core import SingletonFactory class OpenaAILLMFactory(SingletonFactory): _configuration_class: Type = OpenAILLMConfiguration @classmethod def _create_instance(cls, configuration: OpenAILLMConfiguration) -> OpenAI: return OpenAI( api_key=configuration.secrets.api_key.get_secret_value(), model=configuration.name, max_tokens=configuration.max_tokens, max_retries=configuration.max_retries, )","title":"Step 4: LLM Implementation"},{"location":"how_to/how_to_add_new_llm/#step-5-llm-output-extractor","text":"Human feedback feature between Chainlit and Langfuse require extraction of information about LLM response. Each provider returns the differently structured output dictionary. Therefore, we need to implement an extractor of required fields. Create output_extractor.py : from typing import Type from langfuse.api.resources.commons.types.trace_with_details import ( TraceWithDetails, ) from augmentation.components.llms.core.base_output_extractor import ( BaseLlamaindexLLMOutputExtractor, ) from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from core.base_factory import Factory class OpenAILlamaindexLLMOutputExtractor(BaseLlamaindexLLMOutputExtractor): def get_text(self, trace: TraceWithDetails) -> str: return trace.output[\"blocks\"][0][\"text\"] def get_generated_by_model(self, trace: TraceWithDetails) -> str: return self.configuration.name Implemented interface BaseLlamaindexLLMOutputExtractor , provide sufficient extractor for ChainlitFeedbackService purposes. Now just add correspodning factory: class OpenAILlamaindexLLMOutputExtractorFactory(Factory): _configuration_class: Type = OpenAILLMConfiguration @classmethod def _create_instance( cls, configuration: OpenAILLMConfiguration ) -> OpenAILlamaindexLLMOutputExtractor: return OpenAILlamaindexLLMOutputExtractor(configuration)","title":"Step 5: LLM Output Extractor"},{"location":"how_to/how_to_add_new_llm/#step-6-llm-integration","text":"Create an __init__.py file as follows: from augmentation.bootstrap.configuration.components.llm_configuration import ( LLMConfigurationRegistry, LLMProviderName, ) from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from augmentation.components.llms.openai.llm import OpenaAILLMFactory from augmentation.components.llms.openai.output_extractor import ( OpenAILlamaindexLLMOutputExtractorFactory, ) from augmentation.components.llms.registry import ( LlamaindexLLMOutputExtractorRegistry, LLMRegistry, ) def register() -> None: LLMRegistry.register(LLMProviderName.OPENAI, OpenaAILLMFactory) LLMConfigurationRegistry.register( LLMProviderName.OPENAI, OpenAILLMConfiguration ) LlamaindexLLMOutputExtractorRegistry.register( LLMProviderName.OPENAI, OpenAILlamaindexLLMOutputExtractorFactory ) The initialization file includes a register() method responsible for registering our configuration, output extractor and LLM factories. Registries are used to dynamically inform the system about available implementations. This way, with the following OpenAI configuration in configurations/configuration.{environment}.json file: \"augmentation\": { \"chat_engine\": { \"llm\": { \"provider\": \"openai\", \"name\": \"gpt-4o\", // any model name compatible with OpenAI API } } ... } Note : You can use any name exposed by OpenAI We can dynamically retrieve the corresponding LLM implementation by using the name specified in the configuration: llm_config = read_llm_from_config() llm_model = LLMRegistry.get(llm_config.name).create(llm_config) This mechanism is later used by the chat engine to initialize the llm defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 augmentation/ \u2514\u2500\u2500 components/ \u2514\u2500\u2500 llms/ \u2514\u2500\u2500 openai/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 llm.py \u2514\u2500\u2500 output_extractor.py","title":"Step 6: LLM Integration"},{"location":"how_to/how_to_add_new_vector_store/","text":"How to Add a New Vector Store Implementation This guide demonstrates how to add support for a new vector store implementation, using Chroma as an example. Architecture Vector store is used for storing and retrieving embeddings of datasource nodes. Implementation Step 1: Add Dependencies Add the required packages to pyproject.toml : [project.optional-dependencies] embedding = [ \"chromadb>=0.6.3\", \"llama-index-vector-stores-chroma>=0.4.1\", ... ] Step 2: Docker Service Add the vector store service to docker-compose.yml : name: rag services: ... chroma: image: chromadb/chroma:0.6.4.dev19 environment: CHROMA_HOST_PORT: ${RAG__VECTOR_STORE__PORT_REST} ports: - \"${RAG__VECTOR_STORE__PORT_REST}:${RAG__VECTOR_STORE__PORT_REST}\" restart: unless-stopped volumes: - ./.docker-data/chroma:/chroma/chroma/ ... It enables easy vector store initialization using init.sh script. Step 3: Vector Store Enum Add the vector store to the VectorStoreName enum in vector_store_configuration.py : class VectorStoreName(str, Enum): ... CHROMA = \"chroma\" The enum value must match the service name in the Docker configuration. Step 4: Vector Store Configuration Create a new directory src/embedding/vector_stores/chroma and create a configuration.py file in it. This configuration file will contain necessary fields for setup. from typing import Literal from pydantic import Field from embedding.bootstrap.configuration.vector_store_configuration import ( VectorStoreConfiguration, VectorStoreName, ) class ChromaVectorStoreConfiguration(VectorStoreConfiguration): \"\"\"Configuration for the ChromaDB vector store.\"\"\" name: Literal[VectorStoreName.CHROMA] = Field( ..., description=\"The name of the vector store.\" ) The first part is to create a configuration that extends VectorStoreConfiguration . name field constraints the value to VectorStoreName.CHROMA , which serves as an indicator for pydantic validator. Note : For adding potentially needed secrets support follow the same approach as explained in How to Add a New LLM Implementation guide. Step 5: Vector Store Implementation In the vector_store.py file, create singleton vector store factory. It provides a framework, where vector store can be retrieved through ChromaVectorStoreFactory and is initialized only once per runtime, saving up the memory. To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Type from llama_index.vector_stores.chroma import ChromaVectorStore from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) class ChromaVectorStoreFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaVectorStore: return ChromaVectorStore( host=configuration.host, port=str(configuration.port), collection_name=configuration.collection_name, ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding vector store initialization. Step 6: Vector Store Client We will want to validate our vector store before the run, for that we need and HTTP client. To create a Chroma client, we implement ChromaVectorStoreClientFactory in client.py . It extends SingletonFactory , which provides an interface for initializing a single instance for the duration of the application runtime. from typing import Type from chromadb import HttpClient as ChromaHttpClient from chromadb.api import ClientAPI as ChromaClient from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) class ChromaVectorStoreClientFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaClient: return ChromaHttpClient( host=configuration.host, port=configuration.port, ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding client initialization. Step 7: Vector Store Validator Now we can implement the validator that will check if defined vector store collection already exists. Nevertheless, it can be extended to validate other apsects as well. Create validator.py file and create ChromaVectorStoreValidator that implements BaseVectorStoreValidator interface: from typing import Type from chromadb.api import ClientAPI as ChromaClient from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.client import ChromaVectorStoreClientFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) from embedding.vector_stores.core.exceptions import CollectionExistsException from embedding.vector_stores.core.validator import BaseVectorStoreValidator class ChromaVectorStoreValidator(BaseVectorStoreValidator): def __init__( self, configuration: ChromaVectorStoreConfiguration, client: ChromaClient, ): self.configuration = configuration self.client = client def validate(self) -> None: self.validate_collection() def validate_collection(self) -> None: collection_name = self.configuration.collection_name if collection_name in self.client.list_collections(): raise CollectionExistsException(collection_name) Now add the factory that defines validator initialization. class ChromaVectorStoreValidatorFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaVectorStoreValidator: client = ChromaVectorStoreClientFactory.create(configuration) return ChromaVectorStoreValidator( configuration=configuration, client=client ) You can notice that we use previously implemented ChromaVectorStoreClientFactory to get required client instance. Step 8: Vector Store Integration Create an __init__.py file as follows: from embedding.bootstrap.configuration.vector_store_configuration import ( VectorStoreConfigurationRegistry, VectorStoreName, ) from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) from embedding.vector_stores.chroma.validator import ( ChromaVectorStoreValidatorFactory, ) from embedding.vector_stores.chroma.vector_store import ChromaVectorStoreFactory from embedding.vector_stores.registry import ( VectorStoreRegistry, VectorStoreValidatorRegistry, ) def register() -> None: VectorStoreConfigurationRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreConfiguration, ) VectorStoreRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreFactory ) VectorStoreValidatorRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreValidatorFactory ) The initialization file includes a register() method responsible for registering our configuration, and validator and vector store factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Chroma configuration in configurations/configuration.{environment}.json file: \"embedding\": { \"vector_store\": { \"name\": \"chroma\", \"collection_name\": \"new-collection\", \"host\": \"chroma\", \"protocol\": \"http\", \"port\": 6000 } ... }, We can dynamically retrieve the corresponding vector store implementation by using the name specified in the configuration: vector_store_config = read_vector_store_from_config() vector_store = VectorStoreRegistry.get(vector_store_config.name).create(vector_store_config) vector_store_validator = VectorStoreValidatorRegistry.get(vector_store_config.name).create(vector_store_config) This mechanism is later used by the embedding orchestrator to initialize the vector store defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 embedding/ \u2514\u2500\u2500 vector_stores/ \u2514\u2500\u2500 chroma/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 client.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 validator.py \u2514\u2500\u2500 vector_store.py","title":"Add a New Vector Store"},{"location":"how_to/how_to_add_new_vector_store/#how-to-add-a-new-vector-store-implementation","text":"This guide demonstrates how to add support for a new vector store implementation, using Chroma as an example.","title":"How to Add a New Vector Store Implementation"},{"location":"how_to/how_to_add_new_vector_store/#architecture","text":"Vector store is used for storing and retrieving embeddings of datasource nodes.","title":"Architecture"},{"location":"how_to/how_to_add_new_vector_store/#implementation","text":"","title":"Implementation"},{"location":"how_to/how_to_add_new_vector_store/#step-1-add-dependencies","text":"Add the required packages to pyproject.toml : [project.optional-dependencies] embedding = [ \"chromadb>=0.6.3\", \"llama-index-vector-stores-chroma>=0.4.1\", ... ]","title":"Step 1: Add Dependencies"},{"location":"how_to/how_to_add_new_vector_store/#step-2-docker-service","text":"Add the vector store service to docker-compose.yml : name: rag services: ... chroma: image: chromadb/chroma:0.6.4.dev19 environment: CHROMA_HOST_PORT: ${RAG__VECTOR_STORE__PORT_REST} ports: - \"${RAG__VECTOR_STORE__PORT_REST}:${RAG__VECTOR_STORE__PORT_REST}\" restart: unless-stopped volumes: - ./.docker-data/chroma:/chroma/chroma/ ... It enables easy vector store initialization using init.sh script.","title":"Step 2: Docker Service"},{"location":"how_to/how_to_add_new_vector_store/#step-3-vector-store-enum","text":"Add the vector store to the VectorStoreName enum in vector_store_configuration.py : class VectorStoreName(str, Enum): ... CHROMA = \"chroma\" The enum value must match the service name in the Docker configuration.","title":"Step 3: Vector Store Enum"},{"location":"how_to/how_to_add_new_vector_store/#step-4-vector-store-configuration","text":"Create a new directory src/embedding/vector_stores/chroma and create a configuration.py file in it. This configuration file will contain necessary fields for setup. from typing import Literal from pydantic import Field from embedding.bootstrap.configuration.vector_store_configuration import ( VectorStoreConfiguration, VectorStoreName, ) class ChromaVectorStoreConfiguration(VectorStoreConfiguration): \"\"\"Configuration for the ChromaDB vector store.\"\"\" name: Literal[VectorStoreName.CHROMA] = Field( ..., description=\"The name of the vector store.\" ) The first part is to create a configuration that extends VectorStoreConfiguration . name field constraints the value to VectorStoreName.CHROMA , which serves as an indicator for pydantic validator. Note : For adding potentially needed secrets support follow the same approach as explained in How to Add a New LLM Implementation guide.","title":"Step 4: Vector Store Configuration"},{"location":"how_to/how_to_add_new_vector_store/#step-5-vector-store-implementation","text":"In the vector_store.py file, create singleton vector store factory. It provides a framework, where vector store can be retrieved through ChromaVectorStoreFactory and is initialized only once per runtime, saving up the memory. To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Type from llama_index.vector_stores.chroma import ChromaVectorStore from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) class ChromaVectorStoreFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaVectorStore: return ChromaVectorStore( host=configuration.host, port=str(configuration.port), collection_name=configuration.collection_name, ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding vector store initialization.","title":"Step 5: Vector Store Implementation"},{"location":"how_to/how_to_add_new_vector_store/#step-6-vector-store-client","text":"We will want to validate our vector store before the run, for that we need and HTTP client. To create a Chroma client, we implement ChromaVectorStoreClientFactory in client.py . It extends SingletonFactory , which provides an interface for initializing a single instance for the duration of the application runtime. from typing import Type from chromadb import HttpClient as ChromaHttpClient from chromadb.api import ClientAPI as ChromaClient from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) class ChromaVectorStoreClientFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaClient: return ChromaHttpClient( host=configuration.host, port=configuration.port, ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding client initialization.","title":"Step 6: Vector Store Client"},{"location":"how_to/how_to_add_new_vector_store/#step-7-vector-store-validator","text":"Now we can implement the validator that will check if defined vector store collection already exists. Nevertheless, it can be extended to validate other apsects as well. Create validator.py file and create ChromaVectorStoreValidator that implements BaseVectorStoreValidator interface: from typing import Type from chromadb.api import ClientAPI as ChromaClient from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.client import ChromaVectorStoreClientFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) from embedding.vector_stores.core.exceptions import CollectionExistsException from embedding.vector_stores.core.validator import BaseVectorStoreValidator class ChromaVectorStoreValidator(BaseVectorStoreValidator): def __init__( self, configuration: ChromaVectorStoreConfiguration, client: ChromaClient, ): self.configuration = configuration self.client = client def validate(self) -> None: self.validate_collection() def validate_collection(self) -> None: collection_name = self.configuration.collection_name if collection_name in self.client.list_collections(): raise CollectionExistsException(collection_name) Now add the factory that defines validator initialization. class ChromaVectorStoreValidatorFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaVectorStoreValidator: client = ChromaVectorStoreClientFactory.create(configuration) return ChromaVectorStoreValidator( configuration=configuration, client=client ) You can notice that we use previously implemented ChromaVectorStoreClientFactory to get required client instance.","title":"Step 7: Vector Store Validator"},{"location":"how_to/how_to_add_new_vector_store/#step-8-vector-store-integration","text":"Create an __init__.py file as follows: from embedding.bootstrap.configuration.vector_store_configuration import ( VectorStoreConfigurationRegistry, VectorStoreName, ) from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) from embedding.vector_stores.chroma.validator import ( ChromaVectorStoreValidatorFactory, ) from embedding.vector_stores.chroma.vector_store import ChromaVectorStoreFactory from embedding.vector_stores.registry import ( VectorStoreRegistry, VectorStoreValidatorRegistry, ) def register() -> None: VectorStoreConfigurationRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreConfiguration, ) VectorStoreRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreFactory ) VectorStoreValidatorRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreValidatorFactory ) The initialization file includes a register() method responsible for registering our configuration, and validator and vector store factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Chroma configuration in configurations/configuration.{environment}.json file: \"embedding\": { \"vector_store\": { \"name\": \"chroma\", \"collection_name\": \"new-collection\", \"host\": \"chroma\", \"protocol\": \"http\", \"port\": 6000 } ... }, We can dynamically retrieve the corresponding vector store implementation by using the name specified in the configuration: vector_store_config = read_vector_store_from_config() vector_store = VectorStoreRegistry.get(vector_store_config.name).create(vector_store_config) vector_store_validator = VectorStoreValidatorRegistry.get(vector_store_config.name).create(vector_store_config) This mechanism is later used by the embedding orchestrator to initialize the vector store defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 embedding/ \u2514\u2500\u2500 vector_stores/ \u2514\u2500\u2500 chroma/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 client.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 validator.py \u2514\u2500\u2500 vector_store.py","title":"Step 8: Vector Store Integration"},{"location":"how_to/how_to_configure/","text":"How to Configure the RAG System This guide explains how to customize the RAG system pipeline through configuration files. Environments Definition The following environments are supported: class EnvironmentName(str, Enum): DEFAULT = \"default\" LOCAL = \"local\" DEV = \"dev\" TEST = \"test\" PROD = \"prod\" Each environment requires corresponding configuration and secrets files in the configurations directory: Configuration files: configuration.{environment}.json Secrets files: secrets.{environment}.env The configuration files define the pipeline setup, while secrets files store credentials and tokens. For security, all files in the configurations directory are git-ignored except for configuration.default.json and configuration.local.json . Usage Run the pipeline with a specific configuration using the --env flag: build/workstation/init.sh --env default python src/embed.py --env default Datasource Configuration Currently, the following datasources are available: class DatasourceName(str, Enum): NOTION = \"notion\" CONFLUENCE = \"confluence\" PDF = \"pdf\" Blueprint allows the usage of single or multiple datasources. Adjust the corresponding configuration accordingly: { \"extraction\": { \"datasources\": [ { \"name\": \"notion\", \"export_limit\": 100 }, { \"name\": \"pdf\", \"export_limit\": 100, \"base_path\": \"data/\" } ] } } Each entry in datasources corresponds to a single source that will be sequentially used for the extraction of documents to be further processed. The name of each entry must correspond to one of the implemented enums. Datasources' secrets must be added to the environment's secret file. To check configurable options for specific datasources, visit configuration.py of a datasource. LLM Configuration The system supports the following LLM providers: class LLMProviderName(str, Enum): LITE_LLM = \"lite_llm\" LITE_LLM leverages the LiteLLM service, providing a unified interface for cloud-hosted models (e.g., OpenAI, Google, Anthropic) and self-hosted LLMs. Minimal setup requires the use of LLMs in augmentation and evaluation processes. To configure this, adjust the following JSON entries: { \"pipeline\": { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", }, }, \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } }, \"evaluation\": { \"judge_llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } } } The provider field must be one of the values from LLMProviderName , and the name field indicates the specific model exposed by the provider. To check configurable options for specific providers, visit configuration.py of an LLM. In the above case, augmentation and evaluation processes use the same LLM, which might be suboptimal. To change it, simply adjust the entry of one of these: { \"pipeline\": { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", }, }, \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } }, \"evaluation\": { \"judge_llm\": { \"provider\": \"lite_llm\", \"name\": \"gpt-4o-mini\", // another llm \"max_tokens\": 512 // different parameters } } } } Note : You can also use different LLM for any other component like guardrails . Secrets Each model requires api key. For the above LLMs add following secrets: RAG__LLMS__GEMINI_2_0_FLASH_EXP_API_KEY={your-gemini-api-key} RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} The variable name includes the uppercased name of the model you are using, whereas all non-alphanumeric characters are replaced by underscores e.g. gpt-3.5-turbo -> GPT_3_5_TURBO . If you want to use your local model called for instance my-llm , which is exposed via openai-like API you can use the following configuration: \"provider\": \"lite_llm\", \"name\": \"openai/my-llm\", \"api_base\": \"http://127.0.0.1/v1 And the secrets will look as follows: RAG__LLMS__OPENAI_MY_LLM__API_KEY={your-api-key} Note You can use different API structure for your local LLMs, then just replace openai/ prefix with corresponding provider. Embedding Model Configuration Currently, embedding models from these providers are supported: class EmbeddingModelProviderName(str, Enum): HUGGING_FACE = \"hugging_face\" OPENAI = \"openai\" VOYAGE = \"voyage\" Any model exposed by these providers can be used in the setup. Minimal setup requires the use of embedding models in different processes. To configure this, adjust the following JSON entries: { \"embedding\": { \"embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\", \"splitter\": { \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } }, \"evaluation\": { \"judge_embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\" } } } Providers' secrets must be added to the environment's secret file. The provider field must be one of the values from EmbeddingModelProviderName , and the name field indicates the specific model exposed by the provider. The tokenizer_name field indicates the tokenizer used in pair with the embedding model, and it should be compatible with the specified embedding model. The splitter defines how the documents should be chunked in the embedding process and is required for embedding configuration. To check configurable options for specific providers, visit configuration.py of a embedding model. Note : The same embedding model is used for embedding and retrieval processes, therefore it is defined in the embedding configuration only. In the above case, embedding/retrieval and evaluation processes use the same embedding model, which might be suboptimal. To change it, simply adjust the entry of one of these: { \"embedding\": { \"embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\", \"splitting\": { \"name\": \"basic\", \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } }, \"evaluation\": { \"judge_embedding_model\": { \"provider\": \"openai\", // different provider \"name\": \"text-embedding-3-small\", // different embedding model \"tokenizer_name\": \"text-embedding-3-small\", // different tokenizer \"batch_size\": 64 // different parameters } } } Vector Store Configuration Currently, the following vector stores are supported: class VectorStoreName(str, Enum): QDRANT = \"qdrant\" CHROMA = \"chroma\" PGVECTOR = \"pgvector\" To configure the vector store, update the following entry: { \"embedding\": { \"vector_store\": { \"name\": \"qdrant\", \"collection_name\": \"collection-default\", \"host\": \"qdrant\", \"protocol\": \"http\", \"port\": 6333 } } } The name field indicates one of the vector stores from VectorStoreName , and the collection_name defines the vector store collection for embedded documents. The next fields define the connection to the vector store. Corresponding secrets must be added to the environment's secrets file. To check configurable options for specific datasources, visit configuration.py of a vector store. Note : If collection_name already exists in the vector store, the embedding process will be skipped. To run it, delete the collection or use a different name. Langfuse and Chainlit Configuration Configuration contains the entries related to Langfuse and Chainlit: { \"augmentation\": { \"langfuse\": { \"host\": \"langfuse\", \"protocol\": \"http\", \"port\": 3000, \"database\": { \"host\": \"langfuse-db\", \"port\": 5432, \"db\": \"langfuse\" } }, \"chainlit\": { \"port\": 8000 } } } Field chailit.port defines on which port chat UI should be run. Fields in langfuse define connection details to Langfuse server and langfuse.database details of its database. Corresponding secrets for Langfuse have to be added to environment's secrets file. Prompt Templates Configuration For prompts management system uses Langfuse Prompt service. By default four prompt templates are created in Langfuse Prompts service - default_system_prompt , default_context_refine_prompt , default_context_prompt , default_condense_prompt , default_input_guardrail_prompt and default_output_guardrail_prompt . To find out more about these templates visit Llamaindex guide . Prompts are used during the augmentation process, which affects the final answers of the system. They can be adjusted via Langfuse Prompts UI. If you want to provide and use templates under different names, you need to add them to Langfuse Prompts and change the configuration as follows: { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"input_prompt_name\": \"default_input_guardrail_prompt\", \"output_prompt_name\": \"default_output_guardrail_prompt\", }, \"prompt_templates\": { \"condense_prompt_name\": \"new_condense_prompt\", \"context_prompt_name\": \"new_context_prompt\", \"context_refine_prompt_name\": \"new_context_refine_prompt\", \"system_prompt_name\": \"new_system_prompt\" } } } } Upcoming Docs Docs about configurable retrievers, postprocessors and others are in progress..","title":"Configure RAG System"},{"location":"how_to/how_to_configure/#how-to-configure-the-rag-system","text":"This guide explains how to customize the RAG system pipeline through configuration files.","title":"How to Configure the RAG System"},{"location":"how_to/how_to_configure/#environments","text":"","title":"Environments"},{"location":"how_to/how_to_configure/#definition","text":"The following environments are supported: class EnvironmentName(str, Enum): DEFAULT = \"default\" LOCAL = \"local\" DEV = \"dev\" TEST = \"test\" PROD = \"prod\" Each environment requires corresponding configuration and secrets files in the configurations directory: Configuration files: configuration.{environment}.json Secrets files: secrets.{environment}.env The configuration files define the pipeline setup, while secrets files store credentials and tokens. For security, all files in the configurations directory are git-ignored except for configuration.default.json and configuration.local.json .","title":"Definition"},{"location":"how_to/how_to_configure/#usage","text":"Run the pipeline with a specific configuration using the --env flag: build/workstation/init.sh --env default python src/embed.py --env default","title":"Usage"},{"location":"how_to/how_to_configure/#datasource-configuration","text":"Currently, the following datasources are available: class DatasourceName(str, Enum): NOTION = \"notion\" CONFLUENCE = \"confluence\" PDF = \"pdf\" Blueprint allows the usage of single or multiple datasources. Adjust the corresponding configuration accordingly: { \"extraction\": { \"datasources\": [ { \"name\": \"notion\", \"export_limit\": 100 }, { \"name\": \"pdf\", \"export_limit\": 100, \"base_path\": \"data/\" } ] } } Each entry in datasources corresponds to a single source that will be sequentially used for the extraction of documents to be further processed. The name of each entry must correspond to one of the implemented enums. Datasources' secrets must be added to the environment's secret file. To check configurable options for specific datasources, visit configuration.py of a datasource.","title":"Datasource Configuration"},{"location":"how_to/how_to_configure/#llm-configuration","text":"The system supports the following LLM providers: class LLMProviderName(str, Enum): LITE_LLM = \"lite_llm\" LITE_LLM leverages the LiteLLM service, providing a unified interface for cloud-hosted models (e.g., OpenAI, Google, Anthropic) and self-hosted LLMs. Minimal setup requires the use of LLMs in augmentation and evaluation processes. To configure this, adjust the following JSON entries: { \"pipeline\": { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", }, }, \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } }, \"evaluation\": { \"judge_llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } } } The provider field must be one of the values from LLMProviderName , and the name field indicates the specific model exposed by the provider. To check configurable options for specific providers, visit configuration.py of an LLM. In the above case, augmentation and evaluation processes use the same LLM, which might be suboptimal. To change it, simply adjust the entry of one of these: { \"pipeline\": { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", }, }, \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } }, \"evaluation\": { \"judge_llm\": { \"provider\": \"lite_llm\", \"name\": \"gpt-4o-mini\", // another llm \"max_tokens\": 512 // different parameters } } } } Note : You can also use different LLM for any other component like guardrails .","title":"LLM Configuration"},{"location":"how_to/how_to_configure/#secrets","text":"Each model requires api key. For the above LLMs add following secrets: RAG__LLMS__GEMINI_2_0_FLASH_EXP_API_KEY={your-gemini-api-key} RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} The variable name includes the uppercased name of the model you are using, whereas all non-alphanumeric characters are replaced by underscores e.g. gpt-3.5-turbo -> GPT_3_5_TURBO . If you want to use your local model called for instance my-llm , which is exposed via openai-like API you can use the following configuration: \"provider\": \"lite_llm\", \"name\": \"openai/my-llm\", \"api_base\": \"http://127.0.0.1/v1 And the secrets will look as follows: RAG__LLMS__OPENAI_MY_LLM__API_KEY={your-api-key} Note You can use different API structure for your local LLMs, then just replace openai/ prefix with corresponding provider.","title":"Secrets"},{"location":"how_to/how_to_configure/#embedding-model-configuration","text":"Currently, embedding models from these providers are supported: class EmbeddingModelProviderName(str, Enum): HUGGING_FACE = \"hugging_face\" OPENAI = \"openai\" VOYAGE = \"voyage\" Any model exposed by these providers can be used in the setup. Minimal setup requires the use of embedding models in different processes. To configure this, adjust the following JSON entries: { \"embedding\": { \"embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\", \"splitter\": { \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } }, \"evaluation\": { \"judge_embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\" } } } Providers' secrets must be added to the environment's secret file. The provider field must be one of the values from EmbeddingModelProviderName , and the name field indicates the specific model exposed by the provider. The tokenizer_name field indicates the tokenizer used in pair with the embedding model, and it should be compatible with the specified embedding model. The splitter defines how the documents should be chunked in the embedding process and is required for embedding configuration. To check configurable options for specific providers, visit configuration.py of a embedding model. Note : The same embedding model is used for embedding and retrieval processes, therefore it is defined in the embedding configuration only. In the above case, embedding/retrieval and evaluation processes use the same embedding model, which might be suboptimal. To change it, simply adjust the entry of one of these: { \"embedding\": { \"embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\", \"splitting\": { \"name\": \"basic\", \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } }, \"evaluation\": { \"judge_embedding_model\": { \"provider\": \"openai\", // different provider \"name\": \"text-embedding-3-small\", // different embedding model \"tokenizer_name\": \"text-embedding-3-small\", // different tokenizer \"batch_size\": 64 // different parameters } } }","title":"Embedding Model Configuration"},{"location":"how_to/how_to_configure/#vector-store-configuration","text":"Currently, the following vector stores are supported: class VectorStoreName(str, Enum): QDRANT = \"qdrant\" CHROMA = \"chroma\" PGVECTOR = \"pgvector\" To configure the vector store, update the following entry: { \"embedding\": { \"vector_store\": { \"name\": \"qdrant\", \"collection_name\": \"collection-default\", \"host\": \"qdrant\", \"protocol\": \"http\", \"port\": 6333 } } } The name field indicates one of the vector stores from VectorStoreName , and the collection_name defines the vector store collection for embedded documents. The next fields define the connection to the vector store. Corresponding secrets must be added to the environment's secrets file. To check configurable options for specific datasources, visit configuration.py of a vector store. Note : If collection_name already exists in the vector store, the embedding process will be skipped. To run it, delete the collection or use a different name.","title":"Vector Store Configuration"},{"location":"how_to/how_to_configure/#langfuse-and-chainlit-configuration","text":"Configuration contains the entries related to Langfuse and Chainlit: { \"augmentation\": { \"langfuse\": { \"host\": \"langfuse\", \"protocol\": \"http\", \"port\": 3000, \"database\": { \"host\": \"langfuse-db\", \"port\": 5432, \"db\": \"langfuse\" } }, \"chainlit\": { \"port\": 8000 } } } Field chailit.port defines on which port chat UI should be run. Fields in langfuse define connection details to Langfuse server and langfuse.database details of its database. Corresponding secrets for Langfuse have to be added to environment's secrets file.","title":"Langfuse and Chainlit Configuration"},{"location":"how_to/how_to_configure/#prompt-templates-configuration","text":"For prompts management system uses Langfuse Prompt service. By default four prompt templates are created in Langfuse Prompts service - default_system_prompt , default_context_refine_prompt , default_context_prompt , default_condense_prompt , default_input_guardrail_prompt and default_output_guardrail_prompt . To find out more about these templates visit Llamaindex guide . Prompts are used during the augmentation process, which affects the final answers of the system. They can be adjusted via Langfuse Prompts UI. If you want to provide and use templates under different names, you need to add them to Langfuse Prompts and change the configuration as follows: { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"input_prompt_name\": \"default_input_guardrail_prompt\", \"output_prompt_name\": \"default_output_guardrail_prompt\", }, \"prompt_templates\": { \"condense_prompt_name\": \"new_condense_prompt\", \"context_prompt_name\": \"new_context_prompt\", \"context_refine_prompt_name\": \"new_context_refine_prompt\", \"system_prompt_name\": \"new_system_prompt\" } } } }","title":"Prompt Templates Configuration"},{"location":"how_to/how_to_configure/#upcoming-docs","text":"Docs about configurable retrievers, postprocessors and others are in progress..","title":"Upcoming Docs"},{"location":"monitoring/in_progress/","text":"In progres..","title":"In progress"},{"location":"quickstart/developer_setup/","text":"Local Development Setup This guide outlines the steps required to set up the RAG system on your local machine for development purposes. Requirements: Python >=3.10,<3.13 Docker Configuration & Secrets First, clone the repository and navigate to rag_blueprint : git clone https://github.com/feld-m/rag_blueprint.git cd rag_blueprint The local configuration is located in configuration.local.json . This file configures toy PDF dataset as the document datasource and defines local settings for embedding, augmentation, and evaluation stages. To customize the setup, refer to the How to Configure the RAG System guide. Secrets Configuration Create a secrets file at configurations/secrets.local.env . Below is a template: # LLMs RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} # Langfuse RAG__LANGFUSE__DATABASE__USER=user RAG__LANGFUSE__DATABASE__PASSWORD=password RAG__LANGFUSE__SECRET_KEY=required_placeholder RAG__LANGFUSE__PUBLIC_KEY=required_placeholder RAG__LLMS__GPT_4O_MINI__API_KEY : Required for connecting to OpenAI GPT-4o-mini LLM. Langfuse Keys : RAG__LANGFUSE__SECRET_KEY and RAG__LANGFUSE__PUBLIC_KEY are generated after initialization and will need to be updated later. Initialization Python Environment Install uv on your OS following this installation guide. In the root of the project, create a virtual environment and activate it: uv venv source .venv/bin/activate Install the required dependencies: uv sync --all-extras Services Initialization To initialize the Langfuse and vector store services, run the initialization script: build/workstation/init.sh --env local NOTE: Depending on your OS and the setup you might need to give execute permission to the initialization script e.g. chmod u+x build/workstation/init.sh . Once initialized, access the Langfuse web server on your localhost (port defined in configuration.local.json under augmentation.langfuse.port ). Use the Langfuse UI to: Create a user. Set up a project for the application. Generate secret and public keys for the project. Add the generated keys to the configurations/secrets.local.env file as follows: RAG__LANGFUSE__SECRET_KEY=<generated_secret_key> RAG__LANGFUSE__PUBLIC_KEY=<generated_public_key> Development Running RAG For the first run, it is recommended to execute the scripts in the specified order to ensure proper initialization of resources like vector store collections. Embedding Stage Run the embedding stage script: python src/embed.py --env local --on-prem-config Note : The embedding process may take significant time, depending on the size of your datasource. Use export_limit fields in configuration to speed up the process. Moreover, for configuration.local.json setup you can keep required_placeholders in configurations/secrets.local.env . Augmentation Stage Run the augmentation stage script: python src/augment.py --env local --on-prem-config This initializes the RAG system's chat engine and the Chainlit application, leveraging the embeddings generated in the previous step. Evaluation Stage Run the evaluation stage script: python src/evaluation.py --env local --on-prem-config Important : For evaluation to proceed, Langfuse datasets must be populated either manually or via Chainlit's human feedback feature. For additional details, refer to the Evaluation Docs . Git setup The .pre-commit-config.yaml file configures code formatters to enforce consistency before committing changes. After cloning the repository and installing dependencies, enable pre-commit hooks: pre-commit install","title":"Local Developement Setup"},{"location":"quickstart/developer_setup/#local-development-setup","text":"This guide outlines the steps required to set up the RAG system on your local machine for development purposes. Requirements: Python >=3.10,<3.13 Docker","title":"Local Development Setup"},{"location":"quickstart/developer_setup/#configuration-secrets","text":"First, clone the repository and navigate to rag_blueprint : git clone https://github.com/feld-m/rag_blueprint.git cd rag_blueprint The local configuration is located in configuration.local.json . This file configures toy PDF dataset as the document datasource and defines local settings for embedding, augmentation, and evaluation stages. To customize the setup, refer to the How to Configure the RAG System guide.","title":"Configuration &amp; Secrets"},{"location":"quickstart/developer_setup/#secrets-configuration","text":"Create a secrets file at configurations/secrets.local.env . Below is a template: # LLMs RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} # Langfuse RAG__LANGFUSE__DATABASE__USER=user RAG__LANGFUSE__DATABASE__PASSWORD=password RAG__LANGFUSE__SECRET_KEY=required_placeholder RAG__LANGFUSE__PUBLIC_KEY=required_placeholder RAG__LLMS__GPT_4O_MINI__API_KEY : Required for connecting to OpenAI GPT-4o-mini LLM. Langfuse Keys : RAG__LANGFUSE__SECRET_KEY and RAG__LANGFUSE__PUBLIC_KEY are generated after initialization and will need to be updated later.","title":"Secrets Configuration"},{"location":"quickstart/developer_setup/#initialization","text":"","title":"Initialization"},{"location":"quickstart/developer_setup/#python-environment","text":"Install uv on your OS following this installation guide. In the root of the project, create a virtual environment and activate it: uv venv source .venv/bin/activate Install the required dependencies: uv sync --all-extras","title":"Python Environment"},{"location":"quickstart/developer_setup/#services-initialization","text":"To initialize the Langfuse and vector store services, run the initialization script: build/workstation/init.sh --env local NOTE: Depending on your OS and the setup you might need to give execute permission to the initialization script e.g. chmod u+x build/workstation/init.sh . Once initialized, access the Langfuse web server on your localhost (port defined in configuration.local.json under augmentation.langfuse.port ). Use the Langfuse UI to: Create a user. Set up a project for the application. Generate secret and public keys for the project. Add the generated keys to the configurations/secrets.local.env file as follows: RAG__LANGFUSE__SECRET_KEY=<generated_secret_key> RAG__LANGFUSE__PUBLIC_KEY=<generated_public_key>","title":"Services Initialization"},{"location":"quickstart/developer_setup/#development","text":"","title":"Development"},{"location":"quickstart/developer_setup/#running-rag","text":"For the first run, it is recommended to execute the scripts in the specified order to ensure proper initialization of resources like vector store collections.","title":"Running RAG"},{"location":"quickstart/developer_setup/#embedding-stage","text":"Run the embedding stage script: python src/embed.py --env local --on-prem-config Note : The embedding process may take significant time, depending on the size of your datasource. Use export_limit fields in configuration to speed up the process. Moreover, for configuration.local.json setup you can keep required_placeholders in configurations/secrets.local.env .","title":"Embedding Stage"},{"location":"quickstart/developer_setup/#augmentation-stage","text":"Run the augmentation stage script: python src/augment.py --env local --on-prem-config This initializes the RAG system's chat engine and the Chainlit application, leveraging the embeddings generated in the previous step.","title":"Augmentation Stage"},{"location":"quickstart/developer_setup/#evaluation-stage","text":"Run the evaluation stage script: python src/evaluation.py --env local --on-prem-config Important : For evaluation to proceed, Langfuse datasets must be populated either manually or via Chainlit's human feedback feature. For additional details, refer to the Evaluation Docs .","title":"Evaluation Stage"},{"location":"quickstart/developer_setup/#git-setup","text":"The .pre-commit-config.yaml file configures code formatters to enforce consistency before committing changes. After cloning the repository and installing dependencies, enable pre-commit hooks: pre-commit install","title":"Git setup"},{"location":"quickstart/quickstart_setup/","text":"Quickstart Setup This guide outlines the steps to set up and deploy the RAG system on your server or local machine. Requirements: Python >=3.10,<3.13 Docker Configuration & Secrets First, clone the repository and navigate to rag_blueprint : git clone https://github.com/feld-m/rag_blueprint.git cd rag_blueprint The default configuration is located in configuration.default.json . This file configures toy PDF dataset as the document datasource and defines default settings for embedding, augmentation, and evaluation stages. To customize the setup, refer to the How to Configure the RAG System guide. Secrets Configuration Create a secrets file at configurations/secrets.default.env . Below is a template: # LLMs RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} # Langfuse RAG__LANGFUSE__DATABASE__USER=user RAG__LANGFUSE__DATABASE__PASSWORD=password RAG__LANGFUSE__SECRET_KEY=required_placeholder RAG__LANGFUSE__PUBLIC_KEY=required_placeholder RAG__LLMS__GPT_4O_MINI__API_KEY : Required for connecting to OpenAI GPT-4o-mini LLM. Langfuse Keys : RAG__LANGFUSE__SECRET_KEY and RAG__LANGFUSE__PUBLIC_KEY are generated after initialization and will need to be updated later. Initialization Python Environment Install uv on your OS following this installation guide. In the root of the project, create a virtual environment and activate it: uv venv source .venv/bin/activate Install the required dependencies: uv sync --all-extras Services Initialization To initialize the Langfuse and vector store services, run the initialization script: build/workstation/init.sh --env default NOTE: Depending on your OS and the setup you might need to give execute permission to the initialization script e.g. chmod u+x build/workstation/init.sh Once initialized, access the Langfuse web server on your localhost (port defined in configuration.default.json under augmentation.langfuse.port ). Use the Langfuse UI to: Create a user. Set up a project for the application. Generate secret and public keys for the project. Add the generated keys to the configurations/secrets.default.env file as follows: RAG__LANGFUSE__SECRET_KEY=<generated_secret_key> RAG__LANGFUSE__PUBLIC_KEY=<generated_public_key> Deployment After completing the initialization, deploy the RAG system using the following command: build/workstation/deploy.sh --env default This command sets up and runs the RAG system on your workstation, enabling it for use.","title":"Quickstart Setup"},{"location":"quickstart/quickstart_setup/#quickstart-setup","text":"This guide outlines the steps to set up and deploy the RAG system on your server or local machine. Requirements: Python >=3.10,<3.13 Docker","title":"Quickstart Setup"},{"location":"quickstart/quickstart_setup/#configuration-secrets","text":"First, clone the repository and navigate to rag_blueprint : git clone https://github.com/feld-m/rag_blueprint.git cd rag_blueprint The default configuration is located in configuration.default.json . This file configures toy PDF dataset as the document datasource and defines default settings for embedding, augmentation, and evaluation stages. To customize the setup, refer to the How to Configure the RAG System guide.","title":"Configuration &amp; Secrets"},{"location":"quickstart/quickstart_setup/#secrets-configuration","text":"Create a secrets file at configurations/secrets.default.env . Below is a template: # LLMs RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} # Langfuse RAG__LANGFUSE__DATABASE__USER=user RAG__LANGFUSE__DATABASE__PASSWORD=password RAG__LANGFUSE__SECRET_KEY=required_placeholder RAG__LANGFUSE__PUBLIC_KEY=required_placeholder RAG__LLMS__GPT_4O_MINI__API_KEY : Required for connecting to OpenAI GPT-4o-mini LLM. Langfuse Keys : RAG__LANGFUSE__SECRET_KEY and RAG__LANGFUSE__PUBLIC_KEY are generated after initialization and will need to be updated later.","title":"Secrets Configuration"},{"location":"quickstart/quickstart_setup/#initialization","text":"","title":"Initialization"},{"location":"quickstart/quickstart_setup/#python-environment","text":"Install uv on your OS following this installation guide. In the root of the project, create a virtual environment and activate it: uv venv source .venv/bin/activate Install the required dependencies: uv sync --all-extras","title":"Python Environment"},{"location":"quickstart/quickstart_setup/#services-initialization","text":"To initialize the Langfuse and vector store services, run the initialization script: build/workstation/init.sh --env default NOTE: Depending on your OS and the setup you might need to give execute permission to the initialization script e.g. chmod u+x build/workstation/init.sh Once initialized, access the Langfuse web server on your localhost (port defined in configuration.default.json under augmentation.langfuse.port ). Use the Langfuse UI to: Create a user. Set up a project for the application. Generate secret and public keys for the project. Add the generated keys to the configurations/secrets.default.env file as follows: RAG__LANGFUSE__SECRET_KEY=<generated_secret_key> RAG__LANGFUSE__PUBLIC_KEY=<generated_public_key>","title":"Services Initialization"},{"location":"quickstart/quickstart_setup/#deployment","text":"After completing the initialization, deploy the RAG system using the following command: build/workstation/deploy.sh --env default This command sets up and runs the RAG system on your workstation, enabling it for use.","title":"Deployment"},{"location":"src/augment/","text":"Augment This module contains functionality related to the the augment script. Augment This script is used to handle chat interactions using the ChainLit library and a chat engine. Actions are observed by Langfuse. To make it work vector storage should be filled with the embeddings of the documents. To run the script execute the following command from the root directory of the project: python src/chat.py app_shutdown () async Clean up resources on application shutdown. Stops the scheduler if it is running. Source code in src/augment.py 105 106 107 108 109 110 111 112 113 114 115 @cl . on_app_shutdown async def app_shutdown () -> None : \"\"\" Clean up resources on application shutdown. Stops the scheduler if it is running. \"\"\" try : initializer . get_scheduler () . stop () logger . info ( \"Scheduler stopped successfully\" ) except Exception as e : logger . warning ( f \"Failed to stop scheduler: { e } \" ) app_startup () async Initialize the application on startup. Sets up the augmentation initializer and configuration, and starts the scheduler. Source code in src/augment.py 29 30 31 32 33 34 35 36 37 38 @cl . on_app_startup async def app_startup () -> None : \"\"\" Initialize the application on startup. Sets up the augmentation initializer and configuration, and starts the scheduler. \"\"\" global initializer , configuration initializer = AugmentationInitializer () configuration = initializer . get_configuration () initializer . get_scheduler () . start () get_data_layer () Initialize Chainlit's data layer with the custom service. Note: This function may be called before app_startup(), so we need to initialize configuration lazily if it's not already set. Returns: ChainlitService ( ChainlitService ) \u2013 The custom service for data layer. Source code in src/augment.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @cl . data_layer def get_data_layer () -> ChainlitService : \"\"\" Initialize Chainlit's data layer with the custom service. Note: This function may be called before app_startup(), so we need to initialize configuration lazily if it's not already set. Returns: ChainlitService: The custom service for data layer. \"\"\" global initializer , configuration if configuration is None : initializer = AugmentationInitializer () configuration = initializer . get_configuration () return ChainlitServiceFactory . create ( configuration . augmentation ) main ( user_message ) async Process user messages and generate responses. Parameters: user_message ( Message ) \u2013 Message received from user Source code in src/augment.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @cl . on_message async def main ( user_message : cl . Message ) -> None : \"\"\" Process user messages and generate responses. Args: user_message: Message received from user \"\"\" try : chat_engine = cl . user_session . get ( \"chat_engine\" ) assistant_message = cl . Message ( content = \"\" , author = \"Assistant\" ) response = await cl . make_async ( chat_engine . stream_chat )( message = user_message . content , chainlit_message_id = assistant_message . parent_id , ) for token in response . response_gen : await assistant_message . stream_token ( token ) utils = ChainlitUtilsFactory . create ( configuration . augmentation . chainlit ) utils . add_references ( assistant_message , response ) await assistant_message . send () except Exception as e : # It is imprecise to catch all exceptions, but llamaindex doesn't provide unified RateLimitError logger . error ( f \"Error in main: { e } \" , exc_info = True ) await cl . ErrorMessage ( content = \"You have reached the request rate limit. Please try again later.\" , ) . send () start () async Initialize chat session with chat engine. Sets up session-specific chat engine and displays welcome message. Source code in src/augment.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @cl . on_chat_start async def start () -> None : \"\"\" Initialize chat session with chat engine. Sets up session-specific chat engine and displays welcome message. \"\"\" chat_engine = ChatEngineRegistry . get ( configuration . augmentation . chat_engine . name ) . create ( configuration ) chat_engine . set_session_id ( cl . user_session . get ( \"id\" )) cl . user_session . set ( \"chat_engine\" , chat_engine ) utils = ChainlitUtilsFactory . create ( configuration . augmentation . chainlit ) await utils . get_disclaimer_message () . send () await utils . get_welcome_message () . send ()","title":"Augment"},{"location":"src/augment/#augment","text":"This module contains functionality related to the the augment script.","title":"Augment"},{"location":"src/augment/#augment_1","text":"This script is used to handle chat interactions using the ChainLit library and a chat engine. Actions are observed by Langfuse. To make it work vector storage should be filled with the embeddings of the documents. To run the script execute the following command from the root directory of the project: python src/chat.py","title":"Augment"},{"location":"src/augment/#src.augment.app_shutdown","text":"Clean up resources on application shutdown. Stops the scheduler if it is running. Source code in src/augment.py 105 106 107 108 109 110 111 112 113 114 115 @cl . on_app_shutdown async def app_shutdown () -> None : \"\"\" Clean up resources on application shutdown. Stops the scheduler if it is running. \"\"\" try : initializer . get_scheduler () . stop () logger . info ( \"Scheduler stopped successfully\" ) except Exception as e : logger . warning ( f \"Failed to stop scheduler: { e } \" )","title":"app_shutdown"},{"location":"src/augment/#src.augment.app_startup","text":"Initialize the application on startup. Sets up the augmentation initializer and configuration, and starts the scheduler. Source code in src/augment.py 29 30 31 32 33 34 35 36 37 38 @cl . on_app_startup async def app_startup () -> None : \"\"\" Initialize the application on startup. Sets up the augmentation initializer and configuration, and starts the scheduler. \"\"\" global initializer , configuration initializer = AugmentationInitializer () configuration = initializer . get_configuration () initializer . get_scheduler () . start ()","title":"app_startup"},{"location":"src/augment/#src.augment.get_data_layer","text":"Initialize Chainlit's data layer with the custom service. Note: This function may be called before app_startup(), so we need to initialize configuration lazily if it's not already set. Returns: ChainlitService ( ChainlitService ) \u2013 The custom service for data layer. Source code in src/augment.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 @cl . data_layer def get_data_layer () -> ChainlitService : \"\"\" Initialize Chainlit's data layer with the custom service. Note: This function may be called before app_startup(), so we need to initialize configuration lazily if it's not already set. Returns: ChainlitService: The custom service for data layer. \"\"\" global initializer , configuration if configuration is None : initializer = AugmentationInitializer () configuration = initializer . get_configuration () return ChainlitServiceFactory . create ( configuration . augmentation )","title":"get_data_layer"},{"location":"src/augment/#src.augment.main","text":"Process user messages and generate responses. Parameters: user_message ( Message ) \u2013 Message received from user Source code in src/augment.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 @cl . on_message async def main ( user_message : cl . Message ) -> None : \"\"\" Process user messages and generate responses. Args: user_message: Message received from user \"\"\" try : chat_engine = cl . user_session . get ( \"chat_engine\" ) assistant_message = cl . Message ( content = \"\" , author = \"Assistant\" ) response = await cl . make_async ( chat_engine . stream_chat )( message = user_message . content , chainlit_message_id = assistant_message . parent_id , ) for token in response . response_gen : await assistant_message . stream_token ( token ) utils = ChainlitUtilsFactory . create ( configuration . augmentation . chainlit ) utils . add_references ( assistant_message , response ) await assistant_message . send () except Exception as e : # It is imprecise to catch all exceptions, but llamaindex doesn't provide unified RateLimitError logger . error ( f \"Error in main: { e } \" , exc_info = True ) await cl . ErrorMessage ( content = \"You have reached the request rate limit. Please try again later.\" , ) . send ()","title":"main"},{"location":"src/augment/#src.augment.start","text":"Initialize chat session with chat engine. Sets up session-specific chat engine and displays welcome message. Source code in src/augment.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 @cl . on_chat_start async def start () -> None : \"\"\" Initialize chat session with chat engine. Sets up session-specific chat engine and displays welcome message. \"\"\" chat_engine = ChatEngineRegistry . get ( configuration . augmentation . chat_engine . name ) . create ( configuration ) chat_engine . set_session_id ( cl . user_session . get ( \"id\" )) cl . user_session . set ( \"chat_engine\" , chat_engine ) utils = ChainlitUtilsFactory . create ( configuration . augmentation . chainlit ) await utils . get_disclaimer_message () . send () await utils . get_welcome_message () . send ()","title":"start"},{"location":"src/embed/","text":"Embed This module contains functionality related to the the embed script. Embed This script is the entry point for the embedding process. It initializes the embedding orchestrator and starts the embedding workflow. To run the script, execute the following command from the root directory of the project: python src/embed.py Optional flags --clear-collection: Clear/delete the vector store collection before embedding --on-prem-config: Use on-premise configuration files --env: Specify the environment (local, test, dev, prod) run ( clear_collection = False , logger = LoggerConfiguration . get_logger ( __name__ )) async Execute the embedding process. Parameters: clear_collection ( bool , default: False ) \u2013 If True, clear the collection before embedding logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/embed.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 async def run ( clear_collection : bool = False , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Execute the embedding process. Args: clear_collection: If True, clear the collection before embedding logger: Logger instance for logging messages \"\"\" initializer = EmbeddingInitializer () configuration = initializer . get_configuration () vector_store_config = configuration . embedding . vector_store # Clear collection if requested if clear_collection : logger . info ( f \"Clearing collection ' { vector_store_config . collection_name } '...\" ) vector_store = VectorStoreRegistry . get ( vector_store_config . name ) . create ( vector_store_config ) vector_store . clear () logger . info ( f \"Collection ' { vector_store_config . collection_name } ' cleared successfully.\" ) else : # Only validate if we didn't just clear (to avoid false positives) validator = VectorStoreValidatorRegistry . get ( vector_store_config . name ) . create ( vector_store_config ) try : validator . validate () except CollectionExistsException as e : logger . info ( f \"Collection ' { e . collection_name } ' already exists. \" \"Skipping embedding process. Use --clear-collection to override.\" ) return logger . info ( \"Starting embedding process.\" ) orchestrator = EmbeddingOrchestratorRegistry . get ( configuration . embedding . orchestrator_name ) . create ( configuration ) await orchestrator . embed () logger . info ( \"Embedding process finished.\" )","title":"Embed"},{"location":"src/embed/#embed","text":"This module contains functionality related to the the embed script.","title":"Embed"},{"location":"src/embed/#embed_1","text":"This script is the entry point for the embedding process. It initializes the embedding orchestrator and starts the embedding workflow. To run the script, execute the following command from the root directory of the project: python src/embed.py Optional flags --clear-collection: Clear/delete the vector store collection before embedding --on-prem-config: Use on-premise configuration files --env: Specify the environment (local, test, dev, prod)","title":"Embed"},{"location":"src/embed/#src.embed.run","text":"Execute the embedding process. Parameters: clear_collection ( bool , default: False ) \u2013 If True, clear the collection before embedding logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/embed.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 async def run ( clear_collection : bool = False , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Execute the embedding process. Args: clear_collection: If True, clear the collection before embedding logger: Logger instance for logging messages \"\"\" initializer = EmbeddingInitializer () configuration = initializer . get_configuration () vector_store_config = configuration . embedding . vector_store # Clear collection if requested if clear_collection : logger . info ( f \"Clearing collection ' { vector_store_config . collection_name } '...\" ) vector_store = VectorStoreRegistry . get ( vector_store_config . name ) . create ( vector_store_config ) vector_store . clear () logger . info ( f \"Collection ' { vector_store_config . collection_name } ' cleared successfully.\" ) else : # Only validate if we didn't just clear (to avoid false positives) validator = VectorStoreValidatorRegistry . get ( vector_store_config . name ) . create ( vector_store_config ) try : validator . validate () except CollectionExistsException as e : logger . info ( f \"Collection ' { e . collection_name } ' already exists. \" \"Skipping embedding process. Use --clear-collection to override.\" ) return logger . info ( \"Starting embedding process.\" ) orchestrator = EmbeddingOrchestratorRegistry . get ( configuration . embedding . orchestrator_name ) . create ( configuration ) await orchestrator . embed () logger . info ( \"Embedding process finished.\" )","title":"run"},{"location":"src/evaluate/","text":"Evaluate This module contains functionality related to the the evaluate script. Evaluate This script is used to evaluate RAG system using langfuse datasets. To add a new item to manual dataset, visit Langfuse UI. Vector storage should be running with ready collection of embeddings. To run the script execute the following command from the root directory of the project: python src/evaluate.py run ( logger = LoggerConfiguration . get_logger ( __name__ )) Execute RAG system evaluation workflow. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/evaluate.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def run ( logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ) -> None : \"\"\" Execute RAG system evaluation workflow. Args: logger: Logger instance for logging messages \"\"\" initializer = EvaluationInitializer () configuration = initializer . get_configuration () langfuse_evaluator = LangfuseEvaluatorFactory . create ( configuration ) logger . info ( f \"Evaluating { langfuse_evaluator . run_name } ...\" ) langfuse_evaluator . evaluate ( dataset_name = configuration . augmentation . langfuse . datasets . feedback_dataset . name ) langfuse_evaluator . evaluate ( dataset_name = configuration . augmentation . langfuse . datasets . manual_dataset . name ) logger . info ( f \"Evaluation complete for { configuration . metadata . build_name } .\" )","title":"Evaluate"},{"location":"src/evaluate/#evaluate","text":"This module contains functionality related to the the evaluate script.","title":"Evaluate"},{"location":"src/evaluate/#evaluate_1","text":"This script is used to evaluate RAG system using langfuse datasets. To add a new item to manual dataset, visit Langfuse UI. Vector storage should be running with ready collection of embeddings. To run the script execute the following command from the root directory of the project: python src/evaluate.py","title":"Evaluate"},{"location":"src/evaluate/#src.evaluate.run","text":"Execute RAG system evaluation workflow. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/evaluate.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def run ( logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ) -> None : \"\"\" Execute RAG system evaluation workflow. Args: logger: Logger instance for logging messages \"\"\" initializer = EvaluationInitializer () configuration = initializer . get_configuration () langfuse_evaluator = LangfuseEvaluatorFactory . create ( configuration ) logger . info ( f \"Evaluating { langfuse_evaluator . run_name } ...\" ) langfuse_evaluator . evaluate ( dataset_name = configuration . augmentation . langfuse . datasets . feedback_dataset . name ) langfuse_evaluator . evaluate ( dataset_name = configuration . augmentation . langfuse . datasets . manual_dataset . name ) logger . info ( f \"Evaluation complete for { configuration . metadata . build_name } .\" )","title":"run"},{"location":"src/augmentation/bootstrap/initializer/","text":"Initializer This module contains functionality related to the the initializer module for augmentation.bootstrap . Initializer AugmentationInitializer Bases: EmbeddingInitializer Initializer for the augmentation process. Extends the EmbeddingInitializer to set up the environment for augmentation tasks. This initializer is responsible for loading the required configuration and registering all necessary components with the dependency injection container. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. Source code in src/augmentation/bootstrap/initializer.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 class AugmentationInitializer ( EmbeddingInitializer ): \"\"\"Initializer for the augmentation process. Extends the EmbeddingInitializer to set up the environment for augmentation tasks. This initializer is responsible for loading the required configuration and registering all necessary components with the dependency injection container. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = AugmentationConfiguration , package_loader : BasePackageLoader = AugmentationPackageLoader (), ): \"\"\"Initialize the AugmentationInitializer. Args: configuration_class: The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader: Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) self . scheduler = AugmentationScheduler ( configuration = self . get_configuration (), logger = LoggerConfiguration . get_logger ( __name__ ), ) self . _initialize_default_prompt () def get_scheduler ( self ) -> AugmentationScheduler : \"\"\"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler: The scheduler instance used for scheduling jobs. \"\"\" return self . scheduler def _initialize_default_prompt ( self ) -> None : \"\"\" Initialize the default prompt templates for the augmentation process managed by Langfuse. \"\"\" configuration = self . get_configuration () langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . augmentation . langfuse ) # Use LlamaIndex's default condense prompt # Domain-specific prompts can be configured in Langfuse or loaded from prompts/ directory langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_condense_prompt\" , prompt_template = \"\"\"Given the following conversation between a user and an AI assistant and a follow up question from user, rephrase the follow up question to be a standalone question. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\"\"\" , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_context_prompt\" , prompt_template = DEFAULT_CONTEXT_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_context_refine_prompt\" , prompt_template = DEFAULT_CONTEXT_REFINE_PROMPT_TEMPLATE , ) # Generic system prompt for RAG assistant # Domain-specific prompts (e.g., Bundestag) should be configured in Langfuse # or loaded from prompts/ directory for specific deployments langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_system_prompt\" , prompt_template = \"\"\"You are a helpful AI assistant that answers questions based on the provided context documents. CRITICAL: GROUNDING IN RETRIEVED DOCUMENTS - Base your answers ONLY on the information in the provided context documents - Do not use your training data or prior knowledge unless explicitly asked - If the retrieved documents do not contain information about a topic, clearly state this - Always cite or reference the source documents when possible IMPORTANT GUIDELINES: - Provide accurate, objective information based on the retrieved documents - If information is ambiguous or conflicting, acknowledge this - Be concise but thorough in your responses - Maintain a helpful and professional tone\"\"\" , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_input_guardrail_prompt\" , prompt_template = DEFAULT_INPUT_GUARDRAIL_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_output_guardrail_prompt\" , prompt_template = DEFAULT_OUTPUT_GUARDRAIL_PROMPT_TEMPLATE , ) __init__ ( configuration_class = AugmentationConfiguration , package_loader = AugmentationPackageLoader ()) Initialize the AugmentationInitializer. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: AugmentationConfiguration ) \u2013 The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader ( BasePackageLoader , default: AugmentationPackageLoader () ) \u2013 Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. Source code in src/augmentation/bootstrap/initializer.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = AugmentationConfiguration , package_loader : BasePackageLoader = AugmentationPackageLoader (), ): \"\"\"Initialize the AugmentationInitializer. Args: configuration_class: The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader: Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) self . scheduler = AugmentationScheduler ( configuration = self . get_configuration (), logger = LoggerConfiguration . get_logger ( __name__ ), ) self . _initialize_default_prompt () get_scheduler () Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler ( AugmentationScheduler ) \u2013 The scheduler instance used for scheduling jobs. Source code in src/augmentation/bootstrap/initializer.py 189 190 191 192 193 194 195 def get_scheduler ( self ) -> AugmentationScheduler : \"\"\"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler: The scheduler instance used for scheduling jobs. \"\"\" return self . scheduler AugmentationPackageLoader Bases: EmbeddingPackageLoader Package loader for augmentation components. Extends the EmbeddingPackageLoader to load additional packages required for the augmentation process, including LLMs, retrievers, postprocessors, and chat engines. Source code in src/augmentation/bootstrap/initializer.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class AugmentationPackageLoader ( EmbeddingPackageLoader ): \"\"\"Package loader for augmentation components. Extends the EmbeddingPackageLoader to load additional packages required for the augmentation process, including LLMs, retrievers, postprocessors, and chat engines. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the AugmentationPackageLoader. Args: logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.augmentation.components.guardrails\" , \"src.augmentation.components.llms\" , \"src.augmentation.components.retrievers\" , \"src.augmentation.components.postprocessors\" , \"src.augmentation.components.chat_engines\" , ] ) __init__ ( logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the AugmentationPackageLoader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information. Defaults to a logger configured with the current module name. Source code in src/augmentation/bootstrap/initializer.py 123 124 125 126 127 128 129 130 131 132 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the AugmentationPackageLoader. Args: logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" super () . __init__ ( logger ) load_packages () Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. Source code in src/augmentation/bootstrap/initializer.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def load_packages ( self ) -> None : \"\"\"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.augmentation.components.guardrails\" , \"src.augmentation.components.llms\" , \"src.augmentation.components.retrievers\" , \"src.augmentation.components.postprocessors\" , \"src.augmentation.components.chat_engines\" , ] ) AugmentationScheduler Source code in src/augmentation/bootstrap/initializer.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class AugmentationScheduler : def __init__ ( self , configuration : AugmentationConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: configuration (AugmentationConfiguration): The configuration object for the augmentation process. logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" self . logger = logger self . configuration = configuration self . scheduler = AsyncIOScheduler () def start ( self ) -> None : \"\"\"Start the scheduler and schedule the daily queries retention job.\"\"\" langfuse_configuration = self . configuration . augmentation . langfuse queries_retention_job = LangfuseRenetionJobFactory . create ( langfuse_configuration ) try : self . scheduler . add_job ( queries_retention_job . run , CronTrigger . from_crontab ( langfuse_configuration . retention_job . crontab ), id = langfuse_configuration . retention_job . name , replace_existing = True , ) self . logger . info ( \"Daily queries retention job scheduled successfully\" ) self . scheduler . start () self . logger . info ( \"Scheduler started successfully\" ) except Exception as e : self . logger . error ( f \"Failed to initialize scheduler: { e } \" ) def stop ( self ) -> None : \"\"\"Stop the scheduler if it is running.\"\"\" if self . scheduler . running : self . scheduler . shutdown () self . logger . info ( \"Scheduler stopped successfully\" ) else : self . logger . warning ( \"Scheduler was not running\" ) __init__ ( configuration , logger = LoggerConfiguration . get_logger ( __name__ )) Parameters: configuration ( AugmentationConfiguration ) \u2013 The configuration object for the augmentation process. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information. Defaults to a logger configured with the current module name. Source code in src/augmentation/bootstrap/initializer.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , configuration : AugmentationConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: configuration (AugmentationConfiguration): The configuration object for the augmentation process. logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" self . logger = logger self . configuration = configuration self . scheduler = AsyncIOScheduler () start () Start the scheduler and schedule the daily queries retention job. Source code in src/augmentation/bootstrap/initializer.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def start ( self ) -> None : \"\"\"Start the scheduler and schedule the daily queries retention job.\"\"\" langfuse_configuration = self . configuration . augmentation . langfuse queries_retention_job = LangfuseRenetionJobFactory . create ( langfuse_configuration ) try : self . scheduler . add_job ( queries_retention_job . run , CronTrigger . from_crontab ( langfuse_configuration . retention_job . crontab ), id = langfuse_configuration . retention_job . name , replace_existing = True , ) self . logger . info ( \"Daily queries retention job scheduled successfully\" ) self . scheduler . start () self . logger . info ( \"Scheduler started successfully\" ) except Exception as e : self . logger . error ( f \"Failed to initialize scheduler: { e } \" ) stop () Stop the scheduler if it is running. Source code in src/augmentation/bootstrap/initializer.py 106 107 108 109 110 111 112 def stop ( self ) -> None : \"\"\"Stop the scheduler if it is running.\"\"\" if self . scheduler . running : self . scheduler . shutdown () self . logger . info ( \"Scheduler stopped successfully\" ) else : self . logger . warning ( \"Scheduler was not running\" )","title":"Initializer"},{"location":"src/augmentation/bootstrap/initializer/#initializer","text":"This module contains functionality related to the the initializer module for augmentation.bootstrap .","title":"Initializer"},{"location":"src/augmentation/bootstrap/initializer/#initializer_1","text":"","title":"Initializer"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationInitializer","text":"Bases: EmbeddingInitializer Initializer for the augmentation process. Extends the EmbeddingInitializer to set up the environment for augmentation tasks. This initializer is responsible for loading the required configuration and registering all necessary components with the dependency injection container. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. Source code in src/augmentation/bootstrap/initializer.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 class AugmentationInitializer ( EmbeddingInitializer ): \"\"\"Initializer for the augmentation process. Extends the EmbeddingInitializer to set up the environment for augmentation tasks. This initializer is responsible for loading the required configuration and registering all necessary components with the dependency injection container. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = AugmentationConfiguration , package_loader : BasePackageLoader = AugmentationPackageLoader (), ): \"\"\"Initialize the AugmentationInitializer. Args: configuration_class: The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader: Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) self . scheduler = AugmentationScheduler ( configuration = self . get_configuration (), logger = LoggerConfiguration . get_logger ( __name__ ), ) self . _initialize_default_prompt () def get_scheduler ( self ) -> AugmentationScheduler : \"\"\"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler: The scheduler instance used for scheduling jobs. \"\"\" return self . scheduler def _initialize_default_prompt ( self ) -> None : \"\"\" Initialize the default prompt templates for the augmentation process managed by Langfuse. \"\"\" configuration = self . get_configuration () langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . augmentation . langfuse ) # Use LlamaIndex's default condense prompt # Domain-specific prompts can be configured in Langfuse or loaded from prompts/ directory langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_condense_prompt\" , prompt_template = \"\"\"Given the following conversation between a user and an AI assistant and a follow up question from user, rephrase the follow up question to be a standalone question. Chat History: {chat_history} Follow Up Input: {question} Standalone question:\"\"\" , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_context_prompt\" , prompt_template = DEFAULT_CONTEXT_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_context_refine_prompt\" , prompt_template = DEFAULT_CONTEXT_REFINE_PROMPT_TEMPLATE , ) # Generic system prompt for RAG assistant # Domain-specific prompts (e.g., Bundestag) should be configured in Langfuse # or loaded from prompts/ directory for specific deployments langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_system_prompt\" , prompt_template = \"\"\"You are a helpful AI assistant that answers questions based on the provided context documents. CRITICAL: GROUNDING IN RETRIEVED DOCUMENTS - Base your answers ONLY on the information in the provided context documents - Do not use your training data or prior knowledge unless explicitly asked - If the retrieved documents do not contain information about a topic, clearly state this - Always cite or reference the source documents when possible IMPORTANT GUIDELINES: - Provide accurate, objective information based on the retrieved documents - If information is ambiguous or conflicting, acknowledge this - Be concise but thorough in your responses - Maintain a helpful and professional tone\"\"\" , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_input_guardrail_prompt\" , prompt_template = DEFAULT_INPUT_GUARDRAIL_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_output_guardrail_prompt\" , prompt_template = DEFAULT_OUTPUT_GUARDRAIL_PROMPT_TEMPLATE , )","title":"AugmentationInitializer"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationInitializer.__init__","text":"Initialize the AugmentationInitializer. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: AugmentationConfiguration ) \u2013 The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader ( BasePackageLoader , default: AugmentationPackageLoader () ) \u2013 Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. Source code in src/augmentation/bootstrap/initializer.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = AugmentationConfiguration , package_loader : BasePackageLoader = AugmentationPackageLoader (), ): \"\"\"Initialize the AugmentationInitializer. Args: configuration_class: The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader: Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) self . scheduler = AugmentationScheduler ( configuration = self . get_configuration (), logger = LoggerConfiguration . get_logger ( __name__ ), ) self . _initialize_default_prompt ()","title":"__init__"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationInitializer.get_scheduler","text":"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler ( AugmentationScheduler ) \u2013 The scheduler instance used for scheduling jobs. Source code in src/augmentation/bootstrap/initializer.py 189 190 191 192 193 194 195 def get_scheduler ( self ) -> AugmentationScheduler : \"\"\"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler: The scheduler instance used for scheduling jobs. \"\"\" return self . scheduler","title":"get_scheduler"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationPackageLoader","text":"Bases: EmbeddingPackageLoader Package loader for augmentation components. Extends the EmbeddingPackageLoader to load additional packages required for the augmentation process, including LLMs, retrievers, postprocessors, and chat engines. Source code in src/augmentation/bootstrap/initializer.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class AugmentationPackageLoader ( EmbeddingPackageLoader ): \"\"\"Package loader for augmentation components. Extends the EmbeddingPackageLoader to load additional packages required for the augmentation process, including LLMs, retrievers, postprocessors, and chat engines. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the AugmentationPackageLoader. Args: logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.augmentation.components.guardrails\" , \"src.augmentation.components.llms\" , \"src.augmentation.components.retrievers\" , \"src.augmentation.components.postprocessors\" , \"src.augmentation.components.chat_engines\" , ] )","title":"AugmentationPackageLoader"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationPackageLoader.__init__","text":"Initialize the AugmentationPackageLoader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information. Defaults to a logger configured with the current module name. Source code in src/augmentation/bootstrap/initializer.py 123 124 125 126 127 128 129 130 131 132 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the AugmentationPackageLoader. Args: logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" super () . __init__ ( logger )","title":"__init__"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationPackageLoader.load_packages","text":"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. Source code in src/augmentation/bootstrap/initializer.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def load_packages ( self ) -> None : \"\"\"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.augmentation.components.guardrails\" , \"src.augmentation.components.llms\" , \"src.augmentation.components.retrievers\" , \"src.augmentation.components.postprocessors\" , \"src.augmentation.components.chat_engines\" , ] )","title":"load_packages"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationScheduler","text":"Source code in src/augmentation/bootstrap/initializer.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class AugmentationScheduler : def __init__ ( self , configuration : AugmentationConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: configuration (AugmentationConfiguration): The configuration object for the augmentation process. logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" self . logger = logger self . configuration = configuration self . scheduler = AsyncIOScheduler () def start ( self ) -> None : \"\"\"Start the scheduler and schedule the daily queries retention job.\"\"\" langfuse_configuration = self . configuration . augmentation . langfuse queries_retention_job = LangfuseRenetionJobFactory . create ( langfuse_configuration ) try : self . scheduler . add_job ( queries_retention_job . run , CronTrigger . from_crontab ( langfuse_configuration . retention_job . crontab ), id = langfuse_configuration . retention_job . name , replace_existing = True , ) self . logger . info ( \"Daily queries retention job scheduled successfully\" ) self . scheduler . start () self . logger . info ( \"Scheduler started successfully\" ) except Exception as e : self . logger . error ( f \"Failed to initialize scheduler: { e } \" ) def stop ( self ) -> None : \"\"\"Stop the scheduler if it is running.\"\"\" if self . scheduler . running : self . scheduler . shutdown () self . logger . info ( \"Scheduler stopped successfully\" ) else : self . logger . warning ( \"Scheduler was not running\" )","title":"AugmentationScheduler"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationScheduler.__init__","text":"Parameters: configuration ( AugmentationConfiguration ) \u2013 The configuration object for the augmentation process. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information. Defaults to a logger configured with the current module name. Source code in src/augmentation/bootstrap/initializer.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , configuration : AugmentationConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: configuration (AugmentationConfiguration): The configuration object for the augmentation process. logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" self . logger = logger self . configuration = configuration self . scheduler = AsyncIOScheduler ()","title":"__init__"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationScheduler.start","text":"Start the scheduler and schedule the daily queries retention job. Source code in src/augmentation/bootstrap/initializer.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def start ( self ) -> None : \"\"\"Start the scheduler and schedule the daily queries retention job.\"\"\" langfuse_configuration = self . configuration . augmentation . langfuse queries_retention_job = LangfuseRenetionJobFactory . create ( langfuse_configuration ) try : self . scheduler . add_job ( queries_retention_job . run , CronTrigger . from_crontab ( langfuse_configuration . retention_job . crontab ), id = langfuse_configuration . retention_job . name , replace_existing = True , ) self . logger . info ( \"Daily queries retention job scheduled successfully\" ) self . scheduler . start () self . logger . info ( \"Scheduler started successfully\" ) except Exception as e : self . logger . error ( f \"Failed to initialize scheduler: { e } \" )","title":"start"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationScheduler.stop","text":"Stop the scheduler if it is running. Source code in src/augmentation/bootstrap/initializer.py 106 107 108 109 110 111 112 def stop ( self ) -> None : \"\"\"Stop the scheduler if it is running.\"\"\" if self . scheduler . running : self . scheduler . shutdown () self . logger . info ( \"Scheduler stopped successfully\" ) else : self . logger . warning ( \"Scheduler was not running\" )","title":"stop"},{"location":"src/augmentation/bootstrap/configuration/chainlit_configuration/","text":"Chainlit_configuration This module contains functionality related to the the chainlit_configuration module for augmentation.bootstrap.configuration . Chainlit_configuration ChainlitConfiguration Bases: BaseConfiguration Configuration for Chainlit service. This class handles the configuration parameters needed to run a Chainlit service for the RAG application interface. Source code in src/augmentation/bootstrap/configuration/chainlit_configuration.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class ChainlitConfiguration ( BaseConfiguration ): \"\"\"Configuration for Chainlit service. This class handles the configuration parameters needed to run a Chainlit service for the RAG application interface. \"\"\" port : int = Field ( 8000 , description = \"Port to run the chainlit service on.\" ) disclaimer_title : str = Field ( \"Bavarian Beer Chat\" , description = \"Title of the disclaimer message to be displayed.\" , ) disclaimer_text : str = Field ( \"This content is AI-generated and may contain inaccuracies. Please verify any critical information independently. Additional details, including imprint and privacy policy information, can be found in the Readme file located a the top-right corner of the page.\" , description = \"Disclaimer text to be displayed to users.\" , ) welcome_message : str = Field ( \"Welcome to our Bavarian Beer Chat! \ud83c\udf7b We're here to guide you through the rich tapestry of Bavarian beer culture. Whether you're curious about traditional brews, local beer festivals, or the history behind Bavaria's renowned beer purity law, you've come to the right place. Type your question below, and let's embark on this flavorful journey together. Prost!\" , description = \"Welcome message to display to users when they start a conversation.\" , )","title":"Chainlit_configuration"},{"location":"src/augmentation/bootstrap/configuration/chainlit_configuration/#chainlit_configuration","text":"This module contains functionality related to the the chainlit_configuration module for augmentation.bootstrap.configuration .","title":"Chainlit_configuration"},{"location":"src/augmentation/bootstrap/configuration/chainlit_configuration/#chainlit_configuration_1","text":"","title":"Chainlit_configuration"},{"location":"src/augmentation/bootstrap/configuration/chainlit_configuration/#src.augmentation.bootstrap.configuration.chainlit_configuration.ChainlitConfiguration","text":"Bases: BaseConfiguration Configuration for Chainlit service. This class handles the configuration parameters needed to run a Chainlit service for the RAG application interface. Source code in src/augmentation/bootstrap/configuration/chainlit_configuration.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class ChainlitConfiguration ( BaseConfiguration ): \"\"\"Configuration for Chainlit service. This class handles the configuration parameters needed to run a Chainlit service for the RAG application interface. \"\"\" port : int = Field ( 8000 , description = \"Port to run the chainlit service on.\" ) disclaimer_title : str = Field ( \"Bavarian Beer Chat\" , description = \"Title of the disclaimer message to be displayed.\" , ) disclaimer_text : str = Field ( \"This content is AI-generated and may contain inaccuracies. Please verify any critical information independently. Additional details, including imprint and privacy policy information, can be found in the Readme file located a the top-right corner of the page.\" , description = \"Disclaimer text to be displayed to users.\" , ) welcome_message : str = Field ( \"Welcome to our Bavarian Beer Chat! \ud83c\udf7b We're here to guide you through the rich tapestry of Bavarian beer culture. Whether you're curious about traditional brews, local beer festivals, or the history behind Bavaria's renowned beer purity law, you've come to the right place. Type your question below, and let's embark on this flavorful journey together. Prost!\" , description = \"Welcome message to display to users when they start a conversation.\" , )","title":"ChainlitConfiguration"},{"location":"src/augmentation/bootstrap/configuration/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.bootstrap.configuration . Configuration AugmentationConfiguration Bases: EmbeddingConfiguration Main configuration class for the augmentation module. Extends the base embedding configuration with additional augmentation-specific settings to provide a complete configuration for text augmentation processes. Source code in src/augmentation/bootstrap/configuration/configuration.py 134 135 136 137 138 139 140 141 142 143 144 class AugmentationConfiguration ( EmbeddingConfiguration ): \"\"\" Main configuration class for the augmentation module. Extends the base embedding configuration with additional augmentation-specific settings to provide a complete configuration for text augmentation processes. \"\"\" augmentation : _AugmentationConfiguration = Field ( ... , description = \"Configuration of the augmentation process.\" )","title":"Configuration"},{"location":"src/augmentation/bootstrap/configuration/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.bootstrap.configuration .","title":"Configuration"},{"location":"src/augmentation/bootstrap/configuration/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/bootstrap/configuration/configuration/#src.augmentation.bootstrap.configuration.configuration.AugmentationConfiguration","text":"Bases: EmbeddingConfiguration Main configuration class for the augmentation module. Extends the base embedding configuration with additional augmentation-specific settings to provide a complete configuration for text augmentation processes. Source code in src/augmentation/bootstrap/configuration/configuration.py 134 135 136 137 138 139 140 141 142 143 144 class AugmentationConfiguration ( EmbeddingConfiguration ): \"\"\" Main configuration class for the augmentation module. Extends the base embedding configuration with additional augmentation-specific settings to provide a complete configuration for text augmentation processes. \"\"\" augmentation : _AugmentationConfiguration = Field ( ... , description = \"Configuration of the augmentation process.\" )","title":"AugmentationConfiguration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/","text":"Langfuse_configuration This module contains functionality related to the the langfuse_configuration module for augmentation.bootstrap.configuration . Langfuse_configuration LangfuseConfiguration Bases: BaseConfigurationWithSecrets Main configuration for the Langfuse integration. Contains all settings required to connect to and use the Langfuse platform, including server connection details, authentication, and dataset configurations. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class LangfuseConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Main configuration for the Langfuse integration. Contains all settings required to connect to and use the Langfuse platform, including server connection details, authentication, and dataset configurations. \"\"\" class Secrets ( BaseSecrets ): \"\"\"API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) public_key : SecretStr = Field ( ... , description = \"Public API key for authenticating with the Langfuse service\" , ) secret_key : SecretStr = Field ( ... , description = \"Secret API key for authenticating with the Langfuse service\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Langfuse server\" ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"Network protocol to use when connecting to the Langfuse server\" , ) port : int = Field ( 3000 , description = \"TCP port number on which the Langfuse server is listening\" , ) database : LangfuseDatabaseConfiguration = Field ( description = \"Database connection configuration for the Langfuse server\" , default_factory = LangfuseDatabaseConfiguration , ) datasets : LangfuseDatasetsConfiguration = Field ( description = \"Configuration for all datasets managed in the Langfuse platform\" , default_factory = LangfuseDatasetsConfiguration , ) chainlit_tag_format : str = Field ( \"chainlit_message_id: {message_id} \" , description = \"Template string for generating tags that link Chainlit messages to Langfuse traces\" , ) retention_job : LangfuseRetentionJobConfiguration = Field ( description = \"Configuration for the Langfuse retention job that manages trace data lifecycle\" , default_factory = LangfuseRetentionJobConfiguration , ) secrets : Secrets = Field ( None , description = \"API authentication credentials for the Langfuse service\" , ) @property def url ( self ) -> str : \"\"\"Generate the complete URL for connecting to the Langfuse server. Returns: str: Fully formatted URL with protocol, host, and port \"\"\" return f \" { self . protocol } :// { self . host } : { self . port } \" url property Generate the complete URL for connecting to the Langfuse server. Returns: str ( str ) \u2013 Fully formatted URL with protocol, host, and port Secrets Bases: BaseSecrets API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class Secrets ( BaseSecrets ): \"\"\"API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) public_key : SecretStr = Field ( ... , description = \"Public API key for authenticating with the Langfuse service\" , ) secret_key : SecretStr = Field ( ... , description = \"Secret API key for authenticating with the Langfuse service\" , ) LangfuseDatabaseConfiguration Bases: BaseConfigurationWithSecrets Configuration for connecting to the Langfuse PostgreSQL database. Contains all database connection parameters and credentials required for the Langfuse observation and analytics platform. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class LangfuseDatabaseConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Configuration for connecting to the Langfuse PostgreSQL database. Contains all database connection parameters and credentials required for the Langfuse observation and analytics platform. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__DATABASE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) user : SecretStr = Field ( ... , description = \"Username for authenticating with the Langfuse database server\" , ) password : SecretStr = Field ( ... , description = \"Password for authenticating with the Langfuse database server\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Langfuse database server\" , ) port : int = Field ( 5432 , description = \"TCP port number on which the Langfuse database server is listening\" , ) db : str = Field ( \"langfuse\" , description = \"Name of the specific database to connect to on the Langfuse server\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials for the Langfuse database\" ) Secrets Bases: BaseSecrets Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Secrets ( BaseSecrets ): \"\"\"Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__DATABASE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) user : SecretStr = Field ( ... , description = \"Username for authenticating with the Langfuse database server\" , ) password : SecretStr = Field ( ... , description = \"Password for authenticating with the Langfuse database server\" , ) LangfuseDatasetConfiguration Bases: BaseConfiguration Configuration for a single dataset in Langfuse. Defines properties of a dataset that will be created or used in the Langfuse platform. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class LangfuseDatasetConfiguration ( BaseConfiguration ): \"\"\"Configuration for a single dataset in Langfuse. Defines properties of a dataset that will be created or used in the Langfuse platform. \"\"\" name : str = Field ( ... , description = \"Unique identifier for the dataset in the Langfuse server\" , ) description : str = Field ( ... , description = \"Human-readable explanation of the dataset's purpose and contents\" , ) metadata : dict = Field ( {}, description = \"Additional structured information about the dataset as key-value pairs\" , ) LangfuseDatasetsConfiguration Bases: BaseConfiguration Configuration for all datasets used in the Langfuse platform. Contains definitions for specialized datasets used for different purposes. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class LangfuseDatasetsConfiguration ( BaseConfiguration ): \"\"\"Configuration for all datasets used in the Langfuse platform. Contains definitions for specialized datasets used for different purposes. \"\"\" feedback_dataset : LangfuseDatasetConfiguration = Field ( LangfuseDatasetConfiguration ( name = \"feedback-dataset\" , description = \"Dataset created out of positive feedbacks from the chatbot\" , ), description = \"Dataset for storing and analyzing user feedback collected from the chatbot interactions\" , ) manual_dataset : LangfuseDatasetConfiguration = Field ( LangfuseDatasetConfiguration ( name = \"manual-dataset\" , description = \"Dataset created directly by the user indicating the query and the correct answer\" , ), description = \"Dataset for storing manually curated query-answer pairs for evaluation and training\" , )","title":"Langfuse_configuration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#langfuse_configuration","text":"This module contains functionality related to the the langfuse_configuration module for augmentation.bootstrap.configuration .","title":"Langfuse_configuration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#langfuse_configuration_1","text":"","title":"Langfuse_configuration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseConfiguration","text":"Bases: BaseConfigurationWithSecrets Main configuration for the Langfuse integration. Contains all settings required to connect to and use the Langfuse platform, including server connection details, authentication, and dataset configurations. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class LangfuseConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Main configuration for the Langfuse integration. Contains all settings required to connect to and use the Langfuse platform, including server connection details, authentication, and dataset configurations. \"\"\" class Secrets ( BaseSecrets ): \"\"\"API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) public_key : SecretStr = Field ( ... , description = \"Public API key for authenticating with the Langfuse service\" , ) secret_key : SecretStr = Field ( ... , description = \"Secret API key for authenticating with the Langfuse service\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Langfuse server\" ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"Network protocol to use when connecting to the Langfuse server\" , ) port : int = Field ( 3000 , description = \"TCP port number on which the Langfuse server is listening\" , ) database : LangfuseDatabaseConfiguration = Field ( description = \"Database connection configuration for the Langfuse server\" , default_factory = LangfuseDatabaseConfiguration , ) datasets : LangfuseDatasetsConfiguration = Field ( description = \"Configuration for all datasets managed in the Langfuse platform\" , default_factory = LangfuseDatasetsConfiguration , ) chainlit_tag_format : str = Field ( \"chainlit_message_id: {message_id} \" , description = \"Template string for generating tags that link Chainlit messages to Langfuse traces\" , ) retention_job : LangfuseRetentionJobConfiguration = Field ( description = \"Configuration for the Langfuse retention job that manages trace data lifecycle\" , default_factory = LangfuseRetentionJobConfiguration , ) secrets : Secrets = Field ( None , description = \"API authentication credentials for the Langfuse service\" , ) @property def url ( self ) -> str : \"\"\"Generate the complete URL for connecting to the Langfuse server. Returns: str: Fully formatted URL with protocol, host, and port \"\"\" return f \" { self . protocol } :// { self . host } : { self . port } \"","title":"LangfuseConfiguration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseConfiguration.url","text":"Generate the complete URL for connecting to the Langfuse server. Returns: str ( str ) \u2013 Fully formatted URL with protocol, host, and port","title":"url"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseConfiguration.Secrets","text":"Bases: BaseSecrets API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class Secrets ( BaseSecrets ): \"\"\"API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) public_key : SecretStr = Field ( ... , description = \"Public API key for authenticating with the Langfuse service\" , ) secret_key : SecretStr = Field ( ... , description = \"Secret API key for authenticating with the Langfuse service\" , )","title":"Secrets"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseDatabaseConfiguration","text":"Bases: BaseConfigurationWithSecrets Configuration for connecting to the Langfuse PostgreSQL database. Contains all database connection parameters and credentials required for the Langfuse observation and analytics platform. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class LangfuseDatabaseConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Configuration for connecting to the Langfuse PostgreSQL database. Contains all database connection parameters and credentials required for the Langfuse observation and analytics platform. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__DATABASE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) user : SecretStr = Field ( ... , description = \"Username for authenticating with the Langfuse database server\" , ) password : SecretStr = Field ( ... , description = \"Password for authenticating with the Langfuse database server\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Langfuse database server\" , ) port : int = Field ( 5432 , description = \"TCP port number on which the Langfuse database server is listening\" , ) db : str = Field ( \"langfuse\" , description = \"Name of the specific database to connect to on the Langfuse server\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials for the Langfuse database\" )","title":"LangfuseDatabaseConfiguration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseDatabaseConfiguration.Secrets","text":"Bases: BaseSecrets Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Secrets ( BaseSecrets ): \"\"\"Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__DATABASE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) user : SecretStr = Field ( ... , description = \"Username for authenticating with the Langfuse database server\" , ) password : SecretStr = Field ( ... , description = \"Password for authenticating with the Langfuse database server\" , )","title":"Secrets"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseDatasetConfiguration","text":"Bases: BaseConfiguration Configuration for a single dataset in Langfuse. Defines properties of a dataset that will be created or used in the Langfuse platform. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class LangfuseDatasetConfiguration ( BaseConfiguration ): \"\"\"Configuration for a single dataset in Langfuse. Defines properties of a dataset that will be created or used in the Langfuse platform. \"\"\" name : str = Field ( ... , description = \"Unique identifier for the dataset in the Langfuse server\" , ) description : str = Field ( ... , description = \"Human-readable explanation of the dataset's purpose and contents\" , ) metadata : dict = Field ( {}, description = \"Additional structured information about the dataset as key-value pairs\" , )","title":"LangfuseDatasetConfiguration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseDatasetsConfiguration","text":"Bases: BaseConfiguration Configuration for all datasets used in the Langfuse platform. Contains definitions for specialized datasets used for different purposes. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class LangfuseDatasetsConfiguration ( BaseConfiguration ): \"\"\"Configuration for all datasets used in the Langfuse platform. Contains definitions for specialized datasets used for different purposes. \"\"\" feedback_dataset : LangfuseDatasetConfiguration = Field ( LangfuseDatasetConfiguration ( name = \"feedback-dataset\" , description = \"Dataset created out of positive feedbacks from the chatbot\" , ), description = \"Dataset for storing and analyzing user feedback collected from the chatbot interactions\" , ) manual_dataset : LangfuseDatasetConfiguration = Field ( LangfuseDatasetConfiguration ( name = \"manual-dataset\" , description = \"Dataset created directly by the user indicating the query and the correct answer\" , ), description = \"Dataset for storing manually curated query-answer pairs for evaluation and training\" , )","title":"LangfuseDatasetsConfiguration"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/","text":"Temporal_domain_config This module contains functionality related to the the temporal_domain_config module for augmentation.bootstrap.configuration . Temporal_domain_config Temporal domain configuration for domain-specific temporal logic. This module provides configuration for temporal filtering, query expansion, and metadata handling that varies by deployment domain (e.g., Bundestag, EU Parliament, etc.). PeriodDefinition Bases: BaseModel Definition of a temporal period. Example { \"names\": [\"20. Wahlperiode\", \"WP20\", \"20th legislature\"], \"years\": [2021, 2022, 2023, 2024], \"temporal_type\": \"historical\" } Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class PeriodDefinition ( BaseModel ): \"\"\"Definition of a temporal period. Example: { \"names\": [\"20. Wahlperiode\", \"WP20\", \"20th legislature\"], \"years\": [2021, 2022, 2023, 2024], \"temporal_type\": \"historical\" } \"\"\" names : List [ str ] = Field ( default_factory = list , description = \"Period identifiers and variations\" ) years : List [ int ] = Field ( default_factory = list , description = \"Years covered by this period\" ) temporal_type : str = Field ( description = \"Temporal classification: 'current' or 'historical'\" ) QueryExpansionTerms Bases: BaseModel Query expansion terms by language. Example { \"de\": \"Bundestag Fraktionen parlamentarische Gruppen\", \"en\": \"parliament fractions parliamentary groups\" } Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class QueryExpansionTerms ( BaseModel ): \"\"\"Query expansion terms by language. Example: { \"de\": \"Bundestag Fraktionen parlamentarische Gruppen\", \"en\": \"parliament fractions parliamentary groups\" } \"\"\" de : str = Field ( default = \"\" , description = \"German expansion terms\" ) en : str = Field ( default = \"\" , description = \"English expansion terms\" ) fr : str = Field ( default = \"\" , description = \"French expansion terms\" ) es : str = Field ( default = \"\" , description = \"Spanish expansion terms\" ) TemporalDomainConfiguration Bases: BaseModel Domain-specific configuration for temporal RAG system. Configures temporal filtering, query expansion, and metadata handling for specific deployment domains (parliamentary systems, news archives, etc.). If not provided, the system runs in generic mode without temporal filtering. Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class TemporalDomainConfiguration ( BaseModel ): \"\"\"Domain-specific configuration for temporal RAG system. Configures temporal filtering, query expansion, and metadata handling for specific deployment domains (parliamentary systems, news archives, etc.). If not provided, the system runs in generic mode without temporal filtering. \"\"\" name : str = Field ( description = \"Domain identifier (e.g., 'bundestag', 'eu_parliament')\" ) metadata_schema : Dict [ str , Any ] = Field ( description = \"Metadata field names and period definitions\" , examples = [ { \"temporal_field\" : \"legislature_period\" , \"current_period\" : 21 , \"historical_period\" : 20 , } ], ) temporal_keywords : Dict [ str , TemporalKeywords ] = Field ( default_factory = dict , description = \"Keywords for 'current' and 'historical' filtering\" , examples = [ { \"current\" : { \"en\" : [ \"current\" , \"recent\" ], \"de\" : [ \"aktuell\" , \"neueste\" ], }, \"historical\" : { \"en\" : [ \"previous\" , \"past\" ], \"de\" : [ \"vorherig\" , \"vergangen\" ], }, } ], ) period_identifiers : Dict [ str , PeriodDefinition ] = Field ( default_factory = dict , description = \"Period definitions with years and identifiers\" , examples = [ { \"20\" : { \"names\" : [ \"20. Wahlperiode\" , \"WP20\" ], \"years\" : [ 2021 , 2022 , 2023 , 2024 ], \"temporal_type\" : \"historical\" , } } ], ) query_expansion : Dict [ str , QueryExpansionTerms ] = Field ( default_factory = dict , description = \"Query expansion templates for different query types\" , examples = [ { \"temporal_current\" : { \"de\" : \"21. Wahlperiode 2025 aktuelle Bundesregierung\" }, \"temporal_historical\" : { \"de\" : \"20. Wahlperiode 2021-2024 fr\u00fchere Bundesregierung\" }, } ], ) language_detection : Dict [ str , List [ str ]] = Field ( default_factory = dict , description = \"Language detection patterns\" , examples = [ { \"de\" : [ \"wer\" , \"was\" , \"welche\" , \"der\" , \"die\" , \"das\" ], \"en\" : [ \"who\" , \"what\" , \"which\" , \"the\" ], } ], ) @property def current_period_value ( self ) -> Optional [ int ]: \"\"\"Get current period number from metadata schema. Returns: Current period number, or None if not configured \"\"\" return self . metadata_schema . get ( \"current_period\" ) @property def historical_period_value ( self ) -> Optional [ int ]: \"\"\"Get historical period number from metadata schema. Returns: Historical period number, or None if not configured \"\"\" return self . metadata_schema . get ( \"historical_period\" ) @property def temporal_field_name ( self ) -> str : \"\"\"Get metadata field name for temporal filtering. Returns: Metadata field name (default: 'period') \"\"\" return self . metadata_schema . get ( \"temporal_field\" , \"period\" ) def get_all_current_keywords ( self ) -> List [ str ]: \"\"\"Get all 'current' keywords across all configured languages. Returns: List of all current keywords \"\"\" keywords = [] current_kw = self . temporal_keywords . get ( \"current\" ) if current_kw : for lang_keywords in [ current_kw . en , current_kw . de , current_kw . fr , current_kw . es , ]: keywords . extend ( lang_keywords ) return keywords def get_all_historical_keywords ( self ) -> List [ str ]: \"\"\"Get all 'historical' keywords across all configured languages. Includes both language keywords and period identifiers. Returns: List of all historical keywords \"\"\" keywords = [] # Add language-specific keywords historical_kw = self . temporal_keywords . get ( \"historical\" ) if historical_kw : for lang_keywords in [ historical_kw . en , historical_kw . de , historical_kw . fr , historical_kw . es , ]: keywords . extend ( lang_keywords ) # Add period identifiers for historical periods for period_id , period_def in self . period_identifiers . items (): if period_def . temporal_type == \"historical\" : keywords . extend ( period_def . names ) return keywords def detect_language ( self , query : str ) -> str : \"\"\"Detect query language using configured patterns. Args: query: User query string Returns: Detected language code (default: 'en') \"\"\" query_lower = query . lower () for lang , indicators in self . language_detection . items (): if any ( indicator in query_lower for indicator in indicators ): return lang return \"en\" # Default to English def get_expansion_terms ( self , expansion_type : str , language : str = \"en\" ) -> str : \"\"\"Get query expansion terms for a specific type and language. Args: expansion_type: Type of expansion ('temporal_current', 'temporal_historical', 'entity_terms') language: Language code ('en', 'de', 'fr', 'es') Returns: Expansion term string, or empty string if not found \"\"\" expansion = self . query_expansion . get ( expansion_type ) if not expansion : return \"\" return getattr ( expansion , language , \"\" ) current_period_value property Get current period number from metadata schema. Returns: Optional [ int ] \u2013 Current period number, or None if not configured historical_period_value property Get historical period number from metadata schema. Returns: Optional [ int ] \u2013 Historical period number, or None if not configured temporal_field_name property Get metadata field name for temporal filtering. Returns: str \u2013 Metadata field name (default: 'period') detect_language ( query ) Detect query language using configured patterns. Parameters: query ( str ) \u2013 User query string Returns: str \u2013 Detected language code (default: 'en') Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def detect_language ( self , query : str ) -> str : \"\"\"Detect query language using configured patterns. Args: query: User query string Returns: Detected language code (default: 'en') \"\"\" query_lower = query . lower () for lang , indicators in self . language_detection . items (): if any ( indicator in query_lower for indicator in indicators ): return lang return \"en\" # Default to English get_all_current_keywords () Get all 'current' keywords across all configured languages. Returns: List [ str ] \u2013 List of all current keywords Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def get_all_current_keywords ( self ) -> List [ str ]: \"\"\"Get all 'current' keywords across all configured languages. Returns: List of all current keywords \"\"\" keywords = [] current_kw = self . temporal_keywords . get ( \"current\" ) if current_kw : for lang_keywords in [ current_kw . en , current_kw . de , current_kw . fr , current_kw . es , ]: keywords . extend ( lang_keywords ) return keywords get_all_historical_keywords () Get all 'historical' keywords across all configured languages. Includes both language keywords and period identifiers. Returns: List [ str ] \u2013 List of all historical keywords Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def get_all_historical_keywords ( self ) -> List [ str ]: \"\"\"Get all 'historical' keywords across all configured languages. Includes both language keywords and period identifiers. Returns: List of all historical keywords \"\"\" keywords = [] # Add language-specific keywords historical_kw = self . temporal_keywords . get ( \"historical\" ) if historical_kw : for lang_keywords in [ historical_kw . en , historical_kw . de , historical_kw . fr , historical_kw . es , ]: keywords . extend ( lang_keywords ) # Add period identifiers for historical periods for period_id , period_def in self . period_identifiers . items (): if period_def . temporal_type == \"historical\" : keywords . extend ( period_def . names ) return keywords get_expansion_terms ( expansion_type , language = 'en' ) Get query expansion terms for a specific type and language. Parameters: expansion_type ( str ) \u2013 Type of expansion ('temporal_current', 'temporal_historical', 'entity_terms') language ( str , default: 'en' ) \u2013 Language code ('en', 'de', 'fr', 'es') Returns: str \u2013 Expansion term string, or empty string if not found Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def get_expansion_terms ( self , expansion_type : str , language : str = \"en\" ) -> str : \"\"\"Get query expansion terms for a specific type and language. Args: expansion_type: Type of expansion ('temporal_current', 'temporal_historical', 'entity_terms') language: Language code ('en', 'de', 'fr', 'es') Returns: Expansion term string, or empty string if not found \"\"\" expansion = self . query_expansion . get ( expansion_type ) if not expansion : return \"\" return getattr ( expansion , language , \"\" ) TemporalKeywords Bases: BaseModel Keywords for temporal filtering by language. Example { \"en\": [\"current\", \"recent\", \"latest\"], \"de\": [\"aktuell\", \"neueste\", \"jetzt\"] } Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class TemporalKeywords ( BaseModel ): \"\"\"Keywords for temporal filtering by language. Example: { \"en\": [\"current\", \"recent\", \"latest\"], \"de\": [\"aktuell\", \"neueste\", \"jetzt\"] } \"\"\" en : List [ str ] = Field ( default_factory = list , description = \"English keywords\" ) de : List [ str ] = Field ( default_factory = list , description = \"German keywords\" ) fr : List [ str ] = Field ( default_factory = list , description = \"French keywords\" ) es : List [ str ] = Field ( default_factory = list , description = \"Spanish keywords\" )","title":"Temporal_domain_config"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#temporal_domain_config","text":"This module contains functionality related to the the temporal_domain_config module for augmentation.bootstrap.configuration .","title":"Temporal_domain_config"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#temporal_domain_config_1","text":"Temporal domain configuration for domain-specific temporal logic. This module provides configuration for temporal filtering, query expansion, and metadata handling that varies by deployment domain (e.g., Bundestag, EU Parliament, etc.).","title":"Temporal_domain_config"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.PeriodDefinition","text":"Bases: BaseModel Definition of a temporal period. Example { \"names\": [\"20. Wahlperiode\", \"WP20\", \"20th legislature\"], \"years\": [2021, 2022, 2023, 2024], \"temporal_type\": \"historical\" } Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class PeriodDefinition ( BaseModel ): \"\"\"Definition of a temporal period. Example: { \"names\": [\"20. Wahlperiode\", \"WP20\", \"20th legislature\"], \"years\": [2021, 2022, 2023, 2024], \"temporal_type\": \"historical\" } \"\"\" names : List [ str ] = Field ( default_factory = list , description = \"Period identifiers and variations\" ) years : List [ int ] = Field ( default_factory = list , description = \"Years covered by this period\" ) temporal_type : str = Field ( description = \"Temporal classification: 'current' or 'historical'\" )","title":"PeriodDefinition"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.QueryExpansionTerms","text":"Bases: BaseModel Query expansion terms by language. Example { \"de\": \"Bundestag Fraktionen parlamentarische Gruppen\", \"en\": \"parliament fractions parliamentary groups\" } Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class QueryExpansionTerms ( BaseModel ): \"\"\"Query expansion terms by language. Example: { \"de\": \"Bundestag Fraktionen parlamentarische Gruppen\", \"en\": \"parliament fractions parliamentary groups\" } \"\"\" de : str = Field ( default = \"\" , description = \"German expansion terms\" ) en : str = Field ( default = \"\" , description = \"English expansion terms\" ) fr : str = Field ( default = \"\" , description = \"French expansion terms\" ) es : str = Field ( default = \"\" , description = \"Spanish expansion terms\" )","title":"QueryExpansionTerms"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalDomainConfiguration","text":"Bases: BaseModel Domain-specific configuration for temporal RAG system. Configures temporal filtering, query expansion, and metadata handling for specific deployment domains (parliamentary systems, news archives, etc.). If not provided, the system runs in generic mode without temporal filtering. Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class TemporalDomainConfiguration ( BaseModel ): \"\"\"Domain-specific configuration for temporal RAG system. Configures temporal filtering, query expansion, and metadata handling for specific deployment domains (parliamentary systems, news archives, etc.). If not provided, the system runs in generic mode without temporal filtering. \"\"\" name : str = Field ( description = \"Domain identifier (e.g., 'bundestag', 'eu_parliament')\" ) metadata_schema : Dict [ str , Any ] = Field ( description = \"Metadata field names and period definitions\" , examples = [ { \"temporal_field\" : \"legislature_period\" , \"current_period\" : 21 , \"historical_period\" : 20 , } ], ) temporal_keywords : Dict [ str , TemporalKeywords ] = Field ( default_factory = dict , description = \"Keywords for 'current' and 'historical' filtering\" , examples = [ { \"current\" : { \"en\" : [ \"current\" , \"recent\" ], \"de\" : [ \"aktuell\" , \"neueste\" ], }, \"historical\" : { \"en\" : [ \"previous\" , \"past\" ], \"de\" : [ \"vorherig\" , \"vergangen\" ], }, } ], ) period_identifiers : Dict [ str , PeriodDefinition ] = Field ( default_factory = dict , description = \"Period definitions with years and identifiers\" , examples = [ { \"20\" : { \"names\" : [ \"20. Wahlperiode\" , \"WP20\" ], \"years\" : [ 2021 , 2022 , 2023 , 2024 ], \"temporal_type\" : \"historical\" , } } ], ) query_expansion : Dict [ str , QueryExpansionTerms ] = Field ( default_factory = dict , description = \"Query expansion templates for different query types\" , examples = [ { \"temporal_current\" : { \"de\" : \"21. Wahlperiode 2025 aktuelle Bundesregierung\" }, \"temporal_historical\" : { \"de\" : \"20. Wahlperiode 2021-2024 fr\u00fchere Bundesregierung\" }, } ], ) language_detection : Dict [ str , List [ str ]] = Field ( default_factory = dict , description = \"Language detection patterns\" , examples = [ { \"de\" : [ \"wer\" , \"was\" , \"welche\" , \"der\" , \"die\" , \"das\" ], \"en\" : [ \"who\" , \"what\" , \"which\" , \"the\" ], } ], ) @property def current_period_value ( self ) -> Optional [ int ]: \"\"\"Get current period number from metadata schema. Returns: Current period number, or None if not configured \"\"\" return self . metadata_schema . get ( \"current_period\" ) @property def historical_period_value ( self ) -> Optional [ int ]: \"\"\"Get historical period number from metadata schema. Returns: Historical period number, or None if not configured \"\"\" return self . metadata_schema . get ( \"historical_period\" ) @property def temporal_field_name ( self ) -> str : \"\"\"Get metadata field name for temporal filtering. Returns: Metadata field name (default: 'period') \"\"\" return self . metadata_schema . get ( \"temporal_field\" , \"period\" ) def get_all_current_keywords ( self ) -> List [ str ]: \"\"\"Get all 'current' keywords across all configured languages. Returns: List of all current keywords \"\"\" keywords = [] current_kw = self . temporal_keywords . get ( \"current\" ) if current_kw : for lang_keywords in [ current_kw . en , current_kw . de , current_kw . fr , current_kw . es , ]: keywords . extend ( lang_keywords ) return keywords def get_all_historical_keywords ( self ) -> List [ str ]: \"\"\"Get all 'historical' keywords across all configured languages. Includes both language keywords and period identifiers. Returns: List of all historical keywords \"\"\" keywords = [] # Add language-specific keywords historical_kw = self . temporal_keywords . get ( \"historical\" ) if historical_kw : for lang_keywords in [ historical_kw . en , historical_kw . de , historical_kw . fr , historical_kw . es , ]: keywords . extend ( lang_keywords ) # Add period identifiers for historical periods for period_id , period_def in self . period_identifiers . items (): if period_def . temporal_type == \"historical\" : keywords . extend ( period_def . names ) return keywords def detect_language ( self , query : str ) -> str : \"\"\"Detect query language using configured patterns. Args: query: User query string Returns: Detected language code (default: 'en') \"\"\" query_lower = query . lower () for lang , indicators in self . language_detection . items (): if any ( indicator in query_lower for indicator in indicators ): return lang return \"en\" # Default to English def get_expansion_terms ( self , expansion_type : str , language : str = \"en\" ) -> str : \"\"\"Get query expansion terms for a specific type and language. Args: expansion_type: Type of expansion ('temporal_current', 'temporal_historical', 'entity_terms') language: Language code ('en', 'de', 'fr', 'es') Returns: Expansion term string, or empty string if not found \"\"\" expansion = self . query_expansion . get ( expansion_type ) if not expansion : return \"\" return getattr ( expansion , language , \"\" )","title":"TemporalDomainConfiguration"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalDomainConfiguration.current_period_value","text":"Get current period number from metadata schema. Returns: Optional [ int ] \u2013 Current period number, or None if not configured","title":"current_period_value"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalDomainConfiguration.historical_period_value","text":"Get historical period number from metadata schema. Returns: Optional [ int ] \u2013 Historical period number, or None if not configured","title":"historical_period_value"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalDomainConfiguration.temporal_field_name","text":"Get metadata field name for temporal filtering. Returns: str \u2013 Metadata field name (default: 'period')","title":"temporal_field_name"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalDomainConfiguration.detect_language","text":"Detect query language using configured patterns. Parameters: query ( str ) \u2013 User query string Returns: str \u2013 Detected language code (default: 'en') Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def detect_language ( self , query : str ) -> str : \"\"\"Detect query language using configured patterns. Args: query: User query string Returns: Detected language code (default: 'en') \"\"\" query_lower = query . lower () for lang , indicators in self . language_detection . items (): if any ( indicator in query_lower for indicator in indicators ): return lang return \"en\" # Default to English","title":"detect_language"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalDomainConfiguration.get_all_current_keywords","text":"Get all 'current' keywords across all configured languages. Returns: List [ str ] \u2013 List of all current keywords Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def get_all_current_keywords ( self ) -> List [ str ]: \"\"\"Get all 'current' keywords across all configured languages. Returns: List of all current keywords \"\"\" keywords = [] current_kw = self . temporal_keywords . get ( \"current\" ) if current_kw : for lang_keywords in [ current_kw . en , current_kw . de , current_kw . fr , current_kw . es , ]: keywords . extend ( lang_keywords ) return keywords","title":"get_all_current_keywords"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalDomainConfiguration.get_all_historical_keywords","text":"Get all 'historical' keywords across all configured languages. Includes both language keywords and period identifiers. Returns: List [ str ] \u2013 List of all historical keywords Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def get_all_historical_keywords ( self ) -> List [ str ]: \"\"\"Get all 'historical' keywords across all configured languages. Includes both language keywords and period identifiers. Returns: List of all historical keywords \"\"\" keywords = [] # Add language-specific keywords historical_kw = self . temporal_keywords . get ( \"historical\" ) if historical_kw : for lang_keywords in [ historical_kw . en , historical_kw . de , historical_kw . fr , historical_kw . es , ]: keywords . extend ( lang_keywords ) # Add period identifiers for historical periods for period_id , period_def in self . period_identifiers . items (): if period_def . temporal_type == \"historical\" : keywords . extend ( period_def . names ) return keywords","title":"get_all_historical_keywords"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalDomainConfiguration.get_expansion_terms","text":"Get query expansion terms for a specific type and language. Parameters: expansion_type ( str ) \u2013 Type of expansion ('temporal_current', 'temporal_historical', 'entity_terms') language ( str , default: 'en' ) \u2013 Language code ('en', 'de', 'fr', 'es') Returns: str \u2013 Expansion term string, or empty string if not found Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def get_expansion_terms ( self , expansion_type : str , language : str = \"en\" ) -> str : \"\"\"Get query expansion terms for a specific type and language. Args: expansion_type: Type of expansion ('temporal_current', 'temporal_historical', 'entity_terms') language: Language code ('en', 'de', 'fr', 'es') Returns: Expansion term string, or empty string if not found \"\"\" expansion = self . query_expansion . get ( expansion_type ) if not expansion : return \"\" return getattr ( expansion , language , \"\" )","title":"get_expansion_terms"},{"location":"src/augmentation/bootstrap/configuration/temporal_domain_config/#src.augmentation.bootstrap.configuration.temporal_domain_config.TemporalKeywords","text":"Bases: BaseModel Keywords for temporal filtering by language. Example { \"en\": [\"current\", \"recent\", \"latest\"], \"de\": [\"aktuell\", \"neueste\", \"jetzt\"] } Source code in src/augmentation/bootstrap/configuration/temporal_domain_config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class TemporalKeywords ( BaseModel ): \"\"\"Keywords for temporal filtering by language. Example: { \"en\": [\"current\", \"recent\", \"latest\"], \"de\": [\"aktuell\", \"neueste\", \"jetzt\"] } \"\"\" en : List [ str ] = Field ( default_factory = list , description = \"English keywords\" ) de : List [ str ] = Field ( default_factory = list , description = \"German keywords\" ) fr : List [ str ] = Field ( default_factory = list , description = \"French keywords\" ) es : List [ str ] = Field ( default_factory = list , description = \"Spanish keywords\" )","title":"TemporalKeywords"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/","text":"Chat_engine_configuration This module contains functionality related to the the chat_engine_configuration module for augmentation.bootstrap.configuration.components . Chat_engine_configuration BaseChatEngineConfiguration Bases: BaseConfiguration Base configuration class for chat engines. This class defines the standard configuration structure for all chat engines, including retriever, llm, and postprocessor components. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class BaseChatEngineConfiguration ( BaseConfiguration ): \"\"\"Base configuration class for chat engines. This class defines the standard configuration structure for all chat engines, including retriever, llm, and postprocessor components. \"\"\" guardrails : Any = Field ( ... , description = \"Optional guardrails configuration for the chat engine.\" , ) retriever : Any = Field ( ... , description = \"The retriever configuration for the augmentation pipeline.\" , ) llm : Any = Field ( ... , description = \"The llm configuration for the chat engine.\" ) postprocessors : List [ Any ] = Field ( ... , description = \"The list of postprocessors for the chat engine.\" ) prompt_templates : ChatEnginePromptTemplates = Field ( ... , description = \"The prompt templates configuration for the chat engine.\" , default_factory = ChatEnginePromptTemplates , ) @field_validator ( \"guardrails\" ) @classmethod def _validate_guardrails ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate retriever configuration against registered retriever types. Args: value: The retriever configuration to validate info: Validation context information Returns: Validated retriever configuration \"\"\" return super () . _validate ( value , info = info , registry = GurdrailsConfigurationRegistry , ) @field_validator ( \"retriever\" ) @classmethod def _validate_retriever ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate retriever configuration against registered retriever types. Args: value: The retriever configuration to validate info: Validation context information Returns: Validated retriever configuration \"\"\" return super () . _validate ( value , info = info , registry = RetrieverConfigurationRegistry , ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate llm configuration against registered llm types. Args: value: The llm configuration to validate info: Validation context information Returns: Validated llm configuration \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , ) @field_validator ( \"postprocessors\" ) @classmethod def _validate_postprocessors ( cls , value : Any , info : ValidationInfo ) -> List [ Any ]: \"\"\"Validate postprocessors configuration against registered postprocessor types. Args: value: The postprocessor configurations to validate info: Validation context information Returns: List of validated postprocessor configurations \"\"\" return super () . _validate ( value , info = info , registry = PostProcessorConfigurationRegistry , ) ChatEngineConfigurationRegistry Bases: ConfigurationRegistry Registry for chat engine configurations. Maps chat engine names to their corresponding configuration classes to facilitate configuration validation and factory creation. Attributes: _key_class ( Type ) \u2013 The enumeration class for chat engine names. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 159 160 161 162 163 164 165 166 167 168 169 class ChatEngineConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for chat engine configurations. Maps chat engine names to their corresponding configuration classes to facilitate configuration validation and factory creation. Attributes: _key_class: The enumeration class for chat engine names. \"\"\" _key_class : Type = ChatEngineName ChatEngineName Bases: str , Enum Enum defining available chat engine types. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 22 23 24 25 class ChatEngineName ( str , Enum ): \"\"\"Enum defining available chat engine types.\"\"\" LANGFUSE = \"langfuse\"","title":"Chat_engine_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#chat_engine_configuration","text":"This module contains functionality related to the the chat_engine_configuration module for augmentation.bootstrap.configuration.components .","title":"Chat_engine_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#chat_engine_configuration_1","text":"","title":"Chat_engine_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#src.augmentation.bootstrap.configuration.components.chat_engine_configuration.BaseChatEngineConfiguration","text":"Bases: BaseConfiguration Base configuration class for chat engines. This class defines the standard configuration structure for all chat engines, including retriever, llm, and postprocessor components. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class BaseChatEngineConfiguration ( BaseConfiguration ): \"\"\"Base configuration class for chat engines. This class defines the standard configuration structure for all chat engines, including retriever, llm, and postprocessor components. \"\"\" guardrails : Any = Field ( ... , description = \"Optional guardrails configuration for the chat engine.\" , ) retriever : Any = Field ( ... , description = \"The retriever configuration for the augmentation pipeline.\" , ) llm : Any = Field ( ... , description = \"The llm configuration for the chat engine.\" ) postprocessors : List [ Any ] = Field ( ... , description = \"The list of postprocessors for the chat engine.\" ) prompt_templates : ChatEnginePromptTemplates = Field ( ... , description = \"The prompt templates configuration for the chat engine.\" , default_factory = ChatEnginePromptTemplates , ) @field_validator ( \"guardrails\" ) @classmethod def _validate_guardrails ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate retriever configuration against registered retriever types. Args: value: The retriever configuration to validate info: Validation context information Returns: Validated retriever configuration \"\"\" return super () . _validate ( value , info = info , registry = GurdrailsConfigurationRegistry , ) @field_validator ( \"retriever\" ) @classmethod def _validate_retriever ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate retriever configuration against registered retriever types. Args: value: The retriever configuration to validate info: Validation context information Returns: Validated retriever configuration \"\"\" return super () . _validate ( value , info = info , registry = RetrieverConfigurationRegistry , ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate llm configuration against registered llm types. Args: value: The llm configuration to validate info: Validation context information Returns: Validated llm configuration \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , ) @field_validator ( \"postprocessors\" ) @classmethod def _validate_postprocessors ( cls , value : Any , info : ValidationInfo ) -> List [ Any ]: \"\"\"Validate postprocessors configuration against registered postprocessor types. Args: value: The postprocessor configurations to validate info: Validation context information Returns: List of validated postprocessor configurations \"\"\" return super () . _validate ( value , info = info , registry = PostProcessorConfigurationRegistry , )","title":"BaseChatEngineConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#src.augmentation.bootstrap.configuration.components.chat_engine_configuration.ChatEngineConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for chat engine configurations. Maps chat engine names to their corresponding configuration classes to facilitate configuration validation and factory creation. Attributes: _key_class ( Type ) \u2013 The enumeration class for chat engine names. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 159 160 161 162 163 164 165 166 167 168 169 class ChatEngineConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for chat engine configurations. Maps chat engine names to their corresponding configuration classes to facilitate configuration validation and factory creation. Attributes: _key_class: The enumeration class for chat engine names. \"\"\" _key_class : Type = ChatEngineName","title":"ChatEngineConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#src.augmentation.bootstrap.configuration.components.chat_engine_configuration.ChatEngineName","text":"Bases: str , Enum Enum defining available chat engine types. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 22 23 24 25 class ChatEngineName ( str , Enum ): \"\"\"Enum defining available chat engine types.\"\"\" LANGFUSE = \"langfuse\"","title":"ChatEngineName"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/","text":"Guardrails_configuration This module contains functionality related to the the guardrails_configuration module for augmentation.bootstrap.configuration.components . Guardrails_configuration GuardrailsConfiguration Bases: BaseConfiguration Configuration settings for guardrails. This class defines the necessary parameters for configuring and interacting with guardrails used in the system. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class GuardrailsConfiguration ( BaseConfiguration ): \"\"\" Configuration settings for guardrails. This class defines the necessary parameters for configuring and interacting with guardrails used in the system. \"\"\" llm : Any = Field ( ... , description = \"The llm configuration for the guardrails.\" ) input_prompt_name : str = Field ( \"default_input_guardrail_prompt\" , description = ( \"The name of the input guardrail prompt to use available in Langfuse prompts.\" , \"The prompt is used to determine if the input is valid to be passed to the chat engine.\" , \"The LLM should respond with 'yes' or 'true' if the input should be blocked.\" , ), ) output_prompt_name : str = Field ( \"default_output_guardrail_prompt\" , description = ( \"The name of the output guardrail prompt to use available in Langfuse prompts.\" , \"The prompt is used to determine if the output of the chat engine is valid to be returned to the user.\" , \"The LLM should respond with 'yes' or 'true' if the output should be blocked.\" , ), ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate llm configuration against registered llm types. Args: value: The llm configuration to validate info: Validation context information Returns: Validated llm configuration \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , ) GuardrailsName Bases: str , Enum Enumeration of supported guardrails. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 13 14 15 16 17 18 class GuardrailsName ( str , Enum ): \"\"\" Enumeration of supported guardrails. \"\"\" BASIC = \"basic\" GurdrailsConfigurationRegistry Bases: ConfigurationRegistry Registry for guardrails configurations. This registry maintains a mapping between guardrails names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class ( Type ) \u2013 The enumeration class for guardrails names. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class GurdrailsConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for guardrails configurations. This registry maintains a mapping between guardrails names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class: The enumeration class for guardrails names. \"\"\" _key_class : Type = GuardrailsName","title":"Guardrails_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#guardrails_configuration","text":"This module contains functionality related to the the guardrails_configuration module for augmentation.bootstrap.configuration.components .","title":"Guardrails_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#guardrails_configuration_1","text":"","title":"Guardrails_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#src.augmentation.bootstrap.configuration.components.guardrails_configuration.GuardrailsConfiguration","text":"Bases: BaseConfiguration Configuration settings for guardrails. This class defines the necessary parameters for configuring and interacting with guardrails used in the system. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class GuardrailsConfiguration ( BaseConfiguration ): \"\"\" Configuration settings for guardrails. This class defines the necessary parameters for configuring and interacting with guardrails used in the system. \"\"\" llm : Any = Field ( ... , description = \"The llm configuration for the guardrails.\" ) input_prompt_name : str = Field ( \"default_input_guardrail_prompt\" , description = ( \"The name of the input guardrail prompt to use available in Langfuse prompts.\" , \"The prompt is used to determine if the input is valid to be passed to the chat engine.\" , \"The LLM should respond with 'yes' or 'true' if the input should be blocked.\" , ), ) output_prompt_name : str = Field ( \"default_output_guardrail_prompt\" , description = ( \"The name of the output guardrail prompt to use available in Langfuse prompts.\" , \"The prompt is used to determine if the output of the chat engine is valid to be returned to the user.\" , \"The LLM should respond with 'yes' or 'true' if the output should be blocked.\" , ), ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate llm configuration against registered llm types. Args: value: The llm configuration to validate info: Validation context information Returns: Validated llm configuration \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , )","title":"GuardrailsConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#src.augmentation.bootstrap.configuration.components.guardrails_configuration.GuardrailsName","text":"Bases: str , Enum Enumeration of supported guardrails. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 13 14 15 16 17 18 class GuardrailsName ( str , Enum ): \"\"\" Enumeration of supported guardrails. \"\"\" BASIC = \"basic\"","title":"GuardrailsName"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#src.augmentation.bootstrap.configuration.components.guardrails_configuration.GurdrailsConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for guardrails configurations. This registry maintains a mapping between guardrails names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class ( Type ) \u2013 The enumeration class for guardrails names. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class GurdrailsConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for guardrails configurations. This registry maintains a mapping between guardrails names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class: The enumeration class for guardrails names. \"\"\" _key_class : Type = GuardrailsName","title":"GurdrailsConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/","text":"Llm_configuration This module contains functionality related to the the llm_configuration module for augmentation.bootstrap.configuration.components . Llm_configuration LLMConfiguration Bases: BaseConfigurationWithSecrets Configuration settings for language models. This class defines the necessary parameters for configuring and interacting with language models (LLMs) used in the system. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class LLMConfiguration ( BaseConfigurationWithSecrets ): \"\"\" Configuration settings for language models. This class defines the necessary parameters for configuring and interacting with language models (LLMs) used in the system. \"\"\" name : str = Field ( ... , description = \"The name of the language model.\" ) max_tokens : int = Field ( ... , description = \"The maximum number of tokens for the language model.\" ) max_retries : int = Field ( ... , description = \"The maximum number of retries for the language model.\" ) context_window : Optional [ int ] = Field ( None , description = \"The maximum context window size for the language model. \" \"If not specified, uses the model's default from LiteLLM metadata.\" , ) num_output : Optional [ int ] = Field ( None , description = \"The maximum number of output tokens for the language model. \" \"If not specified, uses the model's default from LiteLLM metadata.\" , ) LLMConfigurationRegistry Bases: ConfigurationRegistry Registry for language model configurations. This registry maintains a mapping between LLM provider names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class ( Type ) \u2013 The enumeration class for LLM provider names. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class LLMConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for language model configurations. This registry maintains a mapping between LLM provider names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class: The enumeration class for LLM provider names. \"\"\" _key_class : Type = LLMProviderName LLMProviderName Bases: str , Enum Enumeration of supported language model providers. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 10 11 12 13 14 15 class LLMProviderName ( str , Enum ): \"\"\" Enumeration of supported language model providers. \"\"\" LITE_LLM = \"lite_llm\"","title":"Llm_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#llm_configuration","text":"This module contains functionality related to the the llm_configuration module for augmentation.bootstrap.configuration.components .","title":"Llm_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#llm_configuration_1","text":"","title":"Llm_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#src.augmentation.bootstrap.configuration.components.llm_configuration.LLMConfiguration","text":"Bases: BaseConfigurationWithSecrets Configuration settings for language models. This class defines the necessary parameters for configuring and interacting with language models (LLMs) used in the system. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class LLMConfiguration ( BaseConfigurationWithSecrets ): \"\"\" Configuration settings for language models. This class defines the necessary parameters for configuring and interacting with language models (LLMs) used in the system. \"\"\" name : str = Field ( ... , description = \"The name of the language model.\" ) max_tokens : int = Field ( ... , description = \"The maximum number of tokens for the language model.\" ) max_retries : int = Field ( ... , description = \"The maximum number of retries for the language model.\" ) context_window : Optional [ int ] = Field ( None , description = \"The maximum context window size for the language model. \" \"If not specified, uses the model's default from LiteLLM metadata.\" , ) num_output : Optional [ int ] = Field ( None , description = \"The maximum number of output tokens for the language model. \" \"If not specified, uses the model's default from LiteLLM metadata.\" , )","title":"LLMConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#src.augmentation.bootstrap.configuration.components.llm_configuration.LLMConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for language model configurations. This registry maintains a mapping between LLM provider names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class ( Type ) \u2013 The enumeration class for LLM provider names. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class LLMConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for language model configurations. This registry maintains a mapping between LLM provider names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class: The enumeration class for LLM provider names. \"\"\" _key_class : Type = LLMProviderName","title":"LLMConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#src.augmentation.bootstrap.configuration.components.llm_configuration.LLMProviderName","text":"Bases: str , Enum Enumeration of supported language model providers. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 10 11 12 13 14 15 class LLMProviderName ( str , Enum ): \"\"\" Enumeration of supported language model providers. \"\"\" LITE_LLM = \"lite_llm\"","title":"LLMProviderName"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/","text":"Postprocessors_configuration This module contains functionality related to the the postprocessors_configuration module for augmentation.bootstrap.configuration.components . Postprocessors_configuration PostProcessorConfiguration Bases: BaseConfiguration Base configuration for post-processors. Post-processors refine search results after initial retrieval to improve relevance and quality. Each post-processor implementation should inherit from this class and add its specific configuration parameters. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 20 21 22 23 24 25 26 27 28 29 30 class PostProcessorConfiguration ( BaseConfiguration ): \"\"\"Base configuration for post-processors. Post-processors refine search results after initial retrieval to improve relevance and quality. Each post-processor implementation should inherit from this class and add its specific configuration parameters. \"\"\" name : PostProcessorName = Field ( ... , description = \"The name of the postprocessor.\" ) PostProcessorConfigurationRegistry Bases: ConfigurationRegistry Registry for post-processor configurations. This registry maintains a collection of available post-processor configurations and provides methods for retrieving them based on the PostProcessorName. Attributes: _key_class ( Type ) \u2013 The enumeration class for post-processor names. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class PostProcessorConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for post-processor configurations. This registry maintains a collection of available post-processor configurations and provides methods for retrieving them based on the PostProcessorName. Attributes: _key_class: The enumeration class for post-processor names. \"\"\" _key_class : Type = PostProcessorName @classmethod def get_union_type ( self ) -> List [ PostProcessorConfiguration ]: \"\"\"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List[PostProcessorConfiguration]: A type representing all available post-processor configurations. \"\"\" return List [ super () . get_union_type ()] get_union_type () classmethod Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List [ PostProcessorConfiguration ] \u2013 List[PostProcessorConfiguration]: A type representing all available post-processor configurations. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 45 46 47 48 49 50 51 52 53 54 55 @classmethod def get_union_type ( self ) -> List [ PostProcessorConfiguration ]: \"\"\"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List[PostProcessorConfiguration]: A type representing all available post-processor configurations. \"\"\" return List [ super () . get_union_type ()] PostProcessorName Bases: str , Enum Enumeration of supported post-processor names. These identify the different post-processing options available for refining search results. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 10 11 12 13 14 15 16 17 class PostProcessorName ( str , Enum ): \"\"\"Enumeration of supported post-processor names. These identify the different post-processing options available for refining search results. \"\"\" COLBERT_RERANK = \"colbert_reranker\" HYBRID_FILTER = \"hybrid_filter\"","title":"Postprocessors_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#postprocessors_configuration","text":"This module contains functionality related to the the postprocessors_configuration module for augmentation.bootstrap.configuration.components .","title":"Postprocessors_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#postprocessors_configuration_1","text":"","title":"Postprocessors_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#src.augmentation.bootstrap.configuration.components.postprocessors_configuration.PostProcessorConfiguration","text":"Bases: BaseConfiguration Base configuration for post-processors. Post-processors refine search results after initial retrieval to improve relevance and quality. Each post-processor implementation should inherit from this class and add its specific configuration parameters. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 20 21 22 23 24 25 26 27 28 29 30 class PostProcessorConfiguration ( BaseConfiguration ): \"\"\"Base configuration for post-processors. Post-processors refine search results after initial retrieval to improve relevance and quality. Each post-processor implementation should inherit from this class and add its specific configuration parameters. \"\"\" name : PostProcessorName = Field ( ... , description = \"The name of the postprocessor.\" )","title":"PostProcessorConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#src.augmentation.bootstrap.configuration.components.postprocessors_configuration.PostProcessorConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for post-processor configurations. This registry maintains a collection of available post-processor configurations and provides methods for retrieving them based on the PostProcessorName. Attributes: _key_class ( Type ) \u2013 The enumeration class for post-processor names. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class PostProcessorConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for post-processor configurations. This registry maintains a collection of available post-processor configurations and provides methods for retrieving them based on the PostProcessorName. Attributes: _key_class: The enumeration class for post-processor names. \"\"\" _key_class : Type = PostProcessorName @classmethod def get_union_type ( self ) -> List [ PostProcessorConfiguration ]: \"\"\"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List[PostProcessorConfiguration]: A type representing all available post-processor configurations. \"\"\" return List [ super () . get_union_type ()]","title":"PostProcessorConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#src.augmentation.bootstrap.configuration.components.postprocessors_configuration.PostProcessorConfigurationRegistry.get_union_type","text":"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List [ PostProcessorConfiguration ] \u2013 List[PostProcessorConfiguration]: A type representing all available post-processor configurations. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 45 46 47 48 49 50 51 52 53 54 55 @classmethod def get_union_type ( self ) -> List [ PostProcessorConfiguration ]: \"\"\"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List[PostProcessorConfiguration]: A type representing all available post-processor configurations. \"\"\" return List [ super () . get_union_type ()]","title":"get_union_type"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#src.augmentation.bootstrap.configuration.components.postprocessors_configuration.PostProcessorName","text":"Bases: str , Enum Enumeration of supported post-processor names. These identify the different post-processing options available for refining search results. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 10 11 12 13 14 15 16 17 class PostProcessorName ( str , Enum ): \"\"\"Enumeration of supported post-processor names. These identify the different post-processing options available for refining search results. \"\"\" COLBERT_RERANK = \"colbert_reranker\" HYBRID_FILTER = \"hybrid_filter\"","title":"PostProcessorName"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/","text":"Retriever_configuration This module contains functionality related to the the retriever_configuration module for augmentation.bootstrap.configuration.components . Retriever_configuration RetrieverConfiguration Bases: BaseConfiguration Configuration class for retrievers. This class defines the parameters needed to configure a retriever component in the RAG pipeline. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 18 19 20 21 22 23 24 25 26 27 28 29 class RetrieverConfiguration ( BaseConfiguration ): \"\"\" Configuration class for retrievers. This class defines the parameters needed to configure a retriever component in the RAG pipeline. \"\"\" name : RetrieverName = Field ( ... , description = \"The name of the retriever.\" ) similarity_top_k : int = Field ( ... , description = \"The number of top similar items to retrieve.\" ) RetrieverConfigurationRegistry Bases: ConfigurationRegistry Registry for retriever configurations. Maps RetrieverName enum values to their corresponding configuration classes. Used for dynamic instantiation of retriever components based on configuration. Attributes: _key_class ( Type ) \u2013 The enumeration class for retriever names. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 32 33 34 35 36 37 38 39 40 41 42 43 class RetrieverConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for retriever configurations. Maps RetrieverName enum values to their corresponding configuration classes. Used for dynamic instantiation of retriever components based on configuration. Attributes: _key_class: The enumeration class for retriever names. \"\"\" _key_class : Type = RetrieverName RetrieverName Bases: str , Enum Enumeration of supported retriever types. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 10 11 12 13 14 15 class RetrieverName ( str , Enum ): \"\"\"Enumeration of supported retriever types.\"\"\" BASIC = \"basic\" AUTO = \"auto\" DYNAMIC_TEMPORAL = \"dynamic_temporal\"","title":"Retriever_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#retriever_configuration","text":"This module contains functionality related to the the retriever_configuration module for augmentation.bootstrap.configuration.components .","title":"Retriever_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#retriever_configuration_1","text":"","title":"Retriever_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#src.augmentation.bootstrap.configuration.components.retriever_configuration.RetrieverConfiguration","text":"Bases: BaseConfiguration Configuration class for retrievers. This class defines the parameters needed to configure a retriever component in the RAG pipeline. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 18 19 20 21 22 23 24 25 26 27 28 29 class RetrieverConfiguration ( BaseConfiguration ): \"\"\" Configuration class for retrievers. This class defines the parameters needed to configure a retriever component in the RAG pipeline. \"\"\" name : RetrieverName = Field ( ... , description = \"The name of the retriever.\" ) similarity_top_k : int = Field ( ... , description = \"The number of top similar items to retrieve.\" )","title":"RetrieverConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#src.augmentation.bootstrap.configuration.components.retriever_configuration.RetrieverConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for retriever configurations. Maps RetrieverName enum values to their corresponding configuration classes. Used for dynamic instantiation of retriever components based on configuration. Attributes: _key_class ( Type ) \u2013 The enumeration class for retriever names. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 32 33 34 35 36 37 38 39 40 41 42 43 class RetrieverConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for retriever configurations. Maps RetrieverName enum values to their corresponding configuration classes. Used for dynamic instantiation of retriever components based on configuration. Attributes: _key_class: The enumeration class for retriever names. \"\"\" _key_class : Type = RetrieverName","title":"RetrieverConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#src.augmentation.bootstrap.configuration.components.retriever_configuration.RetrieverName","text":"Bases: str , Enum Enumeration of supported retriever types. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 10 11 12 13 14 15 class RetrieverName ( str , Enum ): \"\"\"Enumeration of supported retriever types.\"\"\" BASIC = \"basic\" AUTO = \"auto\" DYNAMIC_TEMPORAL = \"dynamic_temporal\"","title":"RetrieverName"},{"location":"src/augmentation/chainlit/exceptions/","text":"Exceptions This module contains functionality related to the the exceptions module for augmentation.chainlit . Exceptions TraceNotFoundException Bases: Exception Exception raised when a trace for a given message ID cannot be found. This exception is typically thrown when attempting to access or manipulate a trace using a message ID that doesn't exist in the system. It helps in handling missing trace scenarios gracefully. Source code in src/augmentation/chainlit/exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class TraceNotFoundException ( Exception ): \"\"\"Exception raised when a trace for a given message ID cannot be found. This exception is typically thrown when attempting to access or manipulate a trace using a message ID that doesn't exist in the system. It helps in handling missing trace scenarios gracefully. \"\"\" def __init__ ( self , message_id : str ) -> None : \"\"\"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Args: message_id: ID of the message whose trace was not found. This ID is stored and used in the error message. \"\"\" self . message_id = message_id self . message = f \"Trace for message with id { message_id } not found\" super () . __init__ ( self . message ) __init__ ( message_id ) Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Parameters: message_id ( str ) \u2013 ID of the message whose trace was not found. This ID is stored and used in the error message. Source code in src/augmentation/chainlit/exceptions.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , message_id : str ) -> None : \"\"\"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Args: message_id: ID of the message whose trace was not found. This ID is stored and used in the error message. \"\"\" self . message_id = message_id self . message = f \"Trace for message with id { message_id } not found\" super () . __init__ ( self . message )","title":"Exceptions"},{"location":"src/augmentation/chainlit/exceptions/#exceptions","text":"This module contains functionality related to the the exceptions module for augmentation.chainlit .","title":"Exceptions"},{"location":"src/augmentation/chainlit/exceptions/#exceptions_1","text":"","title":"Exceptions"},{"location":"src/augmentation/chainlit/exceptions/#src.augmentation.chainlit.exceptions.TraceNotFoundException","text":"Bases: Exception Exception raised when a trace for a given message ID cannot be found. This exception is typically thrown when attempting to access or manipulate a trace using a message ID that doesn't exist in the system. It helps in handling missing trace scenarios gracefully. Source code in src/augmentation/chainlit/exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class TraceNotFoundException ( Exception ): \"\"\"Exception raised when a trace for a given message ID cannot be found. This exception is typically thrown when attempting to access or manipulate a trace using a message ID that doesn't exist in the system. It helps in handling missing trace scenarios gracefully. \"\"\" def __init__ ( self , message_id : str ) -> None : \"\"\"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Args: message_id: ID of the message whose trace was not found. This ID is stored and used in the error message. \"\"\" self . message_id = message_id self . message = f \"Trace for message with id { message_id } not found\" super () . __init__ ( self . message )","title":"TraceNotFoundException"},{"location":"src/augmentation/chainlit/exceptions/#src.augmentation.chainlit.exceptions.TraceNotFoundException.__init__","text":"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Parameters: message_id ( str ) \u2013 ID of the message whose trace was not found. This ID is stored and used in the error message. Source code in src/augmentation/chainlit/exceptions.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , message_id : str ) -> None : \"\"\"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Args: message_id: ID of the message whose trace was not found. This ID is stored and used in the error message. \"\"\" self . message_id = message_id self . message = f \"Trace for message with id { message_id } not found\" super () . __init__ ( self . message )","title":"__init__"},{"location":"src/augmentation/chainlit/feedback_service/","text":"Feedback_service This module contains functionality related to the the feedback_service module for augmentation.chainlit . Feedback_service ChainlitFeedbackService Service for handling Chainlit feedback and Langfuse integration. This service processes user feedback from the Chainlit UI and integrates it with Langfuse for tracking and analysis. It associates feedback with traces in Langfuse, saves positive feedback examples to datasets, and provides mechanisms for retrieving relevant trace information. The service tracks feedback scores, comments, and message content, allowing for detailed analytics in the Langfuse UI and quality improvement of responses over time. Attributes: SCORE_NAME \u2013 The standardized name used for feedback scores in Langfuse. Source code in src/augmentation/chainlit/feedback_service.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class ChainlitFeedbackService : \"\"\"Service for handling Chainlit feedback and Langfuse integration. This service processes user feedback from the Chainlit UI and integrates it with Langfuse for tracking and analysis. It associates feedback with traces in Langfuse, saves positive feedback examples to datasets, and provides mechanisms for retrieving relevant trace information. The service tracks feedback scores, comments, and message content, allowing for detailed analytics in the Langfuse UI and quality improvement of responses over time. Attributes: SCORE_NAME: The standardized name used for feedback scores in Langfuse. \"\"\" SCORE_NAME = \"User Feedback\" def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , langfuse_client : Langfuse , feedback_dataset : LangfuseDatasetConfiguration , chainlit_tag_format : str , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Args: langfuse_dataset_service: Service for creating and managing Langfuse datasets. langfuse_client: Client for interacting with the Langfuse API. feedback_dataset: Configuration for the dataset where positive feedback is stored. chainlit_tag_format: Format string for generating tags to retrieve traces by message ID. logger: Logger instance for recording service activities. \"\"\" self . langfuse_dataset_service = langfuse_dataset_service self . langfuse_client = langfuse_client self . feedback_dataset = feedback_dataset self . chainlit_tag_format = chainlit_tag_format self . logger = logger self . langfuse_dataset_service . create_if_does_not_exist ( feedback_dataset ) async def upsert ( self , feedback : Feedback ) -> bool : \"\"\"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Args: feedback: Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool: True if feedback was successfully processed and stored, False if an error occurred. \"\"\" trace = None try : trace = self . _fetch_trace ( feedback . forId ) if self . _is_positive ( feedback ): self . logger . info ( f \"Uploading trace { trace . id } to dataset { self . feedback_dataset . name } .\" ) self . _upload_trace_to_dataset ( trace ) self . langfuse_client . score ( trace_id = trace . id , name = ChainlitFeedbackService . SCORE_NAME , value = feedback . value , comment = feedback . comment , ) self . logger . info ( f \"Upserted feedback for { trace . id } trace with value { feedback . value } .\" ) return True except Exception as e : trace_id = trace . id if trace else None self . logger . warning ( f \"Failed to upsert feedback for { trace_id } trace: { e } \" ) return False def _fetch_trace ( self , message_id : str ) -> TraceWithDetails : \"\"\"Retrieve Langfuse trace associated with a Chainlit message ID. Uses the configured tag format to locate the trace related to a specific Chainlit message. Args: message_id: The unique identifier of the Chainlit message. Returns: TraceWithDetails: The complete trace data for the message. Raises: TraceNotFoundException: If no trace exists with the tag for this message ID. \"\"\" response = self . langfuse_client . fetch_traces ( tags = [ self . chainlit_tag_format . format ( message_id = message_id )] ) trace = response . data [ 0 ] if response . data else None if trace is None : raise TraceNotFoundException ( message_id ) return trace def _upload_trace_to_dataset ( self , trace : TraceWithDetails ) -> None : \"\"\"Save a trace to the feedback dataset for model improvement. Extracts relevant data from the trace including input query, retrieved nodes, templating information, and the final response, then creates a dataset item that can be used for model evaluation or fine-tuning. Args: trace: The trace containing the complete interaction details. \"\"\" retrieve_observation = self . _fetch_last_retrieve_observation ( trace ) last_templating_observation = self . _fetch_last_templating_observation ( trace ) output_generation_observation = ( self . _fetch_pre_last_generation_observation ( trace ) ) self . langfuse_client . create_dataset_item ( dataset_name = self . feedback_dataset . name , input = { \"query_str\" : trace . input , \"nodes\" : retrieve_observation . output . get ( \"nodes\" ), \"templating\" : last_templating_observation . input , }, expected_output = { \"result\" : output_generation_observation . output [ \"blocks\" ][ 0 ][ \"text\" ], }, source_trace_id = trace . id , metadata = { \"generated_by\" : output_generation_observation . model , }, ) def _fetch_last_retrieve_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the most recent retrieval observation from a trace. Retrieves information about the knowledge retrieval step in the RAG pipeline, including which nodes/documents were retrieved. Args: trace: The trace containing all observations. Returns: ObservationsView: The most recent retrieval observation, containing retrieved nodes. \"\"\" retrieve_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , name = \"retrieve\" , ) return max ( retrieve_observations . data , key = lambda x : x . createdAt ) def _fetch_last_templating_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the most recent templating observation from a trace. Retrieves information about how the prompt was constructed before being sent to the language model. Args: trace: The trace containing all observations. Returns: ObservationsView: The most recent templating observation, containing prompt construction details. \"\"\" templating_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , name = \"templating\" , ) return max ( templating_observations . data , key = lambda x : x . createdAt ) def _fetch_pre_last_generation_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the second most recent output observation from a trace. Retrieves information about the output generated by the language model, specifically the one before the last one. Args: trace: The trace containing all observations. Returns: ObservationsView: The second most recent output observation. \"\"\" output_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , type = \"GENERATION\" , ) return sorted ( output_observations . data , key = lambda x : x . createdAt )[ - 2 ] @staticmethod def _is_positive ( feedback : Feedback ) -> bool : \"\"\"Determine if the feedback is positive. Classifies feedback as positive if its numeric value is greater than zero. Positive feedback is used to identify high-quality examples for the dataset. Args: feedback: The feedback object containing the user's rating. Returns: bool: True if the feedback value is positive (greater than 0), False otherwise. \"\"\" return feedback . value > 0 __init__ ( langfuse_dataset_service , langfuse_client , feedback_dataset , chainlit_tag_format , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Parameters: langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service for creating and managing Langfuse datasets. langfuse_client ( Langfuse ) \u2013 Client for interacting with the Langfuse API. feedback_dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration for the dataset where positive feedback is stored. chainlit_tag_format ( str ) \u2013 Format string for generating tags to retrieve traces by message ID. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording service activities. Source code in src/augmentation/chainlit/feedback_service.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , langfuse_client : Langfuse , feedback_dataset : LangfuseDatasetConfiguration , chainlit_tag_format : str , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Args: langfuse_dataset_service: Service for creating and managing Langfuse datasets. langfuse_client: Client for interacting with the Langfuse API. feedback_dataset: Configuration for the dataset where positive feedback is stored. chainlit_tag_format: Format string for generating tags to retrieve traces by message ID. logger: Logger instance for recording service activities. \"\"\" self . langfuse_dataset_service = langfuse_dataset_service self . langfuse_client = langfuse_client self . feedback_dataset = feedback_dataset self . chainlit_tag_format = chainlit_tag_format self . logger = logger self . langfuse_dataset_service . create_if_does_not_exist ( feedback_dataset ) upsert ( feedback ) async Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Parameters: feedback ( Feedback ) \u2013 Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool ( bool ) \u2013 True if feedback was successfully processed and stored, False if an error occurred. Source code in src/augmentation/chainlit/feedback_service.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 async def upsert ( self , feedback : Feedback ) -> bool : \"\"\"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Args: feedback: Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool: True if feedback was successfully processed and stored, False if an error occurred. \"\"\" trace = None try : trace = self . _fetch_trace ( feedback . forId ) if self . _is_positive ( feedback ): self . logger . info ( f \"Uploading trace { trace . id } to dataset { self . feedback_dataset . name } .\" ) self . _upload_trace_to_dataset ( trace ) self . langfuse_client . score ( trace_id = trace . id , name = ChainlitFeedbackService . SCORE_NAME , value = feedback . value , comment = feedback . comment , ) self . logger . info ( f \"Upserted feedback for { trace . id } trace with value { feedback . value } .\" ) return True except Exception as e : trace_id = trace . id if trace else None self . logger . warning ( f \"Failed to upsert feedback for { trace_id } trace: { e } \" ) return False ChainlitFeedbackServiceFactory Bases: Factory Factory for creating ChainlitFeedbackService instances. This factory creates properly configured ChainlitFeedbackService instances using the application configuration. It handles initializing dependencies like the Langfuse client and dataset service. Source code in src/augmentation/chainlit/feedback_service.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 class ChainlitFeedbackServiceFactory ( Factory ): \"\"\"Factory for creating ChainlitFeedbackService instances. This factory creates properly configured ChainlitFeedbackService instances using the application configuration. It handles initializing dependencies like the Langfuse client and dataset service. \"\"\" _configuration_class : Type = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> ChainlitFeedbackService : \"\"\"Create a new ChainlitFeedbackService instance. Creates and configures a feedback service with the proper Langfuse client, dataset service, and configuration settings. Args: configuration: Application configuration containing Langfuse settings. Returns: ChainlitFeedbackService: A fully configured feedback service instance. \"\"\" langfuse_client = LangfuseClientFactory . create ( configuration . langfuse ) langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . langfuse ) feedback_dataset = configuration . langfuse . datasets . feedback_dataset chainlit_tag_format = configuration . langfuse . chainlit_tag_format return ChainlitFeedbackService ( langfuse_client = langfuse_client , langfuse_dataset_service = langfuse_dataset_service , feedback_dataset = feedback_dataset , chainlit_tag_format = chainlit_tag_format , )","title":"Feedback_service"},{"location":"src/augmentation/chainlit/feedback_service/#feedback_service","text":"This module contains functionality related to the the feedback_service module for augmentation.chainlit .","title":"Feedback_service"},{"location":"src/augmentation/chainlit/feedback_service/#feedback_service_1","text":"","title":"Feedback_service"},{"location":"src/augmentation/chainlit/feedback_service/#src.augmentation.chainlit.feedback_service.ChainlitFeedbackService","text":"Service for handling Chainlit feedback and Langfuse integration. This service processes user feedback from the Chainlit UI and integrates it with Langfuse for tracking and analysis. It associates feedback with traces in Langfuse, saves positive feedback examples to datasets, and provides mechanisms for retrieving relevant trace information. The service tracks feedback scores, comments, and message content, allowing for detailed analytics in the Langfuse UI and quality improvement of responses over time. Attributes: SCORE_NAME \u2013 The standardized name used for feedback scores in Langfuse. Source code in src/augmentation/chainlit/feedback_service.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class ChainlitFeedbackService : \"\"\"Service for handling Chainlit feedback and Langfuse integration. This service processes user feedback from the Chainlit UI and integrates it with Langfuse for tracking and analysis. It associates feedback with traces in Langfuse, saves positive feedback examples to datasets, and provides mechanisms for retrieving relevant trace information. The service tracks feedback scores, comments, and message content, allowing for detailed analytics in the Langfuse UI and quality improvement of responses over time. Attributes: SCORE_NAME: The standardized name used for feedback scores in Langfuse. \"\"\" SCORE_NAME = \"User Feedback\" def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , langfuse_client : Langfuse , feedback_dataset : LangfuseDatasetConfiguration , chainlit_tag_format : str , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Args: langfuse_dataset_service: Service for creating and managing Langfuse datasets. langfuse_client: Client for interacting with the Langfuse API. feedback_dataset: Configuration for the dataset where positive feedback is stored. chainlit_tag_format: Format string for generating tags to retrieve traces by message ID. logger: Logger instance for recording service activities. \"\"\" self . langfuse_dataset_service = langfuse_dataset_service self . langfuse_client = langfuse_client self . feedback_dataset = feedback_dataset self . chainlit_tag_format = chainlit_tag_format self . logger = logger self . langfuse_dataset_service . create_if_does_not_exist ( feedback_dataset ) async def upsert ( self , feedback : Feedback ) -> bool : \"\"\"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Args: feedback: Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool: True if feedback was successfully processed and stored, False if an error occurred. \"\"\" trace = None try : trace = self . _fetch_trace ( feedback . forId ) if self . _is_positive ( feedback ): self . logger . info ( f \"Uploading trace { trace . id } to dataset { self . feedback_dataset . name } .\" ) self . _upload_trace_to_dataset ( trace ) self . langfuse_client . score ( trace_id = trace . id , name = ChainlitFeedbackService . SCORE_NAME , value = feedback . value , comment = feedback . comment , ) self . logger . info ( f \"Upserted feedback for { trace . id } trace with value { feedback . value } .\" ) return True except Exception as e : trace_id = trace . id if trace else None self . logger . warning ( f \"Failed to upsert feedback for { trace_id } trace: { e } \" ) return False def _fetch_trace ( self , message_id : str ) -> TraceWithDetails : \"\"\"Retrieve Langfuse trace associated with a Chainlit message ID. Uses the configured tag format to locate the trace related to a specific Chainlit message. Args: message_id: The unique identifier of the Chainlit message. Returns: TraceWithDetails: The complete trace data for the message. Raises: TraceNotFoundException: If no trace exists with the tag for this message ID. \"\"\" response = self . langfuse_client . fetch_traces ( tags = [ self . chainlit_tag_format . format ( message_id = message_id )] ) trace = response . data [ 0 ] if response . data else None if trace is None : raise TraceNotFoundException ( message_id ) return trace def _upload_trace_to_dataset ( self , trace : TraceWithDetails ) -> None : \"\"\"Save a trace to the feedback dataset for model improvement. Extracts relevant data from the trace including input query, retrieved nodes, templating information, and the final response, then creates a dataset item that can be used for model evaluation or fine-tuning. Args: trace: The trace containing the complete interaction details. \"\"\" retrieve_observation = self . _fetch_last_retrieve_observation ( trace ) last_templating_observation = self . _fetch_last_templating_observation ( trace ) output_generation_observation = ( self . _fetch_pre_last_generation_observation ( trace ) ) self . langfuse_client . create_dataset_item ( dataset_name = self . feedback_dataset . name , input = { \"query_str\" : trace . input , \"nodes\" : retrieve_observation . output . get ( \"nodes\" ), \"templating\" : last_templating_observation . input , }, expected_output = { \"result\" : output_generation_observation . output [ \"blocks\" ][ 0 ][ \"text\" ], }, source_trace_id = trace . id , metadata = { \"generated_by\" : output_generation_observation . model , }, ) def _fetch_last_retrieve_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the most recent retrieval observation from a trace. Retrieves information about the knowledge retrieval step in the RAG pipeline, including which nodes/documents were retrieved. Args: trace: The trace containing all observations. Returns: ObservationsView: The most recent retrieval observation, containing retrieved nodes. \"\"\" retrieve_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , name = \"retrieve\" , ) return max ( retrieve_observations . data , key = lambda x : x . createdAt ) def _fetch_last_templating_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the most recent templating observation from a trace. Retrieves information about how the prompt was constructed before being sent to the language model. Args: trace: The trace containing all observations. Returns: ObservationsView: The most recent templating observation, containing prompt construction details. \"\"\" templating_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , name = \"templating\" , ) return max ( templating_observations . data , key = lambda x : x . createdAt ) def _fetch_pre_last_generation_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the second most recent output observation from a trace. Retrieves information about the output generated by the language model, specifically the one before the last one. Args: trace: The trace containing all observations. Returns: ObservationsView: The second most recent output observation. \"\"\" output_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , type = \"GENERATION\" , ) return sorted ( output_observations . data , key = lambda x : x . createdAt )[ - 2 ] @staticmethod def _is_positive ( feedback : Feedback ) -> bool : \"\"\"Determine if the feedback is positive. Classifies feedback as positive if its numeric value is greater than zero. Positive feedback is used to identify high-quality examples for the dataset. Args: feedback: The feedback object containing the user's rating. Returns: bool: True if the feedback value is positive (greater than 0), False otherwise. \"\"\" return feedback . value > 0","title":"ChainlitFeedbackService"},{"location":"src/augmentation/chainlit/feedback_service/#src.augmentation.chainlit.feedback_service.ChainlitFeedbackService.__init__","text":"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Parameters: langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service for creating and managing Langfuse datasets. langfuse_client ( Langfuse ) \u2013 Client for interacting with the Langfuse API. feedback_dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration for the dataset where positive feedback is stored. chainlit_tag_format ( str ) \u2013 Format string for generating tags to retrieve traces by message ID. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording service activities. Source code in src/augmentation/chainlit/feedback_service.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , langfuse_client : Langfuse , feedback_dataset : LangfuseDatasetConfiguration , chainlit_tag_format : str , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Args: langfuse_dataset_service: Service for creating and managing Langfuse datasets. langfuse_client: Client for interacting with the Langfuse API. feedback_dataset: Configuration for the dataset where positive feedback is stored. chainlit_tag_format: Format string for generating tags to retrieve traces by message ID. logger: Logger instance for recording service activities. \"\"\" self . langfuse_dataset_service = langfuse_dataset_service self . langfuse_client = langfuse_client self . feedback_dataset = feedback_dataset self . chainlit_tag_format = chainlit_tag_format self . logger = logger self . langfuse_dataset_service . create_if_does_not_exist ( feedback_dataset )","title":"__init__"},{"location":"src/augmentation/chainlit/feedback_service/#src.augmentation.chainlit.feedback_service.ChainlitFeedbackService.upsert","text":"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Parameters: feedback ( Feedback ) \u2013 Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool ( bool ) \u2013 True if feedback was successfully processed and stored, False if an error occurred. Source code in src/augmentation/chainlit/feedback_service.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 async def upsert ( self , feedback : Feedback ) -> bool : \"\"\"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Args: feedback: Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool: True if feedback was successfully processed and stored, False if an error occurred. \"\"\" trace = None try : trace = self . _fetch_trace ( feedback . forId ) if self . _is_positive ( feedback ): self . logger . info ( f \"Uploading trace { trace . id } to dataset { self . feedback_dataset . name } .\" ) self . _upload_trace_to_dataset ( trace ) self . langfuse_client . score ( trace_id = trace . id , name = ChainlitFeedbackService . SCORE_NAME , value = feedback . value , comment = feedback . comment , ) self . logger . info ( f \"Upserted feedback for { trace . id } trace with value { feedback . value } .\" ) return True except Exception as e : trace_id = trace . id if trace else None self . logger . warning ( f \"Failed to upsert feedback for { trace_id } trace: { e } \" ) return False","title":"upsert"},{"location":"src/augmentation/chainlit/feedback_service/#src.augmentation.chainlit.feedback_service.ChainlitFeedbackServiceFactory","text":"Bases: Factory Factory for creating ChainlitFeedbackService instances. This factory creates properly configured ChainlitFeedbackService instances using the application configuration. It handles initializing dependencies like the Langfuse client and dataset service. Source code in src/augmentation/chainlit/feedback_service.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 class ChainlitFeedbackServiceFactory ( Factory ): \"\"\"Factory for creating ChainlitFeedbackService instances. This factory creates properly configured ChainlitFeedbackService instances using the application configuration. It handles initializing dependencies like the Langfuse client and dataset service. \"\"\" _configuration_class : Type = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> ChainlitFeedbackService : \"\"\"Create a new ChainlitFeedbackService instance. Creates and configures a feedback service with the proper Langfuse client, dataset service, and configuration settings. Args: configuration: Application configuration containing Langfuse settings. Returns: ChainlitFeedbackService: A fully configured feedback service instance. \"\"\" langfuse_client = LangfuseClientFactory . create ( configuration . langfuse ) langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . langfuse ) feedback_dataset = configuration . langfuse . datasets . feedback_dataset chainlit_tag_format = configuration . langfuse . chainlit_tag_format return ChainlitFeedbackService ( langfuse_client = langfuse_client , langfuse_dataset_service = langfuse_dataset_service , feedback_dataset = feedback_dataset , chainlit_tag_format = chainlit_tag_format , )","title":"ChainlitFeedbackServiceFactory"},{"location":"src/augmentation/chainlit/service/","text":"Service This module contains functionality related to the the service module for augmentation.chainlit . Service ChainlitService Bases: BaseDataLayer Data layer implementation for Chainlit integration with Langfuse. Handles persistence of feedback and dataset management through Langfuse. Source code in src/augmentation/chainlit/service.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class ChainlitService ( BaseDataLayer ): \"\"\"Data layer implementation for Chainlit integration with Langfuse. Handles persistence of feedback and dataset management through Langfuse. \"\"\" def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , feedback_service : ChainlitFeedbackService , manual_dataset : LangfuseDatasetConfiguration , ): \"\"\"Initialize the Chainlit service. Args: langfuse_dataset_service: Service for managing Langfuse datasets. feedback_service: Service handling Chainlit feedback. manual_dataset: Configuration for manual dataset. \"\"\" self . manual_dataset = manual_dataset self . feedback_service = feedback_service langfuse_dataset_service . create_if_does_not_exist ( self . manual_dataset ) async def upsert_feedback ( self , feedback : Feedback ) -> bool : \"\"\"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Args: feedback: Feedback object containing user feedback details and metadata. Returns: bool: True if feedback was successfully upserted, False otherwise. \"\"\" return await self . feedback_service . upsert ( feedback ) async def build_debug_url ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a UI element in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a user record in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_feedback ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete user feedback from storage. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation step from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_thread_author ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve the author of a conversation thread. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a user record from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def list_threads ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"List all conversation threads in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def update_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def update_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation thread in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass __init__ ( langfuse_dataset_service , feedback_service , manual_dataset ) Initialize the Chainlit service. Parameters: langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service for managing Langfuse datasets. feedback_service ( ChainlitFeedbackService ) \u2013 Service handling Chainlit feedback. manual_dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration for manual dataset. Source code in src/augmentation/chainlit/service.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , feedback_service : ChainlitFeedbackService , manual_dataset : LangfuseDatasetConfiguration , ): \"\"\"Initialize the Chainlit service. Args: langfuse_dataset_service: Service for managing Langfuse datasets. feedback_service: Service handling Chainlit feedback. manual_dataset: Configuration for manual dataset. \"\"\" self . manual_dataset = manual_dataset self . feedback_service = feedback_service langfuse_dataset_service . create_if_does_not_exist ( self . manual_dataset ) build_debug_url ( * args , ** kwargs ) async Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 59 60 61 62 63 64 65 66 67 68 async def build_debug_url ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass create_element ( * args , ** kwargs ) async Create a UI element in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 70 71 72 73 74 75 76 77 78 79 async def create_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a UI element in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass create_step ( * args , ** kwargs ) async Create a conversation step in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 81 82 83 84 85 86 87 88 89 90 async def create_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass create_user ( * args , ** kwargs ) async Create a user record in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 92 93 94 95 96 97 98 99 100 101 async def create_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a user record in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass delete_element ( * args , ** kwargs ) async Delete a UI element from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 103 104 105 106 107 108 109 110 111 112 async def delete_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass delete_feedback ( * args , ** kwargs ) async Delete user feedback from storage. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 114 115 116 117 118 119 120 121 122 123 async def delete_feedback ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete user feedback from storage. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass delete_step ( * args , ** kwargs ) async Delete a conversation step from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 125 126 127 128 129 130 131 132 133 134 async def delete_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation step from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass delete_thread ( * args , ** kwargs ) async Delete a conversation thread from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 136 137 138 139 140 141 142 143 144 145 async def delete_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass get_element ( * args , ** kwargs ) async Retrieve a UI element from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 147 148 149 150 151 152 153 154 155 156 async def get_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass get_thread ( * args , ** kwargs ) async Retrieve a conversation thread from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 158 159 160 161 162 163 164 165 166 167 async def get_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass get_thread_author ( * args , ** kwargs ) async Retrieve the author of a conversation thread. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 169 170 171 172 173 174 175 176 177 178 async def get_thread_author ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve the author of a conversation thread. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass get_user ( * args , ** kwargs ) async Retrieve a user record from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 180 181 182 183 184 185 186 187 188 189 async def get_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a user record from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass list_threads ( * args , ** kwargs ) async List all conversation threads in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 191 192 193 194 195 196 197 198 199 200 async def list_threads ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"List all conversation threads in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass update_step ( * args , ** kwargs ) async Update a conversation step in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 202 203 204 205 206 207 208 209 210 211 async def update_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass update_thread ( * args , ** kwargs ) async Update a conversation thread in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 213 214 215 216 217 218 219 220 221 222 async def update_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation thread in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass upsert_feedback ( feedback ) async Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Parameters: feedback ( Feedback ) \u2013 Feedback object containing user feedback details and metadata. Returns: bool ( bool ) \u2013 True if feedback was successfully upserted, False otherwise. Source code in src/augmentation/chainlit/service.py 45 46 47 48 49 50 51 52 53 54 55 56 57 async def upsert_feedback ( self , feedback : Feedback ) -> bool : \"\"\"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Args: feedback: Feedback object containing user feedback details and metadata. Returns: bool: True if feedback was successfully upserted, False otherwise. \"\"\" return await self . feedback_service . upsert ( feedback ) ChainlitServiceFactory Bases: Factory Factory for creating ChainlitService instances. Creates and configures ChainlitService instances using application configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating ChainlitService instances. In this case, it is _AugmentationConfiguration. Source code in src/augmentation/chainlit/service.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 class ChainlitServiceFactory ( Factory ): \"\"\"Factory for creating ChainlitService instances. Creates and configures ChainlitService instances using application configuration. Attributes: _configuration_class (Type): The configuration class used for creating ChainlitService instances. In this case, it is _AugmentationConfiguration. \"\"\" _configuration_class : Type = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> ChainlitService : \"\"\"Create a configured ChainlitService instance. Args: configuration: Application configuration containing Langfuse settings. Returns: ChainlitService: Configured service instance ready for use. \"\"\" langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . langfuse ) feedback_service = ChainlitFeedbackServiceFactory . create ( configuration ) manual_dataset = configuration . langfuse . datasets . manual_dataset return ChainlitService ( langfuse_dataset_service = langfuse_dataset_service , feedback_service = feedback_service , manual_dataset = manual_dataset , )","title":"Service"},{"location":"src/augmentation/chainlit/service/#service","text":"This module contains functionality related to the the service module for augmentation.chainlit .","title":"Service"},{"location":"src/augmentation/chainlit/service/#service_1","text":"","title":"Service"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService","text":"Bases: BaseDataLayer Data layer implementation for Chainlit integration with Langfuse. Handles persistence of feedback and dataset management through Langfuse. Source code in src/augmentation/chainlit/service.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class ChainlitService ( BaseDataLayer ): \"\"\"Data layer implementation for Chainlit integration with Langfuse. Handles persistence of feedback and dataset management through Langfuse. \"\"\" def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , feedback_service : ChainlitFeedbackService , manual_dataset : LangfuseDatasetConfiguration , ): \"\"\"Initialize the Chainlit service. Args: langfuse_dataset_service: Service for managing Langfuse datasets. feedback_service: Service handling Chainlit feedback. manual_dataset: Configuration for manual dataset. \"\"\" self . manual_dataset = manual_dataset self . feedback_service = feedback_service langfuse_dataset_service . create_if_does_not_exist ( self . manual_dataset ) async def upsert_feedback ( self , feedback : Feedback ) -> bool : \"\"\"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Args: feedback: Feedback object containing user feedback details and metadata. Returns: bool: True if feedback was successfully upserted, False otherwise. \"\"\" return await self . feedback_service . upsert ( feedback ) async def build_debug_url ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a UI element in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a user record in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_feedback ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete user feedback from storage. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation step from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_thread_author ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve the author of a conversation thread. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a user record from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def list_threads ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"List all conversation threads in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def update_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def update_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation thread in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"ChainlitService"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.__init__","text":"Initialize the Chainlit service. Parameters: langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service for managing Langfuse datasets. feedback_service ( ChainlitFeedbackService ) \u2013 Service handling Chainlit feedback. manual_dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration for manual dataset. Source code in src/augmentation/chainlit/service.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , feedback_service : ChainlitFeedbackService , manual_dataset : LangfuseDatasetConfiguration , ): \"\"\"Initialize the Chainlit service. Args: langfuse_dataset_service: Service for managing Langfuse datasets. feedback_service: Service handling Chainlit feedback. manual_dataset: Configuration for manual dataset. \"\"\" self . manual_dataset = manual_dataset self . feedback_service = feedback_service langfuse_dataset_service . create_if_does_not_exist ( self . manual_dataset )","title":"__init__"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.build_debug_url","text":"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 59 60 61 62 63 64 65 66 67 68 async def build_debug_url ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"build_debug_url"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.create_element","text":"Create a UI element in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 70 71 72 73 74 75 76 77 78 79 async def create_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a UI element in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"create_element"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.create_step","text":"Create a conversation step in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 81 82 83 84 85 86 87 88 89 90 async def create_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"create_step"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.create_user","text":"Create a user record in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 92 93 94 95 96 97 98 99 100 101 async def create_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a user record in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"create_user"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.delete_element","text":"Delete a UI element from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 103 104 105 106 107 108 109 110 111 112 async def delete_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"delete_element"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.delete_feedback","text":"Delete user feedback from storage. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 114 115 116 117 118 119 120 121 122 123 async def delete_feedback ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete user feedback from storage. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"delete_feedback"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.delete_step","text":"Delete a conversation step from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 125 126 127 128 129 130 131 132 133 134 async def delete_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation step from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"delete_step"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.delete_thread","text":"Delete a conversation thread from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 136 137 138 139 140 141 142 143 144 145 async def delete_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"delete_thread"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.get_element","text":"Retrieve a UI element from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 147 148 149 150 151 152 153 154 155 156 async def get_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"get_element"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.get_thread","text":"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 158 159 160 161 162 163 164 165 166 167 async def get_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"get_thread"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.get_thread_author","text":"Retrieve the author of a conversation thread. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 169 170 171 172 173 174 175 176 177 178 async def get_thread_author ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve the author of a conversation thread. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"get_thread_author"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.get_user","text":"Retrieve a user record from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 180 181 182 183 184 185 186 187 188 189 async def get_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a user record from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"get_user"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.list_threads","text":"List all conversation threads in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 191 192 193 194 195 196 197 198 199 200 async def list_threads ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"List all conversation threads in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"list_threads"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.update_step","text":"Update a conversation step in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 202 203 204 205 206 207 208 209 210 211 async def update_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"update_step"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.update_thread","text":"Update a conversation thread in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 213 214 215 216 217 218 219 220 221 222 async def update_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation thread in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"update_thread"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.upsert_feedback","text":"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Parameters: feedback ( Feedback ) \u2013 Feedback object containing user feedback details and metadata. Returns: bool ( bool ) \u2013 True if feedback was successfully upserted, False otherwise. Source code in src/augmentation/chainlit/service.py 45 46 47 48 49 50 51 52 53 54 55 56 57 async def upsert_feedback ( self , feedback : Feedback ) -> bool : \"\"\"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Args: feedback: Feedback object containing user feedback details and metadata. Returns: bool: True if feedback was successfully upserted, False otherwise. \"\"\" return await self . feedback_service . upsert ( feedback )","title":"upsert_feedback"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitServiceFactory","text":"Bases: Factory Factory for creating ChainlitService instances. Creates and configures ChainlitService instances using application configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating ChainlitService instances. In this case, it is _AugmentationConfiguration. Source code in src/augmentation/chainlit/service.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 class ChainlitServiceFactory ( Factory ): \"\"\"Factory for creating ChainlitService instances. Creates and configures ChainlitService instances using application configuration. Attributes: _configuration_class (Type): The configuration class used for creating ChainlitService instances. In this case, it is _AugmentationConfiguration. \"\"\" _configuration_class : Type = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> ChainlitService : \"\"\"Create a configured ChainlitService instance. Args: configuration: Application configuration containing Langfuse settings. Returns: ChainlitService: Configured service instance ready for use. \"\"\" langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . langfuse ) feedback_service = ChainlitFeedbackServiceFactory . create ( configuration ) manual_dataset = configuration . langfuse . datasets . manual_dataset return ChainlitService ( langfuse_dataset_service = langfuse_dataset_service , feedback_service = feedback_service , manual_dataset = manual_dataset , )","title":"ChainlitServiceFactory"},{"location":"src/augmentation/chainlit/utils/","text":"Utils This module contains functionality related to the the utils module for augmentation.chainlit . Utils ChainlitUtils Utility class for handling conversation messages. Provides methods for managing welcome messages and reference handling in conversations. Attributes: REFERENCES_TEMPLATE \u2013 Template string for formatting references section. Source code in src/augmentation/chainlit/utils.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class ChainlitUtils : \"\"\"Utility class for handling conversation messages. Provides methods for managing welcome messages and reference handling in conversations. Attributes: REFERENCES_TEMPLATE: Template string for formatting references section. \"\"\" REFERENCES_TEMPLATE = \" \\n\\n **References**: \\n \" \" {references} \\n \" def __init__ ( self , configuration : ChainlitConfiguration ): \"\"\"Initialize ChainlitUtils with configuration. Args: configuration (ChainlitConfiguration): Configuration containing settings like welcome message. \"\"\" self . configuration = configuration def get_disclaimer_message ( self ) -> Message : \"\"\"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message: A chainlit Message object with the Assistant as author and the disclaimer message as content. \"\"\" return Message ( author = \"System\" , content = \"\" , elements = [ Text ( name = self . configuration . disclaimer_title , content = self . configuration . disclaimer_text , ) ], ) def get_welcome_message ( self ) -> Message : \"\"\"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message: A chainlit Message object with the Assistant as author and the welcome message as content. \"\"\" return Message ( author = \"Assistant\" , content = self . configuration . welcome_message ) def add_references ( self , message : Message , response : StreamingResponse ) -> None : \"\"\"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Args: message (Message): The chainlit Message object to modify by adding references. response (StreamingResponse): A response object containing source_nodes with reference information. \"\"\" message . content += self . _get_references_str ( response . source_nodes ) def _get_references_str ( self , nodes : List [ NodeWithScore ]) -> str : \"\"\"Generate a formatted references section from source nodes. Processes a list of source nodes to create a deduplicated, formatted string of references. Args: nodes (List[NodeWithScore]): List of source nodes with relevance scores and metadata containing reference information. Returns: str: A formatted string containing unique references in the structure defined by REFERENCES_TEMPLATE. \"\"\" raw_references = [ self . _get_reference_str ( node ) for node in nodes ] references = \" \\n \" . join ( set ( raw_references )) return self . REFERENCES_TEMPLATE . format ( references = references ) def _get_reference_str ( self , node : NodeWithScore ) -> str : \"\"\"Format a single node's reference as a markdown string. Extracts title and URL from a node's metadata and formats it as a markdown list item, with link formatting if a URL is available. Args: node (NodeWithScore): A source node containing metadata with title and optional URL information. Returns: str: A formatted markdown list item representing the reference, with a clickable link if URL is available. \"\"\" title = node . metadata . get ( \"title\" ) if not title : title = node . metadata . get ( \"Title\" ) url = node . metadata . get ( \"url\" ) if url : return \"- [ {} ]( {} )\" . format ( title , url ) else : return f \"- { title } \" __init__ ( configuration ) Initialize ChainlitUtils with configuration. Parameters: configuration ( ChainlitConfiguration ) \u2013 Configuration containing settings like welcome message. Source code in src/augmentation/chainlit/utils.py 24 25 26 27 28 29 30 31 def __init__ ( self , configuration : ChainlitConfiguration ): \"\"\"Initialize ChainlitUtils with configuration. Args: configuration (ChainlitConfiguration): Configuration containing settings like welcome message. \"\"\" self . configuration = configuration add_references ( message , response ) Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Parameters: message ( Message ) \u2013 The chainlit Message object to modify by adding references. response ( StreamingResponse ) \u2013 A response object containing source_nodes with reference information. Source code in src/augmentation/chainlit/utils.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def add_references ( self , message : Message , response : StreamingResponse ) -> None : \"\"\"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Args: message (Message): The chainlit Message object to modify by adding references. response (StreamingResponse): A response object containing source_nodes with reference information. \"\"\" message . content += self . _get_references_str ( response . source_nodes ) get_disclaimer_message () Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message ( Message ) \u2013 A chainlit Message object with the Assistant as author and the disclaimer message as content. Source code in src/augmentation/chainlit/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def get_disclaimer_message ( self ) -> Message : \"\"\"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message: A chainlit Message object with the Assistant as author and the disclaimer message as content. \"\"\" return Message ( author = \"System\" , content = \"\" , elements = [ Text ( name = self . configuration . disclaimer_title , content = self . configuration . disclaimer_text , ) ], ) get_welcome_message () Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message ( Message ) \u2013 A chainlit Message object with the Assistant as author and the welcome message as content. Source code in src/augmentation/chainlit/utils.py 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_welcome_message ( self ) -> Message : \"\"\"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message: A chainlit Message object with the Assistant as author and the welcome message as content. \"\"\" return Message ( author = \"Assistant\" , content = self . configuration . welcome_message ) ChainlitUtilsFactory Bases: SingletonFactory Factory class for creating ChainlitUtils instances. This class extends the Factory pattern to create instances of ChainlitUtils based on a given ChainlitConfiguration. It ensures that the configuration provided is of the correct type and handles the creation of new instances. Attributes: _configuration_class ( Type ) \u2013 The expected type of configuration objects. Source code in src/augmentation/chainlit/utils.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class ChainlitUtilsFactory ( SingletonFactory ): \"\"\" Factory class for creating ChainlitUtils instances. This class extends the Factory pattern to create instances of ChainlitUtils based on a given ChainlitConfiguration. It ensures that the configuration provided is of the correct type and handles the creation of new instances. Attributes: _configuration_class (Type): The expected type of configuration objects. \"\"\" _configuration_class : Type = ChainlitConfiguration @classmethod def _create_instance ( cls , configuration : ChainlitConfiguration ) -> ChainlitUtils : \"\"\"Create a new ChainlitUtils instance. Args: configuration (ChainlitConfiguration): Configuration object for ChainlitUtils. Returns: ChainlitUtils: A new instance of ChainlitUtils initialized with the given configuration. \"\"\" return ChainlitUtils ( configuration )","title":"Utils"},{"location":"src/augmentation/chainlit/utils/#utils","text":"This module contains functionality related to the the utils module for augmentation.chainlit .","title":"Utils"},{"location":"src/augmentation/chainlit/utils/#utils_1","text":"","title":"Utils"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils","text":"Utility class for handling conversation messages. Provides methods for managing welcome messages and reference handling in conversations. Attributes: REFERENCES_TEMPLATE \u2013 Template string for formatting references section. Source code in src/augmentation/chainlit/utils.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class ChainlitUtils : \"\"\"Utility class for handling conversation messages. Provides methods for managing welcome messages and reference handling in conversations. Attributes: REFERENCES_TEMPLATE: Template string for formatting references section. \"\"\" REFERENCES_TEMPLATE = \" \\n\\n **References**: \\n \" \" {references} \\n \" def __init__ ( self , configuration : ChainlitConfiguration ): \"\"\"Initialize ChainlitUtils with configuration. Args: configuration (ChainlitConfiguration): Configuration containing settings like welcome message. \"\"\" self . configuration = configuration def get_disclaimer_message ( self ) -> Message : \"\"\"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message: A chainlit Message object with the Assistant as author and the disclaimer message as content. \"\"\" return Message ( author = \"System\" , content = \"\" , elements = [ Text ( name = self . configuration . disclaimer_title , content = self . configuration . disclaimer_text , ) ], ) def get_welcome_message ( self ) -> Message : \"\"\"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message: A chainlit Message object with the Assistant as author and the welcome message as content. \"\"\" return Message ( author = \"Assistant\" , content = self . configuration . welcome_message ) def add_references ( self , message : Message , response : StreamingResponse ) -> None : \"\"\"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Args: message (Message): The chainlit Message object to modify by adding references. response (StreamingResponse): A response object containing source_nodes with reference information. \"\"\" message . content += self . _get_references_str ( response . source_nodes ) def _get_references_str ( self , nodes : List [ NodeWithScore ]) -> str : \"\"\"Generate a formatted references section from source nodes. Processes a list of source nodes to create a deduplicated, formatted string of references. Args: nodes (List[NodeWithScore]): List of source nodes with relevance scores and metadata containing reference information. Returns: str: A formatted string containing unique references in the structure defined by REFERENCES_TEMPLATE. \"\"\" raw_references = [ self . _get_reference_str ( node ) for node in nodes ] references = \" \\n \" . join ( set ( raw_references )) return self . REFERENCES_TEMPLATE . format ( references = references ) def _get_reference_str ( self , node : NodeWithScore ) -> str : \"\"\"Format a single node's reference as a markdown string. Extracts title and URL from a node's metadata and formats it as a markdown list item, with link formatting if a URL is available. Args: node (NodeWithScore): A source node containing metadata with title and optional URL information. Returns: str: A formatted markdown list item representing the reference, with a clickable link if URL is available. \"\"\" title = node . metadata . get ( \"title\" ) if not title : title = node . metadata . get ( \"Title\" ) url = node . metadata . get ( \"url\" ) if url : return \"- [ {} ]( {} )\" . format ( title , url ) else : return f \"- { title } \"","title":"ChainlitUtils"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils.__init__","text":"Initialize ChainlitUtils with configuration. Parameters: configuration ( ChainlitConfiguration ) \u2013 Configuration containing settings like welcome message. Source code in src/augmentation/chainlit/utils.py 24 25 26 27 28 29 30 31 def __init__ ( self , configuration : ChainlitConfiguration ): \"\"\"Initialize ChainlitUtils with configuration. Args: configuration (ChainlitConfiguration): Configuration containing settings like welcome message. \"\"\" self . configuration = configuration","title":"__init__"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils.add_references","text":"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Parameters: message ( Message ) \u2013 The chainlit Message object to modify by adding references. response ( StreamingResponse ) \u2013 A response object containing source_nodes with reference information. Source code in src/augmentation/chainlit/utils.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def add_references ( self , message : Message , response : StreamingResponse ) -> None : \"\"\"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Args: message (Message): The chainlit Message object to modify by adding references. response (StreamingResponse): A response object containing source_nodes with reference information. \"\"\" message . content += self . _get_references_str ( response . source_nodes )","title":"add_references"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils.get_disclaimer_message","text":"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message ( Message ) \u2013 A chainlit Message object with the Assistant as author and the disclaimer message as content. Source code in src/augmentation/chainlit/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def get_disclaimer_message ( self ) -> Message : \"\"\"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message: A chainlit Message object with the Assistant as author and the disclaimer message as content. \"\"\" return Message ( author = \"System\" , content = \"\" , elements = [ Text ( name = self . configuration . disclaimer_title , content = self . configuration . disclaimer_text , ) ], )","title":"get_disclaimer_message"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils.get_welcome_message","text":"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message ( Message ) \u2013 A chainlit Message object with the Assistant as author and the welcome message as content. Source code in src/augmentation/chainlit/utils.py 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_welcome_message ( self ) -> Message : \"\"\"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message: A chainlit Message object with the Assistant as author and the welcome message as content. \"\"\" return Message ( author = \"Assistant\" , content = self . configuration . welcome_message )","title":"get_welcome_message"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtilsFactory","text":"Bases: SingletonFactory Factory class for creating ChainlitUtils instances. This class extends the Factory pattern to create instances of ChainlitUtils based on a given ChainlitConfiguration. It ensures that the configuration provided is of the correct type and handles the creation of new instances. Attributes: _configuration_class ( Type ) \u2013 The expected type of configuration objects. Source code in src/augmentation/chainlit/utils.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class ChainlitUtilsFactory ( SingletonFactory ): \"\"\" Factory class for creating ChainlitUtils instances. This class extends the Factory pattern to create instances of ChainlitUtils based on a given ChainlitConfiguration. It ensures that the configuration provided is of the correct type and handles the creation of new instances. Attributes: _configuration_class (Type): The expected type of configuration objects. \"\"\" _configuration_class : Type = ChainlitConfiguration @classmethod def _create_instance ( cls , configuration : ChainlitConfiguration ) -> ChainlitUtils : \"\"\"Create a new ChainlitUtils instance. Args: configuration (ChainlitConfiguration): Configuration object for ChainlitUtils. Returns: ChainlitUtils: A new instance of ChainlitUtils initialized with the given configuration. \"\"\" return ChainlitUtils ( configuration )","title":"ChainlitUtilsFactory"},{"location":"src/augmentation/components/chat_engines/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.chat_engines . Registry ChatEngineRegistry Bases: Registry Registry for chat engine components. This registry provides a centralized mechanism for registering, retrieving, and managing chat engine implementations. Chat engines are indexed by their ChatEngineName enum value, allowing for dynamic selection and instantiation of different chat engine strategies. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to ChatEngineName. Source code in src/augmentation/components/chat_engines/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class ChatEngineRegistry ( Registry ): \"\"\" Registry for chat engine components. This registry provides a centralized mechanism for registering, retrieving, and managing chat engine implementations. Chat engines are indexed by their ChatEngineName enum value, allowing for dynamic selection and instantiation of different chat engine strategies. Attributes: _key_class (Type): The class type used for registry keys, set to ChatEngineName. \"\"\" _key_class : Type = ChatEngineName","title":"Registry"},{"location":"src/augmentation/components/chat_engines/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.chat_engines .","title":"Registry"},{"location":"src/augmentation/components/chat_engines/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/chat_engines/registry/#src.augmentation.components.chat_engines.registry.ChatEngineRegistry","text":"Bases: Registry Registry for chat engine components. This registry provides a centralized mechanism for registering, retrieving, and managing chat engine implementations. Chat engines are indexed by their ChatEngineName enum value, allowing for dynamic selection and instantiation of different chat engine strategies. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to ChatEngineName. Source code in src/augmentation/components/chat_engines/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class ChatEngineRegistry ( Registry ): \"\"\" Registry for chat engine components. This registry provides a centralized mechanism for registering, retrieving, and managing chat engine implementations. Chat engines are indexed by their ChatEngineName enum value, allowing for dynamic selection and instantiation of different chat engine strategies. Attributes: _key_class (Type): The class type used for registry keys, set to ChatEngineName. \"\"\" _key_class : Type = ChatEngineName","title":"ChatEngineRegistry"},{"location":"src/augmentation/components/chat_engines/langfuse/callback_manager/","text":"Callback_manager This module contains functionality related to the the callback_manager module for augmentation.components.chat_engines.langfuse . Callback_manager LlamaIndexCallbackManagerFactory Bases: Factory Factory class for creating LlamaIndex callback managers with Langfuse integration. This factory creates a callback manager with a Langfuse handler to enable tracking and observability for LlamaIndex operations. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Source code in src/augmentation/components/chat_engines/langfuse/callback_manager.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class LlamaIndexCallbackManagerFactory ( Factory ): \"\"\"Factory class for creating LlamaIndex callback managers with Langfuse integration. This factory creates a callback manager with a Langfuse handler to enable tracking and observability for LlamaIndex operations. Attributes: _configuration_class (Type): The configuration class used for creating \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration , session_id : str = \"\" ) -> LlamaIndexCallbackHandler : \"\"\"Create a CallbackManager with a LlamaIndexCallbackHandler for Langfuse integration. This method creates a Langfuse callback handler using the provided configuration and wraps it in a CallbackManager. Args: configuration: Langfuse configuration containing API keys and URL session_id: Optional identifier for the session to group related traces Returns: A configured CallbackManager with the Langfuse callback handler \"\"\" handler = LlamaIndexCallbackHandler ( secret_key = configuration . secrets . secret_key . get_secret_value (), public_key = configuration . secrets . public_key . get_secret_value (), host = configuration . url , session_id = session_id , ) return CallbackManager ( handlers = [ handler ])","title":"Callback_manager"},{"location":"src/augmentation/components/chat_engines/langfuse/callback_manager/#callback_manager","text":"This module contains functionality related to the the callback_manager module for augmentation.components.chat_engines.langfuse .","title":"Callback_manager"},{"location":"src/augmentation/components/chat_engines/langfuse/callback_manager/#callback_manager_1","text":"","title":"Callback_manager"},{"location":"src/augmentation/components/chat_engines/langfuse/callback_manager/#src.augmentation.components.chat_engines.langfuse.callback_manager.LlamaIndexCallbackManagerFactory","text":"Bases: Factory Factory class for creating LlamaIndex callback managers with Langfuse integration. This factory creates a callback manager with a Langfuse handler to enable tracking and observability for LlamaIndex operations. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Source code in src/augmentation/components/chat_engines/langfuse/callback_manager.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class LlamaIndexCallbackManagerFactory ( Factory ): \"\"\"Factory class for creating LlamaIndex callback managers with Langfuse integration. This factory creates a callback manager with a Langfuse handler to enable tracking and observability for LlamaIndex operations. Attributes: _configuration_class (Type): The configuration class used for creating \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration , session_id : str = \"\" ) -> LlamaIndexCallbackHandler : \"\"\"Create a CallbackManager with a LlamaIndexCallbackHandler for Langfuse integration. This method creates a Langfuse callback handler using the provided configuration and wraps it in a CallbackManager. Args: configuration: Langfuse configuration containing API keys and URL session_id: Optional identifier for the session to group related traces Returns: A configured CallbackManager with the Langfuse callback handler \"\"\" handler = LlamaIndexCallbackHandler ( secret_key = configuration . secrets . secret_key . get_secret_value (), public_key = configuration . secrets . public_key . get_secret_value (), host = configuration . url , session_id = session_id , ) return CallbackManager ( handlers = [ handler ])","title":"LlamaIndexCallbackManagerFactory"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/","text":"Chat_engine This module contains functionality related to the the chat_engine module for augmentation.components.chat_engines.langfuse . Chat_engine LangfuseChatEngine Bases: CondensePlusContextChatEngine Custom chat engine implementing Retrieval-Augmented Generation (RAG). Coordinates retrieval, post-processing, and response generation for RAG workflow. Integrates with Langfuse for tracing and Chainlit for message tracking. Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 class LangfuseChatEngine ( CondensePlusContextChatEngine ): \"\"\"Custom chat engine implementing Retrieval-Augmented Generation (RAG). Coordinates retrieval, post-processing, and response generation for RAG workflow. Integrates with Langfuse for tracing and Chainlit for message tracking. \"\"\" chainlit_tag_format : str = Field ( description = \"Format of the tag used to retrieve the trace by chainlit message id in Langfuse.\" ) input_guardrail_prompt_template : Optional [ PromptTemplate ] = Field ( description = \"Prompt template for validating user input compliance\" , default = None , ) output_guardrail_prompt_template : Optional [ PromptTemplate ] = Field ( description = \"Prompt template for validating response output compliance\" , default = None , ) def __init__ ( self , retriever : BaseRetriever , llm : LLM , memory : BaseMemory , chainlit_tag_format : str , guardrails_engine : BaseGuardrailsEngine , context_prompt : Optional [ Union [ str , PromptTemplate ]] = None , context_refine_prompt : Optional [ Union [ str , PromptTemplate ]] = None , condense_prompt : Optional [ Union [ str , PromptTemplate ]] = None , system_prompt : Optional [ str ] = None , skip_condense : bool = False , node_postprocessors : Optional [ List [ BaseNodePostprocessor ]] = None , callback_manager : Optional [ CallbackManager ] = None , verbose : bool = False , ): \"\"\" Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Args: retriever: Document retriever for RAG llm: Language model for response generation memory: Memory buffer for chat history chainlit_tag_format: Format for Chainlit message ID in Langfuse guardrails_engine: Guardrail engine for input/output validation context_prompt: Prompt for context generation context_refine_prompt: Prompt for refining context condense_prompt: Prompt for condensing context system_prompt: System prompt for LLM input_guardrail_prompt_template: Prompt template for input validation output_guardrail_prompt_template: Prompt template for output validation skip_condense: Flag to skip context condensing node_postprocessors: List of postprocessors for node processing callback_manager: Callback manager for tracing verbose: Flag for verbose output \"\"\" super () . __init__ ( retriever = retriever , llm = llm , memory = memory , context_prompt = context_prompt , context_refine_prompt = context_refine_prompt , condense_prompt = condense_prompt , system_prompt = system_prompt , skip_condense = skip_condense , node_postprocessors = node_postprocessors , callback_manager = callback_manager , verbose = verbose , ) self . guardrails_engine = guardrails_engine self . chainlit_tag_format = chainlit_tag_format def _get_response_synthesizer ( self , chat_history : List [ ChatMessage ], streaming : bool = False ): \"\"\"Override to use SIMPLE_SUMMARIZE mode instead of COMPACT (which uses refine). This eliminates the iterative refinement loop that makes N LLM calls for N documents. Instead, all documents are concatenated and processed in a single LLM call. Args: chat_history: Chat history for context streaming: Whether to stream the response Returns: BaseSynthesizer: Response synthesizer configured for single-pass generation \"\"\" system_prompt = self . _system_prompt or \"\" qa_messages = get_prefix_messages_with_context ( self . _context_prompt_template , system_prompt , [], chat_history , self . _llm . metadata . system_role , ) # Use SIMPLE_SUMMARIZE instead of COMPACT to avoid refinement iterations # This concatenates all retrieved documents and makes ONE LLM call from llama_index.core.prompts import ChatPromptTemplate return get_llama_response_synthesizer ( llm = self . _llm , callback_manager = self . callback_manager , text_qa_template = ChatPromptTemplate . from_messages ( qa_messages , function_mappings = self . _context_prompt_template . function_mappings , ), response_mode = ResponseMode . SIMPLE_SUMMARIZE , streaming = streaming , ) def stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" response = self . _stream_chat ( message = message , chat_history = chat_history , chainlit_message_id = chainlit_message_id , source_process = source_process , ) trace = self . get_current_langfuse_trace () trace . input = message return response @trace_method ( \"chat\" ) def _stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" self . _set_chainlit_message_id ( message_id = chainlit_message_id , source_process = source_process ) guarded_response = self . guardrails_engine . input_guard ( message = message , is_stream = True ) if guarded_response : self . _save_chat_history ( input_message = message , output_message = guarded_response . response ) return guarded_response response = super () . chat ( message = message , chat_history = chat_history ) guarded_response = self . guardrails_engine . output_guard ( message = response . response , is_stream = True ) if guarded_response : self . _save_chat_history ( input_message = message , output_message = guarded_response . response ) return guarded_response response . is_dummy_stream = True return response def get_current_langfuse_trace ( self ) -> StatefulTraceClient : \"\"\"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient: Active Langfuse trace or None if not found \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): return handler . trace return None def set_session_id ( self , session_id : str ) -> None : \"\"\"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Args: session_id: Unique identifier for current user session \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . session_id = session_id def _set_chainlit_message_id ( self , message_id : str , source_process : SourceProcess ) -> None : \"\"\"Configure Chainlit message tracking in Langfuse trace. Links the current Langfuse trace to a Chainlit message ID and tags with the processing source context for traceability in the Langfuse UI. Args: message_id: Chainlit message identifier to reference source_process: Source context enum categorizing the query origin \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . set_trace_params ( tags = [ self . chainlit_tag_format . format ( message_id = message_id ), source_process . name . lower (), ] ) def _save_chat_history ( self , input_message : str , output_message : str ) -> None : \"\"\"Save chat history to memory buffer. Args: input_message: User input message output_message: Generated response message \"\"\" self . _memory . put ( ChatMessage ( role = MessageRole . USER , content = input_message ) ) self . _memory . put ( ChatMessage ( role = MessageRole . ASSISTANT , content = output_message ) ) __init__ ( retriever , llm , memory , chainlit_tag_format , guardrails_engine , context_prompt = None , context_refine_prompt = None , condense_prompt = None , system_prompt = None , skip_condense = False , node_postprocessors = None , callback_manager = None , verbose = False ) Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Parameters: retriever ( BaseRetriever ) \u2013 Document retriever for RAG llm ( LLM ) \u2013 Language model for response generation memory ( BaseMemory ) \u2013 Memory buffer for chat history chainlit_tag_format ( str ) \u2013 Format for Chainlit message ID in Langfuse guardrails_engine ( BaseGuardrailsEngine ) \u2013 Guardrail engine for input/output validation context_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for context generation context_refine_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for refining context condense_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for condensing context system_prompt ( Optional [ str ] , default: None ) \u2013 System prompt for LLM input_guardrail_prompt_template \u2013 Prompt template for input validation output_guardrail_prompt_template \u2013 Prompt template for output validation skip_condense ( bool , default: False ) \u2013 Flag to skip context condensing node_postprocessors ( Optional [ List [ BaseNodePostprocessor ]] , default: None ) \u2013 List of postprocessors for node processing callback_manager ( Optional [ CallbackManager ] , default: None ) \u2013 Callback manager for tracing verbose ( bool , default: False ) \u2013 Flag for verbose output Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def __init__ ( self , retriever : BaseRetriever , llm : LLM , memory : BaseMemory , chainlit_tag_format : str , guardrails_engine : BaseGuardrailsEngine , context_prompt : Optional [ Union [ str , PromptTemplate ]] = None , context_refine_prompt : Optional [ Union [ str , PromptTemplate ]] = None , condense_prompt : Optional [ Union [ str , PromptTemplate ]] = None , system_prompt : Optional [ str ] = None , skip_condense : bool = False , node_postprocessors : Optional [ List [ BaseNodePostprocessor ]] = None , callback_manager : Optional [ CallbackManager ] = None , verbose : bool = False , ): \"\"\" Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Args: retriever: Document retriever for RAG llm: Language model for response generation memory: Memory buffer for chat history chainlit_tag_format: Format for Chainlit message ID in Langfuse guardrails_engine: Guardrail engine for input/output validation context_prompt: Prompt for context generation context_refine_prompt: Prompt for refining context condense_prompt: Prompt for condensing context system_prompt: System prompt for LLM input_guardrail_prompt_template: Prompt template for input validation output_guardrail_prompt_template: Prompt template for output validation skip_condense: Flag to skip context condensing node_postprocessors: List of postprocessors for node processing callback_manager: Callback manager for tracing verbose: Flag for verbose output \"\"\" super () . __init__ ( retriever = retriever , llm = llm , memory = memory , context_prompt = context_prompt , context_refine_prompt = context_refine_prompt , condense_prompt = condense_prompt , system_prompt = system_prompt , skip_condense = skip_condense , node_postprocessors = node_postprocessors , callback_manager = callback_manager , verbose = verbose , ) self . guardrails_engine = guardrails_engine self . chainlit_tag_format = chainlit_tag_format get_current_langfuse_trace () Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient ( StatefulTraceClient ) \u2013 Active Langfuse trace or None if not found Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 242 243 244 245 246 247 248 249 250 251 252 253 254 def get_current_langfuse_trace ( self ) -> StatefulTraceClient : \"\"\"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient: Active Langfuse trace or None if not found \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): return handler . trace return None set_session_id ( session_id ) Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Parameters: session_id ( str ) \u2013 Unique identifier for current user session Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 256 257 258 259 260 261 262 263 264 265 266 267 def set_session_id ( self , session_id : str ) -> None : \"\"\"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Args: session_id: Unique identifier for current user session \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . session_id = session_id stream_chat ( message , chat_history = None , chainlit_message_id = None , source_process = SourceProcess . CHAT_COMPLETION ) Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Parameters: message ( str ) \u2013 Raw query string to process chat_history ( Optional [ List [ ChatMessage ]] , default: None ) \u2013 Optional chat history for context chainlit_message_id ( str , default: None ) \u2013 Optional ID for linking to Chainlit message in UI source_process ( SourceProcess , default: CHAT_COMPLETION ) \u2013 Context identifier indicating query's origin source Returns: AgentChatResponse ( AgentChatResponse ) \u2013 Generated response from RAG pipeline with dummy streaming Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" response = self . _stream_chat ( message = message , chat_history = chat_history , chainlit_message_id = chainlit_message_id , source_process = source_process , ) trace = self . get_current_langfuse_trace () trace . input = message return response LangfuseChatEngineFactory Bases: Factory Factory for creating configured LangfuseChatEngine instances. Constructs and connects components needed for the RAG pipeline. Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 class LangfuseChatEngineFactory ( Factory ): \"\"\"Factory for creating configured LangfuseChatEngine instances. Constructs and connects components needed for the RAG pipeline. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> LangfuseChatEngine : \"\"\"Create and configure a LangfuseChatEngine instance from configuration. Instantiates all RAG pipeline components based on configuration settings, connects them with a shared callback manager for tracing, and assembles them into a complete chat engine. Args: configuration: Complete augmentation configuration containing settings for all components Returns: LangfuseChatEngine: Fully configured RAG chat engine with tracing \"\"\" chat_engine_configuration = configuration . augmentation . chat_engine llm = LLMRegistry . get ( chat_engine_configuration . llm . provider ) . create ( chat_engine_configuration . llm ) retriever = RetrieverRegistry . get ( chat_engine_configuration . retriever . name ) . create ( configuration ) # Set temporal_domain_config for hybrid filter factory before creating postprocessors temporal_domain_config = configuration . augmentation . temporal_domain HybridFilterFactory . set_temporal_domain_config ( temporal_domain_config ) postprocessors = [ PostprocessorRegistry . get ( postprocessor_configuration . name ) . create ( postprocessor_configuration ) for postprocessor_configuration in chat_engine_configuration . postprocessors ] langfuse_callback_manager = LlamaIndexCallbackManagerFactory . create ( configuration . augmentation . langfuse ) memory = ChatMemoryBuffer ( chat_history = [], token_limit = llm . metadata . context_window - 256 ) ( condense_prompt_template , context_prompt_template , context_refine_prompt_template , system_prompt_template , ) = cls . _get_prompt_templates ( configuration = configuration . augmentation ) guardrails_configuration = ( configuration . augmentation . chat_engine . guardrails ) guardrails_engine = GuardrailsRegistry . get ( guardrails_configuration . name ) . create ( configuration = configuration . augmentation ) retriever . callback_manager = langfuse_callback_manager llm . callback_manager = langfuse_callback_manager for postprocessor in postprocessors : postprocessor . callback_manager = langfuse_callback_manager return LangfuseChatEngine ( retriever = retriever , llm = llm , node_postprocessors = postprocessors , callback_manager = langfuse_callback_manager , memory = memory , context_prompt = context_prompt_template , system_prompt = system_prompt_template , context_refine_prompt = context_refine_prompt_template , condense_prompt = condense_prompt_template , chainlit_tag_format = configuration . augmentation . langfuse . chainlit_tag_format , guardrails_engine = guardrails_engine , ) @staticmethod def _get_prompt_templates ( configuration : _AugmentationConfiguration , ) -> tuple : \"\"\"Retrieves the prompt template for the augmentation process. Args: configuration: Configuration object containing prompt templates settings. Returns: Tuple of prompt templates for condensing, context generation, context refinement, system prompts. \"\"\" langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . langfuse ) condense_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . condense_prompt_name ) context_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . context_prompt_name ) context_refine_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . context_refine_prompt_name ) system_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . system_prompt_name ) return ( condense_prompt_template , context_prompt_template , context_refine_prompt_template , system_prompt_template , ) SourceProcess Bases: Enum Enumeration of possible chat processing sources. Attributes: CHAT_COMPLETION \u2013 Query from interactive chat completion interface DEPLOYMENT_EVALUATION \u2013 Query from automated deployment testing and evaluation Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 45 46 47 48 49 50 51 52 53 54 class SourceProcess ( Enum ): \"\"\"Enumeration of possible chat processing sources. Attributes: CHAT_COMPLETION: Query from interactive chat completion interface DEPLOYMENT_EVALUATION: Query from automated deployment testing and evaluation \"\"\" CHAT_COMPLETION = 1 DEPLOYMENT_EVALUATION = 2","title":"Chat_engine"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#chat_engine","text":"This module contains functionality related to the the chat_engine module for augmentation.components.chat_engines.langfuse .","title":"Chat_engine"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#chat_engine_1","text":"","title":"Chat_engine"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine","text":"Bases: CondensePlusContextChatEngine Custom chat engine implementing Retrieval-Augmented Generation (RAG). Coordinates retrieval, post-processing, and response generation for RAG workflow. Integrates with Langfuse for tracing and Chainlit for message tracking. Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 class LangfuseChatEngine ( CondensePlusContextChatEngine ): \"\"\"Custom chat engine implementing Retrieval-Augmented Generation (RAG). Coordinates retrieval, post-processing, and response generation for RAG workflow. Integrates with Langfuse for tracing and Chainlit for message tracking. \"\"\" chainlit_tag_format : str = Field ( description = \"Format of the tag used to retrieve the trace by chainlit message id in Langfuse.\" ) input_guardrail_prompt_template : Optional [ PromptTemplate ] = Field ( description = \"Prompt template for validating user input compliance\" , default = None , ) output_guardrail_prompt_template : Optional [ PromptTemplate ] = Field ( description = \"Prompt template for validating response output compliance\" , default = None , ) def __init__ ( self , retriever : BaseRetriever , llm : LLM , memory : BaseMemory , chainlit_tag_format : str , guardrails_engine : BaseGuardrailsEngine , context_prompt : Optional [ Union [ str , PromptTemplate ]] = None , context_refine_prompt : Optional [ Union [ str , PromptTemplate ]] = None , condense_prompt : Optional [ Union [ str , PromptTemplate ]] = None , system_prompt : Optional [ str ] = None , skip_condense : bool = False , node_postprocessors : Optional [ List [ BaseNodePostprocessor ]] = None , callback_manager : Optional [ CallbackManager ] = None , verbose : bool = False , ): \"\"\" Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Args: retriever: Document retriever for RAG llm: Language model for response generation memory: Memory buffer for chat history chainlit_tag_format: Format for Chainlit message ID in Langfuse guardrails_engine: Guardrail engine for input/output validation context_prompt: Prompt for context generation context_refine_prompt: Prompt for refining context condense_prompt: Prompt for condensing context system_prompt: System prompt for LLM input_guardrail_prompt_template: Prompt template for input validation output_guardrail_prompt_template: Prompt template for output validation skip_condense: Flag to skip context condensing node_postprocessors: List of postprocessors for node processing callback_manager: Callback manager for tracing verbose: Flag for verbose output \"\"\" super () . __init__ ( retriever = retriever , llm = llm , memory = memory , context_prompt = context_prompt , context_refine_prompt = context_refine_prompt , condense_prompt = condense_prompt , system_prompt = system_prompt , skip_condense = skip_condense , node_postprocessors = node_postprocessors , callback_manager = callback_manager , verbose = verbose , ) self . guardrails_engine = guardrails_engine self . chainlit_tag_format = chainlit_tag_format def _get_response_synthesizer ( self , chat_history : List [ ChatMessage ], streaming : bool = False ): \"\"\"Override to use SIMPLE_SUMMARIZE mode instead of COMPACT (which uses refine). This eliminates the iterative refinement loop that makes N LLM calls for N documents. Instead, all documents are concatenated and processed in a single LLM call. Args: chat_history: Chat history for context streaming: Whether to stream the response Returns: BaseSynthesizer: Response synthesizer configured for single-pass generation \"\"\" system_prompt = self . _system_prompt or \"\" qa_messages = get_prefix_messages_with_context ( self . _context_prompt_template , system_prompt , [], chat_history , self . _llm . metadata . system_role , ) # Use SIMPLE_SUMMARIZE instead of COMPACT to avoid refinement iterations # This concatenates all retrieved documents and makes ONE LLM call from llama_index.core.prompts import ChatPromptTemplate return get_llama_response_synthesizer ( llm = self . _llm , callback_manager = self . callback_manager , text_qa_template = ChatPromptTemplate . from_messages ( qa_messages , function_mappings = self . _context_prompt_template . function_mappings , ), response_mode = ResponseMode . SIMPLE_SUMMARIZE , streaming = streaming , ) def stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" response = self . _stream_chat ( message = message , chat_history = chat_history , chainlit_message_id = chainlit_message_id , source_process = source_process , ) trace = self . get_current_langfuse_trace () trace . input = message return response @trace_method ( \"chat\" ) def _stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" self . _set_chainlit_message_id ( message_id = chainlit_message_id , source_process = source_process ) guarded_response = self . guardrails_engine . input_guard ( message = message , is_stream = True ) if guarded_response : self . _save_chat_history ( input_message = message , output_message = guarded_response . response ) return guarded_response response = super () . chat ( message = message , chat_history = chat_history ) guarded_response = self . guardrails_engine . output_guard ( message = response . response , is_stream = True ) if guarded_response : self . _save_chat_history ( input_message = message , output_message = guarded_response . response ) return guarded_response response . is_dummy_stream = True return response def get_current_langfuse_trace ( self ) -> StatefulTraceClient : \"\"\"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient: Active Langfuse trace or None if not found \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): return handler . trace return None def set_session_id ( self , session_id : str ) -> None : \"\"\"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Args: session_id: Unique identifier for current user session \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . session_id = session_id def _set_chainlit_message_id ( self , message_id : str , source_process : SourceProcess ) -> None : \"\"\"Configure Chainlit message tracking in Langfuse trace. Links the current Langfuse trace to a Chainlit message ID and tags with the processing source context for traceability in the Langfuse UI. Args: message_id: Chainlit message identifier to reference source_process: Source context enum categorizing the query origin \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . set_trace_params ( tags = [ self . chainlit_tag_format . format ( message_id = message_id ), source_process . name . lower (), ] ) def _save_chat_history ( self , input_message : str , output_message : str ) -> None : \"\"\"Save chat history to memory buffer. Args: input_message: User input message output_message: Generated response message \"\"\" self . _memory . put ( ChatMessage ( role = MessageRole . USER , content = input_message ) ) self . _memory . put ( ChatMessage ( role = MessageRole . ASSISTANT , content = output_message ) )","title":"LangfuseChatEngine"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine.__init__","text":"Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Parameters: retriever ( BaseRetriever ) \u2013 Document retriever for RAG llm ( LLM ) \u2013 Language model for response generation memory ( BaseMemory ) \u2013 Memory buffer for chat history chainlit_tag_format ( str ) \u2013 Format for Chainlit message ID in Langfuse guardrails_engine ( BaseGuardrailsEngine ) \u2013 Guardrail engine for input/output validation context_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for context generation context_refine_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for refining context condense_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for condensing context system_prompt ( Optional [ str ] , default: None ) \u2013 System prompt for LLM input_guardrail_prompt_template \u2013 Prompt template for input validation output_guardrail_prompt_template \u2013 Prompt template for output validation skip_condense ( bool , default: False ) \u2013 Flag to skip context condensing node_postprocessors ( Optional [ List [ BaseNodePostprocessor ]] , default: None ) \u2013 List of postprocessors for node processing callback_manager ( Optional [ CallbackManager ] , default: None ) \u2013 Callback manager for tracing verbose ( bool , default: False ) \u2013 Flag for verbose output Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def __init__ ( self , retriever : BaseRetriever , llm : LLM , memory : BaseMemory , chainlit_tag_format : str , guardrails_engine : BaseGuardrailsEngine , context_prompt : Optional [ Union [ str , PromptTemplate ]] = None , context_refine_prompt : Optional [ Union [ str , PromptTemplate ]] = None , condense_prompt : Optional [ Union [ str , PromptTemplate ]] = None , system_prompt : Optional [ str ] = None , skip_condense : bool = False , node_postprocessors : Optional [ List [ BaseNodePostprocessor ]] = None , callback_manager : Optional [ CallbackManager ] = None , verbose : bool = False , ): \"\"\" Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Args: retriever: Document retriever for RAG llm: Language model for response generation memory: Memory buffer for chat history chainlit_tag_format: Format for Chainlit message ID in Langfuse guardrails_engine: Guardrail engine for input/output validation context_prompt: Prompt for context generation context_refine_prompt: Prompt for refining context condense_prompt: Prompt for condensing context system_prompt: System prompt for LLM input_guardrail_prompt_template: Prompt template for input validation output_guardrail_prompt_template: Prompt template for output validation skip_condense: Flag to skip context condensing node_postprocessors: List of postprocessors for node processing callback_manager: Callback manager for tracing verbose: Flag for verbose output \"\"\" super () . __init__ ( retriever = retriever , llm = llm , memory = memory , context_prompt = context_prompt , context_refine_prompt = context_refine_prompt , condense_prompt = condense_prompt , system_prompt = system_prompt , skip_condense = skip_condense , node_postprocessors = node_postprocessors , callback_manager = callback_manager , verbose = verbose , ) self . guardrails_engine = guardrails_engine self . chainlit_tag_format = chainlit_tag_format","title":"__init__"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine.get_current_langfuse_trace","text":"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient ( StatefulTraceClient ) \u2013 Active Langfuse trace or None if not found Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 242 243 244 245 246 247 248 249 250 251 252 253 254 def get_current_langfuse_trace ( self ) -> StatefulTraceClient : \"\"\"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient: Active Langfuse trace or None if not found \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): return handler . trace return None","title":"get_current_langfuse_trace"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine.set_session_id","text":"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Parameters: session_id ( str ) \u2013 Unique identifier for current user session Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 256 257 258 259 260 261 262 263 264 265 266 267 def set_session_id ( self , session_id : str ) -> None : \"\"\"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Args: session_id: Unique identifier for current user session \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . session_id = session_id","title":"set_session_id"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine.stream_chat","text":"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Parameters: message ( str ) \u2013 Raw query string to process chat_history ( Optional [ List [ ChatMessage ]] , default: None ) \u2013 Optional chat history for context chainlit_message_id ( str , default: None ) \u2013 Optional ID for linking to Chainlit message in UI source_process ( SourceProcess , default: CHAT_COMPLETION ) \u2013 Context identifier indicating query's origin source Returns: AgentChatResponse ( AgentChatResponse ) \u2013 Generated response from RAG pipeline with dummy streaming Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" response = self . _stream_chat ( message = message , chat_history = chat_history , chainlit_message_id = chainlit_message_id , source_process = source_process , ) trace = self . get_current_langfuse_trace () trace . input = message return response","title":"stream_chat"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngineFactory","text":"Bases: Factory Factory for creating configured LangfuseChatEngine instances. Constructs and connects components needed for the RAG pipeline. Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 class LangfuseChatEngineFactory ( Factory ): \"\"\"Factory for creating configured LangfuseChatEngine instances. Constructs and connects components needed for the RAG pipeline. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> LangfuseChatEngine : \"\"\"Create and configure a LangfuseChatEngine instance from configuration. Instantiates all RAG pipeline components based on configuration settings, connects them with a shared callback manager for tracing, and assembles them into a complete chat engine. Args: configuration: Complete augmentation configuration containing settings for all components Returns: LangfuseChatEngine: Fully configured RAG chat engine with tracing \"\"\" chat_engine_configuration = configuration . augmentation . chat_engine llm = LLMRegistry . get ( chat_engine_configuration . llm . provider ) . create ( chat_engine_configuration . llm ) retriever = RetrieverRegistry . get ( chat_engine_configuration . retriever . name ) . create ( configuration ) # Set temporal_domain_config for hybrid filter factory before creating postprocessors temporal_domain_config = configuration . augmentation . temporal_domain HybridFilterFactory . set_temporal_domain_config ( temporal_domain_config ) postprocessors = [ PostprocessorRegistry . get ( postprocessor_configuration . name ) . create ( postprocessor_configuration ) for postprocessor_configuration in chat_engine_configuration . postprocessors ] langfuse_callback_manager = LlamaIndexCallbackManagerFactory . create ( configuration . augmentation . langfuse ) memory = ChatMemoryBuffer ( chat_history = [], token_limit = llm . metadata . context_window - 256 ) ( condense_prompt_template , context_prompt_template , context_refine_prompt_template , system_prompt_template , ) = cls . _get_prompt_templates ( configuration = configuration . augmentation ) guardrails_configuration = ( configuration . augmentation . chat_engine . guardrails ) guardrails_engine = GuardrailsRegistry . get ( guardrails_configuration . name ) . create ( configuration = configuration . augmentation ) retriever . callback_manager = langfuse_callback_manager llm . callback_manager = langfuse_callback_manager for postprocessor in postprocessors : postprocessor . callback_manager = langfuse_callback_manager return LangfuseChatEngine ( retriever = retriever , llm = llm , node_postprocessors = postprocessors , callback_manager = langfuse_callback_manager , memory = memory , context_prompt = context_prompt_template , system_prompt = system_prompt_template , context_refine_prompt = context_refine_prompt_template , condense_prompt = condense_prompt_template , chainlit_tag_format = configuration . augmentation . langfuse . chainlit_tag_format , guardrails_engine = guardrails_engine , ) @staticmethod def _get_prompt_templates ( configuration : _AugmentationConfiguration , ) -> tuple : \"\"\"Retrieves the prompt template for the augmentation process. Args: configuration: Configuration object containing prompt templates settings. Returns: Tuple of prompt templates for condensing, context generation, context refinement, system prompts. \"\"\" langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . langfuse ) condense_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . condense_prompt_name ) context_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . context_prompt_name ) context_refine_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . context_refine_prompt_name ) system_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . system_prompt_name ) return ( condense_prompt_template , context_prompt_template , context_refine_prompt_template , system_prompt_template , )","title":"LangfuseChatEngineFactory"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.SourceProcess","text":"Bases: Enum Enumeration of possible chat processing sources. Attributes: CHAT_COMPLETION \u2013 Query from interactive chat completion interface DEPLOYMENT_EVALUATION \u2013 Query from automated deployment testing and evaluation Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 45 46 47 48 49 50 51 52 53 54 class SourceProcess ( Enum ): \"\"\"Enumeration of possible chat processing sources. Attributes: CHAT_COMPLETION: Query from interactive chat completion interface DEPLOYMENT_EVALUATION: Query from automated deployment testing and evaluation \"\"\" CHAT_COMPLETION = 1 DEPLOYMENT_EVALUATION = 2","title":"SourceProcess"},{"location":"src/augmentation/components/chat_engines/langfuse/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.chat_engines.langfuse . Configuration LangfuseChatEngineConfiguration Bases: BaseChatEngineConfiguration Configuration class for the Langfuse chat engine. This class represents the configuration settings required for setting up and operating a chat engine with Langfuse integration. Langfuse provides observability and analytics capabilities for LLM applications. Source code in src/augmentation/components/chat_engines/langfuse/configuration.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class LangfuseChatEngineConfiguration ( BaseChatEngineConfiguration ): \"\"\" Configuration class for the Langfuse chat engine. This class represents the configuration settings required for setting up and operating a chat engine with Langfuse integration. Langfuse provides observability and analytics capabilities for LLM applications. \"\"\" name : ChatEngineName = Field ( ChatEngineName . LANGFUSE , description = \"The name of the chat engine configuration integrated with langfuse.\" , )","title":"Configuration"},{"location":"src/augmentation/components/chat_engines/langfuse/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.chat_engines.langfuse .","title":"Configuration"},{"location":"src/augmentation/components/chat_engines/langfuse/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/chat_engines/langfuse/configuration/#src.augmentation.components.chat_engines.langfuse.configuration.LangfuseChatEngineConfiguration","text":"Bases: BaseChatEngineConfiguration Configuration class for the Langfuse chat engine. This class represents the configuration settings required for setting up and operating a chat engine with Langfuse integration. Langfuse provides observability and analytics capabilities for LLM applications. Source code in src/augmentation/components/chat_engines/langfuse/configuration.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class LangfuseChatEngineConfiguration ( BaseChatEngineConfiguration ): \"\"\" Configuration class for the Langfuse chat engine. This class represents the configuration settings required for setting up and operating a chat engine with Langfuse integration. Langfuse provides observability and analytics capabilities for LLM applications. \"\"\" name : ChatEngineName = Field ( ChatEngineName . LANGFUSE , description = \"The name of the chat engine configuration integrated with langfuse.\" , )","title":"LangfuseChatEngineConfiguration"},{"location":"src/augmentation/components/guardrails/base_guardrails/","text":"Base_guardrails This module contains functionality related to the the base_guardrails module for augmentation.components.guardrails . Base_guardrails BaseGuardrailsEngine Bases: ABC Source code in src/augmentation/components/guardrails/base_guardrails.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class BaseGuardrailsEngine ( ABC ): @abstractmethod def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the input is allowed, otherwise an AgentChatResponse containing a message indicating the input is not allowed. \"\"\" pass @abstractmethod def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the output is allowed, otherwise an AgentChatResponse containing a message indicating the output is not allowed. \"\"\" pass input_guard ( message , is_stream ) abstractmethod Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Parameters: message ( str ) \u2013 Generated response message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: bool ( Optional [ AgentChatResponse ] ) \u2013 None if the input is allowed, otherwise an AgentChatResponse Optional [ AgentChatResponse ] \u2013 containing a message indicating the input is not allowed. Source code in src/augmentation/components/guardrails/base_guardrails.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @abstractmethod def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the input is allowed, otherwise an AgentChatResponse containing a message indicating the input is not allowed. \"\"\" pass output_guard ( message , is_stream ) abstractmethod Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Parameters: message ( str ) \u2013 User input message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: bool ( Optional [ AgentChatResponse ] ) \u2013 None if the output is allowed, otherwise an AgentChatResponse Optional [ AgentChatResponse ] \u2013 containing a message indicating the output is not allowed. Source code in src/augmentation/components/guardrails/base_guardrails.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @abstractmethod def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the output is allowed, otherwise an AgentChatResponse containing a message indicating the output is not allowed. \"\"\" pass","title":"Base_guardrails"},{"location":"src/augmentation/components/guardrails/base_guardrails/#base_guardrails","text":"This module contains functionality related to the the base_guardrails module for augmentation.components.guardrails .","title":"Base_guardrails"},{"location":"src/augmentation/components/guardrails/base_guardrails/#base_guardrails_1","text":"","title":"Base_guardrails"},{"location":"src/augmentation/components/guardrails/base_guardrails/#src.augmentation.components.guardrails.base_guardrails.BaseGuardrailsEngine","text":"Bases: ABC Source code in src/augmentation/components/guardrails/base_guardrails.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class BaseGuardrailsEngine ( ABC ): @abstractmethod def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the input is allowed, otherwise an AgentChatResponse containing a message indicating the input is not allowed. \"\"\" pass @abstractmethod def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the output is allowed, otherwise an AgentChatResponse containing a message indicating the output is not allowed. \"\"\" pass","title":"BaseGuardrailsEngine"},{"location":"src/augmentation/components/guardrails/base_guardrails/#src.augmentation.components.guardrails.base_guardrails.BaseGuardrailsEngine.input_guard","text":"Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Parameters: message ( str ) \u2013 Generated response message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: bool ( Optional [ AgentChatResponse ] ) \u2013 None if the input is allowed, otherwise an AgentChatResponse Optional [ AgentChatResponse ] \u2013 containing a message indicating the input is not allowed. Source code in src/augmentation/components/guardrails/base_guardrails.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @abstractmethod def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the input is allowed, otherwise an AgentChatResponse containing a message indicating the input is not allowed. \"\"\" pass","title":"input_guard"},{"location":"src/augmentation/components/guardrails/base_guardrails/#src.augmentation.components.guardrails.base_guardrails.BaseGuardrailsEngine.output_guard","text":"Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Parameters: message ( str ) \u2013 User input message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: bool ( Optional [ AgentChatResponse ] ) \u2013 None if the output is allowed, otherwise an AgentChatResponse Optional [ AgentChatResponse ] \u2013 containing a message indicating the output is not allowed. Source code in src/augmentation/components/guardrails/base_guardrails.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @abstractmethod def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the output is allowed, otherwise an AgentChatResponse containing a message indicating the output is not allowed. \"\"\" pass","title":"output_guard"},{"location":"src/augmentation/components/guardrails/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.guardrails . Registry","title":"Registry"},{"location":"src/augmentation/components/guardrails/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.guardrails .","title":"Registry"},{"location":"src/augmentation/components/guardrails/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/guardrails/basic/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.guardrails.basic . Configuration","title":"Configuration"},{"location":"src/augmentation/components/guardrails/basic/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.guardrails.basic .","title":"Configuration"},{"location":"src/augmentation/components/guardrails/basic/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/guardrails/basic/guardrails/","text":"Guardrails This module contains functionality related to the the guardrails module for augmentation.components.guardrails.basic . Guardrails BasicGuardrailsEngine Bases: BaseGuardrailsEngine Source code in src/augmentation/components/guardrails/basic/guardrails.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class BasicGuardrailsEngine ( BaseGuardrailsEngine ): def __init__ ( self , llm : LLM , input_prompt_template : PromptTemplate , output_prompt_template : PromptTemplate , ): \"\"\" Initialize GuardrailsEngine with LLM and prompt templates. Args: llm: Language model for response generation input_prompt_template: Prompt template for validating user input compliance output_prompt_template: Prompt template for validating response output compliance \"\"\" self . llm = llm self . input_prompt_template = input_prompt_template self . output_prompt_template = output_prompt_template def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate user input message against guardrail rules. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the input is allowed \"\"\" if not self . _is_input_allowed ( message ): return AgentChatResponse ( response = \"I'm unable to answer this question as it doesn't comply with our usage guidelines.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate generated response message against guardrail rules. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the output is allowed \"\"\" if not self . _is_output_allowed ( message ): return AgentChatResponse ( response = \"I apologize, but I'm unable to provide a response to this question.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None def _is_input_allowed ( self , message : str ) -> bool : \"\"\" Check if the input message is allowed based on guardrail rules. Args: message: User input message to validate Returns: bool: True if the input is allowed, False otherwise \"\"\" prompt = self . input_prompt_template . format ( message ) resp = self . llm . complete ( prompt ) text = resp . text . lower () return not ( \"yes\" in text or \"true\" in text ) def _is_output_allowed ( self , message : str ) -> bool : \"\"\" Check if the output message is allowed based on guardrail rules. Args: message: Generated response message to validate Returns: bool: True if the output is allowed, False otherwise \"\"\" prompt = self . output_prompt_template . format ( message ) response = self . llm . complete ( prompt ) text = response . text . lower () return not ( \"yes\" in text or \"true\" in text ) __init__ ( llm , input_prompt_template , output_prompt_template ) Initialize GuardrailsEngine with LLM and prompt templates. Parameters: llm ( LLM ) \u2013 Language model for response generation input_prompt_template ( PromptTemplate ) \u2013 Prompt template for validating user input compliance output_prompt_template ( PromptTemplate ) \u2013 Prompt template for validating response output compliance Source code in src/augmentation/components/guardrails/basic/guardrails.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , llm : LLM , input_prompt_template : PromptTemplate , output_prompt_template : PromptTemplate , ): \"\"\" Initialize GuardrailsEngine with LLM and prompt templates. Args: llm: Language model for response generation input_prompt_template: Prompt template for validating user input compliance output_prompt_template: Prompt template for validating response output compliance \"\"\" self . llm = llm self . input_prompt_template = input_prompt_template self . output_prompt_template = output_prompt_template input_guard ( message , is_stream ) Validate user input message against guardrail rules. Parameters: message ( str ) \u2013 User input message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: Optional [ AgentChatResponse ] \u2013 Optional[AgentChatResponse]: Response indicating if the input is allowed Source code in src/augmentation/components/guardrails/basic/guardrails.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate user input message against guardrail rules. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the input is allowed \"\"\" if not self . _is_input_allowed ( message ): return AgentChatResponse ( response = \"I'm unable to answer this question as it doesn't comply with our usage guidelines.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None output_guard ( message , is_stream ) Validate generated response message against guardrail rules. Parameters: message ( str ) \u2013 Generated response message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: Optional [ AgentChatResponse ] \u2013 Optional[AgentChatResponse]: Response indicating if the output is allowed Source code in src/augmentation/components/guardrails/basic/guardrails.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate generated response message against guardrail rules. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the output is allowed \"\"\" if not self . _is_output_allowed ( message ): return AgentChatResponse ( response = \"I apologize, but I'm unable to provide a response to this question.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None BasicGuardrailsEngineFactory Bases: Factory Factory for creating a GuardrailsEngine from AugmentationConfiguration. Source code in src/augmentation/components/guardrails/basic/guardrails.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class BasicGuardrailsEngineFactory ( Factory ): \"\"\"Factory for creating a GuardrailsEngine from AugmentationConfiguration.\"\"\" _configuration_class = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> BasicGuardrailsEngine : \"\"\" Create and configure a GuardrailsEngine instance from configuration. Instantiates the LLM and retrieves the prompt templates for input and output guardrails. Args: configuration: Configuration object containing settings for LLM and prompt templates Returns: GuardrailsEngine: Fully configured guardrail engine with LLM and prompt templates \"\"\" llm_configuration = configuration . chat_engine . guardrails . llm llm = LLMRegistry . get ( llm_configuration . provider ) . create ( llm_configuration ) input_guardrail_prompt_template , output_guardrail_prompt_template = ( cls . _get_prompt_templates ( configuration = configuration ) ) return BasicGuardrailsEngine ( llm = llm , input_prompt_template = input_guardrail_prompt_template , output_prompt_template = output_guardrail_prompt_template , ) @staticmethod def _get_prompt_templates ( configuration : _AugmentationConfiguration , ) -> tuple : \"\"\"Retrieves the prompt template for the guardrail process. Args: configuration: Configuration object containing langfuse and prompt templates settings. Returns: Tuple of prompt templates for condensing, context generation, context refinement, system prompts, and guardrail prompts. \"\"\" langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . langfuse ) guardrails_configuration = configuration . chat_engine . guardrails input_guardrail_prompt_template = ( langfuse_prompt_service . get_prompt_template ( prompt_name = guardrails_configuration . input_prompt_name ) ) output_guardrail_prompt_template = ( langfuse_prompt_service . get_prompt_template ( prompt_name = guardrails_configuration . output_prompt_name ) ) return ( input_guardrail_prompt_template , output_guardrail_prompt_template , )","title":"Guardrails"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#guardrails","text":"This module contains functionality related to the the guardrails module for augmentation.components.guardrails.basic .","title":"Guardrails"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#guardrails_1","text":"","title":"Guardrails"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngine","text":"Bases: BaseGuardrailsEngine Source code in src/augmentation/components/guardrails/basic/guardrails.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class BasicGuardrailsEngine ( BaseGuardrailsEngine ): def __init__ ( self , llm : LLM , input_prompt_template : PromptTemplate , output_prompt_template : PromptTemplate , ): \"\"\" Initialize GuardrailsEngine with LLM and prompt templates. Args: llm: Language model for response generation input_prompt_template: Prompt template for validating user input compliance output_prompt_template: Prompt template for validating response output compliance \"\"\" self . llm = llm self . input_prompt_template = input_prompt_template self . output_prompt_template = output_prompt_template def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate user input message against guardrail rules. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the input is allowed \"\"\" if not self . _is_input_allowed ( message ): return AgentChatResponse ( response = \"I'm unable to answer this question as it doesn't comply with our usage guidelines.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate generated response message against guardrail rules. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the output is allowed \"\"\" if not self . _is_output_allowed ( message ): return AgentChatResponse ( response = \"I apologize, but I'm unable to provide a response to this question.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None def _is_input_allowed ( self , message : str ) -> bool : \"\"\" Check if the input message is allowed based on guardrail rules. Args: message: User input message to validate Returns: bool: True if the input is allowed, False otherwise \"\"\" prompt = self . input_prompt_template . format ( message ) resp = self . llm . complete ( prompt ) text = resp . text . lower () return not ( \"yes\" in text or \"true\" in text ) def _is_output_allowed ( self , message : str ) -> bool : \"\"\" Check if the output message is allowed based on guardrail rules. Args: message: Generated response message to validate Returns: bool: True if the output is allowed, False otherwise \"\"\" prompt = self . output_prompt_template . format ( message ) response = self . llm . complete ( prompt ) text = response . text . lower () return not ( \"yes\" in text or \"true\" in text )","title":"BasicGuardrailsEngine"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngine.__init__","text":"Initialize GuardrailsEngine with LLM and prompt templates. Parameters: llm ( LLM ) \u2013 Language model for response generation input_prompt_template ( PromptTemplate ) \u2013 Prompt template for validating user input compliance output_prompt_template ( PromptTemplate ) \u2013 Prompt template for validating response output compliance Source code in src/augmentation/components/guardrails/basic/guardrails.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , llm : LLM , input_prompt_template : PromptTemplate , output_prompt_template : PromptTemplate , ): \"\"\" Initialize GuardrailsEngine with LLM and prompt templates. Args: llm: Language model for response generation input_prompt_template: Prompt template for validating user input compliance output_prompt_template: Prompt template for validating response output compliance \"\"\" self . llm = llm self . input_prompt_template = input_prompt_template self . output_prompt_template = output_prompt_template","title":"__init__"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngine.input_guard","text":"Validate user input message against guardrail rules. Parameters: message ( str ) \u2013 User input message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: Optional [ AgentChatResponse ] \u2013 Optional[AgentChatResponse]: Response indicating if the input is allowed Source code in src/augmentation/components/guardrails/basic/guardrails.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate user input message against guardrail rules. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the input is allowed \"\"\" if not self . _is_input_allowed ( message ): return AgentChatResponse ( response = \"I'm unable to answer this question as it doesn't comply with our usage guidelines.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None","title":"input_guard"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngine.output_guard","text":"Validate generated response message against guardrail rules. Parameters: message ( str ) \u2013 Generated response message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: Optional [ AgentChatResponse ] \u2013 Optional[AgentChatResponse]: Response indicating if the output is allowed Source code in src/augmentation/components/guardrails/basic/guardrails.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate generated response message against guardrail rules. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the output is allowed \"\"\" if not self . _is_output_allowed ( message ): return AgentChatResponse ( response = \"I apologize, but I'm unable to provide a response to this question.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None","title":"output_guard"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngineFactory","text":"Bases: Factory Factory for creating a GuardrailsEngine from AugmentationConfiguration. Source code in src/augmentation/components/guardrails/basic/guardrails.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class BasicGuardrailsEngineFactory ( Factory ): \"\"\"Factory for creating a GuardrailsEngine from AugmentationConfiguration.\"\"\" _configuration_class = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> BasicGuardrailsEngine : \"\"\" Create and configure a GuardrailsEngine instance from configuration. Instantiates the LLM and retrieves the prompt templates for input and output guardrails. Args: configuration: Configuration object containing settings for LLM and prompt templates Returns: GuardrailsEngine: Fully configured guardrail engine with LLM and prompt templates \"\"\" llm_configuration = configuration . chat_engine . guardrails . llm llm = LLMRegistry . get ( llm_configuration . provider ) . create ( llm_configuration ) input_guardrail_prompt_template , output_guardrail_prompt_template = ( cls . _get_prompt_templates ( configuration = configuration ) ) return BasicGuardrailsEngine ( llm = llm , input_prompt_template = input_guardrail_prompt_template , output_prompt_template = output_guardrail_prompt_template , ) @staticmethod def _get_prompt_templates ( configuration : _AugmentationConfiguration , ) -> tuple : \"\"\"Retrieves the prompt template for the guardrail process. Args: configuration: Configuration object containing langfuse and prompt templates settings. Returns: Tuple of prompt templates for condensing, context generation, context refinement, system prompts, and guardrail prompts. \"\"\" langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . langfuse ) guardrails_configuration = configuration . chat_engine . guardrails input_guardrail_prompt_template = ( langfuse_prompt_service . get_prompt_template ( prompt_name = guardrails_configuration . input_prompt_name ) ) output_guardrail_prompt_template = ( langfuse_prompt_service . get_prompt_template ( prompt_name = guardrails_configuration . output_prompt_name ) ) return ( input_guardrail_prompt_template , output_guardrail_prompt_template , )","title":"BasicGuardrailsEngineFactory"},{"location":"src/augmentation/components/llms/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.llms . Registry LLMRegistry Bases: Registry Registry for Large Language Model providers. This registry maps LLM provider names (defined in LLMProviderName enum) to their corresponding implementation classes. It allows for dynamic registration and retrieval of LLM provider implementations based on their enumerated types. Attributes: _key_class ( Type ) \u2013 The class used as the key for the registry. Source code in src/augmentation/components/llms/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class LLMRegistry ( Registry ): \"\"\" Registry for Large Language Model providers. This registry maps LLM provider names (defined in LLMProviderName enum) to their corresponding implementation classes. It allows for dynamic registration and retrieval of LLM provider implementations based on their enumerated types. Attributes: _key_class (Type): The class used as the key for the registry. \"\"\" _key_class : Type = LLMProviderName","title":"Registry"},{"location":"src/augmentation/components/llms/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.llms .","title":"Registry"},{"location":"src/augmentation/components/llms/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/llms/registry/#src.augmentation.components.llms.registry.LLMRegistry","text":"Bases: Registry Registry for Large Language Model providers. This registry maps LLM provider names (defined in LLMProviderName enum) to their corresponding implementation classes. It allows for dynamic registration and retrieval of LLM provider implementations based on their enumerated types. Attributes: _key_class ( Type ) \u2013 The class used as the key for the registry. Source code in src/augmentation/components/llms/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class LLMRegistry ( Registry ): \"\"\" Registry for Large Language Model providers. This registry maps LLM provider names (defined in LLMProviderName enum) to their corresponding implementation classes. It allows for dynamic registration and retrieval of LLM provider implementations based on their enumerated types. Attributes: _key_class (Type): The class used as the key for the registry. \"\"\" _key_class : Type = LLMProviderName","title":"LLMRegistry"},{"location":"src/augmentation/components/llms/lite_llm/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.llms.lite_llm . Configuration LiteLLMConfiguration Bases: LLMConfiguration Configuration for LiteLLM language models, which expose multiple providers. To check available providers visit https://docs.litellm.ai/docs/providers. This class extends the base LLMConfiguration to provide LiteLLM-specific configuration options and secrets management. Source code in src/augmentation/components/llms/lite_llm/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LiteLLMConfiguration ( LLMConfiguration ): \"\"\"Configuration for LiteLLM language models, which expose multiple providers. To check available providers visit https://docs.litellm.ai/docs/providers. This class extends the base LLMConfiguration to provide LiteLLM-specific configuration options and secrets management. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix `env_prefix` has to be provided by parent model, so it includes model name. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model provider.\" , ) provider : Literal [ LLMProviderName . LITE_LLM ] = Field ( ... , description = \"The name of the language model provider.\" ) api_base : Optional [ str ] = Field ( None , description = \"Base URL for the API endpoint of the language model provider. If not provided, api_base is determined by LiteLLM.\" , ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) def model_post_init ( self , context : Any ) -> None : \"\"\"Before loading the secrets, its model config's `env_prefix` has to be set to corresponding model. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets_class . model_config [ \"env_prefix\" ] = ( self . _get_secrets_env_prefix () ) super () . model_post_init ( context ) def _get_secrets_env_prefix ( self ) -> str : \"\"\"Returns the environment variable prefix for the secrets. It uses the model name to create a unique prefix for the secrets. All non-alphanumeric characters in model name are replaced with underscores. The prefix is used to load secrets from environment variables. Returns: str: The environment variable prefix for the secrets. \"\"\" model_name = self . name . upper () model_name = \"\" . join ( char if char . isalnum () else \"_\" for char in model_name ) return f \"RAG__LLMS__ { model_name } __\" Secrets Bases: BaseSecrets Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix env_prefix has to be provided by parent model, so it includes model name. Source code in src/augmentation/components/llms/lite_llm/configuration.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Secrets ( BaseSecrets ): \"\"\"Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix `env_prefix` has to be provided by parent model, so it includes model name. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model provider.\" , ) model_post_init ( context ) Before loading the secrets, its model config's env_prefix has to be set to corresponding model. Parameters: context ( Any ) \u2013 The context passed to the pydantic model, must contain 'secrets_file' key. Source code in src/augmentation/components/llms/lite_llm/configuration.py 50 51 52 53 54 55 56 57 58 59 60 def model_post_init ( self , context : Any ) -> None : \"\"\"Before loading the secrets, its model config's `env_prefix` has to be set to corresponding model. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets_class . model_config [ \"env_prefix\" ] = ( self . _get_secrets_env_prefix () ) super () . model_post_init ( context )","title":"Configuration"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.llms.lite_llm .","title":"Configuration"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#src.augmentation.components.llms.lite_llm.configuration.LiteLLMConfiguration","text":"Bases: LLMConfiguration Configuration for LiteLLM language models, which expose multiple providers. To check available providers visit https://docs.litellm.ai/docs/providers. This class extends the base LLMConfiguration to provide LiteLLM-specific configuration options and secrets management. Source code in src/augmentation/components/llms/lite_llm/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LiteLLMConfiguration ( LLMConfiguration ): \"\"\"Configuration for LiteLLM language models, which expose multiple providers. To check available providers visit https://docs.litellm.ai/docs/providers. This class extends the base LLMConfiguration to provide LiteLLM-specific configuration options and secrets management. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix `env_prefix` has to be provided by parent model, so it includes model name. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model provider.\" , ) provider : Literal [ LLMProviderName . LITE_LLM ] = Field ( ... , description = \"The name of the language model provider.\" ) api_base : Optional [ str ] = Field ( None , description = \"Base URL for the API endpoint of the language model provider. If not provided, api_base is determined by LiteLLM.\" , ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) def model_post_init ( self , context : Any ) -> None : \"\"\"Before loading the secrets, its model config's `env_prefix` has to be set to corresponding model. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets_class . model_config [ \"env_prefix\" ] = ( self . _get_secrets_env_prefix () ) super () . model_post_init ( context ) def _get_secrets_env_prefix ( self ) -> str : \"\"\"Returns the environment variable prefix for the secrets. It uses the model name to create a unique prefix for the secrets. All non-alphanumeric characters in model name are replaced with underscores. The prefix is used to load secrets from environment variables. Returns: str: The environment variable prefix for the secrets. \"\"\" model_name = self . name . upper () model_name = \"\" . join ( char if char . isalnum () else \"_\" for char in model_name ) return f \"RAG__LLMS__ { model_name } __\"","title":"LiteLLMConfiguration"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#src.augmentation.components.llms.lite_llm.configuration.LiteLLMConfiguration.Secrets","text":"Bases: BaseSecrets Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix env_prefix has to be provided by parent model, so it includes model name. Source code in src/augmentation/components/llms/lite_llm/configuration.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Secrets ( BaseSecrets ): \"\"\"Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix `env_prefix` has to be provided by parent model, so it includes model name. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model provider.\" , )","title":"Secrets"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#src.augmentation.components.llms.lite_llm.configuration.LiteLLMConfiguration.model_post_init","text":"Before loading the secrets, its model config's env_prefix has to be set to corresponding model. Parameters: context ( Any ) \u2013 The context passed to the pydantic model, must contain 'secrets_file' key. Source code in src/augmentation/components/llms/lite_llm/configuration.py 50 51 52 53 54 55 56 57 58 59 60 def model_post_init ( self , context : Any ) -> None : \"\"\"Before loading the secrets, its model config's `env_prefix` has to be set to corresponding model. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets_class . model_config [ \"env_prefix\" ] = ( self . _get_secrets_env_prefix () ) super () . model_post_init ( context )","title":"model_post_init"},{"location":"src/augmentation/components/llms/lite_llm/llm/","text":"Llm This module contains functionality related to the the llm module for augmentation.components.llms.lite_llm . Llm ConfigurableLiteLLM Bases: LiteLLM Extended LiteLLM that allows context_window and num_output override. This subclass overrides the metadata property to allow configuration-based context_window and num_output values, rather than relying solely on LiteLLM's model registry defaults. Source code in src/augmentation/components/llms/lite_llm/llm.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class ConfigurableLiteLLM ( LiteLLM ): \"\"\" Extended LiteLLM that allows context_window and num_output override. This subclass overrides the metadata property to allow configuration-based context_window and num_output values, rather than relying solely on LiteLLM's model registry defaults. \"\"\" def __init__ ( self , * args , context_window : int | None = None , num_output : int | None = None , ** kwargs , ): \"\"\"Initialize with optional context_window and num_output overrides.\"\"\" super () . __init__ ( * args , ** kwargs ) self . _context_window_override = context_window self . _num_output_override = num_output @property def metadata ( self ) -> LLMMetadata : \"\"\" Get LLM metadata with configuration overrides applied. Returns: LLMMetadata with context_window and num_output from configuration if specified, otherwise uses parent class defaults. \"\"\" # Get default metadata from parent default_metadata = super () . metadata # Apply overrides if configured context_window = ( self . _context_window_override if self . _context_window_override is not None else default_metadata . context_window ) num_output = ( self . _num_output_override if self . _num_output_override is not None else default_metadata . num_output ) return LLMMetadata ( context_window = context_window , num_output = num_output , is_chat_model = default_metadata . is_chat_model , is_function_calling_model = default_metadata . is_function_calling_model , model_name = default_metadata . model_name , system_role = default_metadata . system_role , ) metadata property Get LLM metadata with configuration overrides applied. Returns: LLMMetadata \u2013 LLMMetadata with context_window and num_output from configuration LLMMetadata \u2013 if specified, otherwise uses parent class defaults. __init__ ( * args , context_window = None , num_output = None , ** kwargs ) Initialize with optional context_window and num_output overrides. Source code in src/augmentation/components/llms/lite_llm/llm.py 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , * args , context_window : int | None = None , num_output : int | None = None , ** kwargs , ): \"\"\"Initialize with optional context_window and num_output overrides.\"\"\" super () . __init__ ( * args , ** kwargs ) self . _context_window_override = context_window self . _num_output_override = num_output LiteLLMFactory Bases: SingletonFactory Factory class for creating LiteLLM language model instances. This class implements the Singleton pattern to ensure only one instance of an LiteLLM model with a specific configuration exists in the application. It uses LiteLLMConfiguration to configure the model parameters. Attributes: _configuration_class ( Type ) \u2013 Type of the configuration class used for creating LiteLLM model instances. Source code in src/augmentation/components/llms/lite_llm/llm.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class LiteLLMFactory ( SingletonFactory ): \"\"\" Factory class for creating LiteLLM language model instances. This class implements the Singleton pattern to ensure only one instance of an LiteLLM model with a specific configuration exists in the application. It uses LiteLLMConfiguration to configure the model parameters. Attributes: _configuration_class: Type of the configuration class used for creating LiteLLM model instances. \"\"\" _configuration_class : Type = LiteLLMConfiguration @classmethod def _create_instance ( cls , configuration : LiteLLMConfiguration ) -> LiteLLM : \"\"\" Creates a new instance of the LiteLLM language model. Args: configuration (LiteLLMConfiguration): Configuration object containing settings for the LiteLLM model, including API key, model name, maximum tokens, retry settings, context_window, and num output. Returns: ConfigurableLiteLLM: An instance of the LiteLLM language model configured with the provided settings, including optional context_window and num_output overrides. \"\"\" return ConfigurableLiteLLM ( api_key = configuration . secrets . api_key . get_secret_value (), api_base = configuration . api_base , model = configuration . name , max_tokens = configuration . max_tokens , max_retries = configuration . max_retries , context_window = configuration . context_window , num_output = configuration . num_output , )","title":"Llm"},{"location":"src/augmentation/components/llms/lite_llm/llm/#llm","text":"This module contains functionality related to the the llm module for augmentation.components.llms.lite_llm .","title":"Llm"},{"location":"src/augmentation/components/llms/lite_llm/llm/#llm_1","text":"","title":"Llm"},{"location":"src/augmentation/components/llms/lite_llm/llm/#src.augmentation.components.llms.lite_llm.llm.ConfigurableLiteLLM","text":"Bases: LiteLLM Extended LiteLLM that allows context_window and num_output override. This subclass overrides the metadata property to allow configuration-based context_window and num_output values, rather than relying solely on LiteLLM's model registry defaults. Source code in src/augmentation/components/llms/lite_llm/llm.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class ConfigurableLiteLLM ( LiteLLM ): \"\"\" Extended LiteLLM that allows context_window and num_output override. This subclass overrides the metadata property to allow configuration-based context_window and num_output values, rather than relying solely on LiteLLM's model registry defaults. \"\"\" def __init__ ( self , * args , context_window : int | None = None , num_output : int | None = None , ** kwargs , ): \"\"\"Initialize with optional context_window and num_output overrides.\"\"\" super () . __init__ ( * args , ** kwargs ) self . _context_window_override = context_window self . _num_output_override = num_output @property def metadata ( self ) -> LLMMetadata : \"\"\" Get LLM metadata with configuration overrides applied. Returns: LLMMetadata with context_window and num_output from configuration if specified, otherwise uses parent class defaults. \"\"\" # Get default metadata from parent default_metadata = super () . metadata # Apply overrides if configured context_window = ( self . _context_window_override if self . _context_window_override is not None else default_metadata . context_window ) num_output = ( self . _num_output_override if self . _num_output_override is not None else default_metadata . num_output ) return LLMMetadata ( context_window = context_window , num_output = num_output , is_chat_model = default_metadata . is_chat_model , is_function_calling_model = default_metadata . is_function_calling_model , model_name = default_metadata . model_name , system_role = default_metadata . system_role , )","title":"ConfigurableLiteLLM"},{"location":"src/augmentation/components/llms/lite_llm/llm/#src.augmentation.components.llms.lite_llm.llm.ConfigurableLiteLLM.metadata","text":"Get LLM metadata with configuration overrides applied. Returns: LLMMetadata \u2013 LLMMetadata with context_window and num_output from configuration LLMMetadata \u2013 if specified, otherwise uses parent class defaults.","title":"metadata"},{"location":"src/augmentation/components/llms/lite_llm/llm/#src.augmentation.components.llms.lite_llm.llm.ConfigurableLiteLLM.__init__","text":"Initialize with optional context_window and num_output overrides. Source code in src/augmentation/components/llms/lite_llm/llm.py 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , * args , context_window : int | None = None , num_output : int | None = None , ** kwargs , ): \"\"\"Initialize with optional context_window and num_output overrides.\"\"\" super () . __init__ ( * args , ** kwargs ) self . _context_window_override = context_window self . _num_output_override = num_output","title":"__init__"},{"location":"src/augmentation/components/llms/lite_llm/llm/#src.augmentation.components.llms.lite_llm.llm.LiteLLMFactory","text":"Bases: SingletonFactory Factory class for creating LiteLLM language model instances. This class implements the Singleton pattern to ensure only one instance of an LiteLLM model with a specific configuration exists in the application. It uses LiteLLMConfiguration to configure the model parameters. Attributes: _configuration_class ( Type ) \u2013 Type of the configuration class used for creating LiteLLM model instances. Source code in src/augmentation/components/llms/lite_llm/llm.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class LiteLLMFactory ( SingletonFactory ): \"\"\" Factory class for creating LiteLLM language model instances. This class implements the Singleton pattern to ensure only one instance of an LiteLLM model with a specific configuration exists in the application. It uses LiteLLMConfiguration to configure the model parameters. Attributes: _configuration_class: Type of the configuration class used for creating LiteLLM model instances. \"\"\" _configuration_class : Type = LiteLLMConfiguration @classmethod def _create_instance ( cls , configuration : LiteLLMConfiguration ) -> LiteLLM : \"\"\" Creates a new instance of the LiteLLM language model. Args: configuration (LiteLLMConfiguration): Configuration object containing settings for the LiteLLM model, including API key, model name, maximum tokens, retry settings, context_window, and num output. Returns: ConfigurableLiteLLM: An instance of the LiteLLM language model configured with the provided settings, including optional context_window and num_output overrides. \"\"\" return ConfigurableLiteLLM ( api_key = configuration . secrets . api_key . get_secret_value (), api_base = configuration . api_base , model = configuration . name , max_tokens = configuration . max_tokens , max_retries = configuration . max_retries , context_window = configuration . context_window , num_output = configuration . num_output , )","title":"LiteLLMFactory"},{"location":"src/augmentation/components/postprocessors/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.postprocessors . Registry PostprocessorRegistry Bases: Registry Registry for managing postprocessors in the RAG pipeline. This registry uses PostProcessorName as the key class to identify and retrieve different postprocessor implementations. Postprocessors are components that perform final transformations on data after the main processing is complete. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to PostProcessorName. Source code in src/augmentation/components/postprocessors/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class PostprocessorRegistry ( Registry ): \"\"\" Registry for managing postprocessors in the RAG pipeline. This registry uses PostProcessorName as the key class to identify and retrieve different postprocessor implementations. Postprocessors are components that perform final transformations on data after the main processing is complete. Attributes: _key_class (Type): The class type used for registry keys, set to PostProcessorName. \"\"\" _key_class : Type = PostProcessorName","title":"Registry"},{"location":"src/augmentation/components/postprocessors/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.postprocessors .","title":"Registry"},{"location":"src/augmentation/components/postprocessors/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/postprocessors/registry/#src.augmentation.components.postprocessors.registry.PostprocessorRegistry","text":"Bases: Registry Registry for managing postprocessors in the RAG pipeline. This registry uses PostProcessorName as the key class to identify and retrieve different postprocessor implementations. Postprocessors are components that perform final transformations on data after the main processing is complete. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to PostProcessorName. Source code in src/augmentation/components/postprocessors/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class PostprocessorRegistry ( Registry ): \"\"\" Registry for managing postprocessors in the RAG pipeline. This registry uses PostProcessorName as the key class to identify and retrieve different postprocessor implementations. Postprocessors are components that perform final transformations on data after the main processing is complete. Attributes: _key_class (Type): The class type used for registry keys, set to PostProcessorName. \"\"\" _key_class : Type = PostProcessorName","title":"PostprocessorRegistry"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/","text":"Configuraiton This module contains functionality related to the the configuraiton module for augmentation.components.postprocessors.colbert_rerank . Configuraiton ColbertRerankConfiguration Bases: PostProcessorConfiguration Configuration for the ColBERT reranking postprocessor. This class defines the settings needed to perform document reranking using the ColBERT retrieval model, which improves search quality through late interaction between queries and documents. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class ColbertRerankConfiguration ( PostProcessorConfiguration ): \"\"\" Configuration for the ColBERT reranking postprocessor. This class defines the settings needed to perform document reranking using the ColBERT retrieval model, which improves search quality through late interaction between queries and documents. \"\"\" class Models ( str , Enum ): \"\"\"Supported ColBERT models for reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" class Tokenizers ( str , Enum ): \"\"\"Supported tokenizers for ColBERT reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" name : Literal [ PostProcessorName . COLBERT_RERANK ] = Field ( ... , description = \"The name of the postprocessor.\" ) model : Models = Field ( Models . COLBERTV2 , description = \"Model used for reranking\" ) tokenizer : Tokenizers = Field ( Tokenizers . COLBERTV2 , description = \"Tokenizer used for reranking\" ) top_n : int = Field ( 5 , description = \"Number of documents to be reranked\" ) keep_retrieval_score : bool = Field ( True , description = \"Toggle to keep the retrieval score after the reranking\" , ) Models Bases: str , Enum Supported ColBERT models for reranking. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 21 22 23 24 class Models ( str , Enum ): \"\"\"Supported ColBERT models for reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" Tokenizers Bases: str , Enum Supported tokenizers for ColBERT reranking. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 26 27 28 29 class Tokenizers ( str , Enum ): \"\"\"Supported tokenizers for ColBERT reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\"","title":"Configuraiton"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#configuraiton","text":"This module contains functionality related to the the configuraiton module for augmentation.components.postprocessors.colbert_rerank .","title":"Configuraiton"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#configuraiton_1","text":"","title":"Configuraiton"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#src.augmentation.components.postprocessors.colbert_rerank.configuraiton.ColbertRerankConfiguration","text":"Bases: PostProcessorConfiguration Configuration for the ColBERT reranking postprocessor. This class defines the settings needed to perform document reranking using the ColBERT retrieval model, which improves search quality through late interaction between queries and documents. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class ColbertRerankConfiguration ( PostProcessorConfiguration ): \"\"\" Configuration for the ColBERT reranking postprocessor. This class defines the settings needed to perform document reranking using the ColBERT retrieval model, which improves search quality through late interaction between queries and documents. \"\"\" class Models ( str , Enum ): \"\"\"Supported ColBERT models for reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" class Tokenizers ( str , Enum ): \"\"\"Supported tokenizers for ColBERT reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" name : Literal [ PostProcessorName . COLBERT_RERANK ] = Field ( ... , description = \"The name of the postprocessor.\" ) model : Models = Field ( Models . COLBERTV2 , description = \"Model used for reranking\" ) tokenizer : Tokenizers = Field ( Tokenizers . COLBERTV2 , description = \"Tokenizer used for reranking\" ) top_n : int = Field ( 5 , description = \"Number of documents to be reranked\" ) keep_retrieval_score : bool = Field ( True , description = \"Toggle to keep the retrieval score after the reranking\" , )","title":"ColbertRerankConfiguration"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#src.augmentation.components.postprocessors.colbert_rerank.configuraiton.ColbertRerankConfiguration.Models","text":"Bases: str , Enum Supported ColBERT models for reranking. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 21 22 23 24 class Models ( str , Enum ): \"\"\"Supported ColBERT models for reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\"","title":"Models"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#src.augmentation.components.postprocessors.colbert_rerank.configuraiton.ColbertRerankConfiguration.Tokenizers","text":"Bases: str , Enum Supported tokenizers for ColBERT reranking. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 26 27 28 29 class Tokenizers ( str , Enum ): \"\"\"Supported tokenizers for ColBERT reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\"","title":"Tokenizers"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/postprocessor/","text":"Postprocessor This module contains functionality related to the the postprocessor module for augmentation.components.postprocessors.colbert_rerank . Postprocessor ColbertRerankFactory Bases: Factory Factory for creating ColbertRerank instances. ColbertRerank is a postprocessor that reranks retrieved nodes using ColBERT, a neural information retrieval model that uses contextualized late interaction. This factory handles the creation of ColbertRerank objects based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the ColbertRerank. Source code in src/augmentation/components/postprocessors/colbert_rerank/postprocessor.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class ColbertRerankFactory ( Factory ): \"\"\" Factory for creating ColbertRerank instances. ColbertRerank is a postprocessor that reranks retrieved nodes using ColBERT, a neural information retrieval model that uses contextualized late interaction. This factory handles the creation of ColbertRerank objects based on the provided configuration. Attributes: _configuration_class (Type): The configuration class for the ColbertRerank. \"\"\" _configuration_class : Type = ColbertRerankConfiguration @classmethod def _create_instance ( cls , configuration : ColbertRerankConfiguration ) -> ColbertRerank : \"\"\" Creates a ColbertRerank instance based on the provided configuration. Args: configuration (ColbertRerankConfiguration): Configuration object containing parameters for the ColbertRerank instance. Returns: ColbertRerank: An initialized ColbertRerank postprocessor that can be used to rerank retrieved documents based on their relevance to the query. \"\"\" return ColbertRerank ( top_n = configuration . top_n , model = configuration . model . value , tokenizer = configuration . tokenizer . value , keep_retrieval_score = configuration . keep_retrieval_score , )","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/postprocessor/#postprocessor","text":"This module contains functionality related to the the postprocessor module for augmentation.components.postprocessors.colbert_rerank .","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/postprocessor/#postprocessor_1","text":"","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/postprocessor/#src.augmentation.components.postprocessors.colbert_rerank.postprocessor.ColbertRerankFactory","text":"Bases: Factory Factory for creating ColbertRerank instances. ColbertRerank is a postprocessor that reranks retrieved nodes using ColBERT, a neural information retrieval model that uses contextualized late interaction. This factory handles the creation of ColbertRerank objects based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the ColbertRerank. Source code in src/augmentation/components/postprocessors/colbert_rerank/postprocessor.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class ColbertRerankFactory ( Factory ): \"\"\" Factory for creating ColbertRerank instances. ColbertRerank is a postprocessor that reranks retrieved nodes using ColBERT, a neural information retrieval model that uses contextualized late interaction. This factory handles the creation of ColbertRerank objects based on the provided configuration. Attributes: _configuration_class (Type): The configuration class for the ColbertRerank. \"\"\" _configuration_class : Type = ColbertRerankConfiguration @classmethod def _create_instance ( cls , configuration : ColbertRerankConfiguration ) -> ColbertRerank : \"\"\" Creates a ColbertRerank instance based on the provided configuration. Args: configuration (ColbertRerankConfiguration): Configuration object containing parameters for the ColbertRerank instance. Returns: ColbertRerank: An initialized ColbertRerank postprocessor that can be used to rerank retrieved documents based on their relevance to the query. \"\"\" return ColbertRerank ( top_n = configuration . top_n , model = configuration . model . value , tokenizer = configuration . tokenizer . value , keep_retrieval_score = configuration . keep_retrieval_score , )","title":"ColbertRerankFactory"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.postprocessors.hybrid_filter . Configuration HybridFilterConfiguration Bases: PostProcessorConfiguration Configuration for the Hybrid Filter postprocessor. This postprocessor implements multi-stage filtering: 1. Score threshold - removes low-similarity documents 2. Semantic deduplication - removes near-duplicate content 3. LLM relevance check (optional) - verifies semantic relevance Source code in src/augmentation/components/postprocessors/hybrid_filter/configuration.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class HybridFilterConfiguration ( PostProcessorConfiguration ): \"\"\" Configuration for the Hybrid Filter postprocessor. This postprocessor implements multi-stage filtering: 1. Score threshold - removes low-similarity documents 2. Semantic deduplication - removes near-duplicate content 3. LLM relevance check (optional) - verifies semantic relevance \"\"\" name : Literal [ PostProcessorName . HYBRID_FILTER ] = Field ( default = PostProcessorName . HYBRID_FILTER , description = \"Name of the postprocessor (hybrid_filter)\" , ) score_threshold : float = Field ( default = 0.65 , ge = 0.0 , le = 1.0 , description = ( \"Minimum similarity score to keep a document (0-1). \" \"Documents below this threshold are filtered out. \" \"Typical values: 0.6-0.7 for strict filtering, 0.5-0.6 for lenient.\" ), ) similarity_threshold : float = Field ( default = 0.90 , ge = 0.0 , le = 1.0 , description = ( \"Threshold for considering documents as duplicates (0-1). \" \"Documents with embedding similarity above this are considered duplicates. \" \"Higher values (0.9-0.95) for near-exact duplicates, \" \"lower values (0.8-0.85) for semantic duplicates.\" ), ) max_documents : int = Field ( default = 8 , ge = 1 , le = 20 , description = ( \"Maximum number of documents to return after filtering. \" \"This controls the final document count sent to the LLM for synthesis.\" ), ) enable_llm_filter : bool = Field ( default = False , description = ( \"Whether to use LLM for final relevance checking. \" \"When enabled, each remaining document is checked for relevance using an LLM. \" \"This is most accurate but adds cost and latency (1 LLM call per document). \" \"Recommended: false for fast mode, true for high-accuracy mode.\" ), ) llm : Optional [ LLMConfiguration ] = Field ( default = None , description = ( \"LLM configuration for relevance checking. \" \"Only required if enable_llm_filter is true. \" \"Should be a fast, cheap model (e.g., gpt-4o-mini).\" ), ) @field_validator ( \"llm\" , mode = \"before\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate LLM configuration using the registry to support all LLM providers.\"\"\" if value is None : return value return LLMConfiguration . _validate ( value , info , LLMConfigurationRegistry )","title":"Configuration"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.postprocessors.hybrid_filter .","title":"Configuration"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/configuration/#src.augmentation.components.postprocessors.hybrid_filter.configuration.HybridFilterConfiguration","text":"Bases: PostProcessorConfiguration Configuration for the Hybrid Filter postprocessor. This postprocessor implements multi-stage filtering: 1. Score threshold - removes low-similarity documents 2. Semantic deduplication - removes near-duplicate content 3. LLM relevance check (optional) - verifies semantic relevance Source code in src/augmentation/components/postprocessors/hybrid_filter/configuration.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class HybridFilterConfiguration ( PostProcessorConfiguration ): \"\"\" Configuration for the Hybrid Filter postprocessor. This postprocessor implements multi-stage filtering: 1. Score threshold - removes low-similarity documents 2. Semantic deduplication - removes near-duplicate content 3. LLM relevance check (optional) - verifies semantic relevance \"\"\" name : Literal [ PostProcessorName . HYBRID_FILTER ] = Field ( default = PostProcessorName . HYBRID_FILTER , description = \"Name of the postprocessor (hybrid_filter)\" , ) score_threshold : float = Field ( default = 0.65 , ge = 0.0 , le = 1.0 , description = ( \"Minimum similarity score to keep a document (0-1). \" \"Documents below this threshold are filtered out. \" \"Typical values: 0.6-0.7 for strict filtering, 0.5-0.6 for lenient.\" ), ) similarity_threshold : float = Field ( default = 0.90 , ge = 0.0 , le = 1.0 , description = ( \"Threshold for considering documents as duplicates (0-1). \" \"Documents with embedding similarity above this are considered duplicates. \" \"Higher values (0.9-0.95) for near-exact duplicates, \" \"lower values (0.8-0.85) for semantic duplicates.\" ), ) max_documents : int = Field ( default = 8 , ge = 1 , le = 20 , description = ( \"Maximum number of documents to return after filtering. \" \"This controls the final document count sent to the LLM for synthesis.\" ), ) enable_llm_filter : bool = Field ( default = False , description = ( \"Whether to use LLM for final relevance checking. \" \"When enabled, each remaining document is checked for relevance using an LLM. \" \"This is most accurate but adds cost and latency (1 LLM call per document). \" \"Recommended: false for fast mode, true for high-accuracy mode.\" ), ) llm : Optional [ LLMConfiguration ] = Field ( default = None , description = ( \"LLM configuration for relevance checking. \" \"Only required if enable_llm_filter is true. \" \"Should be a fast, cheap model (e.g., gpt-4o-mini).\" ), ) @field_validator ( \"llm\" , mode = \"before\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate LLM configuration using the registry to support all LLM providers.\"\"\" if value is None : return value return LLMConfiguration . _validate ( value , info , LLMConfigurationRegistry )","title":"HybridFilterConfiguration"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/factory/","text":"Factory This module contains functionality related to the the factory module for augmentation.components.postprocessors.hybrid_filter . Factory HybridFilterFactory Bases: Factory Factory for creating HybridFilterPostprocessor instances. The Hybrid Filter postprocessor provides multi-stage document filtering: 1. Score threshold filtering (fast) 2. Temporal filtering (based on temporal_domain config, optional) 3. Semantic deduplication (fast) 4. LLM relevance checking (optional, slow but accurate) This factory handles the creation of HybridFilterPostprocessor objects based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the HybridFilterPostprocessor. Source code in src/augmentation/components/postprocessors/hybrid_filter/factory.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class HybridFilterFactory ( Factory ): \"\"\" Factory for creating HybridFilterPostprocessor instances. The Hybrid Filter postprocessor provides multi-stage document filtering: 1. Score threshold filtering (fast) 2. Temporal filtering (based on temporal_domain config, optional) 3. Semantic deduplication (fast) 4. LLM relevance checking (optional, slow but accurate) This factory handles the creation of HybridFilterPostprocessor objects based on the provided configuration. Attributes: _configuration_class: The configuration class for the HybridFilterPostprocessor. \"\"\" _configuration_class : Type = HybridFilterConfiguration # Store temporal_domain_config at class level for access during creation _temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None @classmethod def set_temporal_domain_config ( cls , temporal_domain_config : Optional [ TemporalDomainConfiguration ] ): \"\"\"Set the temporal domain config to be used when creating instances. Args: temporal_domain_config: Temporal domain configuration \"\"\" cls . _temporal_domain_config = temporal_domain_config @classmethod def _create_instance ( cls , configuration : HybridFilterConfiguration ) -> HybridFilterPostprocessor : \"\"\" Creates a HybridFilterPostprocessor instance based on the provided configuration. Uses the temporal_domain_config set via set_temporal_domain_config() if available. Args: configuration: HybridFilterConfiguration containing filter thresholds, max documents, LLM settings, etc. Returns: HybridFilterPostprocessor: An initialized postprocessor that can filter and deduplicate retrieved documents before synthesis. \"\"\" return HybridFilterPostprocessor ( configuration = configuration , temporal_domain_config = cls . _temporal_domain_config , ) set_temporal_domain_config ( temporal_domain_config ) classmethod Set the temporal domain config to be used when creating instances. Parameters: temporal_domain_config ( Optional [ TemporalDomainConfiguration ] ) \u2013 Temporal domain configuration Source code in src/augmentation/components/postprocessors/hybrid_filter/factory.py 37 38 39 40 41 42 43 44 45 46 @classmethod def set_temporal_domain_config ( cls , temporal_domain_config : Optional [ TemporalDomainConfiguration ] ): \"\"\"Set the temporal domain config to be used when creating instances. Args: temporal_domain_config: Temporal domain configuration \"\"\" cls . _temporal_domain_config = temporal_domain_config","title":"Factory"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/factory/#factory","text":"This module contains functionality related to the the factory module for augmentation.components.postprocessors.hybrid_filter .","title":"Factory"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/factory/#factory_1","text":"","title":"Factory"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/factory/#src.augmentation.components.postprocessors.hybrid_filter.factory.HybridFilterFactory","text":"Bases: Factory Factory for creating HybridFilterPostprocessor instances. The Hybrid Filter postprocessor provides multi-stage document filtering: 1. Score threshold filtering (fast) 2. Temporal filtering (based on temporal_domain config, optional) 3. Semantic deduplication (fast) 4. LLM relevance checking (optional, slow but accurate) This factory handles the creation of HybridFilterPostprocessor objects based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the HybridFilterPostprocessor. Source code in src/augmentation/components/postprocessors/hybrid_filter/factory.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class HybridFilterFactory ( Factory ): \"\"\" Factory for creating HybridFilterPostprocessor instances. The Hybrid Filter postprocessor provides multi-stage document filtering: 1. Score threshold filtering (fast) 2. Temporal filtering (based on temporal_domain config, optional) 3. Semantic deduplication (fast) 4. LLM relevance checking (optional, slow but accurate) This factory handles the creation of HybridFilterPostprocessor objects based on the provided configuration. Attributes: _configuration_class: The configuration class for the HybridFilterPostprocessor. \"\"\" _configuration_class : Type = HybridFilterConfiguration # Store temporal_domain_config at class level for access during creation _temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None @classmethod def set_temporal_domain_config ( cls , temporal_domain_config : Optional [ TemporalDomainConfiguration ] ): \"\"\"Set the temporal domain config to be used when creating instances. Args: temporal_domain_config: Temporal domain configuration \"\"\" cls . _temporal_domain_config = temporal_domain_config @classmethod def _create_instance ( cls , configuration : HybridFilterConfiguration ) -> HybridFilterPostprocessor : \"\"\" Creates a HybridFilterPostprocessor instance based on the provided configuration. Uses the temporal_domain_config set via set_temporal_domain_config() if available. Args: configuration: HybridFilterConfiguration containing filter thresholds, max documents, LLM settings, etc. Returns: HybridFilterPostprocessor: An initialized postprocessor that can filter and deduplicate retrieved documents before synthesis. \"\"\" return HybridFilterPostprocessor ( configuration = configuration , temporal_domain_config = cls . _temporal_domain_config , )","title":"HybridFilterFactory"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/factory/#src.augmentation.components.postprocessors.hybrid_filter.factory.HybridFilterFactory.set_temporal_domain_config","text":"Set the temporal domain config to be used when creating instances. Parameters: temporal_domain_config ( Optional [ TemporalDomainConfiguration ] ) \u2013 Temporal domain configuration Source code in src/augmentation/components/postprocessors/hybrid_filter/factory.py 37 38 39 40 41 42 43 44 45 46 @classmethod def set_temporal_domain_config ( cls , temporal_domain_config : Optional [ TemporalDomainConfiguration ] ): \"\"\"Set the temporal domain config to be used when creating instances. Args: temporal_domain_config: Temporal domain configuration \"\"\" cls . _temporal_domain_config = temporal_domain_config","title":"set_temporal_domain_config"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/postprocessor/","text":"Postprocessor This module contains functionality related to the the postprocessor module for augmentation.components.postprocessors.hybrid_filter . Postprocessor HybridFilterPostprocessor Bases: BaseNodePostprocessor Multi-stage filtering postprocessor for intelligent document filtering. Applies five stages of filtering to retrieved documents: 1. Score threshold - fast removal of low-similarity documents 2. Temporal filtering - removes old documents when query mentions current/recent 3. Semantic deduplication - removes near-duplicate content 4. LLM relevance check (optional) - verifies semantic relevance to query 5. Max documents limit - caps final document count This approach balances quality and performance by applying cheap filters first, then expensive LLM checks only on remaining high-quality candidates. Source code in src/augmentation/components/postprocessors/hybrid_filter/postprocessor.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 class HybridFilterPostprocessor ( BaseNodePostprocessor ): \"\"\" Multi-stage filtering postprocessor for intelligent document filtering. Applies five stages of filtering to retrieved documents: 1. Score threshold - fast removal of low-similarity documents 2. Temporal filtering - removes old documents when query mentions current/recent 3. Semantic deduplication - removes near-duplicate content 4. LLM relevance check (optional) - verifies semantic relevance to query 5. Max documents limit - caps final document count This approach balances quality and performance by applying cheap filters first, then expensive LLM checks only on remaining high-quality candidates. \"\"\" score_threshold : float = Field ( default = 0.65 , description = \"Minimum similarity score to keep a document\" ) similarity_threshold : float = Field ( default = 0.90 , description = \"Threshold for considering documents as duplicates\" , ) max_documents : int = Field ( default = 8 , description = \"Maximum number of documents to return\" ) enable_llm_filter : bool = Field ( default = False , description = \"Whether to use LLM for final relevance checking\" , ) _llm : Any = PrivateAttr ( default = None ) _logger : Any = PrivateAttr ( default = None ) _temporal_domain_config : Optional [ TemporalDomainConfiguration ] = ( PrivateAttr ( default = None ) ) _current_keywords : List [ str ] = PrivateAttr ( default_factory = list ) _historical_keywords : List [ str ] = PrivateAttr ( default_factory = list ) def __init__ ( self , configuration : HybridFilterConfiguration , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , ** kwargs , ): \"\"\" Initialize the hybrid filter postprocessor. Args: configuration: Configuration containing filter thresholds and LLM settings temporal_domain_config: Optional temporal domain configuration. If None, runs without temporal filtering. \"\"\" super () . __init__ ( score_threshold = configuration . score_threshold , similarity_threshold = configuration . similarity_threshold , max_documents = configuration . max_documents , enable_llm_filter = configuration . enable_llm_filter , ** kwargs , ) self . _logger = LoggerConfiguration . get_logger ( __name__ ) self . _temporal_domain_config = temporal_domain_config # Build keyword lists from config (empty if no config provided) # Lowercase keywords for case-insensitive substring matching if temporal_domain_config : self . _current_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_current_keywords () ] self . _historical_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_historical_keywords () ] self . _logger . info ( f \"[HybridFilter] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _current_keywords = [] self . _historical_keywords = [] self . _logger . info ( \"[HybridFilter] Running without temporal filtering (no temporal_domain config)\" ) # Initialize LLM if relevance filtering is enabled if configuration . enable_llm_filter and configuration . llm : self . _llm = LLMRegistry . get ( configuration . llm . provider ) . create ( configuration . llm ) self . _logger . info ( f \"LLM relevance filtering enabled with { configuration . llm . provider } / { configuration . llm . name } \" ) def _postprocess_nodes ( self , nodes : List [ NodeWithScore ], query_bundle : Optional [ QueryBundle ] = None , ) -> List [ NodeWithScore ]: \"\"\" Apply multi-stage filtering to retrieved nodes. Args: nodes: List of retrieved nodes with similarity scores query_bundle: Optional query information for LLM relevance checking Returns: List of filtered nodes, max length = max_documents \"\"\" if not nodes : return nodes initial_count = len ( nodes ) self . _logger . info ( f \"[HybridFilter] Starting with { initial_count } retrieved documents\" ) # Stage 1: Score threshold filtering nodes = self . _filter_by_score ( nodes ) # Stage 2: Temporal filtering (if query mentions current/recent) # Track if we applied historical filtering (WP20-only) applied_historical_filter = False if query_bundle : nodes , applied_historical_filter = ( self . _filter_by_temporal_relevance ( nodes , query_bundle . query_str ) ) # Stage 3: Semantic deduplication nodes = self . _deduplicate_semantically ( nodes ) # Stage 4: LLM relevance check (optional, expensive) # Skip LLM filtering if we already applied strict WP20 metadata filtering # since all documents are already from the correct period if ( self . enable_llm_filter and self . _llm and query_bundle and not applied_historical_filter ): nodes = self . _filter_by_llm_relevance ( nodes , query_bundle . query_str ) elif applied_historical_filter : self . _logger . info ( \"[HybridFilter] Skipping LLM filtering - documents already strictly filtered to WP20\" ) # Stage 5: Limit to max_documents nodes = nodes [: self . max_documents ] self . _logger . info ( f \"[HybridFilter] Final: { len ( nodes ) } / { initial_count } documents retained\" ) return nodes def _filter_by_temporal_relevance ( self , nodes : List [ NodeWithScore ], query : str ) -> tuple [ List [ NodeWithScore ], bool ]: \"\"\" Stage 2: Filter documents by temporal relevance based on query keywords. Uses configured temporal domain to detect temporal intent and filter accordingly. If no temporal_domain_config is provided, returns all nodes unchanged. Args: nodes: List of nodes to filter query: User query string Returns: Tuple of (filtered nodes, bool indicating if historical filter was applied) \"\"\" # If no temporal domain config, skip temporal filtering if not self . _temporal_domain_config : self . _logger . debug ( \"[HybridFilter] Temporal filtering SKIPPED - no temporal_domain config\" ) return nodes , False query_lower = query . lower () # First check for historical keywords - these trigger historical period filtering has_historical_keyword = any ( keyword in query_lower for keyword in self . _historical_keywords ) if has_historical_keyword : field_name = self . _temporal_domain_config . temporal_field_name target_period = str ( self . _temporal_domain_config . historical_period_value ) self . _logger . info ( f \"[HybridFilter] Historical filtering ACTIVATED - filtering to { field_name } = { target_period } for query: ' { query [: 80 ] } '\" ) filtered = [] for node in nodes : period = node . node . metadata . get ( field_name , \"\" ) document_number = node . node . metadata . get ( \"document_number\" , \"\" ) # Fallback: Try to extract period from document_number if not in metadata field if not period and \"/\" in document_number : period = document_number . split ( \"/\" )[ 0 ] # Convert period to string for comparison period = str ( period ) if period else \"\" # Keep only historical period documents if period == target_period : filtered . append ( node ) self . _logger . debug ( f \"[HybridFilter] \u2713 KEPT { field_name } = { period } doc: { node . node . metadata . get ( 'title' , 'Untitled' )[: 60 ] } \" ) else : self . _logger . debug ( f \"[HybridFilter] \u2717 FILTERED OUT { field_name } = { period } doc (keeping { target_period } only)\" ) removed_count = len ( nodes ) - len ( filtered ) if removed_count > 0 : self . _logger . info ( f \"[HybridFilter] Historical filtering: removed { removed_count } non- { target_period } documents \u2192 { len ( filtered ) } remaining\" ) # If filtering removed everything, fall back to all nodes if not filtered : self . _logger . warning ( \"[HybridFilter] Historical filtering would remove all documents. Keeping all to avoid empty results.\" ) return nodes , False return ( filtered , True , ) # True indicates historical filtering was applied # Then check for current/temporal keywords - these trigger current period filtering has_temporal_keyword = any ( keyword in query_lower for keyword in self . _current_keywords ) if not has_temporal_keyword : self . _logger . info ( f \"[HybridFilter] Temporal filtering SKIPPED - no temporal keywords found in query: ' { query [: 80 ] } '\" ) return nodes , False field_name = self . _temporal_domain_config . temporal_field_name current_period = str ( self . _temporal_domain_config . current_period_value ) self . _logger . info ( f \"[HybridFilter] Temporal filtering ACTIVATED - filtering to { field_name } = { current_period } for query: ' { query [: 80 ] } '\" ) filtered = [] for node in nodes : period = node . node . metadata . get ( field_name , \"\" ) document_number = node . node . metadata . get ( \"document_number\" , \"\" ) title = node . node . metadata . get ( \"title\" , \"Untitled\" )[: 60 ] # Fallback: Try to extract period from document_number if not in metadata field if not period and \"/\" in document_number : period = document_number . split ( \"/\" )[ 0 ] # Convert period to string for comparison period = str ( period ) if period else \"\" # Log what we found self . _logger . info ( f \"[HybridFilter] Doc: ' { title } ' | { field_name } =' { node . node . metadata . get ( field_name , '' ) } ' | document_number=' { document_number } ' | extracted_period=' { period } '\" ) # Keep if from current period or if period is unknown (empty) if period == current_period or period == \"\" : filtered . append ( node ) self . _logger . info ( f \"[HybridFilter] \u2713 KEPT (period= { period or 'unknown' } )\" ) else : self . _logger . info ( f \"[HybridFilter] \u2717 FILTERED OUT (old period= { period } )\" ) removed_count = len ( nodes ) - len ( filtered ) if removed_count > 0 : self . _logger . info ( f \"[HybridFilter] Temporal filtering: removed { removed_count } old documents \" f \"(kept { field_name } = { current_period } only) \u2192 { len ( filtered ) } remaining\" ) # If filtering removed everything, fall back to all nodes if not filtered : self . _logger . warning ( \"[HybridFilter] Temporal filtering would remove all documents. Keeping all to avoid empty results.\" ) return nodes , False return ( filtered , False , ) # False - current period filtering, not historical def _filter_by_score ( self , nodes : List [ NodeWithScore ] ) -> List [ NodeWithScore ]: \"\"\" Stage 1: Remove documents below similarity threshold. Fast and cheap - eliminates clearly irrelevant docs based on vector similarity. Args: nodes: List of nodes with scores Returns: Filtered list of nodes above threshold \"\"\" filtered = [ n for n in nodes if n . score >= self . score_threshold ] removed_count = len ( nodes ) - len ( filtered ) self . _logger . info ( f \"[HybridFilter] Score filtering: removed { removed_count } docs \" f \"(threshold: { self . score_threshold : .2f } ) \u2192 { len ( filtered ) } remaining\" ) if removed_count > 0 and self . _logger . isEnabledFor ( 10 ): # DEBUG level for node in nodes : if node . score < self . score_threshold : title = node . node . metadata . get ( \"title\" , \"Untitled\" )[: 60 ] self . _logger . debug ( f \" Filtered out (score= { node . score : .3f } ): { title } \" ) return filtered def _deduplicate_semantically ( self , nodes : List [ NodeWithScore ] ) -> List [ NodeWithScore ]: \"\"\" Stage 2: Remove near-duplicate documents using embedding similarity. Common in Bundestag where multiple speeches discuss the same topic. Keeps highest-scoring doc from each semantic cluster. Args: nodes: List of nodes to deduplicate Returns: Deduplicated list of nodes \"\"\" if len ( nodes ) <= 1 : return nodes kept_nodes = [] skip_indices = set () for i , node_i in enumerate ( nodes ): if i in skip_indices : continue # Check if similar to any already kept node is_duplicate = False for kept_node in kept_nodes : if self . _are_nodes_similar ( node_i , kept_node ): is_duplicate = True if self . _logger . isEnabledFor ( 10 ): # DEBUG level title_i = node_i . node . metadata . get ( \"title\" , \"Untitled\" )[ : 60 ] title_kept = kept_node . node . metadata . get ( \"title\" , \"Untitled\" )[: 60 ] self . _logger . debug ( f \" Duplicate detected: ' { title_i } ' similar to ' { title_kept } '\" ) break if not is_duplicate : kept_nodes . append ( node_i ) # Mark similar subsequent docs as duplicates for j in range ( i + 1 , len ( nodes )): if j not in skip_indices : node_j = nodes [ j ] if self . _are_nodes_similar ( node_i , node_j ): skip_indices . add ( j ) removed_count = len ( nodes ) - len ( kept_nodes ) self . _logger . info ( f \"[HybridFilter] Deduplication: removed { removed_count } duplicates \" f \"(threshold: { self . similarity_threshold : .2f } ) \u2192 { len ( kept_nodes ) } remaining\" ) return kept_nodes def _are_nodes_similar ( self , node1 : NodeWithScore , node2 : NodeWithScore ) -> bool : \"\"\" Check if two nodes are semantically similar based on embeddings. Args: node1: First node node2: Second node Returns: True if similarity >= similarity_threshold, False otherwise \"\"\" # Use embeddings if available if ( hasattr ( node1 . node , \"embedding\" ) and hasattr ( node2 . node , \"embedding\" ) and node1 . node . embedding is not None and node2 . node . embedding is not None ): similarity = self . _cosine_similarity ( node1 . node . embedding , node2 . node . embedding ) return similarity >= self . similarity_threshold return False def _filter_by_llm_relevance ( self , nodes : List [ NodeWithScore ], query : str ) -> List [ NodeWithScore ]: \"\"\" Stage 3: Use LLM to check if each document actually helps answer the query. Most accurate but expensive - only use after cheaper filters. Only checks remaining documents (typically 3-6 after deduplication). Args: nodes: List of nodes to check query: User query string Returns: List of nodes deemed relevant by LLM \"\"\" if not self . _llm : return nodes relevant_nodes = [] checked_count = 0 for node in nodes : checked_count += 1 # Extract metadata for context title = node . node . metadata . get ( \"title\" , \"N/A\" ) date = node . node . metadata . get ( \"created_time\" , \"N/A\" ) speaker = node . node . metadata . get ( \"speaker\" , \"N/A\" ) doc_type = node . node . metadata . get ( \"document_type\" , \"N/A\" ) # Use more text for better relevance judgment (1500 chars instead of 500) text_excerpt = node . node . text [: 1500 ] prompt = f \"\"\"You are a lenient relevance filter for a German Bundestag document retrieval system. Your job is to filter out ONLY clearly irrelevant documents. When in doubt, keep the document. User Query: { query } Document Information: - Title: { title } - Date: { date } - Speaker: { speaker } - Type: { doc_type } - Similarity Score: { node . score : .3f } (already pre-filtered by semantic search) Document Excerpt (first 1500 characters): { text_excerpt } Task: This document was already retrieved by semantic search with score { node . score : .3f } . Only reject it if it's CLEARLY and OBVIOUSLY irrelevant to the query. If the document might contain ANY useful information for answering the query, keep it. Reply with ONLY \"YES\" (keep) or \"NO\" (reject) followed by a brief reason (max 10 words). Format: YES/NO - <reason> \"\"\" try : response = self . _llm . complete ( prompt ) response_text = response . text . strip () . upper () is_relevant = response_text . startswith ( \"YES\" ) if is_relevant : relevant_nodes . append ( node ) self . _logger . info ( f \"[HybridFilter] LLM kept ( { checked_count } / { len ( nodes ) } ): { title [: 60 ] } \" ) else : self . _logger . info ( f \"[HybridFilter] LLM rejected ( { checked_count } / { len ( nodes ) } ): { title [: 60 ] } - { response_text [: 80 ] } \" ) except Exception as e : self . _logger . warning ( f \"[HybridFilter] LLM check failed for document: { e } . Keeping document.\" ) relevant_nodes . append ( node ) # Keep on error to avoid data loss removed_count = len ( nodes ) - len ( relevant_nodes ) self . _logger . info ( f \"[HybridFilter] LLM filtering: removed { removed_count } irrelevant docs \u2192 { len ( relevant_nodes ) } remaining\" ) return relevant_nodes @staticmethod def _cosine_similarity ( vec1 : List [ float ], vec2 : List [ float ]) -> float : \"\"\" Calculate cosine similarity between two vectors. Args: vec1: First embedding vector vec2: Second embedding vector Returns: Cosine similarity score (0-1) \"\"\" vec1_np = np . array ( vec1 ) vec2_np = np . array ( vec2 ) dot_product = np . dot ( vec1_np , vec2_np ) norm1 = np . linalg . norm ( vec1_np ) norm2 = np . linalg . norm ( vec2_np ) if norm1 == 0 or norm2 == 0 : return 0.0 return float ( dot_product / ( norm1 * norm2 )) __init__ ( configuration , temporal_domain_config = None , ** kwargs ) Initialize the hybrid filter postprocessor. Parameters: configuration ( HybridFilterConfiguration ) \u2013 Configuration containing filter thresholds and LLM settings temporal_domain_config ( Optional [ TemporalDomainConfiguration ] , default: None ) \u2013 Optional temporal domain configuration. If None, runs without temporal filtering. Source code in src/augmentation/components/postprocessors/hybrid_filter/postprocessor.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , configuration : HybridFilterConfiguration , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , ** kwargs , ): \"\"\" Initialize the hybrid filter postprocessor. Args: configuration: Configuration containing filter thresholds and LLM settings temporal_domain_config: Optional temporal domain configuration. If None, runs without temporal filtering. \"\"\" super () . __init__ ( score_threshold = configuration . score_threshold , similarity_threshold = configuration . similarity_threshold , max_documents = configuration . max_documents , enable_llm_filter = configuration . enable_llm_filter , ** kwargs , ) self . _logger = LoggerConfiguration . get_logger ( __name__ ) self . _temporal_domain_config = temporal_domain_config # Build keyword lists from config (empty if no config provided) # Lowercase keywords for case-insensitive substring matching if temporal_domain_config : self . _current_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_current_keywords () ] self . _historical_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_historical_keywords () ] self . _logger . info ( f \"[HybridFilter] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _current_keywords = [] self . _historical_keywords = [] self . _logger . info ( \"[HybridFilter] Running without temporal filtering (no temporal_domain config)\" ) # Initialize LLM if relevance filtering is enabled if configuration . enable_llm_filter and configuration . llm : self . _llm = LLMRegistry . get ( configuration . llm . provider ) . create ( configuration . llm ) self . _logger . info ( f \"LLM relevance filtering enabled with { configuration . llm . provider } / { configuration . llm . name } \" )","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/postprocessor/#postprocessor","text":"This module contains functionality related to the the postprocessor module for augmentation.components.postprocessors.hybrid_filter .","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/postprocessor/#postprocessor_1","text":"","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/postprocessor/#src.augmentation.components.postprocessors.hybrid_filter.postprocessor.HybridFilterPostprocessor","text":"Bases: BaseNodePostprocessor Multi-stage filtering postprocessor for intelligent document filtering. Applies five stages of filtering to retrieved documents: 1. Score threshold - fast removal of low-similarity documents 2. Temporal filtering - removes old documents when query mentions current/recent 3. Semantic deduplication - removes near-duplicate content 4. LLM relevance check (optional) - verifies semantic relevance to query 5. Max documents limit - caps final document count This approach balances quality and performance by applying cheap filters first, then expensive LLM checks only on remaining high-quality candidates. Source code in src/augmentation/components/postprocessors/hybrid_filter/postprocessor.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 class HybridFilterPostprocessor ( BaseNodePostprocessor ): \"\"\" Multi-stage filtering postprocessor for intelligent document filtering. Applies five stages of filtering to retrieved documents: 1. Score threshold - fast removal of low-similarity documents 2. Temporal filtering - removes old documents when query mentions current/recent 3. Semantic deduplication - removes near-duplicate content 4. LLM relevance check (optional) - verifies semantic relevance to query 5. Max documents limit - caps final document count This approach balances quality and performance by applying cheap filters first, then expensive LLM checks only on remaining high-quality candidates. \"\"\" score_threshold : float = Field ( default = 0.65 , description = \"Minimum similarity score to keep a document\" ) similarity_threshold : float = Field ( default = 0.90 , description = \"Threshold for considering documents as duplicates\" , ) max_documents : int = Field ( default = 8 , description = \"Maximum number of documents to return\" ) enable_llm_filter : bool = Field ( default = False , description = \"Whether to use LLM for final relevance checking\" , ) _llm : Any = PrivateAttr ( default = None ) _logger : Any = PrivateAttr ( default = None ) _temporal_domain_config : Optional [ TemporalDomainConfiguration ] = ( PrivateAttr ( default = None ) ) _current_keywords : List [ str ] = PrivateAttr ( default_factory = list ) _historical_keywords : List [ str ] = PrivateAttr ( default_factory = list ) def __init__ ( self , configuration : HybridFilterConfiguration , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , ** kwargs , ): \"\"\" Initialize the hybrid filter postprocessor. Args: configuration: Configuration containing filter thresholds and LLM settings temporal_domain_config: Optional temporal domain configuration. If None, runs without temporal filtering. \"\"\" super () . __init__ ( score_threshold = configuration . score_threshold , similarity_threshold = configuration . similarity_threshold , max_documents = configuration . max_documents , enable_llm_filter = configuration . enable_llm_filter , ** kwargs , ) self . _logger = LoggerConfiguration . get_logger ( __name__ ) self . _temporal_domain_config = temporal_domain_config # Build keyword lists from config (empty if no config provided) # Lowercase keywords for case-insensitive substring matching if temporal_domain_config : self . _current_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_current_keywords () ] self . _historical_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_historical_keywords () ] self . _logger . info ( f \"[HybridFilter] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _current_keywords = [] self . _historical_keywords = [] self . _logger . info ( \"[HybridFilter] Running without temporal filtering (no temporal_domain config)\" ) # Initialize LLM if relevance filtering is enabled if configuration . enable_llm_filter and configuration . llm : self . _llm = LLMRegistry . get ( configuration . llm . provider ) . create ( configuration . llm ) self . _logger . info ( f \"LLM relevance filtering enabled with { configuration . llm . provider } / { configuration . llm . name } \" ) def _postprocess_nodes ( self , nodes : List [ NodeWithScore ], query_bundle : Optional [ QueryBundle ] = None , ) -> List [ NodeWithScore ]: \"\"\" Apply multi-stage filtering to retrieved nodes. Args: nodes: List of retrieved nodes with similarity scores query_bundle: Optional query information for LLM relevance checking Returns: List of filtered nodes, max length = max_documents \"\"\" if not nodes : return nodes initial_count = len ( nodes ) self . _logger . info ( f \"[HybridFilter] Starting with { initial_count } retrieved documents\" ) # Stage 1: Score threshold filtering nodes = self . _filter_by_score ( nodes ) # Stage 2: Temporal filtering (if query mentions current/recent) # Track if we applied historical filtering (WP20-only) applied_historical_filter = False if query_bundle : nodes , applied_historical_filter = ( self . _filter_by_temporal_relevance ( nodes , query_bundle . query_str ) ) # Stage 3: Semantic deduplication nodes = self . _deduplicate_semantically ( nodes ) # Stage 4: LLM relevance check (optional, expensive) # Skip LLM filtering if we already applied strict WP20 metadata filtering # since all documents are already from the correct period if ( self . enable_llm_filter and self . _llm and query_bundle and not applied_historical_filter ): nodes = self . _filter_by_llm_relevance ( nodes , query_bundle . query_str ) elif applied_historical_filter : self . _logger . info ( \"[HybridFilter] Skipping LLM filtering - documents already strictly filtered to WP20\" ) # Stage 5: Limit to max_documents nodes = nodes [: self . max_documents ] self . _logger . info ( f \"[HybridFilter] Final: { len ( nodes ) } / { initial_count } documents retained\" ) return nodes def _filter_by_temporal_relevance ( self , nodes : List [ NodeWithScore ], query : str ) -> tuple [ List [ NodeWithScore ], bool ]: \"\"\" Stage 2: Filter documents by temporal relevance based on query keywords. Uses configured temporal domain to detect temporal intent and filter accordingly. If no temporal_domain_config is provided, returns all nodes unchanged. Args: nodes: List of nodes to filter query: User query string Returns: Tuple of (filtered nodes, bool indicating if historical filter was applied) \"\"\" # If no temporal domain config, skip temporal filtering if not self . _temporal_domain_config : self . _logger . debug ( \"[HybridFilter] Temporal filtering SKIPPED - no temporal_domain config\" ) return nodes , False query_lower = query . lower () # First check for historical keywords - these trigger historical period filtering has_historical_keyword = any ( keyword in query_lower for keyword in self . _historical_keywords ) if has_historical_keyword : field_name = self . _temporal_domain_config . temporal_field_name target_period = str ( self . _temporal_domain_config . historical_period_value ) self . _logger . info ( f \"[HybridFilter] Historical filtering ACTIVATED - filtering to { field_name } = { target_period } for query: ' { query [: 80 ] } '\" ) filtered = [] for node in nodes : period = node . node . metadata . get ( field_name , \"\" ) document_number = node . node . metadata . get ( \"document_number\" , \"\" ) # Fallback: Try to extract period from document_number if not in metadata field if not period and \"/\" in document_number : period = document_number . split ( \"/\" )[ 0 ] # Convert period to string for comparison period = str ( period ) if period else \"\" # Keep only historical period documents if period == target_period : filtered . append ( node ) self . _logger . debug ( f \"[HybridFilter] \u2713 KEPT { field_name } = { period } doc: { node . node . metadata . get ( 'title' , 'Untitled' )[: 60 ] } \" ) else : self . _logger . debug ( f \"[HybridFilter] \u2717 FILTERED OUT { field_name } = { period } doc (keeping { target_period } only)\" ) removed_count = len ( nodes ) - len ( filtered ) if removed_count > 0 : self . _logger . info ( f \"[HybridFilter] Historical filtering: removed { removed_count } non- { target_period } documents \u2192 { len ( filtered ) } remaining\" ) # If filtering removed everything, fall back to all nodes if not filtered : self . _logger . warning ( \"[HybridFilter] Historical filtering would remove all documents. Keeping all to avoid empty results.\" ) return nodes , False return ( filtered , True , ) # True indicates historical filtering was applied # Then check for current/temporal keywords - these trigger current period filtering has_temporal_keyword = any ( keyword in query_lower for keyword in self . _current_keywords ) if not has_temporal_keyword : self . _logger . info ( f \"[HybridFilter] Temporal filtering SKIPPED - no temporal keywords found in query: ' { query [: 80 ] } '\" ) return nodes , False field_name = self . _temporal_domain_config . temporal_field_name current_period = str ( self . _temporal_domain_config . current_period_value ) self . _logger . info ( f \"[HybridFilter] Temporal filtering ACTIVATED - filtering to { field_name } = { current_period } for query: ' { query [: 80 ] } '\" ) filtered = [] for node in nodes : period = node . node . metadata . get ( field_name , \"\" ) document_number = node . node . metadata . get ( \"document_number\" , \"\" ) title = node . node . metadata . get ( \"title\" , \"Untitled\" )[: 60 ] # Fallback: Try to extract period from document_number if not in metadata field if not period and \"/\" in document_number : period = document_number . split ( \"/\" )[ 0 ] # Convert period to string for comparison period = str ( period ) if period else \"\" # Log what we found self . _logger . info ( f \"[HybridFilter] Doc: ' { title } ' | { field_name } =' { node . node . metadata . get ( field_name , '' ) } ' | document_number=' { document_number } ' | extracted_period=' { period } '\" ) # Keep if from current period or if period is unknown (empty) if period == current_period or period == \"\" : filtered . append ( node ) self . _logger . info ( f \"[HybridFilter] \u2713 KEPT (period= { period or 'unknown' } )\" ) else : self . _logger . info ( f \"[HybridFilter] \u2717 FILTERED OUT (old period= { period } )\" ) removed_count = len ( nodes ) - len ( filtered ) if removed_count > 0 : self . _logger . info ( f \"[HybridFilter] Temporal filtering: removed { removed_count } old documents \" f \"(kept { field_name } = { current_period } only) \u2192 { len ( filtered ) } remaining\" ) # If filtering removed everything, fall back to all nodes if not filtered : self . _logger . warning ( \"[HybridFilter] Temporal filtering would remove all documents. Keeping all to avoid empty results.\" ) return nodes , False return ( filtered , False , ) # False - current period filtering, not historical def _filter_by_score ( self , nodes : List [ NodeWithScore ] ) -> List [ NodeWithScore ]: \"\"\" Stage 1: Remove documents below similarity threshold. Fast and cheap - eliminates clearly irrelevant docs based on vector similarity. Args: nodes: List of nodes with scores Returns: Filtered list of nodes above threshold \"\"\" filtered = [ n for n in nodes if n . score >= self . score_threshold ] removed_count = len ( nodes ) - len ( filtered ) self . _logger . info ( f \"[HybridFilter] Score filtering: removed { removed_count } docs \" f \"(threshold: { self . score_threshold : .2f } ) \u2192 { len ( filtered ) } remaining\" ) if removed_count > 0 and self . _logger . isEnabledFor ( 10 ): # DEBUG level for node in nodes : if node . score < self . score_threshold : title = node . node . metadata . get ( \"title\" , \"Untitled\" )[: 60 ] self . _logger . debug ( f \" Filtered out (score= { node . score : .3f } ): { title } \" ) return filtered def _deduplicate_semantically ( self , nodes : List [ NodeWithScore ] ) -> List [ NodeWithScore ]: \"\"\" Stage 2: Remove near-duplicate documents using embedding similarity. Common in Bundestag where multiple speeches discuss the same topic. Keeps highest-scoring doc from each semantic cluster. Args: nodes: List of nodes to deduplicate Returns: Deduplicated list of nodes \"\"\" if len ( nodes ) <= 1 : return nodes kept_nodes = [] skip_indices = set () for i , node_i in enumerate ( nodes ): if i in skip_indices : continue # Check if similar to any already kept node is_duplicate = False for kept_node in kept_nodes : if self . _are_nodes_similar ( node_i , kept_node ): is_duplicate = True if self . _logger . isEnabledFor ( 10 ): # DEBUG level title_i = node_i . node . metadata . get ( \"title\" , \"Untitled\" )[ : 60 ] title_kept = kept_node . node . metadata . get ( \"title\" , \"Untitled\" )[: 60 ] self . _logger . debug ( f \" Duplicate detected: ' { title_i } ' similar to ' { title_kept } '\" ) break if not is_duplicate : kept_nodes . append ( node_i ) # Mark similar subsequent docs as duplicates for j in range ( i + 1 , len ( nodes )): if j not in skip_indices : node_j = nodes [ j ] if self . _are_nodes_similar ( node_i , node_j ): skip_indices . add ( j ) removed_count = len ( nodes ) - len ( kept_nodes ) self . _logger . info ( f \"[HybridFilter] Deduplication: removed { removed_count } duplicates \" f \"(threshold: { self . similarity_threshold : .2f } ) \u2192 { len ( kept_nodes ) } remaining\" ) return kept_nodes def _are_nodes_similar ( self , node1 : NodeWithScore , node2 : NodeWithScore ) -> bool : \"\"\" Check if two nodes are semantically similar based on embeddings. Args: node1: First node node2: Second node Returns: True if similarity >= similarity_threshold, False otherwise \"\"\" # Use embeddings if available if ( hasattr ( node1 . node , \"embedding\" ) and hasattr ( node2 . node , \"embedding\" ) and node1 . node . embedding is not None and node2 . node . embedding is not None ): similarity = self . _cosine_similarity ( node1 . node . embedding , node2 . node . embedding ) return similarity >= self . similarity_threshold return False def _filter_by_llm_relevance ( self , nodes : List [ NodeWithScore ], query : str ) -> List [ NodeWithScore ]: \"\"\" Stage 3: Use LLM to check if each document actually helps answer the query. Most accurate but expensive - only use after cheaper filters. Only checks remaining documents (typically 3-6 after deduplication). Args: nodes: List of nodes to check query: User query string Returns: List of nodes deemed relevant by LLM \"\"\" if not self . _llm : return nodes relevant_nodes = [] checked_count = 0 for node in nodes : checked_count += 1 # Extract metadata for context title = node . node . metadata . get ( \"title\" , \"N/A\" ) date = node . node . metadata . get ( \"created_time\" , \"N/A\" ) speaker = node . node . metadata . get ( \"speaker\" , \"N/A\" ) doc_type = node . node . metadata . get ( \"document_type\" , \"N/A\" ) # Use more text for better relevance judgment (1500 chars instead of 500) text_excerpt = node . node . text [: 1500 ] prompt = f \"\"\"You are a lenient relevance filter for a German Bundestag document retrieval system. Your job is to filter out ONLY clearly irrelevant documents. When in doubt, keep the document. User Query: { query } Document Information: - Title: { title } - Date: { date } - Speaker: { speaker } - Type: { doc_type } - Similarity Score: { node . score : .3f } (already pre-filtered by semantic search) Document Excerpt (first 1500 characters): { text_excerpt } Task: This document was already retrieved by semantic search with score { node . score : .3f } . Only reject it if it's CLEARLY and OBVIOUSLY irrelevant to the query. If the document might contain ANY useful information for answering the query, keep it. Reply with ONLY \"YES\" (keep) or \"NO\" (reject) followed by a brief reason (max 10 words). Format: YES/NO - <reason> \"\"\" try : response = self . _llm . complete ( prompt ) response_text = response . text . strip () . upper () is_relevant = response_text . startswith ( \"YES\" ) if is_relevant : relevant_nodes . append ( node ) self . _logger . info ( f \"[HybridFilter] LLM kept ( { checked_count } / { len ( nodes ) } ): { title [: 60 ] } \" ) else : self . _logger . info ( f \"[HybridFilter] LLM rejected ( { checked_count } / { len ( nodes ) } ): { title [: 60 ] } - { response_text [: 80 ] } \" ) except Exception as e : self . _logger . warning ( f \"[HybridFilter] LLM check failed for document: { e } . Keeping document.\" ) relevant_nodes . append ( node ) # Keep on error to avoid data loss removed_count = len ( nodes ) - len ( relevant_nodes ) self . _logger . info ( f \"[HybridFilter] LLM filtering: removed { removed_count } irrelevant docs \u2192 { len ( relevant_nodes ) } remaining\" ) return relevant_nodes @staticmethod def _cosine_similarity ( vec1 : List [ float ], vec2 : List [ float ]) -> float : \"\"\" Calculate cosine similarity between two vectors. Args: vec1: First embedding vector vec2: Second embedding vector Returns: Cosine similarity score (0-1) \"\"\" vec1_np = np . array ( vec1 ) vec2_np = np . array ( vec2 ) dot_product = np . dot ( vec1_np , vec2_np ) norm1 = np . linalg . norm ( vec1_np ) norm2 = np . linalg . norm ( vec2_np ) if norm1 == 0 or norm2 == 0 : return 0.0 return float ( dot_product / ( norm1 * norm2 ))","title":"HybridFilterPostprocessor"},{"location":"src/augmentation/components/postprocessors/hybrid_filter/postprocessor/#src.augmentation.components.postprocessors.hybrid_filter.postprocessor.HybridFilterPostprocessor.__init__","text":"Initialize the hybrid filter postprocessor. Parameters: configuration ( HybridFilterConfiguration ) \u2013 Configuration containing filter thresholds and LLM settings temporal_domain_config ( Optional [ TemporalDomainConfiguration ] , default: None ) \u2013 Optional temporal domain configuration. If None, runs without temporal filtering. Source code in src/augmentation/components/postprocessors/hybrid_filter/postprocessor.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def __init__ ( self , configuration : HybridFilterConfiguration , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , ** kwargs , ): \"\"\" Initialize the hybrid filter postprocessor. Args: configuration: Configuration containing filter thresholds and LLM settings temporal_domain_config: Optional temporal domain configuration. If None, runs without temporal filtering. \"\"\" super () . __init__ ( score_threshold = configuration . score_threshold , similarity_threshold = configuration . similarity_threshold , max_documents = configuration . max_documents , enable_llm_filter = configuration . enable_llm_filter , ** kwargs , ) self . _logger = LoggerConfiguration . get_logger ( __name__ ) self . _temporal_domain_config = temporal_domain_config # Build keyword lists from config (empty if no config provided) # Lowercase keywords for case-insensitive substring matching if temporal_domain_config : self . _current_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_current_keywords () ] self . _historical_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_historical_keywords () ] self . _logger . info ( f \"[HybridFilter] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _current_keywords = [] self . _historical_keywords = [] self . _logger . info ( \"[HybridFilter] Running without temporal filtering (no temporal_domain config)\" ) # Initialize LLM if relevance filtering is enabled if configuration . enable_llm_filter and configuration . llm : self . _llm = LLMRegistry . get ( configuration . llm . provider ) . create ( configuration . llm ) self . _logger . info ( f \"LLM relevance filtering enabled with { configuration . llm . provider } / { configuration . llm . name } \" )","title":"__init__"},{"location":"src/augmentation/components/retrievers/query_rewriter/","text":"Query_rewriter This module contains functionality related to the the query_rewriter module for augmentation.components.retrievers . Query_rewriter Query rewriter for improving retrieval quality on specific query patterns. QueryRewriter Rewrites queries to improve semantic search retrieval. Uses pattern-based detection to selectively expand queries with domain-specific terminology. If temporal_domain_config is provided, uses configured keywords and expansion terms. Otherwise, performs no query rewriting (generic mode). Source code in src/augmentation/components/retrievers/query_rewriter.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 class QueryRewriter : \"\"\"Rewrites queries to improve semantic search retrieval. Uses pattern-based detection to selectively expand queries with domain-specific terminology. If temporal_domain_config is provided, uses configured keywords and expansion terms. Otherwise, performs no query rewriting (generic mode). \"\"\" def __init__ ( self , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\"Initialize the query rewriter. Args: temporal_domain_config: Optional temporal domain configuration. If None, runs in generic mode (no query rewriting). logger: Logger instance for logging rewrite operations. \"\"\" self . _temporal_domain_config = temporal_domain_config self . _logger = logger or LoggerConfiguration . get_logger ( __name__ ) # Build keyword lists from config # Lowercase keywords for case-insensitive substring matching if temporal_domain_config : self . _historical_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_historical_keywords () ] self . _current_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_current_keywords () ] self . _logger . info ( f \"[QueryRewriter] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _historical_keywords = [] self . _current_keywords = [] self . _logger . info ( \"[QueryRewriter] Running in generic mode (no query rewriting)\" ) def rewrite ( self , query : str ) -> str : \"\"\"Rewrite query if it matches known patterns. Args: query: Original user query Returns: Rewritten query with expansion terms, or original query if no pattern matches \"\"\" if not query or not query . strip (): return query # If no temporal domain config, return query unchanged if not self . _temporal_domain_config : return query query_lower = query . lower () # Pattern 1: Historical queries (previous/past information) # Check this FIRST so \"parties in previous parliament\" gets historical expansion if any ( keyword in query_lower for keyword in self . _historical_keywords ): return self . _expand_historical_query ( query , query_lower ) # Pattern 2: Temporal queries (current/recent information) # Check this SECOND so \"parties in current parliament\" gets temporal expansion if any ( keyword in query_lower for keyword in self . _current_keywords ): return self . _expand_temporal_query ( query , query_lower ) # Pattern 3: Entity queries (if entity_terms expansion is configured) # Check for entity-specific expansion terms entity_expansion = self . _temporal_domain_config . query_expansion . get ( \"entity_terms\" ) if entity_expansion : # Extract entity keywords from expansion config to use as triggers # For Bundestag: checks for \"party\", \"parties\", \"partei\", \"fraktion\", etc. entity_keywords = self . _extract_entity_keywords_from_config () if any ( keyword in query_lower for keyword in entity_keywords ): return self . _expand_entity_query ( query , query_lower ) # No pattern matched - return original query self . _logger . debug ( \"[QueryRewriter] No pattern matched, returning original query\" ) return query def _extract_entity_keywords_from_config ( self ) -> list : \"\"\"Extract entity keywords from config to use as triggers. Returns basic entity-related keywords that might trigger entity expansion. Returns: List of entity trigger keywords \"\"\" # Return common parliamentary/entity keywords # These are generic enough to work across domains return [ \"party\" , \"parties\" , \"partei\" , \"parteien\" , \"fraktion\" , \"fraktionen\" , \"group\" , \"groups\" , \"faction\" , \"factions\" , ] def _expand_entity_query ( self , query : str , query_lower : str ) -> str : \"\"\"Expand entity queries with domain-specific terminology. Uses configured entity_terms expansion if available. Args: query: Original query query_lower: Lowercase version for language detection Returns: Expanded query \"\"\" if not self . _temporal_domain_config : return query # Detect language language = self . _temporal_domain_config . detect_language ( query_lower ) # Get expansion terms expansion = self . _temporal_domain_config . get_expansion_terms ( \"entity_terms\" , language ) if not expansion : # Fallback to other language if primary not available fallback_lang = \"de\" if language != \"de\" else \"en\" expansion = self . _temporal_domain_config . get_expansion_terms ( \"entity_terms\" , fallback_lang ) if expansion : rewritten = f \" { query } { expansion } \" self . _logger . info ( f \"[QueryRewriter] Expanded entity query \\n \" f \" Original: { query [: 80 ] } ... \\n \" f \" Rewritten: { rewritten [: 120 ] } ...\" ) return rewritten return query def _expand_historical_query ( self , query : str , query_lower : str ) -> str : \"\"\"Expand historical queries to boost historical period documents. Uses configured temporal_historical expansion terms. Args: query: Original query query_lower: Lowercase version for language detection Returns: Expanded query \"\"\" if not self . _temporal_domain_config : return query # Detect language language = self . _temporal_domain_config . detect_language ( query_lower ) # Get expansion terms expansion = self . _temporal_domain_config . get_expansion_terms ( \"temporal_historical\" , language ) if not expansion : # Fallback to other language if primary not available fallback_lang = \"de\" if language != \"de\" else \"en\" expansion = self . _temporal_domain_config . get_expansion_terms ( \"temporal_historical\" , fallback_lang ) # Check if query also mentions entities (for additional entity terms) entity_keywords = self . _extract_entity_keywords_from_config () entity_terms_present = any ( keyword in query_lower for keyword in entity_keywords ) if entity_terms_present : entity_expansion = self . _temporal_domain_config . get_expansion_terms ( \"entity_terms\" , language ) if entity_expansion : expansion = f \" { expansion } { entity_expansion } \" if expansion : rewritten = f \" { query } { expansion } \" self . _logger . info ( f \"[QueryRewriter] Expanded historical query \\n \" f \" Original: { query [: 80 ] } ... \\n \" f \" Rewritten: { rewritten [: 120 ] } ...\" ) return rewritten return query def _expand_temporal_query ( self , query : str , query_lower : str ) -> str : \"\"\"Expand temporal queries to boost current period documents. Uses configured temporal_current expansion terms. Args: query: Original query query_lower: Lowercase version for language detection Returns: Expanded query \"\"\" if not self . _temporal_domain_config : return query # Detect language language = self . _temporal_domain_config . detect_language ( query_lower ) # Get expansion terms expansion = self . _temporal_domain_config . get_expansion_terms ( \"temporal_current\" , language ) if not expansion : # Fallback to other language if primary not available fallback_lang = \"de\" if language != \"de\" else \"en\" expansion = self . _temporal_domain_config . get_expansion_terms ( \"temporal_current\" , fallback_lang ) if expansion : rewritten = f \" { query } { expansion } \" self . _logger . info ( f \"[QueryRewriter] Expanded temporal query \\n \" f \" Original: { query [: 80 ] } ... \\n \" f \" Rewritten: { rewritten [: 120 ] } ...\" ) return rewritten return query def should_rewrite ( self , query : str ) -> bool : \"\"\"Check if query would be rewritten. Useful for testing and debugging. Args: query: Query to check Returns: True if query matches a rewrite pattern \"\"\" if not query or not self . _temporal_domain_config : return False query_lower = query . lower () # Check historical pattern if any ( keyword in query_lower for keyword in self . _historical_keywords ): return True # Check temporal/current pattern if any ( keyword in query_lower for keyword in self . _current_keywords ): return True # Check entity pattern entity_keywords = self . _extract_entity_keywords_from_config () if any ( keyword in query_lower for keyword in entity_keywords ): return True return False __init__ ( temporal_domain_config = None , logger = None ) Initialize the query rewriter. Parameters: temporal_domain_config ( Optional [ TemporalDomainConfiguration ] , default: None ) \u2013 Optional temporal domain configuration. If None, runs in generic mode (no query rewriting). logger ( Optional [ Logger ] , default: None ) \u2013 Logger instance for logging rewrite operations. Source code in src/augmentation/components/retrievers/query_rewriter.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\"Initialize the query rewriter. Args: temporal_domain_config: Optional temporal domain configuration. If None, runs in generic mode (no query rewriting). logger: Logger instance for logging rewrite operations. \"\"\" self . _temporal_domain_config = temporal_domain_config self . _logger = logger or LoggerConfiguration . get_logger ( __name__ ) # Build keyword lists from config # Lowercase keywords for case-insensitive substring matching if temporal_domain_config : self . _historical_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_historical_keywords () ] self . _current_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_current_keywords () ] self . _logger . info ( f \"[QueryRewriter] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _historical_keywords = [] self . _current_keywords = [] self . _logger . info ( \"[QueryRewriter] Running in generic mode (no query rewriting)\" ) rewrite ( query ) Rewrite query if it matches known patterns. Parameters: query ( str ) \u2013 Original user query Returns: str \u2013 Rewritten query with expansion terms, or original query if no pattern matches Source code in src/augmentation/components/retrievers/query_rewriter.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def rewrite ( self , query : str ) -> str : \"\"\"Rewrite query if it matches known patterns. Args: query: Original user query Returns: Rewritten query with expansion terms, or original query if no pattern matches \"\"\" if not query or not query . strip (): return query # If no temporal domain config, return query unchanged if not self . _temporal_domain_config : return query query_lower = query . lower () # Pattern 1: Historical queries (previous/past information) # Check this FIRST so \"parties in previous parliament\" gets historical expansion if any ( keyword in query_lower for keyword in self . _historical_keywords ): return self . _expand_historical_query ( query , query_lower ) # Pattern 2: Temporal queries (current/recent information) # Check this SECOND so \"parties in current parliament\" gets temporal expansion if any ( keyword in query_lower for keyword in self . _current_keywords ): return self . _expand_temporal_query ( query , query_lower ) # Pattern 3: Entity queries (if entity_terms expansion is configured) # Check for entity-specific expansion terms entity_expansion = self . _temporal_domain_config . query_expansion . get ( \"entity_terms\" ) if entity_expansion : # Extract entity keywords from expansion config to use as triggers # For Bundestag: checks for \"party\", \"parties\", \"partei\", \"fraktion\", etc. entity_keywords = self . _extract_entity_keywords_from_config () if any ( keyword in query_lower for keyword in entity_keywords ): return self . _expand_entity_query ( query , query_lower ) # No pattern matched - return original query self . _logger . debug ( \"[QueryRewriter] No pattern matched, returning original query\" ) return query should_rewrite ( query ) Check if query would be rewritten. Useful for testing and debugging. Parameters: query ( str ) \u2013 Query to check Returns: bool \u2013 True if query matches a rewrite pattern Source code in src/augmentation/components/retrievers/query_rewriter.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def should_rewrite ( self , query : str ) -> bool : \"\"\"Check if query would be rewritten. Useful for testing and debugging. Args: query: Query to check Returns: True if query matches a rewrite pattern \"\"\" if not query or not self . _temporal_domain_config : return False query_lower = query . lower () # Check historical pattern if any ( keyword in query_lower for keyword in self . _historical_keywords ): return True # Check temporal/current pattern if any ( keyword in query_lower for keyword in self . _current_keywords ): return True # Check entity pattern entity_keywords = self . _extract_entity_keywords_from_config () if any ( keyword in query_lower for keyword in entity_keywords ): return True return False","title":"Query_rewriter"},{"location":"src/augmentation/components/retrievers/query_rewriter/#query_rewriter","text":"This module contains functionality related to the the query_rewriter module for augmentation.components.retrievers .","title":"Query_rewriter"},{"location":"src/augmentation/components/retrievers/query_rewriter/#query_rewriter_1","text":"Query rewriter for improving retrieval quality on specific query patterns.","title":"Query_rewriter"},{"location":"src/augmentation/components/retrievers/query_rewriter/#src.augmentation.components.retrievers.query_rewriter.QueryRewriter","text":"Rewrites queries to improve semantic search retrieval. Uses pattern-based detection to selectively expand queries with domain-specific terminology. If temporal_domain_config is provided, uses configured keywords and expansion terms. Otherwise, performs no query rewriting (generic mode). Source code in src/augmentation/components/retrievers/query_rewriter.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 class QueryRewriter : \"\"\"Rewrites queries to improve semantic search retrieval. Uses pattern-based detection to selectively expand queries with domain-specific terminology. If temporal_domain_config is provided, uses configured keywords and expansion terms. Otherwise, performs no query rewriting (generic mode). \"\"\" def __init__ ( self , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\"Initialize the query rewriter. Args: temporal_domain_config: Optional temporal domain configuration. If None, runs in generic mode (no query rewriting). logger: Logger instance for logging rewrite operations. \"\"\" self . _temporal_domain_config = temporal_domain_config self . _logger = logger or LoggerConfiguration . get_logger ( __name__ ) # Build keyword lists from config # Lowercase keywords for case-insensitive substring matching if temporal_domain_config : self . _historical_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_historical_keywords () ] self . _current_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_current_keywords () ] self . _logger . info ( f \"[QueryRewriter] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _historical_keywords = [] self . _current_keywords = [] self . _logger . info ( \"[QueryRewriter] Running in generic mode (no query rewriting)\" ) def rewrite ( self , query : str ) -> str : \"\"\"Rewrite query if it matches known patterns. Args: query: Original user query Returns: Rewritten query with expansion terms, or original query if no pattern matches \"\"\" if not query or not query . strip (): return query # If no temporal domain config, return query unchanged if not self . _temporal_domain_config : return query query_lower = query . lower () # Pattern 1: Historical queries (previous/past information) # Check this FIRST so \"parties in previous parliament\" gets historical expansion if any ( keyword in query_lower for keyword in self . _historical_keywords ): return self . _expand_historical_query ( query , query_lower ) # Pattern 2: Temporal queries (current/recent information) # Check this SECOND so \"parties in current parliament\" gets temporal expansion if any ( keyword in query_lower for keyword in self . _current_keywords ): return self . _expand_temporal_query ( query , query_lower ) # Pattern 3: Entity queries (if entity_terms expansion is configured) # Check for entity-specific expansion terms entity_expansion = self . _temporal_domain_config . query_expansion . get ( \"entity_terms\" ) if entity_expansion : # Extract entity keywords from expansion config to use as triggers # For Bundestag: checks for \"party\", \"parties\", \"partei\", \"fraktion\", etc. entity_keywords = self . _extract_entity_keywords_from_config () if any ( keyword in query_lower for keyword in entity_keywords ): return self . _expand_entity_query ( query , query_lower ) # No pattern matched - return original query self . _logger . debug ( \"[QueryRewriter] No pattern matched, returning original query\" ) return query def _extract_entity_keywords_from_config ( self ) -> list : \"\"\"Extract entity keywords from config to use as triggers. Returns basic entity-related keywords that might trigger entity expansion. Returns: List of entity trigger keywords \"\"\" # Return common parliamentary/entity keywords # These are generic enough to work across domains return [ \"party\" , \"parties\" , \"partei\" , \"parteien\" , \"fraktion\" , \"fraktionen\" , \"group\" , \"groups\" , \"faction\" , \"factions\" , ] def _expand_entity_query ( self , query : str , query_lower : str ) -> str : \"\"\"Expand entity queries with domain-specific terminology. Uses configured entity_terms expansion if available. Args: query: Original query query_lower: Lowercase version for language detection Returns: Expanded query \"\"\" if not self . _temporal_domain_config : return query # Detect language language = self . _temporal_domain_config . detect_language ( query_lower ) # Get expansion terms expansion = self . _temporal_domain_config . get_expansion_terms ( \"entity_terms\" , language ) if not expansion : # Fallback to other language if primary not available fallback_lang = \"de\" if language != \"de\" else \"en\" expansion = self . _temporal_domain_config . get_expansion_terms ( \"entity_terms\" , fallback_lang ) if expansion : rewritten = f \" { query } { expansion } \" self . _logger . info ( f \"[QueryRewriter] Expanded entity query \\n \" f \" Original: { query [: 80 ] } ... \\n \" f \" Rewritten: { rewritten [: 120 ] } ...\" ) return rewritten return query def _expand_historical_query ( self , query : str , query_lower : str ) -> str : \"\"\"Expand historical queries to boost historical period documents. Uses configured temporal_historical expansion terms. Args: query: Original query query_lower: Lowercase version for language detection Returns: Expanded query \"\"\" if not self . _temporal_domain_config : return query # Detect language language = self . _temporal_domain_config . detect_language ( query_lower ) # Get expansion terms expansion = self . _temporal_domain_config . get_expansion_terms ( \"temporal_historical\" , language ) if not expansion : # Fallback to other language if primary not available fallback_lang = \"de\" if language != \"de\" else \"en\" expansion = self . _temporal_domain_config . get_expansion_terms ( \"temporal_historical\" , fallback_lang ) # Check if query also mentions entities (for additional entity terms) entity_keywords = self . _extract_entity_keywords_from_config () entity_terms_present = any ( keyword in query_lower for keyword in entity_keywords ) if entity_terms_present : entity_expansion = self . _temporal_domain_config . get_expansion_terms ( \"entity_terms\" , language ) if entity_expansion : expansion = f \" { expansion } { entity_expansion } \" if expansion : rewritten = f \" { query } { expansion } \" self . _logger . info ( f \"[QueryRewriter] Expanded historical query \\n \" f \" Original: { query [: 80 ] } ... \\n \" f \" Rewritten: { rewritten [: 120 ] } ...\" ) return rewritten return query def _expand_temporal_query ( self , query : str , query_lower : str ) -> str : \"\"\"Expand temporal queries to boost current period documents. Uses configured temporal_current expansion terms. Args: query: Original query query_lower: Lowercase version for language detection Returns: Expanded query \"\"\" if not self . _temporal_domain_config : return query # Detect language language = self . _temporal_domain_config . detect_language ( query_lower ) # Get expansion terms expansion = self . _temporal_domain_config . get_expansion_terms ( \"temporal_current\" , language ) if not expansion : # Fallback to other language if primary not available fallback_lang = \"de\" if language != \"de\" else \"en\" expansion = self . _temporal_domain_config . get_expansion_terms ( \"temporal_current\" , fallback_lang ) if expansion : rewritten = f \" { query } { expansion } \" self . _logger . info ( f \"[QueryRewriter] Expanded temporal query \\n \" f \" Original: { query [: 80 ] } ... \\n \" f \" Rewritten: { rewritten [: 120 ] } ...\" ) return rewritten return query def should_rewrite ( self , query : str ) -> bool : \"\"\"Check if query would be rewritten. Useful for testing and debugging. Args: query: Query to check Returns: True if query matches a rewrite pattern \"\"\" if not query or not self . _temporal_domain_config : return False query_lower = query . lower () # Check historical pattern if any ( keyword in query_lower for keyword in self . _historical_keywords ): return True # Check temporal/current pattern if any ( keyword in query_lower for keyword in self . _current_keywords ): return True # Check entity pattern entity_keywords = self . _extract_entity_keywords_from_config () if any ( keyword in query_lower for keyword in entity_keywords ): return True return False","title":"QueryRewriter"},{"location":"src/augmentation/components/retrievers/query_rewriter/#src.augmentation.components.retrievers.query_rewriter.QueryRewriter.__init__","text":"Initialize the query rewriter. Parameters: temporal_domain_config ( Optional [ TemporalDomainConfiguration ] , default: None ) \u2013 Optional temporal domain configuration. If None, runs in generic mode (no query rewriting). logger ( Optional [ Logger ] , default: None ) \u2013 Logger instance for logging rewrite operations. Source code in src/augmentation/components/retrievers/query_rewriter.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\"Initialize the query rewriter. Args: temporal_domain_config: Optional temporal domain configuration. If None, runs in generic mode (no query rewriting). logger: Logger instance for logging rewrite operations. \"\"\" self . _temporal_domain_config = temporal_domain_config self . _logger = logger or LoggerConfiguration . get_logger ( __name__ ) # Build keyword lists from config # Lowercase keywords for case-insensitive substring matching if temporal_domain_config : self . _historical_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_historical_keywords () ] self . _current_keywords = [ kw . lower () for kw in temporal_domain_config . get_all_current_keywords () ] self . _logger . info ( f \"[QueryRewriter] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _historical_keywords = [] self . _current_keywords = [] self . _logger . info ( \"[QueryRewriter] Running in generic mode (no query rewriting)\" )","title":"__init__"},{"location":"src/augmentation/components/retrievers/query_rewriter/#src.augmentation.components.retrievers.query_rewriter.QueryRewriter.rewrite","text":"Rewrite query if it matches known patterns. Parameters: query ( str ) \u2013 Original user query Returns: str \u2013 Rewritten query with expansion terms, or original query if no pattern matches Source code in src/augmentation/components/retrievers/query_rewriter.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def rewrite ( self , query : str ) -> str : \"\"\"Rewrite query if it matches known patterns. Args: query: Original user query Returns: Rewritten query with expansion terms, or original query if no pattern matches \"\"\" if not query or not query . strip (): return query # If no temporal domain config, return query unchanged if not self . _temporal_domain_config : return query query_lower = query . lower () # Pattern 1: Historical queries (previous/past information) # Check this FIRST so \"parties in previous parliament\" gets historical expansion if any ( keyword in query_lower for keyword in self . _historical_keywords ): return self . _expand_historical_query ( query , query_lower ) # Pattern 2: Temporal queries (current/recent information) # Check this SECOND so \"parties in current parliament\" gets temporal expansion if any ( keyword in query_lower for keyword in self . _current_keywords ): return self . _expand_temporal_query ( query , query_lower ) # Pattern 3: Entity queries (if entity_terms expansion is configured) # Check for entity-specific expansion terms entity_expansion = self . _temporal_domain_config . query_expansion . get ( \"entity_terms\" ) if entity_expansion : # Extract entity keywords from expansion config to use as triggers # For Bundestag: checks for \"party\", \"parties\", \"partei\", \"fraktion\", etc. entity_keywords = self . _extract_entity_keywords_from_config () if any ( keyword in query_lower for keyword in entity_keywords ): return self . _expand_entity_query ( query , query_lower ) # No pattern matched - return original query self . _logger . debug ( \"[QueryRewriter] No pattern matched, returning original query\" ) return query","title":"rewrite"},{"location":"src/augmentation/components/retrievers/query_rewriter/#src.augmentation.components.retrievers.query_rewriter.QueryRewriter.should_rewrite","text":"Check if query would be rewritten. Useful for testing and debugging. Parameters: query ( str ) \u2013 Query to check Returns: bool \u2013 True if query matches a rewrite pattern Source code in src/augmentation/components/retrievers/query_rewriter.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def should_rewrite ( self , query : str ) -> bool : \"\"\"Check if query would be rewritten. Useful for testing and debugging. Args: query: Query to check Returns: True if query matches a rewrite pattern \"\"\" if not query or not self . _temporal_domain_config : return False query_lower = query . lower () # Check historical pattern if any ( keyword in query_lower for keyword in self . _historical_keywords ): return True # Check temporal/current pattern if any ( keyword in query_lower for keyword in self . _current_keywords ): return True # Check entity pattern entity_keywords = self . _extract_entity_keywords_from_config () if any ( keyword in query_lower for keyword in entity_keywords ): return True return False","title":"should_rewrite"},{"location":"src/augmentation/components/retrievers/query_rewriting_retriever/","text":"Query_rewriting_retriever This module contains functionality related to the the query_rewriting_retriever module for augmentation.components.retrievers . Query_rewriting_retriever Query rewriting retriever wrapper that enhances retrieval with query expansion. QueryRewritingRetriever Bases: BaseRetriever Retriever wrapper that applies query rewriting before retrieval. This wrapper intercepts queries, applies domain-specific expansions to improve semantic search, then delegates to the underlying retriever. Source code in src/augmentation/components/retrievers/query_rewriting_retriever.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class QueryRewritingRetriever ( BaseRetriever ): \"\"\"Retriever wrapper that applies query rewriting before retrieval. This wrapper intercepts queries, applies domain-specific expansions to improve semantic search, then delegates to the underlying retriever. \"\"\" def __init__ ( self , base_retriever : BaseRetriever , query_rewriter : Optional [ QueryRewriter ] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\"Initialize the query rewriting retriever. Args: base_retriever: The underlying retriever to delegate to query_rewriter: Query rewriter instance (creates default if None) logger: Logger instance \"\"\" super () . __init__ () self . _base_retriever = base_retriever self . _query_rewriter = query_rewriter or QueryRewriter () self . _logger = logger or LoggerConfiguration . get_logger ( __name__ ) def _retrieve ( self , query_bundle : QueryBundle ) -> List [ NodeWithScore ]: \"\"\"Retrieve nodes with query rewriting. Args: query_bundle: Query bundle containing the user query Returns: List of nodes with scores from the underlying retriever \"\"\" original_query = query_bundle . query_str # Rewrite query if it matches a pattern rewritten_query = self . _query_rewriter . rewrite ( original_query ) # If query was rewritten, create new query bundle if rewritten_query != original_query : self . _logger . info ( f \"[QueryRewritingRetriever] Query rewritten \\n \" f \" Original: { original_query [: 100 ] } ... \\n \" f \" Rewritten: { rewritten_query [: 150 ] } ...\" ) query_bundle = QueryBundle ( query_str = rewritten_query , custom_embedding_strs = [ rewritten_query ], ) else : self . _logger . debug ( \"[QueryRewritingRetriever] No rewriting applied\" ) # Delegate to base retriever return self . _base_retriever . _retrieve ( query_bundle ) async def _aretrieve ( self , query_bundle : QueryBundle ) -> List [ NodeWithScore ]: \"\"\"Async retrieve (delegates to sync for now).\"\"\" return self . _retrieve ( query_bundle ) __init__ ( base_retriever , query_rewriter = None , logger = None ) Initialize the query rewriting retriever. Parameters: base_retriever ( BaseRetriever ) \u2013 The underlying retriever to delegate to query_rewriter ( Optional [ QueryRewriter ] , default: None ) \u2013 Query rewriter instance (creates default if None) logger ( Optional [ Logger ] , default: None ) \u2013 Logger instance Source code in src/augmentation/components/retrievers/query_rewriting_retriever.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , base_retriever : BaseRetriever , query_rewriter : Optional [ QueryRewriter ] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\"Initialize the query rewriting retriever. Args: base_retriever: The underlying retriever to delegate to query_rewriter: Query rewriter instance (creates default if None) logger: Logger instance \"\"\" super () . __init__ () self . _base_retriever = base_retriever self . _query_rewriter = query_rewriter or QueryRewriter () self . _logger = logger or LoggerConfiguration . get_logger ( __name__ )","title":"Query_rewriting_retriever"},{"location":"src/augmentation/components/retrievers/query_rewriting_retriever/#query_rewriting_retriever","text":"This module contains functionality related to the the query_rewriting_retriever module for augmentation.components.retrievers .","title":"Query_rewriting_retriever"},{"location":"src/augmentation/components/retrievers/query_rewriting_retriever/#query_rewriting_retriever_1","text":"Query rewriting retriever wrapper that enhances retrieval with query expansion.","title":"Query_rewriting_retriever"},{"location":"src/augmentation/components/retrievers/query_rewriting_retriever/#src.augmentation.components.retrievers.query_rewriting_retriever.QueryRewritingRetriever","text":"Bases: BaseRetriever Retriever wrapper that applies query rewriting before retrieval. This wrapper intercepts queries, applies domain-specific expansions to improve semantic search, then delegates to the underlying retriever. Source code in src/augmentation/components/retrievers/query_rewriting_retriever.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 class QueryRewritingRetriever ( BaseRetriever ): \"\"\"Retriever wrapper that applies query rewriting before retrieval. This wrapper intercepts queries, applies domain-specific expansions to improve semantic search, then delegates to the underlying retriever. \"\"\" def __init__ ( self , base_retriever : BaseRetriever , query_rewriter : Optional [ QueryRewriter ] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\"Initialize the query rewriting retriever. Args: base_retriever: The underlying retriever to delegate to query_rewriter: Query rewriter instance (creates default if None) logger: Logger instance \"\"\" super () . __init__ () self . _base_retriever = base_retriever self . _query_rewriter = query_rewriter or QueryRewriter () self . _logger = logger or LoggerConfiguration . get_logger ( __name__ ) def _retrieve ( self , query_bundle : QueryBundle ) -> List [ NodeWithScore ]: \"\"\"Retrieve nodes with query rewriting. Args: query_bundle: Query bundle containing the user query Returns: List of nodes with scores from the underlying retriever \"\"\" original_query = query_bundle . query_str # Rewrite query if it matches a pattern rewritten_query = self . _query_rewriter . rewrite ( original_query ) # If query was rewritten, create new query bundle if rewritten_query != original_query : self . _logger . info ( f \"[QueryRewritingRetriever] Query rewritten \\n \" f \" Original: { original_query [: 100 ] } ... \\n \" f \" Rewritten: { rewritten_query [: 150 ] } ...\" ) query_bundle = QueryBundle ( query_str = rewritten_query , custom_embedding_strs = [ rewritten_query ], ) else : self . _logger . debug ( \"[QueryRewritingRetriever] No rewriting applied\" ) # Delegate to base retriever return self . _base_retriever . _retrieve ( query_bundle ) async def _aretrieve ( self , query_bundle : QueryBundle ) -> List [ NodeWithScore ]: \"\"\"Async retrieve (delegates to sync for now).\"\"\" return self . _retrieve ( query_bundle )","title":"QueryRewritingRetriever"},{"location":"src/augmentation/components/retrievers/query_rewriting_retriever/#src.augmentation.components.retrievers.query_rewriting_retriever.QueryRewritingRetriever.__init__","text":"Initialize the query rewriting retriever. Parameters: base_retriever ( BaseRetriever ) \u2013 The underlying retriever to delegate to query_rewriter ( Optional [ QueryRewriter ] , default: None ) \u2013 Query rewriter instance (creates default if None) logger ( Optional [ Logger ] , default: None ) \u2013 Logger instance Source code in src/augmentation/components/retrievers/query_rewriting_retriever.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , base_retriever : BaseRetriever , query_rewriter : Optional [ QueryRewriter ] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\"Initialize the query rewriting retriever. Args: base_retriever: The underlying retriever to delegate to query_rewriter: Query rewriter instance (creates default if None) logger: Logger instance \"\"\" super () . __init__ () self . _base_retriever = base_retriever self . _query_rewriter = query_rewriter or QueryRewriter () self . _logger = logger or LoggerConfiguration . get_logger ( __name__ )","title":"__init__"},{"location":"src/augmentation/components/retrievers/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.retrievers . Registry RetrieverRegistry Bases: Registry Registry for managing retriever components in the RAG system. This registry maps RetrieverName enum values to their corresponding retriever implementations, facilitating the creation and management of retriever instances based on configuration. It inherits from the base Registry class and specifies RetrieverName as the key type for registration and lookup operations. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to RetrieverName enum. Source code in src/augmentation/components/retrievers/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class RetrieverRegistry ( Registry ): \"\"\"Registry for managing retriever components in the RAG system. This registry maps RetrieverName enum values to their corresponding retriever implementations, facilitating the creation and management of retriever instances based on configuration. It inherits from the base Registry class and specifies RetrieverName as the key type for registration and lookup operations. Attributes: _key_class (Type): The class type used for registry keys, set to RetrieverName enum. \"\"\" _key_class : Type = RetrieverName","title":"Registry"},{"location":"src/augmentation/components/retrievers/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.retrievers .","title":"Registry"},{"location":"src/augmentation/components/retrievers/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/retrievers/registry/#src.augmentation.components.retrievers.registry.RetrieverRegistry","text":"Bases: Registry Registry for managing retriever components in the RAG system. This registry maps RetrieverName enum values to their corresponding retriever implementations, facilitating the creation and management of retriever instances based on configuration. It inherits from the base Registry class and specifies RetrieverName as the key type for registration and lookup operations. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to RetrieverName enum. Source code in src/augmentation/components/retrievers/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class RetrieverRegistry ( Registry ): \"\"\"Registry for managing retriever components in the RAG system. This registry maps RetrieverName enum values to their corresponding retriever implementations, facilitating the creation and management of retriever instances based on configuration. It inherits from the base Registry class and specifies RetrieverName as the key type for registration and lookup operations. Attributes: _key_class (Type): The class type used for registry keys, set to RetrieverName enum. \"\"\" _key_class : Type = RetrieverName","title":"RetrieverRegistry"},{"location":"src/augmentation/components/retrievers/auto/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.retrievers.auto . Configuration AutoRetrieverConfiguration Bases: RetrieverConfiguration Configuration for the Auto Retriever component. The Auto Retriever automatically determines the most appropriate retrieval strategy based on the query content, using an LLM to analyze and extract relevant metadata from the query. Source code in src/augmentation/components/retrievers/auto/configuration.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class AutoRetrieverConfiguration ( RetrieverConfiguration ): \"\"\" Configuration for the Auto Retriever component. The Auto Retriever automatically determines the most appropriate retrieval strategy based on the query content, using an LLM to analyze and extract relevant metadata from the query. \"\"\" name : Literal [ RetrieverName . AUTO ] = Field ( ... , description = \"The name of the retriever.\" ) llm : Any = Field ( ... , description = \"The LLM configuration used to extract metadata from the query.\" , ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\" Validates the LLM configuration using the LLMConfigurationRegistry. This validator ensures that the provided LLM configuration is valid according to the registered LLM configuration classes. Args: value: The LLM configuration value to validate. info: ValidationInfo object containing context about the validation. Returns: The validated LLM configuration object. \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , )","title":"Configuration"},{"location":"src/augmentation/components/retrievers/auto/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.retrievers.auto .","title":"Configuration"},{"location":"src/augmentation/components/retrievers/auto/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/retrievers/auto/configuration/#src.augmentation.components.retrievers.auto.configuration.AutoRetrieverConfiguration","text":"Bases: RetrieverConfiguration Configuration for the Auto Retriever component. The Auto Retriever automatically determines the most appropriate retrieval strategy based on the query content, using an LLM to analyze and extract relevant metadata from the query. Source code in src/augmentation/components/retrievers/auto/configuration.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class AutoRetrieverConfiguration ( RetrieverConfiguration ): \"\"\" Configuration for the Auto Retriever component. The Auto Retriever automatically determines the most appropriate retrieval strategy based on the query content, using an LLM to analyze and extract relevant metadata from the query. \"\"\" name : Literal [ RetrieverName . AUTO ] = Field ( ... , description = \"The name of the retriever.\" ) llm : Any = Field ( ... , description = \"The LLM configuration used to extract metadata from the query.\" , ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\" Validates the LLM configuration using the LLMConfigurationRegistry. This validator ensures that the provided LLM configuration is valid according to the registered LLM configuration classes. Args: value: The LLM configuration value to validate. info: ValidationInfo object containing context about the validation. Returns: The validated LLM configuration object. \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , )","title":"AutoRetrieverConfiguration"},{"location":"src/augmentation/components/retrievers/auto/retriever/","text":"Retriever This module contains functionality related to the the retriever module for augmentation.components.retrievers.auto . Retriever AutoRetrieverFactory Bases: Factory Factory class for creating VectorIndexAutoRetriever instances. This factory builds auto-retriever components that utilize LLMs to dynamically construct queries for vector store retrieval based on user inputs. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the auto retriever. Source code in src/augmentation/components/retrievers/auto/retriever.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class AutoRetrieverFactory ( Factory ): \"\"\" Factory class for creating VectorIndexAutoRetriever instances. This factory builds auto-retriever components that utilize LLMs to dynamically construct queries for vector store retrieval based on user inputs. Attributes: _configuration_class: The configuration class for the auto retriever. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> BaseRetriever : \"\"\" Creates a VectorIndexAutoRetriever wrapped with query rewriting. This method: 1. Sets up the vector store using the configuration 2. Initializes the embedding model 3. Creates a VectorStoreIndex from the vector store and embedding model 4. Configures the LLM for the retriever 5. Wraps the retriever with query rewriting for improved retrieval 6. Returns a fully configured retriever with query rewriting Args: configuration: AugmentationConfiguration object containing all necessary settings for creating the retriever component Returns: BaseRetriever: A query-rewriting retriever wrapping the auto-retriever \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) llm = LLMRegistry . get ( retriever_configuration . llm . provider ) . create ( retriever_configuration . llm ) base_retriever = VectorIndexAutoRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , llm = llm , vector_store_info = VectorStoreInfo ( content_info = \"Knowledge base containing documents for retrieval process in RAG system.\" , metadata_info = [ # No metadata filters defined - rely purely on semantic search. # Metadata filters often become overly specific and eliminate all results, # especially with temporal queries where exact dates may not exist in the DB. # The LLM also tends to hallucinate dates or infer temporal relationships # from context (e.g., applying date from question 1 to question 2). # Semantic search with embeddings is generally more effective for finding # relevant documents than strict metadata filtering. ], ), ) # Wrap with query rewriting for improved retrieval on specific query patterns query_rewriter = QueryRewriter () return QueryRewritingRetriever ( base_retriever = base_retriever , query_rewriter = query_rewriter )","title":"Retriever"},{"location":"src/augmentation/components/retrievers/auto/retriever/#retriever","text":"This module contains functionality related to the the retriever module for augmentation.components.retrievers.auto .","title":"Retriever"},{"location":"src/augmentation/components/retrievers/auto/retriever/#retriever_1","text":"","title":"Retriever"},{"location":"src/augmentation/components/retrievers/auto/retriever/#src.augmentation.components.retrievers.auto.retriever.AutoRetrieverFactory","text":"Bases: Factory Factory class for creating VectorIndexAutoRetriever instances. This factory builds auto-retriever components that utilize LLMs to dynamically construct queries for vector store retrieval based on user inputs. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the auto retriever. Source code in src/augmentation/components/retrievers/auto/retriever.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class AutoRetrieverFactory ( Factory ): \"\"\" Factory class for creating VectorIndexAutoRetriever instances. This factory builds auto-retriever components that utilize LLMs to dynamically construct queries for vector store retrieval based on user inputs. Attributes: _configuration_class: The configuration class for the auto retriever. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> BaseRetriever : \"\"\" Creates a VectorIndexAutoRetriever wrapped with query rewriting. This method: 1. Sets up the vector store using the configuration 2. Initializes the embedding model 3. Creates a VectorStoreIndex from the vector store and embedding model 4. Configures the LLM for the retriever 5. Wraps the retriever with query rewriting for improved retrieval 6. Returns a fully configured retriever with query rewriting Args: configuration: AugmentationConfiguration object containing all necessary settings for creating the retriever component Returns: BaseRetriever: A query-rewriting retriever wrapping the auto-retriever \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) llm = LLMRegistry . get ( retriever_configuration . llm . provider ) . create ( retriever_configuration . llm ) base_retriever = VectorIndexAutoRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , llm = llm , vector_store_info = VectorStoreInfo ( content_info = \"Knowledge base containing documents for retrieval process in RAG system.\" , metadata_info = [ # No metadata filters defined - rely purely on semantic search. # Metadata filters often become overly specific and eliminate all results, # especially with temporal queries where exact dates may not exist in the DB. # The LLM also tends to hallucinate dates or infer temporal relationships # from context (e.g., applying date from question 1 to question 2). # Semantic search with embeddings is generally more effective for finding # relevant documents than strict metadata filtering. ], ), ) # Wrap with query rewriting for improved retrieval on specific query patterns query_rewriter = QueryRewriter () return QueryRewritingRetriever ( base_retriever = base_retriever , query_rewriter = query_rewriter )","title":"AutoRetrieverFactory"},{"location":"src/augmentation/components/retrievers/basic/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.retrievers.basic . Configuration BasicRetrieverConfiguration Bases: RetrieverConfiguration Configuration for the Basic Retriever component. This class defines the configuration parameters needed for initializing and operating the basic retriever, extending the base RetrieverConfiguration. Source code in src/augmentation/components/retrievers/basic/configuration.py 11 12 13 14 15 16 17 18 19 20 21 class BasicRetrieverConfiguration ( RetrieverConfiguration ): \"\"\" Configuration for the Basic Retriever component. This class defines the configuration parameters needed for initializing and operating the basic retriever, extending the base RetrieverConfiguration. \"\"\" name : Literal [ RetrieverName . BASIC ] = Field ( ... , description = \"The name of the retriever.\" )","title":"Configuration"},{"location":"src/augmentation/components/retrievers/basic/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.retrievers.basic .","title":"Configuration"},{"location":"src/augmentation/components/retrievers/basic/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/retrievers/basic/configuration/#src.augmentation.components.retrievers.basic.configuration.BasicRetrieverConfiguration","text":"Bases: RetrieverConfiguration Configuration for the Basic Retriever component. This class defines the configuration parameters needed for initializing and operating the basic retriever, extending the base RetrieverConfiguration. Source code in src/augmentation/components/retrievers/basic/configuration.py 11 12 13 14 15 16 17 18 19 20 21 class BasicRetrieverConfiguration ( RetrieverConfiguration ): \"\"\" Configuration for the Basic Retriever component. This class defines the configuration parameters needed for initializing and operating the basic retriever, extending the base RetrieverConfiguration. \"\"\" name : Literal [ RetrieverName . BASIC ] = Field ( ... , description = \"The name of the retriever.\" )","title":"BasicRetrieverConfiguration"},{"location":"src/augmentation/components/retrievers/basic/retriever/","text":"Retriever This module contains functionality related to the the retriever module for augmentation.components.retrievers.basic . Retriever BasicRetrieverFactory Bases: Factory Factory class for creating VectorIndexRetriever instances. This factory implements the Factory design pattern to create a basic retriever component that uses vector similarity search to retrieve relevant context from a vector store. Source code in src/augmentation/components/retrievers/basic/retriever.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class BasicRetrieverFactory ( Factory ): \"\"\" Factory class for creating VectorIndexRetriever instances. This factory implements the Factory design pattern to create a basic retriever component that uses vector similarity search to retrieve relevant context from a vector store. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> VectorIndexRetriever : \"\"\" Creates a VectorIndexRetriever instance based on the provided configuration. This method: 1. Initializes the vector store from configuration 2. Creates the embedding model 3. Sets up the vector store index 4. Configures and returns the retriever with specified parameters Args: configuration: An AugmentationConfiguration object containing settings for the vector store, embedding model, and retriever parameters. Returns: VectorIndexRetriever: Configured retriever instance ready for similarity searches. \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) return VectorIndexRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , )","title":"Retriever"},{"location":"src/augmentation/components/retrievers/basic/retriever/#retriever","text":"This module contains functionality related to the the retriever module for augmentation.components.retrievers.basic .","title":"Retriever"},{"location":"src/augmentation/components/retrievers/basic/retriever/#retriever_1","text":"","title":"Retriever"},{"location":"src/augmentation/components/retrievers/basic/retriever/#src.augmentation.components.retrievers.basic.retriever.BasicRetrieverFactory","text":"Bases: Factory Factory class for creating VectorIndexRetriever instances. This factory implements the Factory design pattern to create a basic retriever component that uses vector similarity search to retrieve relevant context from a vector store. Source code in src/augmentation/components/retrievers/basic/retriever.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class BasicRetrieverFactory ( Factory ): \"\"\" Factory class for creating VectorIndexRetriever instances. This factory implements the Factory design pattern to create a basic retriever component that uses vector similarity search to retrieve relevant context from a vector store. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> VectorIndexRetriever : \"\"\" Creates a VectorIndexRetriever instance based on the provided configuration. This method: 1. Initializes the vector store from configuration 2. Creates the embedding model 3. Sets up the vector store index 4. Configures and returns the retriever with specified parameters Args: configuration: An AugmentationConfiguration object containing settings for the vector store, embedding model, and retriever parameters. Returns: VectorIndexRetriever: Configured retriever instance ready for similarity searches. \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) return VectorIndexRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , )","title":"BasicRetrieverFactory"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.retrievers.dynamic_temporal . Configuration Configuration for dynamic temporal retriever. DynamicTemporalRetrieverConfiguration Bases: RetrieverConfiguration Configuration for dynamic temporal retriever. This retriever detects temporal keywords and applies filters dynamically. No additional configuration needed beyond base retriever settings. Source code in src/augmentation/components/retrievers/dynamic_temporal/configuration.py 8 9 10 11 12 13 14 15 class DynamicTemporalRetrieverConfiguration ( RetrieverConfiguration ): \"\"\"Configuration for dynamic temporal retriever. This retriever detects temporal keywords and applies filters dynamically. No additional configuration needed beyond base retriever settings. \"\"\" pass","title":"Configuration"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.retrievers.dynamic_temporal .","title":"Configuration"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/configuration/#configuration_1","text":"Configuration for dynamic temporal retriever.","title":"Configuration"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/configuration/#src.augmentation.components.retrievers.dynamic_temporal.configuration.DynamicTemporalRetrieverConfiguration","text":"Bases: RetrieverConfiguration Configuration for dynamic temporal retriever. This retriever detects temporal keywords and applies filters dynamically. No additional configuration needed beyond base retriever settings. Source code in src/augmentation/components/retrievers/dynamic_temporal/configuration.py 8 9 10 11 12 13 14 15 class DynamicTemporalRetrieverConfiguration ( RetrieverConfiguration ): \"\"\"Configuration for dynamic temporal retriever. This retriever detects temporal keywords and applies filters dynamically. No additional configuration needed beyond base retriever settings. \"\"\" pass","title":"DynamicTemporalRetrieverConfiguration"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/retriever/","text":"Retriever This module contains functionality related to the the retriever module for augmentation.components.retrievers.dynamic_temporal . Retriever Dynamic temporal retriever that applies filters based on query content. DynamicTemporalRetriever Bases: BaseRetriever Retriever that dynamically applies temporal filters based on query. Detects temporal keywords in queries and applies appropriate metadata filters based on configured temporal domain. If no temporal_domain_config is provided, runs in generic mode without temporal filtering. Source code in src/augmentation/components/retrievers/dynamic_temporal/retriever.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class DynamicTemporalRetriever ( BaseRetriever ): \"\"\"Retriever that dynamically applies temporal filters based on query. Detects temporal keywords in queries and applies appropriate metadata filters based on configured temporal domain. If no temporal_domain_config is provided, runs in generic mode without temporal filtering. \"\"\" def __init__ ( self , index : VectorStoreIndex , similarity_top_k : int , query_rewriter : Optional [ QueryRewriter ] = None , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , ): \"\"\"Initialize dynamic temporal retriever. Args: index: Vector store index similarity_top_k: Number of top results to retrieve query_rewriter: Optional query rewriter for semantic improvements temporal_domain_config: Optional temporal domain configuration. If None, runs in generic mode (no temporal filtering). \"\"\" super () . __init__ () self . _index = index self . _similarity_top_k = similarity_top_k self . _temporal_domain_config = temporal_domain_config self . _query_rewriter = query_rewriter or QueryRewriter ( temporal_domain_config ) self . _logger = LoggerConfiguration . get_logger ( __name__ ) # Build keyword lists from config (empty if no config provided) if temporal_domain_config : self . _current_keywords = ( temporal_domain_config . get_all_current_keywords () ) self . _historical_keywords = ( temporal_domain_config . get_all_historical_keywords () ) self . _logger . info ( f \"[DynamicTemporal] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _current_keywords = [] self . _historical_keywords = [] self . _logger . info ( \"[DynamicTemporal] Running in generic mode (no temporal filtering)\" ) def _get_temporal_filter_mode ( self , query : str ) -> str : \"\"\"Determine which temporal filter to apply based on query keywords. Uses configured temporal keywords to detect temporal intent. If no temporal domain config is provided, always returns \"none\". Args: query: User query string Returns: One of: \"historical\", \"current\", \"none\" \"\"\" # If no temporal domain config, no filtering if not self . _temporal_domain_config : return \"none\" query_lower = query . lower () # First check for historical keywords - higher priority for keyword in self . _historical_keywords : # Use word boundary matching for keyword detection if re . search ( rf \"\\b { re . escape ( keyword ) } \\b\" , query_lower , re . IGNORECASE ): self . _logger . info ( f \"[DynamicTemporal] Detected historical keyword: ' { keyword } ' \" f \"- will filter to period { self . _temporal_domain_config . historical_period_value } \" ) return \"historical\" # Then check for current/temporal keywords for keyword in self . _current_keywords : if re . search ( rf \"\\b { re . escape ( keyword ) } \\b\" , query_lower , re . IGNORECASE ): self . _logger . info ( f \"[DynamicTemporal] Detected current keyword: ' { keyword } ' \" f \"- will filter to period { self . _temporal_domain_config . current_period_value } \" ) return \"current\" return \"none\" def _retrieve ( self , query_bundle : QueryBundle ) -> List [ NodeWithScore ]: \"\"\"Retrieve with dynamic temporal filtering. Applies metadata filtering based on configured temporal domain and query keywords. If no temporal_domain_config is provided, retrieves without filtering. Args: query_bundle: Query bundle Returns: List of nodes with scores \"\"\" original_query = query_bundle . query_str # Apply query rewriting rewritten_query = self . _query_rewriter . rewrite ( original_query ) if rewritten_query != original_query : query_bundle = QueryBundle ( query_str = rewritten_query ) # Determine which temporal filter to apply filter_mode = self . _get_temporal_filter_mode ( original_query ) if filter_mode == \"current\" and self . _temporal_domain_config : field_name = self . _temporal_domain_config . temporal_field_name period_value = self . _temporal_domain_config . current_period_value self . _logger . info ( f \"[DynamicTemporal] Applying { field_name } = { period_value } filter for current/recent query\" ) # Create filter for current period temporal_filter = MetadataFilters ( filters = [ MetadataFilter ( key = field_name , value = period_value , operator = FilterOperator . EQ , ) ] ) retriever = self . _index . as_retriever ( similarity_top_k = self . _similarity_top_k , filters = temporal_filter , ) elif filter_mode == \"historical\" and self . _temporal_domain_config : field_name = self . _temporal_domain_config . temporal_field_name period_value = self . _temporal_domain_config . historical_period_value self . _logger . info ( f \"[DynamicTemporal] Applying { field_name } = { period_value } filter for historical/previous query\" ) # Create filter for historical period temporal_filter = MetadataFilters ( filters = [ MetadataFilter ( key = field_name , value = period_value , operator = FilterOperator . EQ , ) ] ) retriever = self . _index . as_retriever ( similarity_top_k = self . _similarity_top_k , filters = temporal_filter , ) else : # filter_mode == \"none\" self . _logger . info ( \"[DynamicTemporal] No temporal filtering - searching all documents\" ) # No temporal filter - search all documents retriever = self . _index . as_retriever ( similarity_top_k = self . _similarity_top_k , ) # Retrieve using the configured retriever return retriever . retrieve ( query_bundle . query_str ) async def _aretrieve ( self , query_bundle : QueryBundle ) -> List [ NodeWithScore ]: \"\"\"Async retrieve (delegates to sync).\"\"\" return self . _retrieve ( query_bundle ) __init__ ( index , similarity_top_k , query_rewriter = None , temporal_domain_config = None ) Initialize dynamic temporal retriever. Parameters: index ( VectorStoreIndex ) \u2013 Vector store index similarity_top_k ( int ) \u2013 Number of top results to retrieve query_rewriter ( Optional [ QueryRewriter ] , default: None ) \u2013 Optional query rewriter for semantic improvements temporal_domain_config ( Optional [ TemporalDomainConfiguration ] , default: None ) \u2013 Optional temporal domain configuration. If None, runs in generic mode (no temporal filtering). Source code in src/augmentation/components/retrievers/dynamic_temporal/retriever.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , index : VectorStoreIndex , similarity_top_k : int , query_rewriter : Optional [ QueryRewriter ] = None , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , ): \"\"\"Initialize dynamic temporal retriever. Args: index: Vector store index similarity_top_k: Number of top results to retrieve query_rewriter: Optional query rewriter for semantic improvements temporal_domain_config: Optional temporal domain configuration. If None, runs in generic mode (no temporal filtering). \"\"\" super () . __init__ () self . _index = index self . _similarity_top_k = similarity_top_k self . _temporal_domain_config = temporal_domain_config self . _query_rewriter = query_rewriter or QueryRewriter ( temporal_domain_config ) self . _logger = LoggerConfiguration . get_logger ( __name__ ) # Build keyword lists from config (empty if no config provided) if temporal_domain_config : self . _current_keywords = ( temporal_domain_config . get_all_current_keywords () ) self . _historical_keywords = ( temporal_domain_config . get_all_historical_keywords () ) self . _logger . info ( f \"[DynamicTemporal] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _current_keywords = [] self . _historical_keywords = [] self . _logger . info ( \"[DynamicTemporal] Running in generic mode (no temporal filtering)\" ) DynamicTemporalRetrieverFactory Bases: Factory Factory for creating dynamic temporal retriever. Source code in src/augmentation/components/retrievers/dynamic_temporal/retriever.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 class DynamicTemporalRetrieverFactory ( Factory ): \"\"\"Factory for creating dynamic temporal retriever.\"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> BaseRetriever : \"\"\"Create dynamic temporal retriever. Args: configuration: AugmentationConfiguration Returns: DynamicTemporalRetriever instance \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) # Get temporal domain config (may be None for generic mode) temporal_domain_config = configuration . augmentation . temporal_domain query_rewriter = QueryRewriter ( temporal_domain_config = temporal_domain_config ) return DynamicTemporalRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , query_rewriter = query_rewriter , temporal_domain_config = temporal_domain_config , )","title":"Retriever"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/retriever/#retriever","text":"This module contains functionality related to the the retriever module for augmentation.components.retrievers.dynamic_temporal .","title":"Retriever"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/retriever/#retriever_1","text":"Dynamic temporal retriever that applies filters based on query content.","title":"Retriever"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/retriever/#src.augmentation.components.retrievers.dynamic_temporal.retriever.DynamicTemporalRetriever","text":"Bases: BaseRetriever Retriever that dynamically applies temporal filters based on query. Detects temporal keywords in queries and applies appropriate metadata filters based on configured temporal domain. If no temporal_domain_config is provided, runs in generic mode without temporal filtering. Source code in src/augmentation/components/retrievers/dynamic_temporal/retriever.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class DynamicTemporalRetriever ( BaseRetriever ): \"\"\"Retriever that dynamically applies temporal filters based on query. Detects temporal keywords in queries and applies appropriate metadata filters based on configured temporal domain. If no temporal_domain_config is provided, runs in generic mode without temporal filtering. \"\"\" def __init__ ( self , index : VectorStoreIndex , similarity_top_k : int , query_rewriter : Optional [ QueryRewriter ] = None , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , ): \"\"\"Initialize dynamic temporal retriever. Args: index: Vector store index similarity_top_k: Number of top results to retrieve query_rewriter: Optional query rewriter for semantic improvements temporal_domain_config: Optional temporal domain configuration. If None, runs in generic mode (no temporal filtering). \"\"\" super () . __init__ () self . _index = index self . _similarity_top_k = similarity_top_k self . _temporal_domain_config = temporal_domain_config self . _query_rewriter = query_rewriter or QueryRewriter ( temporal_domain_config ) self . _logger = LoggerConfiguration . get_logger ( __name__ ) # Build keyword lists from config (empty if no config provided) if temporal_domain_config : self . _current_keywords = ( temporal_domain_config . get_all_current_keywords () ) self . _historical_keywords = ( temporal_domain_config . get_all_historical_keywords () ) self . _logger . info ( f \"[DynamicTemporal] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _current_keywords = [] self . _historical_keywords = [] self . _logger . info ( \"[DynamicTemporal] Running in generic mode (no temporal filtering)\" ) def _get_temporal_filter_mode ( self , query : str ) -> str : \"\"\"Determine which temporal filter to apply based on query keywords. Uses configured temporal keywords to detect temporal intent. If no temporal domain config is provided, always returns \"none\". Args: query: User query string Returns: One of: \"historical\", \"current\", \"none\" \"\"\" # If no temporal domain config, no filtering if not self . _temporal_domain_config : return \"none\" query_lower = query . lower () # First check for historical keywords - higher priority for keyword in self . _historical_keywords : # Use word boundary matching for keyword detection if re . search ( rf \"\\b { re . escape ( keyword ) } \\b\" , query_lower , re . IGNORECASE ): self . _logger . info ( f \"[DynamicTemporal] Detected historical keyword: ' { keyword } ' \" f \"- will filter to period { self . _temporal_domain_config . historical_period_value } \" ) return \"historical\" # Then check for current/temporal keywords for keyword in self . _current_keywords : if re . search ( rf \"\\b { re . escape ( keyword ) } \\b\" , query_lower , re . IGNORECASE ): self . _logger . info ( f \"[DynamicTemporal] Detected current keyword: ' { keyword } ' \" f \"- will filter to period { self . _temporal_domain_config . current_period_value } \" ) return \"current\" return \"none\" def _retrieve ( self , query_bundle : QueryBundle ) -> List [ NodeWithScore ]: \"\"\"Retrieve with dynamic temporal filtering. Applies metadata filtering based on configured temporal domain and query keywords. If no temporal_domain_config is provided, retrieves without filtering. Args: query_bundle: Query bundle Returns: List of nodes with scores \"\"\" original_query = query_bundle . query_str # Apply query rewriting rewritten_query = self . _query_rewriter . rewrite ( original_query ) if rewritten_query != original_query : query_bundle = QueryBundle ( query_str = rewritten_query ) # Determine which temporal filter to apply filter_mode = self . _get_temporal_filter_mode ( original_query ) if filter_mode == \"current\" and self . _temporal_domain_config : field_name = self . _temporal_domain_config . temporal_field_name period_value = self . _temporal_domain_config . current_period_value self . _logger . info ( f \"[DynamicTemporal] Applying { field_name } = { period_value } filter for current/recent query\" ) # Create filter for current period temporal_filter = MetadataFilters ( filters = [ MetadataFilter ( key = field_name , value = period_value , operator = FilterOperator . EQ , ) ] ) retriever = self . _index . as_retriever ( similarity_top_k = self . _similarity_top_k , filters = temporal_filter , ) elif filter_mode == \"historical\" and self . _temporal_domain_config : field_name = self . _temporal_domain_config . temporal_field_name period_value = self . _temporal_domain_config . historical_period_value self . _logger . info ( f \"[DynamicTemporal] Applying { field_name } = { period_value } filter for historical/previous query\" ) # Create filter for historical period temporal_filter = MetadataFilters ( filters = [ MetadataFilter ( key = field_name , value = period_value , operator = FilterOperator . EQ , ) ] ) retriever = self . _index . as_retriever ( similarity_top_k = self . _similarity_top_k , filters = temporal_filter , ) else : # filter_mode == \"none\" self . _logger . info ( \"[DynamicTemporal] No temporal filtering - searching all documents\" ) # No temporal filter - search all documents retriever = self . _index . as_retriever ( similarity_top_k = self . _similarity_top_k , ) # Retrieve using the configured retriever return retriever . retrieve ( query_bundle . query_str ) async def _aretrieve ( self , query_bundle : QueryBundle ) -> List [ NodeWithScore ]: \"\"\"Async retrieve (delegates to sync).\"\"\" return self . _retrieve ( query_bundle )","title":"DynamicTemporalRetriever"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/retriever/#src.augmentation.components.retrievers.dynamic_temporal.retriever.DynamicTemporalRetriever.__init__","text":"Initialize dynamic temporal retriever. Parameters: index ( VectorStoreIndex ) \u2013 Vector store index similarity_top_k ( int ) \u2013 Number of top results to retrieve query_rewriter ( Optional [ QueryRewriter ] , default: None ) \u2013 Optional query rewriter for semantic improvements temporal_domain_config ( Optional [ TemporalDomainConfiguration ] , default: None ) \u2013 Optional temporal domain configuration. If None, runs in generic mode (no temporal filtering). Source code in src/augmentation/components/retrievers/dynamic_temporal/retriever.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , index : VectorStoreIndex , similarity_top_k : int , query_rewriter : Optional [ QueryRewriter ] = None , temporal_domain_config : Optional [ TemporalDomainConfiguration ] = None , ): \"\"\"Initialize dynamic temporal retriever. Args: index: Vector store index similarity_top_k: Number of top results to retrieve query_rewriter: Optional query rewriter for semantic improvements temporal_domain_config: Optional temporal domain configuration. If None, runs in generic mode (no temporal filtering). \"\"\" super () . __init__ () self . _index = index self . _similarity_top_k = similarity_top_k self . _temporal_domain_config = temporal_domain_config self . _query_rewriter = query_rewriter or QueryRewriter ( temporal_domain_config ) self . _logger = LoggerConfiguration . get_logger ( __name__ ) # Build keyword lists from config (empty if no config provided) if temporal_domain_config : self . _current_keywords = ( temporal_domain_config . get_all_current_keywords () ) self . _historical_keywords = ( temporal_domain_config . get_all_historical_keywords () ) self . _logger . info ( f \"[DynamicTemporal] Initialized with temporal domain: { temporal_domain_config . name } \" ) else : self . _current_keywords = [] self . _historical_keywords = [] self . _logger . info ( \"[DynamicTemporal] Running in generic mode (no temporal filtering)\" )","title":"__init__"},{"location":"src/augmentation/components/retrievers/dynamic_temporal/retriever/#src.augmentation.components.retrievers.dynamic_temporal.retriever.DynamicTemporalRetrieverFactory","text":"Bases: Factory Factory for creating dynamic temporal retriever. Source code in src/augmentation/components/retrievers/dynamic_temporal/retriever.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 class DynamicTemporalRetrieverFactory ( Factory ): \"\"\"Factory for creating dynamic temporal retriever.\"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> BaseRetriever : \"\"\"Create dynamic temporal retriever. Args: configuration: AugmentationConfiguration Returns: DynamicTemporalRetriever instance \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) # Get temporal domain config (may be None for generic mode) temporal_domain_config = configuration . augmentation . temporal_domain query_rewriter = QueryRewriter ( temporal_domain_config = temporal_domain_config ) return DynamicTemporalRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , query_rewriter = query_rewriter , temporal_domain_config = temporal_domain_config , )","title":"DynamicTemporalRetrieverFactory"},{"location":"src/augmentation/langfuse/client/","text":"Client This module contains functionality related to the the client module for augmentation.langfuse . Client LangfuseClientFactory Bases: SingletonFactory Factory class for creating and managing Langfuse client instances. This class implements the Singleton pattern through inheriting from SingletonFactory, ensuring only one Langfuse client instance exists throughout the application. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Langfuse client instances. In this case, it is LangfuseConfiguration. Source code in src/augmentation/langfuse/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class LangfuseClientFactory ( SingletonFactory ): \"\"\" Factory class for creating and managing Langfuse client instances. This class implements the Singleton pattern through inheriting from SingletonFactory, ensuring only one Langfuse client instance exists throughout the application. Attributes: _configuration_class (Type): The configuration class used for creating Langfuse client instances. In this case, it is LangfuseConfiguration. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> Langfuse : \"\"\" Creates a new Langfuse client instance. Args: configuration (LangfuseConfiguration): Configuration object containing Langfuse API credentials and URL settings. Returns: Langfuse: A configured Langfuse client instance ready for use with the provided credentials and host. \"\"\" return Langfuse ( secret_key = configuration . secrets . secret_key . get_secret_value (), public_key = configuration . secrets . public_key . get_secret_value (), host = configuration . url , )","title":"Client"},{"location":"src/augmentation/langfuse/client/#client","text":"This module contains functionality related to the the client module for augmentation.langfuse .","title":"Client"},{"location":"src/augmentation/langfuse/client/#client_1","text":"","title":"Client"},{"location":"src/augmentation/langfuse/client/#src.augmentation.langfuse.client.LangfuseClientFactory","text":"Bases: SingletonFactory Factory class for creating and managing Langfuse client instances. This class implements the Singleton pattern through inheriting from SingletonFactory, ensuring only one Langfuse client instance exists throughout the application. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Langfuse client instances. In this case, it is LangfuseConfiguration. Source code in src/augmentation/langfuse/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class LangfuseClientFactory ( SingletonFactory ): \"\"\" Factory class for creating and managing Langfuse client instances. This class implements the Singleton pattern through inheriting from SingletonFactory, ensuring only one Langfuse client instance exists throughout the application. Attributes: _configuration_class (Type): The configuration class used for creating Langfuse client instances. In this case, it is LangfuseConfiguration. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> Langfuse : \"\"\" Creates a new Langfuse client instance. Args: configuration (LangfuseConfiguration): Configuration object containing Langfuse API credentials and URL settings. Returns: Langfuse: A configured Langfuse client instance ready for use with the provided credentials and host. \"\"\" return Langfuse ( secret_key = configuration . secrets . secret_key . get_secret_value (), public_key = configuration . secrets . public_key . get_secret_value (), host = configuration . url , )","title":"LangfuseClientFactory"},{"location":"src/augmentation/langfuse/dataset_service/","text":"Dataset_service This module contains functionality related to the the dataset_service module for augmentation.langfuse . Dataset_service LangfuseDatasetService Service for managing Langfuse datasets. This service provides methods to create, manage, and retrieve datasets within the Langfuse platform. It handles the communication with the Langfuse API for all dataset-related operations. Source code in src/augmentation/langfuse/dataset_service.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class LangfuseDatasetService : \"\"\"Service for managing Langfuse datasets. This service provides methods to create, manage, and retrieve datasets within the Langfuse platform. It handles the communication with the Langfuse API for all dataset-related operations. \"\"\" def __init__ ( self , langfuse_client : Langfuse , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Langfuse dataset service. Args: langfuse_client: Authenticated client for Langfuse API interactions. logger: Logger instance for recording operations. Defaults to module logger. \"\"\" self . langfuse_client = langfuse_client self . logger = logger def create_if_does_not_exist ( self , dataset : LangfuseDatasetConfiguration ) -> None : \"\"\"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Args: dataset: Configuration object containing dataset name, description, and metadata for creation. Note: The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. \"\"\" try : self . langfuse_client . get_dataset ( dataset . name ) self . logger . info ( f \"Dataset { dataset . name } exists.\" ) except NotFoundError : self . logger . info ( f \"Dataset { dataset . name } does not exist. Creating...\" ) self . langfuse_client . create_dataset ( name = dataset . name , description = dataset . description , metadata = dataset . metadata , ) def get_dataset ( self , dataset_name : str ) -> DatasetClient : \"\"\"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Args: dataset_name: The unique name identifier of the dataset to retrieve. Returns: DatasetClient: A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError: If a dataset with the specified name doesn't exist. \"\"\" return self . langfuse_client . get_dataset ( dataset_name ) __init__ ( langfuse_client , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the Langfuse dataset service. Parameters: langfuse_client ( Langfuse ) \u2013 Authenticated client for Langfuse API interactions. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operations. Defaults to module logger. Source code in src/augmentation/langfuse/dataset_service.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , langfuse_client : Langfuse , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Langfuse dataset service. Args: langfuse_client: Authenticated client for Langfuse API interactions. logger: Logger instance for recording operations. Defaults to module logger. \"\"\" self . langfuse_client = langfuse_client self . logger = logger create_if_does_not_exist ( dataset ) Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Parameters: dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration object containing dataset name, description, and metadata for creation. Note The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. Source code in src/augmentation/langfuse/dataset_service.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_if_does_not_exist ( self , dataset : LangfuseDatasetConfiguration ) -> None : \"\"\"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Args: dataset: Configuration object containing dataset name, description, and metadata for creation. Note: The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. \"\"\" try : self . langfuse_client . get_dataset ( dataset . name ) self . logger . info ( f \"Dataset { dataset . name } exists.\" ) except NotFoundError : self . logger . info ( f \"Dataset { dataset . name } does not exist. Creating...\" ) self . langfuse_client . create_dataset ( name = dataset . name , description = dataset . description , metadata = dataset . metadata , ) get_dataset ( dataset_name ) Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Parameters: dataset_name ( str ) \u2013 The unique name identifier of the dataset to retrieve. Returns: DatasetClient ( DatasetClient ) \u2013 A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError \u2013 If a dataset with the specified name doesn't exist. Source code in src/augmentation/langfuse/dataset_service.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_dataset ( self , dataset_name : str ) -> DatasetClient : \"\"\"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Args: dataset_name: The unique name identifier of the dataset to retrieve. Returns: DatasetClient: A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError: If a dataset with the specified name doesn't exist. \"\"\" return self . langfuse_client . get_dataset ( dataset_name ) LangfuseDatasetServiceFactory Bases: Factory Factory for creating LangfuseDatasetService instances. Creates and configures LangfuseDatasetService instances with the appropriate client based on provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class type used by this factory. Source code in src/augmentation/langfuse/dataset_service.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class LangfuseDatasetServiceFactory ( Factory ): \"\"\"Factory for creating LangfuseDatasetService instances. Creates and configures LangfuseDatasetService instances with the appropriate client based on provided configuration. Attributes: _configuration_class: The configuration class type used by this factory. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> LangfuseDatasetService : \"\"\"Create a configured LangfuseDatasetService instance. Args: configuration: The Langfuse configuration containing API credentials and other settings. Returns: A fully initialized LangfuseDatasetService instance with an authenticated client. \"\"\" client = LangfuseClientFactory . create ( configuration ) return LangfuseDatasetService ( client )","title":"Dataset_service"},{"location":"src/augmentation/langfuse/dataset_service/#dataset_service","text":"This module contains functionality related to the the dataset_service module for augmentation.langfuse .","title":"Dataset_service"},{"location":"src/augmentation/langfuse/dataset_service/#dataset_service_1","text":"","title":"Dataset_service"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetService","text":"Service for managing Langfuse datasets. This service provides methods to create, manage, and retrieve datasets within the Langfuse platform. It handles the communication with the Langfuse API for all dataset-related operations. Source code in src/augmentation/langfuse/dataset_service.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class LangfuseDatasetService : \"\"\"Service for managing Langfuse datasets. This service provides methods to create, manage, and retrieve datasets within the Langfuse platform. It handles the communication with the Langfuse API for all dataset-related operations. \"\"\" def __init__ ( self , langfuse_client : Langfuse , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Langfuse dataset service. Args: langfuse_client: Authenticated client for Langfuse API interactions. logger: Logger instance for recording operations. Defaults to module logger. \"\"\" self . langfuse_client = langfuse_client self . logger = logger def create_if_does_not_exist ( self , dataset : LangfuseDatasetConfiguration ) -> None : \"\"\"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Args: dataset: Configuration object containing dataset name, description, and metadata for creation. Note: The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. \"\"\" try : self . langfuse_client . get_dataset ( dataset . name ) self . logger . info ( f \"Dataset { dataset . name } exists.\" ) except NotFoundError : self . logger . info ( f \"Dataset { dataset . name } does not exist. Creating...\" ) self . langfuse_client . create_dataset ( name = dataset . name , description = dataset . description , metadata = dataset . metadata , ) def get_dataset ( self , dataset_name : str ) -> DatasetClient : \"\"\"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Args: dataset_name: The unique name identifier of the dataset to retrieve. Returns: DatasetClient: A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError: If a dataset with the specified name doesn't exist. \"\"\" return self . langfuse_client . get_dataset ( dataset_name )","title":"LangfuseDatasetService"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetService.__init__","text":"Initialize the Langfuse dataset service. Parameters: langfuse_client ( Langfuse ) \u2013 Authenticated client for Langfuse API interactions. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operations. Defaults to module logger. Source code in src/augmentation/langfuse/dataset_service.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , langfuse_client : Langfuse , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Langfuse dataset service. Args: langfuse_client: Authenticated client for Langfuse API interactions. logger: Logger instance for recording operations. Defaults to module logger. \"\"\" self . langfuse_client = langfuse_client self . logger = logger","title":"__init__"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetService.create_if_does_not_exist","text":"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Parameters: dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration object containing dataset name, description, and metadata for creation. Note The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. Source code in src/augmentation/langfuse/dataset_service.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_if_does_not_exist ( self , dataset : LangfuseDatasetConfiguration ) -> None : \"\"\"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Args: dataset: Configuration object containing dataset name, description, and metadata for creation. Note: The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. \"\"\" try : self . langfuse_client . get_dataset ( dataset . name ) self . logger . info ( f \"Dataset { dataset . name } exists.\" ) except NotFoundError : self . logger . info ( f \"Dataset { dataset . name } does not exist. Creating...\" ) self . langfuse_client . create_dataset ( name = dataset . name , description = dataset . description , metadata = dataset . metadata , )","title":"create_if_does_not_exist"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetService.get_dataset","text":"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Parameters: dataset_name ( str ) \u2013 The unique name identifier of the dataset to retrieve. Returns: DatasetClient ( DatasetClient ) \u2013 A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError \u2013 If a dataset with the specified name doesn't exist. Source code in src/augmentation/langfuse/dataset_service.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_dataset ( self , dataset_name : str ) -> DatasetClient : \"\"\"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Args: dataset_name: The unique name identifier of the dataset to retrieve. Returns: DatasetClient: A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError: If a dataset with the specified name doesn't exist. \"\"\" return self . langfuse_client . get_dataset ( dataset_name )","title":"get_dataset"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetServiceFactory","text":"Bases: Factory Factory for creating LangfuseDatasetService instances. Creates and configures LangfuseDatasetService instances with the appropriate client based on provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class type used by this factory. Source code in src/augmentation/langfuse/dataset_service.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class LangfuseDatasetServiceFactory ( Factory ): \"\"\"Factory for creating LangfuseDatasetService instances. Creates and configures LangfuseDatasetService instances with the appropriate client based on provided configuration. Attributes: _configuration_class: The configuration class type used by this factory. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> LangfuseDatasetService : \"\"\"Create a configured LangfuseDatasetService instance. Args: configuration: The Langfuse configuration containing API credentials and other settings. Returns: A fully initialized LangfuseDatasetService instance with an authenticated client. \"\"\" client = LangfuseClientFactory . create ( configuration ) return LangfuseDatasetService ( client )","title":"LangfuseDatasetServiceFactory"},{"location":"src/augmentation/langfuse/prompt_service/","text":"Prompt_service This module contains functionality related to the the prompt_service module for augmentation.langfuse . Prompt_service LangfusePromptService Source code in src/augmentation/langfuse/prompt_service.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class LangfusePromptService : def __init__ ( self , client : Langfuse , logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initializes the LangfusePromptService with a Langfuse client and a logger. Args: client (Langfuse): The Langfuse client instance. logger (Logger): The logger instance. \"\"\" self . client = client self . logger = logger def create_prompt_if_not_exists ( self , prompt_name : str , prompt_template : str , ) -> None : \"\"\" Creates a new prompt in Langfuse if it does not already exist. Args: prompt_name (str): The name of the prompt. prompt_template (str): The template of the prompt. \"\"\" if self . prompt_exists ( prompt_name ): return self . logger . info ( f \"Creating { prompt_name } prompt in Langfuse\" ) self . client . create_prompt ( name = prompt_name , prompt = prompt_template , labels = [ \"production\" ] ) def prompt_exists ( self , prompt_name : str , ) -> bool : \"\"\" Checks if a prompt exists in Langfuse. Args: prompt_name (str): The name of the prompt. Returns: bool: True if the prompt exists, False otherwise. \"\"\" try : self . client . get_prompt ( prompt_name ) return True except Exception : return False def get_prompt_template ( self , prompt_name : str ) -> str : \"\"\" Retrieves the prompt template from Langfuse. Args: prompt_name (str): The name of the prompt. Returns: str: The prompt template. \"\"\" prompt = self . client . get_prompt ( prompt_name ) return prompt . prompt __init__ ( client , logger = LoggerConfiguration . get_logger ( __name__ )) Initializes the LangfusePromptService with a Langfuse client and a logger. Parameters: client ( Langfuse ) \u2013 The Langfuse client instance. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 The logger instance. Source code in src/augmentation/langfuse/prompt_service.py 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , client : Langfuse , logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initializes the LangfusePromptService with a Langfuse client and a logger. Args: client (Langfuse): The Langfuse client instance. logger (Logger): The logger instance. \"\"\" self . client = client self . logger = logger create_prompt_if_not_exists ( prompt_name , prompt_template ) Creates a new prompt in Langfuse if it does not already exist. Parameters: prompt_name ( str ) \u2013 The name of the prompt. prompt_template ( str ) \u2013 The template of the prompt. Source code in src/augmentation/langfuse/prompt_service.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def create_prompt_if_not_exists ( self , prompt_name : str , prompt_template : str , ) -> None : \"\"\" Creates a new prompt in Langfuse if it does not already exist. Args: prompt_name (str): The name of the prompt. prompt_template (str): The template of the prompt. \"\"\" if self . prompt_exists ( prompt_name ): return self . logger . info ( f \"Creating { prompt_name } prompt in Langfuse\" ) self . client . create_prompt ( name = prompt_name , prompt = prompt_template , labels = [ \"production\" ] ) get_prompt_template ( prompt_name ) Retrieves the prompt template from Langfuse. Parameters: prompt_name ( str ) \u2013 The name of the prompt. Returns: str ( str ) \u2013 The prompt template. Source code in src/augmentation/langfuse/prompt_service.py 67 68 69 70 71 72 73 74 75 76 77 78 def get_prompt_template ( self , prompt_name : str ) -> str : \"\"\" Retrieves the prompt template from Langfuse. Args: prompt_name (str): The name of the prompt. Returns: str: The prompt template. \"\"\" prompt = self . client . get_prompt ( prompt_name ) return prompt . prompt prompt_exists ( prompt_name ) Checks if a prompt exists in Langfuse. Parameters: prompt_name ( str ) \u2013 The name of the prompt. Returns: bool ( bool ) \u2013 True if the prompt exists, False otherwise. Source code in src/augmentation/langfuse/prompt_service.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def prompt_exists ( self , prompt_name : str , ) -> bool : \"\"\" Checks if a prompt exists in Langfuse. Args: prompt_name (str): The name of the prompt. Returns: bool: True if the prompt exists, False otherwise. \"\"\" try : self . client . get_prompt ( prompt_name ) return True except Exception : return False LangfusePromptServiceFactory Bases: Factory Factory class for creating and managing Langfuse prompt service instances. This class implements the Singleton pattern through inheriting from Factory, ensuring only one Langfuse prompt service instance exists throughout the application. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Langfuse prompt service instances. In this case, it is LangfuseConfiguration. Source code in src/augmentation/langfuse/prompt_service.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class LangfusePromptServiceFactory ( Factory ): \"\"\" Factory class for creating and managing Langfuse prompt service instances. This class implements the Singleton pattern through inheriting from Factory, ensuring only one Langfuse prompt service instance exists throughout the application. Attributes: _configuration_class (Type): The configuration class used for creating Langfuse prompt service instances. In this case, it is LangfuseConfiguration. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> Langfuse : \"\"\" Creates a new Langfuse prompt service instance. Args: configuration (LangfuseConfiguration): Configuration object containing Langfuse API credentials and URL settings. Returns: LangfusePromptService: A configured Langfuse prompt service instance ready for use with the provided credentials and host. \"\"\" client = LangfuseClientFactory . create ( configuration ) return LangfusePromptService ( client = client )","title":"Prompt_service"},{"location":"src/augmentation/langfuse/prompt_service/#prompt_service","text":"This module contains functionality related to the the prompt_service module for augmentation.langfuse .","title":"Prompt_service"},{"location":"src/augmentation/langfuse/prompt_service/#prompt_service_1","text":"","title":"Prompt_service"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService","text":"Source code in src/augmentation/langfuse/prompt_service.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class LangfusePromptService : def __init__ ( self , client : Langfuse , logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initializes the LangfusePromptService with a Langfuse client and a logger. Args: client (Langfuse): The Langfuse client instance. logger (Logger): The logger instance. \"\"\" self . client = client self . logger = logger def create_prompt_if_not_exists ( self , prompt_name : str , prompt_template : str , ) -> None : \"\"\" Creates a new prompt in Langfuse if it does not already exist. Args: prompt_name (str): The name of the prompt. prompt_template (str): The template of the prompt. \"\"\" if self . prompt_exists ( prompt_name ): return self . logger . info ( f \"Creating { prompt_name } prompt in Langfuse\" ) self . client . create_prompt ( name = prompt_name , prompt = prompt_template , labels = [ \"production\" ] ) def prompt_exists ( self , prompt_name : str , ) -> bool : \"\"\" Checks if a prompt exists in Langfuse. Args: prompt_name (str): The name of the prompt. Returns: bool: True if the prompt exists, False otherwise. \"\"\" try : self . client . get_prompt ( prompt_name ) return True except Exception : return False def get_prompt_template ( self , prompt_name : str ) -> str : \"\"\" Retrieves the prompt template from Langfuse. Args: prompt_name (str): The name of the prompt. Returns: str: The prompt template. \"\"\" prompt = self . client . get_prompt ( prompt_name ) return prompt . prompt","title":"LangfusePromptService"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService.__init__","text":"Initializes the LangfusePromptService with a Langfuse client and a logger. Parameters: client ( Langfuse ) \u2013 The Langfuse client instance. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 The logger instance. Source code in src/augmentation/langfuse/prompt_service.py 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , client : Langfuse , logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initializes the LangfusePromptService with a Langfuse client and a logger. Args: client (Langfuse): The Langfuse client instance. logger (Logger): The logger instance. \"\"\" self . client = client self . logger = logger","title":"__init__"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService.create_prompt_if_not_exists","text":"Creates a new prompt in Langfuse if it does not already exist. Parameters: prompt_name ( str ) \u2013 The name of the prompt. prompt_template ( str ) \u2013 The template of the prompt. Source code in src/augmentation/langfuse/prompt_service.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def create_prompt_if_not_exists ( self , prompt_name : str , prompt_template : str , ) -> None : \"\"\" Creates a new prompt in Langfuse if it does not already exist. Args: prompt_name (str): The name of the prompt. prompt_template (str): The template of the prompt. \"\"\" if self . prompt_exists ( prompt_name ): return self . logger . info ( f \"Creating { prompt_name } prompt in Langfuse\" ) self . client . create_prompt ( name = prompt_name , prompt = prompt_template , labels = [ \"production\" ] )","title":"create_prompt_if_not_exists"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService.get_prompt_template","text":"Retrieves the prompt template from Langfuse. Parameters: prompt_name ( str ) \u2013 The name of the prompt. Returns: str ( str ) \u2013 The prompt template. Source code in src/augmentation/langfuse/prompt_service.py 67 68 69 70 71 72 73 74 75 76 77 78 def get_prompt_template ( self , prompt_name : str ) -> str : \"\"\" Retrieves the prompt template from Langfuse. Args: prompt_name (str): The name of the prompt. Returns: str: The prompt template. \"\"\" prompt = self . client . get_prompt ( prompt_name ) return prompt . prompt","title":"get_prompt_template"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService.prompt_exists","text":"Checks if a prompt exists in Langfuse. Parameters: prompt_name ( str ) \u2013 The name of the prompt. Returns: bool ( bool ) \u2013 True if the prompt exists, False otherwise. Source code in src/augmentation/langfuse/prompt_service.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def prompt_exists ( self , prompt_name : str , ) -> bool : \"\"\" Checks if a prompt exists in Langfuse. Args: prompt_name (str): The name of the prompt. Returns: bool: True if the prompt exists, False otherwise. \"\"\" try : self . client . get_prompt ( prompt_name ) return True except Exception : return False","title":"prompt_exists"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptServiceFactory","text":"Bases: Factory Factory class for creating and managing Langfuse prompt service instances. This class implements the Singleton pattern through inheriting from Factory, ensuring only one Langfuse prompt service instance exists throughout the application. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Langfuse prompt service instances. In this case, it is LangfuseConfiguration. Source code in src/augmentation/langfuse/prompt_service.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class LangfusePromptServiceFactory ( Factory ): \"\"\" Factory class for creating and managing Langfuse prompt service instances. This class implements the Singleton pattern through inheriting from Factory, ensuring only one Langfuse prompt service instance exists throughout the application. Attributes: _configuration_class (Type): The configuration class used for creating Langfuse prompt service instances. In this case, it is LangfuseConfiguration. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> Langfuse : \"\"\" Creates a new Langfuse prompt service instance. Args: configuration (LangfuseConfiguration): Configuration object containing Langfuse API credentials and URL settings. Returns: LangfusePromptService: A configured Langfuse prompt service instance ready for use with the provided credentials and host. \"\"\" client = LangfuseClientFactory . create ( configuration ) return LangfusePromptService ( client = client )","title":"LangfusePromptServiceFactory"},{"location":"src/core/base_configuration/","text":"Base_configuration This module contains functionality related to the the base_configuration module for core . Base_configuration BaseConfiguration Bases: BaseModel , ABC Base abstract class for all configuration models. Provides common functionality like hashing for configuration objects. Extend this class to create specific configuration types. Source code in src/core/base_configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BaseConfiguration ( BaseModel , ABC ): \"\"\" Base abstract class for all configuration models. Provides common functionality like hashing for configuration objects. Extend this class to create specific configuration types. \"\"\" def __hash__ ( self ) -> int : \"\"\"Not the most efficient way of hashing, but it works for now.\"\"\" json = self . model_dump_json () return hash ( json ) @classmethod def _validate ( cls , value : Any , info : ValidationInfo , registry : Type [ ConfigurationRegistry ], ) -> Any : \"\"\" Validates the value against the type defined in the registry. Args: value (Any): The value of the field. info (ValidationInfo): The information about the field. registry (Type[ConfigurationRegistry]): The registry to use for validation. Returns: Any: The validated value. \"\"\" type_adapter = TypeAdapter ( registry . get_union_type ()) return type_adapter . validate_python ( value , context = info . context , ) __hash__ () Not the most efficient way of hashing, but it works for now. Source code in src/core/base_configuration.py 29 30 31 32 def __hash__ ( self ) -> int : \"\"\"Not the most efficient way of hashing, but it works for now.\"\"\" json = self . model_dump_json () return hash ( json ) BaseConfigurationWithSecrets Bases: BaseConfiguration Abstract model for configuration's secrets handling. Provides functionality to automatically load and validate secrets from environment variables or files. Extending class has to implement secrets field with corresponding type. Source code in src/core/base_configuration.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class BaseConfigurationWithSecrets ( BaseConfiguration ): \"\"\" Abstract model for configuration's secrets handling. Provides functionality to automatically load and validate secrets from environment variables or files. Extending class has to implement `secrets` field with corresponding type. \"\"\" secrets : BaseSecrets = Field ( None , description = \"`BaseSecrets` is meant for the the configuration that does not require secrets.\" \"In other case `BaseSecrets` should be replaced with the corresponding secrets class.\" , ) def model_post_init ( self , context : Any ) -> None : \"\"\" Function is invoked after the model is initialized. It is used to initialize secrets. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" self . secrets = self . _get_secrets ( secrets_file = context [ \"secrets_file\" ]) def _get_secrets ( self , secrets_file : str ) -> BaseSettings : \"\"\" Function to initialize secrets from the specified file. Args: secrets_file (str): The path to the secrets file. Returns: BaseSettings: The initialized secrets object. Raises: ValueError: If secrets are not found or cannot be loaded. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets = secrets_class ( _env_file = secrets_file ) if secrets is None : raise ValueError ( f \"Secrets for { self . name } not found.\" ) return secrets model_post_init ( context ) Function is invoked after the model is initialized. It is used to initialize secrets. Parameters: context ( Any ) \u2013 The context passed to the pydantic model, must contain 'secrets_file' key. Source code in src/core/base_configuration.py 88 89 90 91 92 93 94 95 def model_post_init ( self , context : Any ) -> None : \"\"\" Function is invoked after the model is initialized. It is used to initialize secrets. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" self . secrets = self . _get_secrets ( secrets_file = context [ \"secrets_file\" ]) BaseSecrets Bases: BaseConfiguration , BaseSettings Base class for secrets management. Extends both BaseConfiguration and BaseSettings to handle sensitive configuration data like API keys, passwords, etc. Uses Pydantic's BaseSettings for environment variable and file-based loading. Source code in src/core/base_configuration.py 59 60 61 62 63 64 65 66 67 68 69 70 class BaseSecrets ( BaseConfiguration , BaseSettings ): \"\"\" Base class for secrets management. Extends both BaseConfiguration and BaseSettings to handle sensitive configuration data like API keys, passwords, etc. Uses Pydantic's BaseSettings for environment variable and file-based loading. \"\"\" model_config = ConfigDict ( extra = \"ignore\" , ) BasicConfiguration Bases: BaseConfiguration Standard configuration class with metadata support. Includes a metadata field for storing application-specific settings. Source code in src/core/base_configuration.py 248 249 250 251 252 253 254 255 256 257 class BasicConfiguration ( BaseConfiguration ): \"\"\" Standard configuration class with metadata support. Includes a metadata field for storing application-specific settings. \"\"\" metadata : Optional [ MetadataConfiguration ] = Field ( None , description = \"Metadata of the run.\" ) EnvironmentName Bases: str , Enum Enumeration of available environments. Defines the possible runtime environments for the application. Source code in src/core/base_configuration.py 137 138 139 140 141 142 143 144 145 146 147 148 class EnvironmentName ( str , Enum ): \"\"\" Enumeration of available environments. Defines the possible runtime environments for the application. \"\"\" DEFAULT = \"default\" LOCAL = \"local\" DEV = \"dev\" TEST = \"test\" PROD = \"prod\" LogLevelName Bases: str , Enum Enumeration of available logging levels. Provides a mapping between string log levels and their corresponding logging constants. Source code in src/core/base_configuration.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class LogLevelName ( str , Enum ): \"\"\" Enumeration of available logging levels. Provides a mapping between string log levels and their corresponding logging constants. \"\"\" DEBUG = \"debug\" INFO = \"info\" WARNING = \"warning\" ERROR = \"error\" CRITICAL = \"critical\" @property def value_as_int ( self ) -> int : \"\"\"Convert string log level to the corresponding logging constant.\"\"\" return logging . _nameToLevel [ self . value . upper ()] value_as_int property Convert string log level to the corresponding logging constant. MetadataConfiguration Bases: BaseConfiguration Configuration for application metadata. Fields are read from the command line arguments. Source code in src/core/base_configuration.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class MetadataConfiguration ( BaseConfiguration ): \"\"\" Configuration for application metadata. Fields are read from the command line arguments. \"\"\" environment : EnvironmentName = Field ( EnvironmentName . LOCAL , description = \"The environment of the application.\" , ) build_name : Optional [ str ] = Field ( None , description = \"The name of the build.\" , ) log_level : LogLevelName = Field ( LogLevelName . INFO , description = \"The log level of the application.\" ) on_prem_config : bool = Field ( False , description = \"If set, then the configuration will be read from on premise configuration.\" \"Otherwise from the orchestrator service.\" , ) @model_validator ( mode = \"before\" ) @classmethod def validate_from_args ( cls , data : dict ) -> dict : \"\"\" Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Args: data (dict): The configuration data. Returns: dict: The validated configuration data. \"\"\" parser = cls . _get_parser () args , _ = parser . parse_known_args () return cls . _get_data ( data = data , args = args ) @classmethod def _get_data ( cls , data : dict , args : argparse . Namespace ) -> dict : \"\"\" Function to parse the arguments. Returns: argparse.Namespace: The parsed arguments. \"\"\" if args . env : data [ \"environment\" ] = EnvironmentName ( args . env ) if args . build_name : data [ \"build_name\" ] = args . build_name if args . on_prem_config : data [ \"on_prem_config\" ] = args . on_prem_config if args . log_level : data [ \"log_level\" ] = args . log_level return data @classmethod def _get_parser ( cls ) -> argparse . ArgumentParser : \"\"\" Function to initialize the argument parser to read arguments from command line. Returns: argparse.ArgumentParser: The argument parser. \"\"\" parser = argparse . ArgumentParser () parser . add_argument ( \"--env\" , type = EnvironmentName , help = \"Runtime environment.\" , default = EnvironmentName . LOCAL , choices = [ env . value for env in EnvironmentName ], ) parser . add_argument ( \"--build-name\" , type = str , help = \"The name of the build.\" , default = f \"build-local- { time . time () } \" , ) parser . add_argument ( \"--on-prem-config\" , help = \"If set, then the configuration will be read from on premise configuration.\" \"Otherwise from the orchestrator service.\" , action = \"store_true\" , ) parser . add_argument ( \"--log-level\" , type = LogLevelName , help = \"Log level.\" , default = LogLevelName . INFO , choices = [ level . value for level in LogLevelName ], ) return parser validate_from_args ( data ) classmethod Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Parameters: data ( dict ) \u2013 The configuration data. Returns: dict ( dict ) \u2013 The validated configuration data. Source code in src/core/base_configuration.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @model_validator ( mode = \"before\" ) @classmethod def validate_from_args ( cls , data : dict ) -> dict : \"\"\" Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Args: data (dict): The configuration data. Returns: dict: The validated configuration data. \"\"\" parser = cls . _get_parser () args , _ = parser . parse_known_args () return cls . _get_data ( data = data , args = args )","title":"Base_configuration"},{"location":"src/core/base_configuration/#base_configuration","text":"This module contains functionality related to the the base_configuration module for core .","title":"Base_configuration"},{"location":"src/core/base_configuration/#base_configuration_1","text":"","title":"Base_configuration"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseConfiguration","text":"Bases: BaseModel , ABC Base abstract class for all configuration models. Provides common functionality like hashing for configuration objects. Extend this class to create specific configuration types. Source code in src/core/base_configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BaseConfiguration ( BaseModel , ABC ): \"\"\" Base abstract class for all configuration models. Provides common functionality like hashing for configuration objects. Extend this class to create specific configuration types. \"\"\" def __hash__ ( self ) -> int : \"\"\"Not the most efficient way of hashing, but it works for now.\"\"\" json = self . model_dump_json () return hash ( json ) @classmethod def _validate ( cls , value : Any , info : ValidationInfo , registry : Type [ ConfigurationRegistry ], ) -> Any : \"\"\" Validates the value against the type defined in the registry. Args: value (Any): The value of the field. info (ValidationInfo): The information about the field. registry (Type[ConfigurationRegistry]): The registry to use for validation. Returns: Any: The validated value. \"\"\" type_adapter = TypeAdapter ( registry . get_union_type ()) return type_adapter . validate_python ( value , context = info . context , )","title":"BaseConfiguration"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseConfiguration.__hash__","text":"Not the most efficient way of hashing, but it works for now. Source code in src/core/base_configuration.py 29 30 31 32 def __hash__ ( self ) -> int : \"\"\"Not the most efficient way of hashing, but it works for now.\"\"\" json = self . model_dump_json () return hash ( json )","title":"__hash__"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseConfigurationWithSecrets","text":"Bases: BaseConfiguration Abstract model for configuration's secrets handling. Provides functionality to automatically load and validate secrets from environment variables or files. Extending class has to implement secrets field with corresponding type. Source code in src/core/base_configuration.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class BaseConfigurationWithSecrets ( BaseConfiguration ): \"\"\" Abstract model for configuration's secrets handling. Provides functionality to automatically load and validate secrets from environment variables or files. Extending class has to implement `secrets` field with corresponding type. \"\"\" secrets : BaseSecrets = Field ( None , description = \"`BaseSecrets` is meant for the the configuration that does not require secrets.\" \"In other case `BaseSecrets` should be replaced with the corresponding secrets class.\" , ) def model_post_init ( self , context : Any ) -> None : \"\"\" Function is invoked after the model is initialized. It is used to initialize secrets. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" self . secrets = self . _get_secrets ( secrets_file = context [ \"secrets_file\" ]) def _get_secrets ( self , secrets_file : str ) -> BaseSettings : \"\"\" Function to initialize secrets from the specified file. Args: secrets_file (str): The path to the secrets file. Returns: BaseSettings: The initialized secrets object. Raises: ValueError: If secrets are not found or cannot be loaded. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets = secrets_class ( _env_file = secrets_file ) if secrets is None : raise ValueError ( f \"Secrets for { self . name } not found.\" ) return secrets","title":"BaseConfigurationWithSecrets"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseConfigurationWithSecrets.model_post_init","text":"Function is invoked after the model is initialized. It is used to initialize secrets. Parameters: context ( Any ) \u2013 The context passed to the pydantic model, must contain 'secrets_file' key. Source code in src/core/base_configuration.py 88 89 90 91 92 93 94 95 def model_post_init ( self , context : Any ) -> None : \"\"\" Function is invoked after the model is initialized. It is used to initialize secrets. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" self . secrets = self . _get_secrets ( secrets_file = context [ \"secrets_file\" ])","title":"model_post_init"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseSecrets","text":"Bases: BaseConfiguration , BaseSettings Base class for secrets management. Extends both BaseConfiguration and BaseSettings to handle sensitive configuration data like API keys, passwords, etc. Uses Pydantic's BaseSettings for environment variable and file-based loading. Source code in src/core/base_configuration.py 59 60 61 62 63 64 65 66 67 68 69 70 class BaseSecrets ( BaseConfiguration , BaseSettings ): \"\"\" Base class for secrets management. Extends both BaseConfiguration and BaseSettings to handle sensitive configuration data like API keys, passwords, etc. Uses Pydantic's BaseSettings for environment variable and file-based loading. \"\"\" model_config = ConfigDict ( extra = \"ignore\" , )","title":"BaseSecrets"},{"location":"src/core/base_configuration/#src.core.base_configuration.BasicConfiguration","text":"Bases: BaseConfiguration Standard configuration class with metadata support. Includes a metadata field for storing application-specific settings. Source code in src/core/base_configuration.py 248 249 250 251 252 253 254 255 256 257 class BasicConfiguration ( BaseConfiguration ): \"\"\" Standard configuration class with metadata support. Includes a metadata field for storing application-specific settings. \"\"\" metadata : Optional [ MetadataConfiguration ] = Field ( None , description = \"Metadata of the run.\" )","title":"BasicConfiguration"},{"location":"src/core/base_configuration/#src.core.base_configuration.EnvironmentName","text":"Bases: str , Enum Enumeration of available environments. Defines the possible runtime environments for the application. Source code in src/core/base_configuration.py 137 138 139 140 141 142 143 144 145 146 147 148 class EnvironmentName ( str , Enum ): \"\"\" Enumeration of available environments. Defines the possible runtime environments for the application. \"\"\" DEFAULT = \"default\" LOCAL = \"local\" DEV = \"dev\" TEST = \"test\" PROD = \"prod\"","title":"EnvironmentName"},{"location":"src/core/base_configuration/#src.core.base_configuration.LogLevelName","text":"Bases: str , Enum Enumeration of available logging levels. Provides a mapping between string log levels and their corresponding logging constants. Source code in src/core/base_configuration.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class LogLevelName ( str , Enum ): \"\"\" Enumeration of available logging levels. Provides a mapping between string log levels and their corresponding logging constants. \"\"\" DEBUG = \"debug\" INFO = \"info\" WARNING = \"warning\" ERROR = \"error\" CRITICAL = \"critical\" @property def value_as_int ( self ) -> int : \"\"\"Convert string log level to the corresponding logging constant.\"\"\" return logging . _nameToLevel [ self . value . upper ()]","title":"LogLevelName"},{"location":"src/core/base_configuration/#src.core.base_configuration.LogLevelName.value_as_int","text":"Convert string log level to the corresponding logging constant.","title":"value_as_int"},{"location":"src/core/base_configuration/#src.core.base_configuration.MetadataConfiguration","text":"Bases: BaseConfiguration Configuration for application metadata. Fields are read from the command line arguments. Source code in src/core/base_configuration.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class MetadataConfiguration ( BaseConfiguration ): \"\"\" Configuration for application metadata. Fields are read from the command line arguments. \"\"\" environment : EnvironmentName = Field ( EnvironmentName . LOCAL , description = \"The environment of the application.\" , ) build_name : Optional [ str ] = Field ( None , description = \"The name of the build.\" , ) log_level : LogLevelName = Field ( LogLevelName . INFO , description = \"The log level of the application.\" ) on_prem_config : bool = Field ( False , description = \"If set, then the configuration will be read from on premise configuration.\" \"Otherwise from the orchestrator service.\" , ) @model_validator ( mode = \"before\" ) @classmethod def validate_from_args ( cls , data : dict ) -> dict : \"\"\" Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Args: data (dict): The configuration data. Returns: dict: The validated configuration data. \"\"\" parser = cls . _get_parser () args , _ = parser . parse_known_args () return cls . _get_data ( data = data , args = args ) @classmethod def _get_data ( cls , data : dict , args : argparse . Namespace ) -> dict : \"\"\" Function to parse the arguments. Returns: argparse.Namespace: The parsed arguments. \"\"\" if args . env : data [ \"environment\" ] = EnvironmentName ( args . env ) if args . build_name : data [ \"build_name\" ] = args . build_name if args . on_prem_config : data [ \"on_prem_config\" ] = args . on_prem_config if args . log_level : data [ \"log_level\" ] = args . log_level return data @classmethod def _get_parser ( cls ) -> argparse . ArgumentParser : \"\"\" Function to initialize the argument parser to read arguments from command line. Returns: argparse.ArgumentParser: The argument parser. \"\"\" parser = argparse . ArgumentParser () parser . add_argument ( \"--env\" , type = EnvironmentName , help = \"Runtime environment.\" , default = EnvironmentName . LOCAL , choices = [ env . value for env in EnvironmentName ], ) parser . add_argument ( \"--build-name\" , type = str , help = \"The name of the build.\" , default = f \"build-local- { time . time () } \" , ) parser . add_argument ( \"--on-prem-config\" , help = \"If set, then the configuration will be read from on premise configuration.\" \"Otherwise from the orchestrator service.\" , action = \"store_true\" , ) parser . add_argument ( \"--log-level\" , type = LogLevelName , help = \"Log level.\" , default = LogLevelName . INFO , choices = [ level . value for level in LogLevelName ], ) return parser","title":"MetadataConfiguration"},{"location":"src/core/base_configuration/#src.core.base_configuration.MetadataConfiguration.validate_from_args","text":"Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Parameters: data ( dict ) \u2013 The configuration data. Returns: dict ( dict ) \u2013 The validated configuration data. Source code in src/core/base_configuration.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @model_validator ( mode = \"before\" ) @classmethod def validate_from_args ( cls , data : dict ) -> dict : \"\"\" Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Args: data (dict): The configuration data. Returns: dict: The validated configuration data. \"\"\" parser = cls . _get_parser () args , _ = parser . parse_known_args () return cls . _get_data ( data = data , args = args )","title":"validate_from_args"},{"location":"src/core/base_factory/","text":"Base_factory This module contains functionality related to the the base_factory module for core . Base_factory ConfigurationRegistry Bases: Registry A specialized registry for configuration classes. This class extends the Registry to provide additional functionality specific to managing configuration classes, particularly the ability to create Union types from all registered configurations. The type is used by pydantic to validate the given configuration. Source code in src/core/base_factory.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class ConfigurationRegistry ( Registry ): \"\"\" A specialized registry for configuration classes. This class extends the Registry to provide additional functionality specific to managing configuration classes, particularly the ability to create Union types from all registered configurations. The type is used by pydantic to validate the given configuration. \"\"\" @classmethod def get_union_type ( cls ) -> Any : \"\"\" Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any: A Union type containing all registered configuration classes. \"\"\" return Union [ tuple ( cls . _objects . values ())] get_union_type () classmethod Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any ( Any ) \u2013 A Union type containing all registered configuration classes. Source code in src/core/base_factory.py 211 212 213 214 215 216 217 218 219 220 221 222 @classmethod def get_union_type ( cls ) -> Any : \"\"\" Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any: A Union type containing all registered configuration classes. \"\"\" return Union [ tuple ( cls . _objects . values ())] Factory Bases: ABC Abstract Factory base class for creating instances based on configurations. This class implements the Factory pattern and provides a standardized way to create instances of objects based on their configuration. Attributes: _configuration_class ( Type ) \u2013 The expected type of configuration objects. Source code in src/core/base_factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Factory ( ABC ): \"\"\" Abstract Factory base class for creating instances based on configurations. This class implements the Factory pattern and provides a standardized way to create instances of objects based on their configuration. Attributes: _configuration_class (Type): The expected type of configuration objects. \"\"\" _configuration_class : Type = None @classmethod @abstractmethod def _create_instance ( cls , configuration : Any ) -> Any : \"\"\" Abstract method that must be implemented by subclasses to create instances. Args: configuration (Any): The configuration object used to create the instance. Returns: Any: A new instance created based on the provided configuration. \"\"\" pass @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create an instance based on the given configuration. Args: configuration (Any): The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any: A new instance created based on the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_instance ( configuration ) create ( configuration ) classmethod Create an instance based on the given configuration. Parameters: configuration ( Any ) \u2013 The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any ( Any ) \u2013 A new instance created based on the provided configuration. Raises: ValueError \u2013 If the configuration is not an instance of the expected type. Source code in src/core/base_factory.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create an instance based on the given configuration. Args: configuration (Any): The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any: A new instance created based on the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_instance ( configuration ) Registry A registry for storing and retrieving objects by key. This class implements a simple registry pattern that allows objects to be registered with a key and retrieved later using that key. It is designed to be subclassed, with each subclass defining its own expected key type. It is used for registering different plugins or components in a system. Attributes: _key_class ( Type ) \u2013 The expected type for registry keys. _objects ( Dict [ Any , Any ] ) \u2013 Dictionary storing registered objects by their keys. Source code in src/core/base_factory.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 class Registry : \"\"\" A registry for storing and retrieving objects by key. This class implements a simple registry pattern that allows objects to be registered with a key and retrieved later using that key. It is designed to be subclassed, with each subclass defining its own expected key type. It is used for registering different plugins or components in a system. Attributes: _key_class (Type): The expected type for registry keys. _objects (Dict[Any, Any]): Dictionary storing registered objects by their keys. \"\"\" _key_class : Type = None _objects : Dict [ Any , Any ] = {} def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _objects = {} @classmethod def register ( cls , key : Any , Any : Any ) -> None : \"\"\" Register an object with the specified key. Args: key (Any): The key to associate with the object. Must be of type _key_class. Any (Any): The object to register. Raises: ValueError: If the key is not an instance of the expected type. \"\"\" if not isinstance ( key , cls . _key_class ): raise ValueError ( f \"Key must be of type: { cls . _key_class } \" ) cls . _objects [ key ] = Any @classmethod def get ( cls , key : Any ) -> Any : \"\"\" Retrieve an object by its key. Args: key (Any): The key associated with the object to retrieve. Returns: Any: The object associated with the key. Raises: ValueError: If no object is registered with the given key. \"\"\" if key not in cls . _objects : raise ValueError ( f \"Factory for ' { key } ' key is not registered.\" ) return cls . _objects [ key ] @classmethod def get_all ( cls ) -> Dict [ Any , Any ]: \"\"\" Get all registered objects. Returns: Dict[Any, Any]: A dictionary containing all registered objects with their keys. \"\"\" return cls . _objects __init_subclass__ ( ** kwargs ) Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Parameters: **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in src/core/base_factory.py 143 144 145 146 147 148 149 150 151 152 153 154 def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _objects = {} get ( key ) classmethod Retrieve an object by its key. Parameters: key ( Any ) \u2013 The key associated with the object to retrieve. Returns: Any ( Any ) \u2013 The object associated with the key. Raises: ValueError \u2013 If no object is registered with the given key. Source code in src/core/base_factory.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 @classmethod def get ( cls , key : Any ) -> Any : \"\"\" Retrieve an object by its key. Args: key (Any): The key associated with the object to retrieve. Returns: Any: The object associated with the key. Raises: ValueError: If no object is registered with the given key. \"\"\" if key not in cls . _objects : raise ValueError ( f \"Factory for ' { key } ' key is not registered.\" ) return cls . _objects [ key ] get_all () classmethod Get all registered objects. Returns: Dict [ Any , Any ] \u2013 Dict[Any, Any]: A dictionary containing all registered objects with their keys. Source code in src/core/base_factory.py 190 191 192 193 194 195 196 197 198 @classmethod def get_all ( cls ) -> Dict [ Any , Any ]: \"\"\" Get all registered objects. Returns: Dict[Any, Any]: A dictionary containing all registered objects with their keys. \"\"\" return cls . _objects register ( key , Any ) classmethod Register an object with the specified key. Parameters: key ( Any ) \u2013 The key to associate with the object. Must be of type _key_class. Any ( Any ) \u2013 The object to register. Raises: ValueError \u2013 If the key is not an instance of the expected type. Source code in src/core/base_factory.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 @classmethod def register ( cls , key : Any , Any : Any ) -> None : \"\"\" Register an object with the specified key. Args: key (Any): The key to associate with the object. Must be of type _key_class. Any (Any): The object to register. Raises: ValueError: If the key is not an instance of the expected type. \"\"\" if not isinstance ( key , cls . _key_class ): raise ValueError ( f \"Key must be of type: { cls . _key_class } \" ) cls . _objects [ key ] = Any SingletonFactory Bases: Factory This class extends the Factory pattern with Singleton functionality, ensuring that only one instance is created for each unique configuration during a single runtime. Attributes: _cache ( dict ) \u2013 Dictionary storing instances by their configurations. Source code in src/core/base_factory.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class SingletonFactory ( Factory ): \"\"\" This class extends the Factory pattern with Singleton functionality, ensuring that only one instance is created for each unique configuration during a single runtime. Attributes: _cache (dict): Dictionary storing instances by their configurations. \"\"\" _cache : dict = {} def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _cache = {} @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create or retrieve a singleton instance based on the given configuration. Args: configuration (Any): The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any: A singleton instance associated with the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_singleton ( configuration ) @classmethod def _create_singleton ( cls , configuration : Any ) -> Any : \"\"\" Create a new instance or return an existing one from the cache. Args: configuration (Any): The configuration object used to create/retrieve the instance. Returns: Any: A singleton instance associated with the provided configuration. \"\"\" if configuration in cls . _cache : return cls . _cache [ configuration ] instance = cls . _create_instance ( configuration ) cls . _cache [ configuration ] = instance return instance @classmethod def clear_cache ( cls ): \"\"\" Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. \"\"\" cls . _cache . clear () __init_subclass__ ( ** kwargs ) Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Parameters: **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in src/core/base_factory.py 65 66 67 68 69 70 71 72 73 74 75 76 def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _cache = {} clear_cache () classmethod Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. Source code in src/core/base_factory.py 115 116 117 118 119 120 121 122 @classmethod def clear_cache ( cls ): \"\"\" Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. \"\"\" cls . _cache . clear () create ( configuration ) classmethod Create or retrieve a singleton instance based on the given configuration. Parameters: configuration ( Any ) \u2013 The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any ( Any ) \u2013 A singleton instance associated with the provided configuration. Raises: ValueError \u2013 If the configuration is not an instance of the expected type. Source code in src/core/base_factory.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create or retrieve a singleton instance based on the given configuration. Args: configuration (Any): The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any: A singleton instance associated with the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_singleton ( configuration )","title":"Base_factory"},{"location":"src/core/base_factory/#base_factory","text":"This module contains functionality related to the the base_factory module for core .","title":"Base_factory"},{"location":"src/core/base_factory/#base_factory_1","text":"","title":"Base_factory"},{"location":"src/core/base_factory/#src.core.base_factory.ConfigurationRegistry","text":"Bases: Registry A specialized registry for configuration classes. This class extends the Registry to provide additional functionality specific to managing configuration classes, particularly the ability to create Union types from all registered configurations. The type is used by pydantic to validate the given configuration. Source code in src/core/base_factory.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class ConfigurationRegistry ( Registry ): \"\"\" A specialized registry for configuration classes. This class extends the Registry to provide additional functionality specific to managing configuration classes, particularly the ability to create Union types from all registered configurations. The type is used by pydantic to validate the given configuration. \"\"\" @classmethod def get_union_type ( cls ) -> Any : \"\"\" Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any: A Union type containing all registered configuration classes. \"\"\" return Union [ tuple ( cls . _objects . values ())]","title":"ConfigurationRegistry"},{"location":"src/core/base_factory/#src.core.base_factory.ConfigurationRegistry.get_union_type","text":"Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any ( Any ) \u2013 A Union type containing all registered configuration classes. Source code in src/core/base_factory.py 211 212 213 214 215 216 217 218 219 220 221 222 @classmethod def get_union_type ( cls ) -> Any : \"\"\" Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any: A Union type containing all registered configuration classes. \"\"\" return Union [ tuple ( cls . _objects . values ())]","title":"get_union_type"},{"location":"src/core/base_factory/#src.core.base_factory.Factory","text":"Bases: ABC Abstract Factory base class for creating instances based on configurations. This class implements the Factory pattern and provides a standardized way to create instances of objects based on their configuration. Attributes: _configuration_class ( Type ) \u2013 The expected type of configuration objects. Source code in src/core/base_factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Factory ( ABC ): \"\"\" Abstract Factory base class for creating instances based on configurations. This class implements the Factory pattern and provides a standardized way to create instances of objects based on their configuration. Attributes: _configuration_class (Type): The expected type of configuration objects. \"\"\" _configuration_class : Type = None @classmethod @abstractmethod def _create_instance ( cls , configuration : Any ) -> Any : \"\"\" Abstract method that must be implemented by subclasses to create instances. Args: configuration (Any): The configuration object used to create the instance. Returns: Any: A new instance created based on the provided configuration. \"\"\" pass @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create an instance based on the given configuration. Args: configuration (Any): The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any: A new instance created based on the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_instance ( configuration )","title":"Factory"},{"location":"src/core/base_factory/#src.core.base_factory.Factory.create","text":"Create an instance based on the given configuration. Parameters: configuration ( Any ) \u2013 The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any ( Any ) \u2013 A new instance created based on the provided configuration. Raises: ValueError \u2013 If the configuration is not an instance of the expected type. Source code in src/core/base_factory.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create an instance based on the given configuration. Args: configuration (Any): The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any: A new instance created based on the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_instance ( configuration )","title":"create"},{"location":"src/core/base_factory/#src.core.base_factory.Registry","text":"A registry for storing and retrieving objects by key. This class implements a simple registry pattern that allows objects to be registered with a key and retrieved later using that key. It is designed to be subclassed, with each subclass defining its own expected key type. It is used for registering different plugins or components in a system. Attributes: _key_class ( Type ) \u2013 The expected type for registry keys. _objects ( Dict [ Any , Any ] ) \u2013 Dictionary storing registered objects by their keys. Source code in src/core/base_factory.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 class Registry : \"\"\" A registry for storing and retrieving objects by key. This class implements a simple registry pattern that allows objects to be registered with a key and retrieved later using that key. It is designed to be subclassed, with each subclass defining its own expected key type. It is used for registering different plugins or components in a system. Attributes: _key_class (Type): The expected type for registry keys. _objects (Dict[Any, Any]): Dictionary storing registered objects by their keys. \"\"\" _key_class : Type = None _objects : Dict [ Any , Any ] = {} def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _objects = {} @classmethod def register ( cls , key : Any , Any : Any ) -> None : \"\"\" Register an object with the specified key. Args: key (Any): The key to associate with the object. Must be of type _key_class. Any (Any): The object to register. Raises: ValueError: If the key is not an instance of the expected type. \"\"\" if not isinstance ( key , cls . _key_class ): raise ValueError ( f \"Key must be of type: { cls . _key_class } \" ) cls . _objects [ key ] = Any @classmethod def get ( cls , key : Any ) -> Any : \"\"\" Retrieve an object by its key. Args: key (Any): The key associated with the object to retrieve. Returns: Any: The object associated with the key. Raises: ValueError: If no object is registered with the given key. \"\"\" if key not in cls . _objects : raise ValueError ( f \"Factory for ' { key } ' key is not registered.\" ) return cls . _objects [ key ] @classmethod def get_all ( cls ) -> Dict [ Any , Any ]: \"\"\" Get all registered objects. Returns: Dict[Any, Any]: A dictionary containing all registered objects with their keys. \"\"\" return cls . _objects","title":"Registry"},{"location":"src/core/base_factory/#src.core.base_factory.Registry.__init_subclass__","text":"Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Parameters: **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in src/core/base_factory.py 143 144 145 146 147 148 149 150 151 152 153 154 def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _objects = {}","title":"__init_subclass__"},{"location":"src/core/base_factory/#src.core.base_factory.Registry.get","text":"Retrieve an object by its key. Parameters: key ( Any ) \u2013 The key associated with the object to retrieve. Returns: Any ( Any ) \u2013 The object associated with the key. Raises: ValueError \u2013 If no object is registered with the given key. Source code in src/core/base_factory.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 @classmethod def get ( cls , key : Any ) -> Any : \"\"\" Retrieve an object by its key. Args: key (Any): The key associated with the object to retrieve. Returns: Any: The object associated with the key. Raises: ValueError: If no object is registered with the given key. \"\"\" if key not in cls . _objects : raise ValueError ( f \"Factory for ' { key } ' key is not registered.\" ) return cls . _objects [ key ]","title":"get"},{"location":"src/core/base_factory/#src.core.base_factory.Registry.get_all","text":"Get all registered objects. Returns: Dict [ Any , Any ] \u2013 Dict[Any, Any]: A dictionary containing all registered objects with their keys. Source code in src/core/base_factory.py 190 191 192 193 194 195 196 197 198 @classmethod def get_all ( cls ) -> Dict [ Any , Any ]: \"\"\" Get all registered objects. Returns: Dict[Any, Any]: A dictionary containing all registered objects with their keys. \"\"\" return cls . _objects","title":"get_all"},{"location":"src/core/base_factory/#src.core.base_factory.Registry.register","text":"Register an object with the specified key. Parameters: key ( Any ) \u2013 The key to associate with the object. Must be of type _key_class. Any ( Any ) \u2013 The object to register. Raises: ValueError \u2013 If the key is not an instance of the expected type. Source code in src/core/base_factory.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 @classmethod def register ( cls , key : Any , Any : Any ) -> None : \"\"\" Register an object with the specified key. Args: key (Any): The key to associate with the object. Must be of type _key_class. Any (Any): The object to register. Raises: ValueError: If the key is not an instance of the expected type. \"\"\" if not isinstance ( key , cls . _key_class ): raise ValueError ( f \"Key must be of type: { cls . _key_class } \" ) cls . _objects [ key ] = Any","title":"register"},{"location":"src/core/base_factory/#src.core.base_factory.SingletonFactory","text":"Bases: Factory This class extends the Factory pattern with Singleton functionality, ensuring that only one instance is created for each unique configuration during a single runtime. Attributes: _cache ( dict ) \u2013 Dictionary storing instances by their configurations. Source code in src/core/base_factory.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class SingletonFactory ( Factory ): \"\"\" This class extends the Factory pattern with Singleton functionality, ensuring that only one instance is created for each unique configuration during a single runtime. Attributes: _cache (dict): Dictionary storing instances by their configurations. \"\"\" _cache : dict = {} def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _cache = {} @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create or retrieve a singleton instance based on the given configuration. Args: configuration (Any): The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any: A singleton instance associated with the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_singleton ( configuration ) @classmethod def _create_singleton ( cls , configuration : Any ) -> Any : \"\"\" Create a new instance or return an existing one from the cache. Args: configuration (Any): The configuration object used to create/retrieve the instance. Returns: Any: A singleton instance associated with the provided configuration. \"\"\" if configuration in cls . _cache : return cls . _cache [ configuration ] instance = cls . _create_instance ( configuration ) cls . _cache [ configuration ] = instance return instance @classmethod def clear_cache ( cls ): \"\"\" Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. \"\"\" cls . _cache . clear ()","title":"SingletonFactory"},{"location":"src/core/base_factory/#src.core.base_factory.SingletonFactory.__init_subclass__","text":"Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Parameters: **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in src/core/base_factory.py 65 66 67 68 69 70 71 72 73 74 75 76 def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _cache = {}","title":"__init_subclass__"},{"location":"src/core/base_factory/#src.core.base_factory.SingletonFactory.clear_cache","text":"Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. Source code in src/core/base_factory.py 115 116 117 118 119 120 121 122 @classmethod def clear_cache ( cls ): \"\"\" Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. \"\"\" cls . _cache . clear ()","title":"clear_cache"},{"location":"src/core/base_factory/#src.core.base_factory.SingletonFactory.create","text":"Create or retrieve a singleton instance based on the given configuration. Parameters: configuration ( Any ) \u2013 The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any ( Any ) \u2013 A singleton instance associated with the provided configuration. Raises: ValueError \u2013 If the configuration is not an instance of the expected type. Source code in src/core/base_factory.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create or retrieve a singleton instance based on the given configuration. Args: configuration (Any): The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any: A singleton instance associated with the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_singleton ( configuration )","title":"create"},{"location":"src/core/base_initializer/","text":"Base_initializer This module contains functionality related to the the base_initializer module for core . Base_initializer BaseInitializer Bases: ABC Abstract base class for configuration initializers. This class defines the interface for initializers that are responsible for retrieving and initializing configuration objects. Subclasses must implement the get_configuration method to provide a concrete initialization strategy. Attributes: _configuration_class \u2013 The class of the configuration object to be initialized. Source code in src/core/base_initializer.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class BaseInitializer ( ABC ): \"\"\"Abstract base class for configuration initializers. This class defines the interface for initializers that are responsible for retrieving and initializing configuration objects. Subclasses must implement the get_configuration method to provide a concrete initialization strategy. Attributes: _configuration_class: The class of the configuration object to be initialized. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ]): \"\"\"Initialize the BaseInitializer. Args: configuration_class: The configuration class to use for initialization. \"\"\" self . _configuration_class = configuration_class @abstractmethod def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass __init__ ( configuration_class ) Initialize the BaseInitializer. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 The configuration class to use for initialization. Source code in src/core/base_initializer.py 26 27 28 29 30 31 32 def __init__ ( self , configuration_class : Type [ BaseConfiguration ]): \"\"\"Initialize the BaseInitializer. Args: configuration_class: The configuration class to use for initialization. \"\"\" self . _configuration_class = configuration_class get_configuration () abstractmethod Retrieve the configuration instance. Returns: BaseConfiguration \u2013 The initialized configuration object. Raises: NotImplementedError \u2013 This method must be implemented by subclasses. Source code in src/core/base_initializer.py 34 35 36 37 38 39 40 41 42 43 44 @abstractmethod def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass BasePackageLoader Bases: ABC Abstract base class for dynamic package loading. This class defines the interface for package loaders that dynamically discover and load packages from specified locations. Subclasses must implement the load_packages method to provide a concrete package loading strategy. The class provides a helper method _load_packages that handles the common logic of dynamically importing modules. Attributes: logger \u2013 Logger instance used for logging package loading activities. Source code in src/core/base_initializer.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class BasePackageLoader ( ABC ): \"\"\"Abstract base class for dynamic package loading. This class defines the interface for package loaders that dynamically discover and load packages from specified locations. Subclasses must implement the load_packages method to provide a concrete package loading strategy. The class provides a helper method _load_packages that handles the common logic of dynamically importing modules. Attributes: logger: Logger instance used for logging package loading activities. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the BasePackageLoader. Args: logger: Logger instance for logging package loading activities. \"\"\" self . logger = logger @abstractmethod def load_packages ( self ) -> None : \"\"\"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass def _load_packages ( self , parent_packages : List [ str ]) -> None : \"\"\"Load packages from the specified parent packages. This method dynamically imports modules from the provided parent packages and calls their register() method to register components with the system. It skips the 'core' package and handles import errors gracefully. Args: parent_packages: List of parent package names to load modules from. \"\"\" for parent_package in parent_packages : self . logger . info ( f \"Loading { parent_package } packages...\" ) package_path = parent_package . replace ( \".\" , \"/\" ) for _ , name , is_package in pkgutil . iter_modules ([ package_path ]): if is_package and name != \"core\" : try : module_path = f \" { parent_package } . { name } \" module = importlib . import_module ( module_path ) module . register () self . logger . info ( f \"Loaded package: { name } .\" ) except ImportError as e : self . logger . error ( f \"Failed to load datasource package { name } : { e } .\" ) except Exception as e : self . logger . error ( f \"Failed to register package { name } : { e } .\" ) __init__ ( logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the BasePackageLoader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging package loading activities. Source code in src/core/base_initializer.py 59 60 61 62 63 64 65 66 67 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the BasePackageLoader. Args: logger: Logger instance for logging package loading activities. \"\"\" self . logger = logger load_packages () abstractmethod Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError \u2013 This method must be implemented by subclasses. Source code in src/core/base_initializer.py 69 70 71 72 73 74 75 76 77 78 79 @abstractmethod def load_packages ( self ) -> None : \"\"\"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass BasicInitializer Bases: BaseInitializer Common initializer for embedding, augmentation and evaluation processes. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. Source code in src/core/base_initializer.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class BasicInitializer ( BaseInitializer ): \"\"\"Common initializer for embedding, augmentation and evaluation processes. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ], package_loader : BasePackageLoader , ): \"\"\"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Args: configuration_class: The configuration class to use for initialization. package_loader: The package loader to use for loading packages. \"\"\" super () . __init__ ( configuration_class ) self . configuration_retriever : BaseConfigurationRetriever = None package_loader . load_packages () self . _init_configuration_retriever () def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. \"\"\" return self . configuration_retriever . get () def _init_configuration_retriever ( self ) -> None : \"\"\"Initialize the configuration retriever. Creates an appropriate configuration retriever based on the metadata configuration and assigns it to the configuration_retriever attribute. \"\"\" metadata = MetadataConfiguration () configuration_retriever_class = ConfiguratioRetriverRegistry . get ( on_prem = metadata . on_prem_config ) self . configuration_retriever = configuration_retriever_class ( configuration_class = self . _configuration_class , metadata = metadata ) __init__ ( configuration_class , package_loader ) Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 The configuration class to use for initialization. package_loader ( BasePackageLoader ) \u2013 The package loader to use for loading packages. Source code in src/core/base_initializer.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __init__ ( self , configuration_class : Type [ BaseConfiguration ], package_loader : BasePackageLoader , ): \"\"\"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Args: configuration_class: The configuration class to use for initialization. package_loader: The package loader to use for loading packages. \"\"\" super () . __init__ ( configuration_class ) self . configuration_retriever : BaseConfigurationRetriever = None package_loader . load_packages () self . _init_configuration_retriever () get_configuration () Retrieve the configuration instance. Returns: BaseConfiguration \u2013 The initialized configuration object. Source code in src/core/base_initializer.py 139 140 141 142 143 144 145 def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. \"\"\" return self . configuration_retriever . get ()","title":"Base_initializer"},{"location":"src/core/base_initializer/#base_initializer","text":"This module contains functionality related to the the base_initializer module for core .","title":"Base_initializer"},{"location":"src/core/base_initializer/#base_initializer_1","text":"","title":"Base_initializer"},{"location":"src/core/base_initializer/#src.core.base_initializer.BaseInitializer","text":"Bases: ABC Abstract base class for configuration initializers. This class defines the interface for initializers that are responsible for retrieving and initializing configuration objects. Subclasses must implement the get_configuration method to provide a concrete initialization strategy. Attributes: _configuration_class \u2013 The class of the configuration object to be initialized. Source code in src/core/base_initializer.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class BaseInitializer ( ABC ): \"\"\"Abstract base class for configuration initializers. This class defines the interface for initializers that are responsible for retrieving and initializing configuration objects. Subclasses must implement the get_configuration method to provide a concrete initialization strategy. Attributes: _configuration_class: The class of the configuration object to be initialized. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ]): \"\"\"Initialize the BaseInitializer. Args: configuration_class: The configuration class to use for initialization. \"\"\" self . _configuration_class = configuration_class @abstractmethod def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass","title":"BaseInitializer"},{"location":"src/core/base_initializer/#src.core.base_initializer.BaseInitializer.__init__","text":"Initialize the BaseInitializer. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 The configuration class to use for initialization. Source code in src/core/base_initializer.py 26 27 28 29 30 31 32 def __init__ ( self , configuration_class : Type [ BaseConfiguration ]): \"\"\"Initialize the BaseInitializer. Args: configuration_class: The configuration class to use for initialization. \"\"\" self . _configuration_class = configuration_class","title":"__init__"},{"location":"src/core/base_initializer/#src.core.base_initializer.BaseInitializer.get_configuration","text":"Retrieve the configuration instance. Returns: BaseConfiguration \u2013 The initialized configuration object. Raises: NotImplementedError \u2013 This method must be implemented by subclasses. Source code in src/core/base_initializer.py 34 35 36 37 38 39 40 41 42 43 44 @abstractmethod def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass","title":"get_configuration"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasePackageLoader","text":"Bases: ABC Abstract base class for dynamic package loading. This class defines the interface for package loaders that dynamically discover and load packages from specified locations. Subclasses must implement the load_packages method to provide a concrete package loading strategy. The class provides a helper method _load_packages that handles the common logic of dynamically importing modules. Attributes: logger \u2013 Logger instance used for logging package loading activities. Source code in src/core/base_initializer.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class BasePackageLoader ( ABC ): \"\"\"Abstract base class for dynamic package loading. This class defines the interface for package loaders that dynamically discover and load packages from specified locations. Subclasses must implement the load_packages method to provide a concrete package loading strategy. The class provides a helper method _load_packages that handles the common logic of dynamically importing modules. Attributes: logger: Logger instance used for logging package loading activities. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the BasePackageLoader. Args: logger: Logger instance for logging package loading activities. \"\"\" self . logger = logger @abstractmethod def load_packages ( self ) -> None : \"\"\"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass def _load_packages ( self , parent_packages : List [ str ]) -> None : \"\"\"Load packages from the specified parent packages. This method dynamically imports modules from the provided parent packages and calls their register() method to register components with the system. It skips the 'core' package and handles import errors gracefully. Args: parent_packages: List of parent package names to load modules from. \"\"\" for parent_package in parent_packages : self . logger . info ( f \"Loading { parent_package } packages...\" ) package_path = parent_package . replace ( \".\" , \"/\" ) for _ , name , is_package in pkgutil . iter_modules ([ package_path ]): if is_package and name != \"core\" : try : module_path = f \" { parent_package } . { name } \" module = importlib . import_module ( module_path ) module . register () self . logger . info ( f \"Loaded package: { name } .\" ) except ImportError as e : self . logger . error ( f \"Failed to load datasource package { name } : { e } .\" ) except Exception as e : self . logger . error ( f \"Failed to register package { name } : { e } .\" )","title":"BasePackageLoader"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasePackageLoader.__init__","text":"Initialize the BasePackageLoader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging package loading activities. Source code in src/core/base_initializer.py 59 60 61 62 63 64 65 66 67 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the BasePackageLoader. Args: logger: Logger instance for logging package loading activities. \"\"\" self . logger = logger","title":"__init__"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasePackageLoader.load_packages","text":"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError \u2013 This method must be implemented by subclasses. Source code in src/core/base_initializer.py 69 70 71 72 73 74 75 76 77 78 79 @abstractmethod def load_packages ( self ) -> None : \"\"\"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass","title":"load_packages"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasicInitializer","text":"Bases: BaseInitializer Common initializer for embedding, augmentation and evaluation processes. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. Source code in src/core/base_initializer.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class BasicInitializer ( BaseInitializer ): \"\"\"Common initializer for embedding, augmentation and evaluation processes. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ], package_loader : BasePackageLoader , ): \"\"\"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Args: configuration_class: The configuration class to use for initialization. package_loader: The package loader to use for loading packages. \"\"\" super () . __init__ ( configuration_class ) self . configuration_retriever : BaseConfigurationRetriever = None package_loader . load_packages () self . _init_configuration_retriever () def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. \"\"\" return self . configuration_retriever . get () def _init_configuration_retriever ( self ) -> None : \"\"\"Initialize the configuration retriever. Creates an appropriate configuration retriever based on the metadata configuration and assigns it to the configuration_retriever attribute. \"\"\" metadata = MetadataConfiguration () configuration_retriever_class = ConfiguratioRetriverRegistry . get ( on_prem = metadata . on_prem_config ) self . configuration_retriever = configuration_retriever_class ( configuration_class = self . _configuration_class , metadata = metadata )","title":"BasicInitializer"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasicInitializer.__init__","text":"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 The configuration class to use for initialization. package_loader ( BasePackageLoader ) \u2013 The package loader to use for loading packages. Source code in src/core/base_initializer.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __init__ ( self , configuration_class : Type [ BaseConfiguration ], package_loader : BasePackageLoader , ): \"\"\"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Args: configuration_class: The configuration class to use for initialization. package_loader: The package loader to use for loading packages. \"\"\" super () . __init__ ( configuration_class ) self . configuration_retriever : BaseConfigurationRetriever = None package_loader . load_packages () self . _init_configuration_retriever ()","title":"__init__"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasicInitializer.get_configuration","text":"Retrieve the configuration instance. Returns: BaseConfiguration \u2013 The initialized configuration object. Source code in src/core/base_initializer.py 139 140 141 142 143 144 145 def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. \"\"\" return self . configuration_retriever . get ()","title":"get_configuration"},{"location":"src/core/configuration_retrievers/","text":"Configuration_retrievers This module contains functionality related to the the configuration_retrievers module for core . Configuration_retrievers BaseConfigurationRetriever Bases: ABC Abstract base class for configuration retrieval. This class defines the common interface and functionality for retrieving application configurations from various sources. Concrete implementations should extend this class to provide specific retrieval mechanisms. Attributes: CONFIGURATIONS_DIR ( str ) \u2013 Directory where configuration files are stored. Source code in src/core/configuration_retrievers.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class BaseConfigurationRetriever ( ABC ): \"\"\"Abstract base class for configuration retrieval. This class defines the common interface and functionality for retrieving application configurations from various sources. Concrete implementations should extend this class to provide specific retrieval mechanisms. Attributes: CONFIGURATIONS_DIR (str): Directory where configuration files are stored. \"\"\" CONFIGURATIONS_DIR = \"configurations\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ], metadata : MetadataConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the configuration retriever. Args: configuration_class: Class of the configuration object to be retrieved. metadata: Applicaton metadata. logger: Logger instance for this class. \"\"\" self . metadata = metadata self . logger = logger self . _configuration_class = configuration_class self . configuration = None @abstractmethod def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve the configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object of `_configuration_class` type. \"\"\" pass def _parse_configuration ( self , configuration_json : dict , verbose = True ): \"\"\"Parse JSON configuration into a configuration object. Args: configuration_json: Dictionary containing the configuration data. verbose: Whether to log the parsed configuration. Returns: Parsed configuration object of `_configuration_class` type.. \"\"\" secrets_filepath = self . _get_secrets_filepath () configuration = self . _configuration_class . model_validate_json ( configuration_json , context = { \"secrets_file\" : secrets_filepath } ) configuration . metadata = self . metadata if verbose : self . logger . info ( f \"::Environment: { self . metadata . environment } \" ) self . logger . info ( configuration . model_dump_json ( indent = 4 )) return configuration def _get_secrets_filepath ( self ) -> str : \"\"\"Get the path to the secrets file based on environment. Returns: Path to the secrets file. \"\"\" return f \" { OnPremConfigurationRetriever . CONFIGURATIONS_DIR } /secrets. { self . metadata . environment . value } .env\" __init__ ( configuration_class , metadata , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the configuration retriever. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 Class of the configuration object to be retrieved. metadata ( MetadataConfiguration ) \u2013 Applicaton metadata. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for this class. Source code in src/core/configuration_retrievers.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , configuration_class : Type [ BaseConfiguration ], metadata : MetadataConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the configuration retriever. Args: configuration_class: Class of the configuration object to be retrieved. metadata: Applicaton metadata. logger: Logger instance for this class. \"\"\" self . metadata = metadata self . logger = logger self . _configuration_class = configuration_class self . configuration = None get ( verbose = True ) abstractmethod Retrieve the configuration. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Returns: BaseConfiguration \u2013 The parsed configuration object of _configuration_class type. Source code in src/core/configuration_retrievers.py 40 41 42 43 44 45 46 47 48 49 50 @abstractmethod def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve the configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object of `_configuration_class` type. \"\"\" pass ConfiguratioRetriverRegistry Registry for configuration retrievers. Provides factory methods to get the appropriate configuration retriever based on context. Source code in src/core/configuration_retrievers.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class ConfiguratioRetriverRegistry : \"\"\"Registry for configuration retrievers. Provides factory methods to get the appropriate configuration retriever based on context. \"\"\" def get ( on_prem : bool ) -> BaseConfigurationRetriever : \"\"\"Get the appropriate configuration retriever class. Args: on_prem: Whether to use on-premise configuration or remote. Returns: The appropriate configuration retriever class. \"\"\" if on_prem : return OnPremConfigurationRetriever else : return RemoteConfigurationRetriever get ( on_prem ) Get the appropriate configuration retriever class. Parameters: on_prem ( bool ) \u2013 Whether to use on-premise configuration or remote. Returns: BaseConfigurationRetriever \u2013 The appropriate configuration retriever class. Source code in src/core/configuration_retrievers.py 159 160 161 162 163 164 165 166 167 168 169 170 171 def get ( on_prem : bool ) -> BaseConfigurationRetriever : \"\"\"Get the appropriate configuration retriever class. Args: on_prem: Whether to use on-premise configuration or remote. Returns: The appropriate configuration retriever class. \"\"\" if on_prem : return OnPremConfigurationRetriever else : return RemoteConfigurationRetriever OnPremConfigurationRetriever Bases: BaseConfigurationRetriever Configuration retriever for on-premise file sources. Retrieves configurations from local JSON files. Source code in src/core/configuration_retrievers.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class OnPremConfigurationRetriever ( BaseConfigurationRetriever ): \"\"\"Configuration retriever for on-premise file sources. Retrieves configurations from local JSON files. \"\"\" def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" if not self . configuration : self . configuration = self . _get ( verbose ) return self . configuration def _get ( self , verbose ) -> BaseConfiguration : \"\"\"Internal method to retrieve and parse configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" configuration_filepath = self . _get_configuration_filepath () with open ( configuration_filepath ) as f : configuration_json = f . read () return self . _parse_configuration ( configuration_json = configuration_json , verbose = verbose ) def _get_configuration_filepath ( self ) -> str : \"\"\"Get the path to the configuration file based on environment. Returns: Path to the configuration JSON file. \"\"\" return f \" { OnPremConfigurationRetriever . CONFIGURATIONS_DIR } /configuration. { self . metadata . environment . value } .json\" get ( verbose = True ) Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Returns: BaseConfiguration \u2013 The parsed configuration object. Source code in src/core/configuration_retrievers.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" if not self . configuration : self . configuration = self . _get ( verbose ) return self . configuration RemoteConfigurationRetriever Bases: BaseConfigurationRetriever Configuration retriever for remote sources. Retrieves configuration from remote sources like config servers or cloud storage. Currently not implemented. Source code in src/core/configuration_retrievers.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class RemoteConfigurationRetriever ( BaseConfigurationRetriever ): \"\"\"Configuration retriever for remote sources. Retrieves configuration from remote sources like config servers or cloud storage. Currently not implemented. \"\"\" def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from a remote source. Args: verbose: Whether to log detailed information during retrieval. Raises: NotImplementedError: This functionality is not yet implemented. Returns: The configuration object. \"\"\" raise NotImplementedError ( \"Remote configuration is not implemented yet.\" ) get ( verbose = True ) Retrieve configuration from a remote source. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Raises: NotImplementedError \u2013 This functionality is not yet implemented. Returns: BaseConfiguration \u2013 The configuration object. Source code in src/core/configuration_retrievers.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from a remote source. Args: verbose: Whether to log detailed information during retrieval. Raises: NotImplementedError: This functionality is not yet implemented. Returns: The configuration object. \"\"\" raise NotImplementedError ( \"Remote configuration is not implemented yet.\" )","title":"Configuration_retrievers"},{"location":"src/core/configuration_retrievers/#configuration_retrievers","text":"This module contains functionality related to the the configuration_retrievers module for core .","title":"Configuration_retrievers"},{"location":"src/core/configuration_retrievers/#configuration_retrievers_1","text":"","title":"Configuration_retrievers"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.BaseConfigurationRetriever","text":"Bases: ABC Abstract base class for configuration retrieval. This class defines the common interface and functionality for retrieving application configurations from various sources. Concrete implementations should extend this class to provide specific retrieval mechanisms. Attributes: CONFIGURATIONS_DIR ( str ) \u2013 Directory where configuration files are stored. Source code in src/core/configuration_retrievers.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class BaseConfigurationRetriever ( ABC ): \"\"\"Abstract base class for configuration retrieval. This class defines the common interface and functionality for retrieving application configurations from various sources. Concrete implementations should extend this class to provide specific retrieval mechanisms. Attributes: CONFIGURATIONS_DIR (str): Directory where configuration files are stored. \"\"\" CONFIGURATIONS_DIR = \"configurations\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ], metadata : MetadataConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the configuration retriever. Args: configuration_class: Class of the configuration object to be retrieved. metadata: Applicaton metadata. logger: Logger instance for this class. \"\"\" self . metadata = metadata self . logger = logger self . _configuration_class = configuration_class self . configuration = None @abstractmethod def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve the configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object of `_configuration_class` type. \"\"\" pass def _parse_configuration ( self , configuration_json : dict , verbose = True ): \"\"\"Parse JSON configuration into a configuration object. Args: configuration_json: Dictionary containing the configuration data. verbose: Whether to log the parsed configuration. Returns: Parsed configuration object of `_configuration_class` type.. \"\"\" secrets_filepath = self . _get_secrets_filepath () configuration = self . _configuration_class . model_validate_json ( configuration_json , context = { \"secrets_file\" : secrets_filepath } ) configuration . metadata = self . metadata if verbose : self . logger . info ( f \"::Environment: { self . metadata . environment } \" ) self . logger . info ( configuration . model_dump_json ( indent = 4 )) return configuration def _get_secrets_filepath ( self ) -> str : \"\"\"Get the path to the secrets file based on environment. Returns: Path to the secrets file. \"\"\" return f \" { OnPremConfigurationRetriever . CONFIGURATIONS_DIR } /secrets. { self . metadata . environment . value } .env\"","title":"BaseConfigurationRetriever"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.BaseConfigurationRetriever.__init__","text":"Initialize the configuration retriever. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 Class of the configuration object to be retrieved. metadata ( MetadataConfiguration ) \u2013 Applicaton metadata. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for this class. Source code in src/core/configuration_retrievers.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , configuration_class : Type [ BaseConfiguration ], metadata : MetadataConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the configuration retriever. Args: configuration_class: Class of the configuration object to be retrieved. metadata: Applicaton metadata. logger: Logger instance for this class. \"\"\" self . metadata = metadata self . logger = logger self . _configuration_class = configuration_class self . configuration = None","title":"__init__"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.BaseConfigurationRetriever.get","text":"Retrieve the configuration. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Returns: BaseConfiguration \u2013 The parsed configuration object of _configuration_class type. Source code in src/core/configuration_retrievers.py 40 41 42 43 44 45 46 47 48 49 50 @abstractmethod def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve the configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object of `_configuration_class` type. \"\"\" pass","title":"get"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.ConfiguratioRetriverRegistry","text":"Registry for configuration retrievers. Provides factory methods to get the appropriate configuration retriever based on context. Source code in src/core/configuration_retrievers.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class ConfiguratioRetriverRegistry : \"\"\"Registry for configuration retrievers. Provides factory methods to get the appropriate configuration retriever based on context. \"\"\" def get ( on_prem : bool ) -> BaseConfigurationRetriever : \"\"\"Get the appropriate configuration retriever class. Args: on_prem: Whether to use on-premise configuration or remote. Returns: The appropriate configuration retriever class. \"\"\" if on_prem : return OnPremConfigurationRetriever else : return RemoteConfigurationRetriever","title":"ConfiguratioRetriverRegistry"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.ConfiguratioRetriverRegistry.get","text":"Get the appropriate configuration retriever class. Parameters: on_prem ( bool ) \u2013 Whether to use on-premise configuration or remote. Returns: BaseConfigurationRetriever \u2013 The appropriate configuration retriever class. Source code in src/core/configuration_retrievers.py 159 160 161 162 163 164 165 166 167 168 169 170 171 def get ( on_prem : bool ) -> BaseConfigurationRetriever : \"\"\"Get the appropriate configuration retriever class. Args: on_prem: Whether to use on-premise configuration or remote. Returns: The appropriate configuration retriever class. \"\"\" if on_prem : return OnPremConfigurationRetriever else : return RemoteConfigurationRetriever","title":"get"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.OnPremConfigurationRetriever","text":"Bases: BaseConfigurationRetriever Configuration retriever for on-premise file sources. Retrieves configurations from local JSON files. Source code in src/core/configuration_retrievers.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class OnPremConfigurationRetriever ( BaseConfigurationRetriever ): \"\"\"Configuration retriever for on-premise file sources. Retrieves configurations from local JSON files. \"\"\" def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" if not self . configuration : self . configuration = self . _get ( verbose ) return self . configuration def _get ( self , verbose ) -> BaseConfiguration : \"\"\"Internal method to retrieve and parse configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" configuration_filepath = self . _get_configuration_filepath () with open ( configuration_filepath ) as f : configuration_json = f . read () return self . _parse_configuration ( configuration_json = configuration_json , verbose = verbose ) def _get_configuration_filepath ( self ) -> str : \"\"\"Get the path to the configuration file based on environment. Returns: Path to the configuration JSON file. \"\"\" return f \" { OnPremConfigurationRetriever . CONFIGURATIONS_DIR } /configuration. { self . metadata . environment . value } .json\"","title":"OnPremConfigurationRetriever"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.OnPremConfigurationRetriever.get","text":"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Returns: BaseConfiguration \u2013 The parsed configuration object. Source code in src/core/configuration_retrievers.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" if not self . configuration : self . configuration = self . _get ( verbose ) return self . configuration","title":"get"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.RemoteConfigurationRetriever","text":"Bases: BaseConfigurationRetriever Configuration retriever for remote sources. Retrieves configuration from remote sources like config servers or cloud storage. Currently not implemented. Source code in src/core/configuration_retrievers.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class RemoteConfigurationRetriever ( BaseConfigurationRetriever ): \"\"\"Configuration retriever for remote sources. Retrieves configuration from remote sources like config servers or cloud storage. Currently not implemented. \"\"\" def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from a remote source. Args: verbose: Whether to log detailed information during retrieval. Raises: NotImplementedError: This functionality is not yet implemented. Returns: The configuration object. \"\"\" raise NotImplementedError ( \"Remote configuration is not implemented yet.\" )","title":"RemoteConfigurationRetriever"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.RemoteConfigurationRetriever.get","text":"Retrieve configuration from a remote source. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Raises: NotImplementedError \u2013 This functionality is not yet implemented. Returns: BaseConfiguration \u2013 The configuration object. Source code in src/core/configuration_retrievers.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from a remote source. Args: verbose: Whether to log detailed information during retrieval. Raises: NotImplementedError: This functionality is not yet implemented. Returns: The configuration object. \"\"\" raise NotImplementedError ( \"Remote configuration is not implemented yet.\" )","title":"get"},{"location":"src/core/logger/","text":"Logger This module contains functionality related to the the logger module for core . Logger LoggerConfiguration Utility class for configuring application-wide logging. This class provides methods to create consistently configured loggers, manage log levels, and control the behavior of third-party loggers. It uses the log level settings from MetadataConfiguration by default. Attributes: LOG_FORMAT \u2013 The format string used for log messages log_level ( LogLevelName ) \u2013 The application-wide log level (cached from configuration) Source code in src/core/logger.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LoggerConfiguration : \"\"\"Utility class for configuring application-wide logging. This class provides methods to create consistently configured loggers, manage log levels, and control the behavior of third-party loggers. It uses the log level settings from MetadataConfiguration by default. Attributes: LOG_FORMAT: The format string used for log messages log_level: The application-wide log level (cached from configuration) \"\"\" LOG_FORMAT = \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" log_level : LogLevelName = None @classmethod def get_logger ( cls , name : str , log_level : LogLevelName = None , propagate : bool = False ) -> logging . Logger : \"\"\"Create a logger with specified configuration. Args: name: Name for the logger (typically __name__) log_level: Logging level propagate: Whether to propagate to parent loggers Returns: logging.Logger: Configured logger instance \"\"\" if not log_level : log_level = cls . get_log_level () logger = logging . getLogger ( name ) logger . setLevel ( cls . log_level . value_as_int ) logger . propagate = propagate if not logger . handlers : formatter = logging . Formatter ( cls . LOG_FORMAT ) handler = logging . StreamHandler ( sys . stdout ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger @classmethod def get_log_level ( cls ) -> LogLevelName : \"\"\"Get the log level for the logger. Returns: LogLevelName: The log level for the logger \"\"\" if not cls . log_level : metadata = MetadataConfiguration () cls . log_level = metadata . log_level return cls . log_level @staticmethod def mute_logs (): \"\"\"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level \"\"\" # Warning logs logging . getLogger ( \"httpx\" ) . setLevel ( logging . WARNING ) # Error logs logging . getLogger ( \"pdfminer\" ) . setLevel ( logging . ERROR ) get_log_level () classmethod Get the log level for the logger. Returns: LogLevelName ( LogLevelName ) \u2013 The log level for the logger Source code in src/core/logger.py 52 53 54 55 56 57 58 59 60 61 62 @classmethod def get_log_level ( cls ) -> LogLevelName : \"\"\"Get the log level for the logger. Returns: LogLevelName: The log level for the logger \"\"\" if not cls . log_level : metadata = MetadataConfiguration () cls . log_level = metadata . log_level return cls . log_level get_logger ( name , log_level = None , propagate = False ) classmethod Create a logger with specified configuration. Parameters: name ( str ) \u2013 Name for the logger (typically name ) log_level ( LogLevelName , default: None ) \u2013 Logging level propagate ( bool , default: False ) \u2013 Whether to propagate to parent loggers Returns: Logger \u2013 logging.Logger: Configured logger instance Source code in src/core/logger.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @classmethod def get_logger ( cls , name : str , log_level : LogLevelName = None , propagate : bool = False ) -> logging . Logger : \"\"\"Create a logger with specified configuration. Args: name: Name for the logger (typically __name__) log_level: Logging level propagate: Whether to propagate to parent loggers Returns: logging.Logger: Configured logger instance \"\"\" if not log_level : log_level = cls . get_log_level () logger = logging . getLogger ( name ) logger . setLevel ( cls . log_level . value_as_int ) logger . propagate = propagate if not logger . handlers : formatter = logging . Formatter ( cls . LOG_FORMAT ) handler = logging . StreamHandler ( sys . stdout ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger mute_logs () staticmethod Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level Source code in src/core/logger.py 64 65 66 67 68 69 70 71 72 73 74 75 76 @staticmethod def mute_logs (): \"\"\"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level \"\"\" # Warning logs logging . getLogger ( \"httpx\" ) . setLevel ( logging . WARNING ) # Error logs logging . getLogger ( \"pdfminer\" ) . setLevel ( logging . ERROR )","title":"Logger"},{"location":"src/core/logger/#logger","text":"This module contains functionality related to the the logger module for core .","title":"Logger"},{"location":"src/core/logger/#logger_1","text":"","title":"Logger"},{"location":"src/core/logger/#src.core.logger.LoggerConfiguration","text":"Utility class for configuring application-wide logging. This class provides methods to create consistently configured loggers, manage log levels, and control the behavior of third-party loggers. It uses the log level settings from MetadataConfiguration by default. Attributes: LOG_FORMAT \u2013 The format string used for log messages log_level ( LogLevelName ) \u2013 The application-wide log level (cached from configuration) Source code in src/core/logger.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LoggerConfiguration : \"\"\"Utility class for configuring application-wide logging. This class provides methods to create consistently configured loggers, manage log levels, and control the behavior of third-party loggers. It uses the log level settings from MetadataConfiguration by default. Attributes: LOG_FORMAT: The format string used for log messages log_level: The application-wide log level (cached from configuration) \"\"\" LOG_FORMAT = \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" log_level : LogLevelName = None @classmethod def get_logger ( cls , name : str , log_level : LogLevelName = None , propagate : bool = False ) -> logging . Logger : \"\"\"Create a logger with specified configuration. Args: name: Name for the logger (typically __name__) log_level: Logging level propagate: Whether to propagate to parent loggers Returns: logging.Logger: Configured logger instance \"\"\" if not log_level : log_level = cls . get_log_level () logger = logging . getLogger ( name ) logger . setLevel ( cls . log_level . value_as_int ) logger . propagate = propagate if not logger . handlers : formatter = logging . Formatter ( cls . LOG_FORMAT ) handler = logging . StreamHandler ( sys . stdout ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger @classmethod def get_log_level ( cls ) -> LogLevelName : \"\"\"Get the log level for the logger. Returns: LogLevelName: The log level for the logger \"\"\" if not cls . log_level : metadata = MetadataConfiguration () cls . log_level = metadata . log_level return cls . log_level @staticmethod def mute_logs (): \"\"\"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level \"\"\" # Warning logs logging . getLogger ( \"httpx\" ) . setLevel ( logging . WARNING ) # Error logs logging . getLogger ( \"pdfminer\" ) . setLevel ( logging . ERROR )","title":"LoggerConfiguration"},{"location":"src/core/logger/#src.core.logger.LoggerConfiguration.get_log_level","text":"Get the log level for the logger. Returns: LogLevelName ( LogLevelName ) \u2013 The log level for the logger Source code in src/core/logger.py 52 53 54 55 56 57 58 59 60 61 62 @classmethod def get_log_level ( cls ) -> LogLevelName : \"\"\"Get the log level for the logger. Returns: LogLevelName: The log level for the logger \"\"\" if not cls . log_level : metadata = MetadataConfiguration () cls . log_level = metadata . log_level return cls . log_level","title":"get_log_level"},{"location":"src/core/logger/#src.core.logger.LoggerConfiguration.get_logger","text":"Create a logger with specified configuration. Parameters: name ( str ) \u2013 Name for the logger (typically name ) log_level ( LogLevelName , default: None ) \u2013 Logging level propagate ( bool , default: False ) \u2013 Whether to propagate to parent loggers Returns: Logger \u2013 logging.Logger: Configured logger instance Source code in src/core/logger.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @classmethod def get_logger ( cls , name : str , log_level : LogLevelName = None , propagate : bool = False ) -> logging . Logger : \"\"\"Create a logger with specified configuration. Args: name: Name for the logger (typically __name__) log_level: Logging level propagate: Whether to propagate to parent loggers Returns: logging.Logger: Configured logger instance \"\"\" if not log_level : log_level = cls . get_log_level () logger = logging . getLogger ( name ) logger . setLevel ( cls . log_level . value_as_int ) logger . propagate = propagate if not logger . handlers : formatter = logging . Formatter ( cls . LOG_FORMAT ) handler = logging . StreamHandler ( sys . stdout ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger","title":"get_logger"},{"location":"src/core/logger/#src.core.logger.LoggerConfiguration.mute_logs","text":"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level Source code in src/core/logger.py 64 65 66 67 68 69 70 71 72 73 74 75 76 @staticmethod def mute_logs (): \"\"\"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level \"\"\" # Warning logs logging . getLogger ( \"httpx\" ) . setLevel ( logging . WARNING ) # Error logs logging . getLogger ( \"pdfminer\" ) . setLevel ( logging . ERROR )","title":"mute_logs"},{"location":"src/embedding/bootstrap/initializer/","text":"Initializer This module contains functionality related to the the initializer module for embedding.bootstrap . Initializer EmbeddingInitializer Bases: ExtractionInitializer Initializer for embedding, augmentation and evaluation processes. Extends the ExtractionInitializer to provide specialized initialization for embedding-related functionality. Responsible for binding embedding components to the dependency injection container and preparing the environment for embedding, augmentation, and evaluation processes. Attributes: configuration \u2013 EmbeddingConfiguration object containing configuration parameters configuration_json \u2013 JSON string representation of the EmbeddingConfiguration package_loader \u2013 Loader responsible for importing required packages Source code in src/embedding/bootstrap/initializer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class EmbeddingInitializer ( ExtractionInitializer ): \"\"\"Initializer for embedding, augmentation and evaluation processes. Extends the ExtractionInitializer to provide specialized initialization for embedding-related functionality. Responsible for binding embedding components to the dependency injection container and preparing the environment for embedding, augmentation, and evaluation processes. Attributes: configuration: EmbeddingConfiguration object containing configuration parameters configuration_json: JSON string representation of the EmbeddingConfiguration package_loader: Loader responsible for importing required packages \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EmbeddingConfiguration , package_loader : BasePackageLoader = EmbeddingPackageLoader (), ): super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) EmbeddingPackageLoader Bases: ExtractionPackageLoader Package loader for embedding components. Extends the ExtractionPackageLoader to include embedding-specific packages. Responsible for dynamically loading embedding-related modules into the application. Attributes: logger \u2013 Logger instance for logging package loading operations Source code in src/embedding/bootstrap/initializer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class EmbeddingPackageLoader ( ExtractionPackageLoader ): \"\"\"Package loader for embedding components. Extends the ExtractionPackageLoader to include embedding-specific packages. Responsible for dynamically loading embedding-related modules into the application. Attributes: logger: Logger instance for logging package loading operations \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.embedding.vector_stores\" , \"src.embedding.embedding_models\" , \"src.embedding.splitters\" , \"src.embedding.embedders\" , \"src.embedding.orchestrators\" , ] ) load_packages () Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. Source code in src/embedding/bootstrap/initializer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def load_packages ( self ) -> None : \"\"\"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.embedding.vector_stores\" , \"src.embedding.embedding_models\" , \"src.embedding.splitters\" , \"src.embedding.embedders\" , \"src.embedding.orchestrators\" , ] )","title":"Initializer"},{"location":"src/embedding/bootstrap/initializer/#initializer","text":"This module contains functionality related to the the initializer module for embedding.bootstrap .","title":"Initializer"},{"location":"src/embedding/bootstrap/initializer/#initializer_1","text":"","title":"Initializer"},{"location":"src/embedding/bootstrap/initializer/#src.embedding.bootstrap.initializer.EmbeddingInitializer","text":"Bases: ExtractionInitializer Initializer for embedding, augmentation and evaluation processes. Extends the ExtractionInitializer to provide specialized initialization for embedding-related functionality. Responsible for binding embedding components to the dependency injection container and preparing the environment for embedding, augmentation, and evaluation processes. Attributes: configuration \u2013 EmbeddingConfiguration object containing configuration parameters configuration_json \u2013 JSON string representation of the EmbeddingConfiguration package_loader \u2013 Loader responsible for importing required packages Source code in src/embedding/bootstrap/initializer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class EmbeddingInitializer ( ExtractionInitializer ): \"\"\"Initializer for embedding, augmentation and evaluation processes. Extends the ExtractionInitializer to provide specialized initialization for embedding-related functionality. Responsible for binding embedding components to the dependency injection container and preparing the environment for embedding, augmentation, and evaluation processes. Attributes: configuration: EmbeddingConfiguration object containing configuration parameters configuration_json: JSON string representation of the EmbeddingConfiguration package_loader: Loader responsible for importing required packages \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EmbeddingConfiguration , package_loader : BasePackageLoader = EmbeddingPackageLoader (), ): super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , )","title":"EmbeddingInitializer"},{"location":"src/embedding/bootstrap/initializer/#src.embedding.bootstrap.initializer.EmbeddingPackageLoader","text":"Bases: ExtractionPackageLoader Package loader for embedding components. Extends the ExtractionPackageLoader to include embedding-specific packages. Responsible for dynamically loading embedding-related modules into the application. Attributes: logger \u2013 Logger instance for logging package loading operations Source code in src/embedding/bootstrap/initializer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class EmbeddingPackageLoader ( ExtractionPackageLoader ): \"\"\"Package loader for embedding components. Extends the ExtractionPackageLoader to include embedding-specific packages. Responsible for dynamically loading embedding-related modules into the application. Attributes: logger: Logger instance for logging package loading operations \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.embedding.vector_stores\" , \"src.embedding.embedding_models\" , \"src.embedding.splitters\" , \"src.embedding.embedders\" , \"src.embedding.orchestrators\" , ] )","title":"EmbeddingPackageLoader"},{"location":"src/embedding/bootstrap/initializer/#src.embedding.bootstrap.initializer.EmbeddingPackageLoader.load_packages","text":"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. Source code in src/embedding/bootstrap/initializer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def load_packages ( self ) -> None : \"\"\"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.embedding.vector_stores\" , \"src.embedding.embedding_models\" , \"src.embedding.splitters\" , \"src.embedding.embedders\" , \"src.embedding.orchestrators\" , ] )","title":"load_packages"},{"location":"src/embedding/bootstrap/configuration/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.bootstrap.configuration . Configuration EmbedderName Bases: str , Enum Enumeration of supported embedder implementations. Currently supports: - BASIC: The default basic embedder Source code in src/embedding/bootstrap/configuration/configuration.py 29 30 31 32 33 34 35 36 37 class EmbedderName ( str , Enum ): \"\"\" Enumeration of supported embedder implementations. Currently supports: - BASIC: The default basic embedder \"\"\" BASIC = \"basic\" EmbeddingConfiguration Bases: ExtractionConfiguration Complete configuration for the embedding process. Extends the extraction configuration with embedding-specific settings, creating a comprehensive configuration for the entire embedding pipeline. Source code in src/embedding/bootstrap/configuration/configuration.py 103 104 105 106 107 108 109 110 111 112 113 class EmbeddingConfiguration ( ExtractionConfiguration ): \"\"\" Complete configuration for the embedding process. Extends the extraction configuration with embedding-specific settings, creating a comprehensive configuration for the entire embedding pipeline. \"\"\" embedding : _EmbeddingConfiguration = Field ( ... , description = \"Configuration of the embedding process.\" ) EmbeddingOrchestratorName Bases: str , Enum Enumeration of supported embedding orchestrator types. Currently supports: - BASIC: The default basic orchestration strategy Source code in src/embedding/bootstrap/configuration/configuration.py 18 19 20 21 22 23 24 25 26 class EmbeddingOrchestratorName ( str , Enum ): \"\"\" Enumeration of supported embedding orchestrator types. Currently supports: - BASIC: The default basic orchestration strategy \"\"\" BASIC = \"basic\"","title":"Configuration"},{"location":"src/embedding/bootstrap/configuration/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.bootstrap.configuration .","title":"Configuration"},{"location":"src/embedding/bootstrap/configuration/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/bootstrap/configuration/configuration/#src.embedding.bootstrap.configuration.configuration.EmbedderName","text":"Bases: str , Enum Enumeration of supported embedder implementations. Currently supports: - BASIC: The default basic embedder Source code in src/embedding/bootstrap/configuration/configuration.py 29 30 31 32 33 34 35 36 37 class EmbedderName ( str , Enum ): \"\"\" Enumeration of supported embedder implementations. Currently supports: - BASIC: The default basic embedder \"\"\" BASIC = \"basic\"","title":"EmbedderName"},{"location":"src/embedding/bootstrap/configuration/configuration/#src.embedding.bootstrap.configuration.configuration.EmbeddingConfiguration","text":"Bases: ExtractionConfiguration Complete configuration for the embedding process. Extends the extraction configuration with embedding-specific settings, creating a comprehensive configuration for the entire embedding pipeline. Source code in src/embedding/bootstrap/configuration/configuration.py 103 104 105 106 107 108 109 110 111 112 113 class EmbeddingConfiguration ( ExtractionConfiguration ): \"\"\" Complete configuration for the embedding process. Extends the extraction configuration with embedding-specific settings, creating a comprehensive configuration for the entire embedding pipeline. \"\"\" embedding : _EmbeddingConfiguration = Field ( ... , description = \"Configuration of the embedding process.\" )","title":"EmbeddingConfiguration"},{"location":"src/embedding/bootstrap/configuration/configuration/#src.embedding.bootstrap.configuration.configuration.EmbeddingOrchestratorName","text":"Bases: str , Enum Enumeration of supported embedding orchestrator types. Currently supports: - BASIC: The default basic orchestration strategy Source code in src/embedding/bootstrap/configuration/configuration.py 18 19 20 21 22 23 24 25 26 class EmbeddingOrchestratorName ( str , Enum ): \"\"\" Enumeration of supported embedding orchestrator types. Currently supports: - BASIC: The default basic orchestration strategy \"\"\" BASIC = \"basic\"","title":"EmbeddingOrchestratorName"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/","text":"Embedding_model_configuration This module contains functionality related to the the embedding_model_configuration module for embedding.bootstrap.configuration . Embedding_model_configuration EmbeddingModelConfiguration Bases: BaseConfigurationWithSecrets Configuration class for embedding models. This class defines the necessary parameters and settings for configuring an embedding model, including provider information, model name, and tokenization settings. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class EmbeddingModelConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Configuration class for embedding models. This class defines the necessary parameters and settings for configuring an embedding model, including provider information, model name, and tokenization settings. \"\"\" provider : EmbeddingModelProviderName = Field ( ... , description = \"The provider of the embedding model.\" ) name : str = Field ( ... , description = \"The name of the embedding model.\" ) tokenizer_name : str = Field ( ... , description = \"The name of the tokenizer used by the embedding model.\" , ) batch_size : int = Field ( 64 , description = \"The batch size for embedding.\" ) splitter : Any = Field ( None , description = \"The splitter configuration for the embedding model.\" ) @field_validator ( \"splitter\" ) @classmethod def _validate_splitter ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate the splitter configuration. This method ensures that the provided splitter configuration is valid according to the SplitterConfigurationRegistry. Args: value: The splitter configuration value to validate. info: Validation context information. Returns: The validated splitter configuration. \"\"\" return super () . _validate ( value , info = info , registry = SplitterConfigurationRegistry , ) EmbeddingModelConfigurationRegistry Bases: ConfigurationRegistry Registry for embedding model configurations. This registry maps embedding model provider names to their respective configuration classes. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 69 70 71 72 73 74 75 76 class EmbeddingModelConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for embedding model configurations. This registry maps embedding model provider names to their respective configuration classes. \"\"\" _key_class : Type = EmbeddingModelProviderName EmbeddingModelProviderName Bases: str , Enum Enumeration of supported embedding model providers. This enum lists all the providers that can be used for embedding models. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 13 14 15 16 17 18 19 20 21 class EmbeddingModelProviderName ( str , Enum ): \"\"\"Enumeration of supported embedding model providers. This enum lists all the providers that can be used for embedding models. \"\"\" HUGGING_FACE = \"hugging_face\" OPENAI = \"openai\" VOYAGE = \"voyage\"","title":"Embedding_model_configuration"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#embedding_model_configuration","text":"This module contains functionality related to the the embedding_model_configuration module for embedding.bootstrap.configuration .","title":"Embedding_model_configuration"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#embedding_model_configuration_1","text":"","title":"Embedding_model_configuration"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#src.embedding.bootstrap.configuration.embedding_model_configuration.EmbeddingModelConfiguration","text":"Bases: BaseConfigurationWithSecrets Configuration class for embedding models. This class defines the necessary parameters and settings for configuring an embedding model, including provider information, model name, and tokenization settings. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class EmbeddingModelConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Configuration class for embedding models. This class defines the necessary parameters and settings for configuring an embedding model, including provider information, model name, and tokenization settings. \"\"\" provider : EmbeddingModelProviderName = Field ( ... , description = \"The provider of the embedding model.\" ) name : str = Field ( ... , description = \"The name of the embedding model.\" ) tokenizer_name : str = Field ( ... , description = \"The name of the tokenizer used by the embedding model.\" , ) batch_size : int = Field ( 64 , description = \"The batch size for embedding.\" ) splitter : Any = Field ( None , description = \"The splitter configuration for the embedding model.\" ) @field_validator ( \"splitter\" ) @classmethod def _validate_splitter ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate the splitter configuration. This method ensures that the provided splitter configuration is valid according to the SplitterConfigurationRegistry. Args: value: The splitter configuration value to validate. info: Validation context information. Returns: The validated splitter configuration. \"\"\" return super () . _validate ( value , info = info , registry = SplitterConfigurationRegistry , )","title":"EmbeddingModelConfiguration"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#src.embedding.bootstrap.configuration.embedding_model_configuration.EmbeddingModelConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for embedding model configurations. This registry maps embedding model provider names to their respective configuration classes. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 69 70 71 72 73 74 75 76 class EmbeddingModelConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for embedding model configurations. This registry maps embedding model provider names to their respective configuration classes. \"\"\" _key_class : Type = EmbeddingModelProviderName","title":"EmbeddingModelConfigurationRegistry"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#src.embedding.bootstrap.configuration.embedding_model_configuration.EmbeddingModelProviderName","text":"Bases: str , Enum Enumeration of supported embedding model providers. This enum lists all the providers that can be used for embedding models. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 13 14 15 16 17 18 19 20 21 class EmbeddingModelProviderName ( str , Enum ): \"\"\"Enumeration of supported embedding model providers. This enum lists all the providers that can be used for embedding models. \"\"\" HUGGING_FACE = \"hugging_face\" OPENAI = \"openai\" VOYAGE = \"voyage\"","title":"EmbeddingModelProviderName"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/","text":"Splitting_configuration This module contains functionality related to the the splitting_configuration module for embedding.bootstrap.configuration . Splitting_configuration SplitterConfiguration Bases: BaseConfiguration Base configuration class for text splitters. This class provides the foundation for defining specific configurations required by different text splitting algorithms. Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 19 20 21 22 23 24 25 26 27 class SplitterConfiguration ( BaseConfiguration ): \"\"\" Base configuration class for text splitters. This class provides the foundation for defining specific configurations required by different text splitting algorithms. \"\"\" pass SplitterConfigurationRegistry Bases: ConfigurationRegistry Registry for managing different splitter configurations. This registry maps SplitterName enum values to their corresponding configuration classes, allowing for dynamic configuration selection based on the chosen splitter type. Attributes: _key_class ( Type ) \u2013 The type used as keys in the registry (SplitterName). Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 31 32 33 34 35 36 37 38 39 40 41 42 43 class SplitterConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for managing different splitter configurations. This registry maps SplitterName enum values to their corresponding configuration classes, allowing for dynamic configuration selection based on the chosen splitter type. Attributes: _key_class: The type used as keys in the registry (SplitterName). \"\"\" _key_class : Type = SplitterName SplitterName Bases: str , Enum Enumeration of available text splitter types. Attributes: BASIC_MARKDOWN \u2013 A basic splitter for markdown documents. Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 8 9 10 11 12 13 14 15 16 class SplitterName ( str , Enum ): \"\"\" Enumeration of available text splitter types. Attributes: BASIC_MARKDOWN: A basic splitter for markdown documents. \"\"\" BASIC_MARKDOWN = \"basic-markdown\"","title":"Splitting_configuration"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#splitting_configuration","text":"This module contains functionality related to the the splitting_configuration module for embedding.bootstrap.configuration .","title":"Splitting_configuration"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#splitting_configuration_1","text":"","title":"Splitting_configuration"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#src.embedding.bootstrap.configuration.splitting_configuration.SplitterConfiguration","text":"Bases: BaseConfiguration Base configuration class for text splitters. This class provides the foundation for defining specific configurations required by different text splitting algorithms. Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 19 20 21 22 23 24 25 26 27 class SplitterConfiguration ( BaseConfiguration ): \"\"\" Base configuration class for text splitters. This class provides the foundation for defining specific configurations required by different text splitting algorithms. \"\"\" pass","title":"SplitterConfiguration"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#src.embedding.bootstrap.configuration.splitting_configuration.SplitterConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for managing different splitter configurations. This registry maps SplitterName enum values to their corresponding configuration classes, allowing for dynamic configuration selection based on the chosen splitter type. Attributes: _key_class ( Type ) \u2013 The type used as keys in the registry (SplitterName). Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 31 32 33 34 35 36 37 38 39 40 41 42 43 class SplitterConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for managing different splitter configurations. This registry maps SplitterName enum values to their corresponding configuration classes, allowing for dynamic configuration selection based on the chosen splitter type. Attributes: _key_class: The type used as keys in the registry (SplitterName). \"\"\" _key_class : Type = SplitterName","title":"SplitterConfigurationRegistry"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#src.embedding.bootstrap.configuration.splitting_configuration.SplitterName","text":"Bases: str , Enum Enumeration of available text splitter types. Attributes: BASIC_MARKDOWN \u2013 A basic splitter for markdown documents. Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 8 9 10 11 12 13 14 15 16 class SplitterName ( str , Enum ): \"\"\" Enumeration of available text splitter types. Attributes: BASIC_MARKDOWN: A basic splitter for markdown documents. \"\"\" BASIC_MARKDOWN = \"basic-markdown\"","title":"SplitterName"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/","text":"Vector_store_configuration This module contains functionality related to the the vector_store_configuration module for embedding.bootstrap.configuration . Vector_store_configuration VectorStoreConfiguration Bases: BaseConfigurationWithSecrets , ABC Abstract base configuration class for vector stores. Inherits from BaseConfigurationWithSecrets to handle secure configuration settings. All specific vector store configurations should inherit from this class. Attributes: port ( int ) \u2013 The port number for connecting to the vector store server. collection_name ( str ) \u2013 Name of the collection in the vector store. host ( str ) \u2013 Hostname or IP address of the vector store server. protocol ( Union [ Literal ['http'], Literal ['https']] ) \u2013 Connection protocol (http or https). Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class VectorStoreConfiguration ( BaseConfigurationWithSecrets , ABC ): \"\"\" Abstract base configuration class for vector stores. Inherits from BaseConfigurationWithSecrets to handle secure configuration settings. All specific vector store configurations should inherit from this class. Attributes: port: The port number for connecting to the vector store server. collection_name: Name of the collection in the vector store. host: Hostname or IP address of the vector store server. protocol: Connection protocol (http or https). \"\"\" port : int = Field ( ... , description = \"The port for the vector store.\" ) collection_name : str = Field ( ... , description = \"The collection name in the vector store.\" ) host : str = Field ( \"127.0.0.1\" , description = \"Host of the vector store server\" ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"The protocol for the vector store.\" ) VectorStoreConfigurationRegistry Bases: ConfigurationRegistry Registry for vector store configurations. Maps VectorStoreName enum values to their corresponding configuration classes. Used to retrieve the appropriate configuration based on the selected vector store. Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 50 51 52 53 54 55 56 57 58 class VectorStoreConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for vector store configurations. Maps VectorStoreName enum values to their corresponding configuration classes. Used to retrieve the appropriate configuration based on the selected vector store. \"\"\" _key_class = VectorStoreName VectorStoreName Bases: str , Enum Enumeration of supported vector store providers. Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 12 13 14 15 16 17 18 19 class VectorStoreName ( str , Enum ): \"\"\" Enumeration of supported vector store providers. \"\"\" QDRANT = \"qdrant\" CHROMA = \"chroma\" PGVECTOR = \"pgvector\"","title":"Vector_store_configuration"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#vector_store_configuration","text":"This module contains functionality related to the the vector_store_configuration module for embedding.bootstrap.configuration .","title":"Vector_store_configuration"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#vector_store_configuration_1","text":"","title":"Vector_store_configuration"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#src.embedding.bootstrap.configuration.vector_store_configuration.VectorStoreConfiguration","text":"Bases: BaseConfigurationWithSecrets , ABC Abstract base configuration class for vector stores. Inherits from BaseConfigurationWithSecrets to handle secure configuration settings. All specific vector store configurations should inherit from this class. Attributes: port ( int ) \u2013 The port number for connecting to the vector store server. collection_name ( str ) \u2013 Name of the collection in the vector store. host ( str ) \u2013 Hostname or IP address of the vector store server. protocol ( Union [ Literal ['http'], Literal ['https']] ) \u2013 Connection protocol (http or https). Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class VectorStoreConfiguration ( BaseConfigurationWithSecrets , ABC ): \"\"\" Abstract base configuration class for vector stores. Inherits from BaseConfigurationWithSecrets to handle secure configuration settings. All specific vector store configurations should inherit from this class. Attributes: port: The port number for connecting to the vector store server. collection_name: Name of the collection in the vector store. host: Hostname or IP address of the vector store server. protocol: Connection protocol (http or https). \"\"\" port : int = Field ( ... , description = \"The port for the vector store.\" ) collection_name : str = Field ( ... , description = \"The collection name in the vector store.\" ) host : str = Field ( \"127.0.0.1\" , description = \"Host of the vector store server\" ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"The protocol for the vector store.\" )","title":"VectorStoreConfiguration"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#src.embedding.bootstrap.configuration.vector_store_configuration.VectorStoreConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for vector store configurations. Maps VectorStoreName enum values to their corresponding configuration classes. Used to retrieve the appropriate configuration based on the selected vector store. Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 50 51 52 53 54 55 56 57 58 class VectorStoreConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for vector store configurations. Maps VectorStoreName enum values to their corresponding configuration classes. Used to retrieve the appropriate configuration based on the selected vector store. \"\"\" _key_class = VectorStoreName","title":"VectorStoreConfigurationRegistry"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#src.embedding.bootstrap.configuration.vector_store_configuration.VectorStoreName","text":"Bases: str , Enum Enumeration of supported vector store providers. Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 12 13 14 15 16 17 18 19 class VectorStoreName ( str , Enum ): \"\"\" Enumeration of supported vector store providers. \"\"\" QDRANT = \"qdrant\" CHROMA = \"chroma\" PGVECTOR = \"pgvector\"","title":"VectorStoreName"},{"location":"src/embedding/embedders/base_embedder/","text":"Base_embedder This module contains functionality related to the the base_embedder module for embedding.embedders . Base_embedder BaseEmbedder Bases: ABC Abstract base class for text node embedding operations. This class provides core functionality for embedding text nodes, with derived classes implementing specific embedding strategies. Source code in src/embedding/embedders/base_embedder.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class BaseEmbedder ( ABC ): \"\"\"Abstract base class for text node embedding operations. This class provides core functionality for embedding text nodes, with derived classes implementing specific embedding strategies. \"\"\" def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , ): \"\"\"Initialize embedder with configuration, model and storage. Args: configuration: Configuration parameters for the embedding process embedding_model: Model to generate text embeddings vector_store: Storage system for persisting embedding vectors \"\"\" super () . __init__ () self . configuration = configuration self . embedding_model = embedding_model self . vector_store = vector_store @abstractmethod def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Args: nodes: Collection of text nodes to embed Note: Implementation should modify nodes in-place by adding embeddings \"\"\" pass @abstractmethod def embed_flush ( self ) -> None : \"\"\"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note: Should be called at the end of processing to ensure no nodes remain unembedded in the buffer \"\"\" pass __init__ ( configuration , embedding_model , vector_store ) Initialize embedder with configuration, model and storage. Parameters: configuration ( EmbeddingConfiguration ) \u2013 Configuration parameters for the embedding process embedding_model ( BaseEmbedding ) \u2013 Model to generate text embeddings vector_store ( VectorStore ) \u2013 Storage system for persisting embedding vectors Source code in src/embedding/embedders/base_embedder.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , ): \"\"\"Initialize embedder with configuration, model and storage. Args: configuration: Configuration parameters for the embedding process embedding_model: Model to generate text embeddings vector_store: Storage system for persisting embedding vectors \"\"\" super () . __init__ () self . configuration = configuration self . embedding_model = embedding_model self . vector_store = vector_store embed ( nodes ) abstractmethod Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Parameters: nodes ( List [ TextNode ] ) \u2013 Collection of text nodes to embed Note Implementation should modify nodes in-place by adding embeddings Source code in src/embedding/embedders/base_embedder.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @abstractmethod def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Args: nodes: Collection of text nodes to embed Note: Implementation should modify nodes in-place by adding embeddings \"\"\" pass embed_flush () abstractmethod Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note Should be called at the end of processing to ensure no nodes remain unembedded in the buffer Source code in src/embedding/embedders/base_embedder.py 53 54 55 56 57 58 59 60 61 62 63 @abstractmethod def embed_flush ( self ) -> None : \"\"\"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note: Should be called at the end of processing to ensure no nodes remain unembedded in the buffer \"\"\" pass","title":"Base_embedder"},{"location":"src/embedding/embedders/base_embedder/#base_embedder","text":"This module contains functionality related to the the base_embedder module for embedding.embedders .","title":"Base_embedder"},{"location":"src/embedding/embedders/base_embedder/#base_embedder_1","text":"","title":"Base_embedder"},{"location":"src/embedding/embedders/base_embedder/#src.embedding.embedders.base_embedder.BaseEmbedder","text":"Bases: ABC Abstract base class for text node embedding operations. This class provides core functionality for embedding text nodes, with derived classes implementing specific embedding strategies. Source code in src/embedding/embedders/base_embedder.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class BaseEmbedder ( ABC ): \"\"\"Abstract base class for text node embedding operations. This class provides core functionality for embedding text nodes, with derived classes implementing specific embedding strategies. \"\"\" def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , ): \"\"\"Initialize embedder with configuration, model and storage. Args: configuration: Configuration parameters for the embedding process embedding_model: Model to generate text embeddings vector_store: Storage system for persisting embedding vectors \"\"\" super () . __init__ () self . configuration = configuration self . embedding_model = embedding_model self . vector_store = vector_store @abstractmethod def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Args: nodes: Collection of text nodes to embed Note: Implementation should modify nodes in-place by adding embeddings \"\"\" pass @abstractmethod def embed_flush ( self ) -> None : \"\"\"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note: Should be called at the end of processing to ensure no nodes remain unembedded in the buffer \"\"\" pass","title":"BaseEmbedder"},{"location":"src/embedding/embedders/base_embedder/#src.embedding.embedders.base_embedder.BaseEmbedder.__init__","text":"Initialize embedder with configuration, model and storage. Parameters: configuration ( EmbeddingConfiguration ) \u2013 Configuration parameters for the embedding process embedding_model ( BaseEmbedding ) \u2013 Model to generate text embeddings vector_store ( VectorStore ) \u2013 Storage system for persisting embedding vectors Source code in src/embedding/embedders/base_embedder.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , ): \"\"\"Initialize embedder with configuration, model and storage. Args: configuration: Configuration parameters for the embedding process embedding_model: Model to generate text embeddings vector_store: Storage system for persisting embedding vectors \"\"\" super () . __init__ () self . configuration = configuration self . embedding_model = embedding_model self . vector_store = vector_store","title":"__init__"},{"location":"src/embedding/embedders/base_embedder/#src.embedding.embedders.base_embedder.BaseEmbedder.embed","text":"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Parameters: nodes ( List [ TextNode ] ) \u2013 Collection of text nodes to embed Note Implementation should modify nodes in-place by adding embeddings Source code in src/embedding/embedders/base_embedder.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @abstractmethod def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Args: nodes: Collection of text nodes to embed Note: Implementation should modify nodes in-place by adding embeddings \"\"\" pass","title":"embed"},{"location":"src/embedding/embedders/base_embedder/#src.embedding.embedders.base_embedder.BaseEmbedder.embed_flush","text":"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note Should be called at the end of processing to ensure no nodes remain unembedded in the buffer Source code in src/embedding/embedders/base_embedder.py 53 54 55 56 57 58 59 60 61 62 63 @abstractmethod def embed_flush ( self ) -> None : \"\"\"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note: Should be called at the end of processing to ensure no nodes remain unembedded in the buffer \"\"\" pass","title":"embed_flush"},{"location":"src/embedding/embedders/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.embedders . Registry EmbedderRegistry Bases: Registry Registry for managing embedding models. This registry stores and provides access to embedding model implementations based on their corresponding EmbedderName enumeration values. It extends the base Registry class to provide type-safe access to embedders. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, which is EmbedderName enum in this case. Source code in src/embedding/embedders/registry.py 7 8 9 10 11 12 13 14 15 16 17 18 19 class EmbedderRegistry ( Registry ): \"\"\"Registry for managing embedding models. This registry stores and provides access to embedding model implementations based on their corresponding EmbedderName enumeration values. It extends the base Registry class to provide type-safe access to embedders. Attributes: _key_class (Type): The class type used as keys in the registry, which is EmbedderName enum in this case. \"\"\" _key_class : Type = EmbedderName","title":"Registry"},{"location":"src/embedding/embedders/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.embedders .","title":"Registry"},{"location":"src/embedding/embedders/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/embedders/registry/#src.embedding.embedders.registry.EmbedderRegistry","text":"Bases: Registry Registry for managing embedding models. This registry stores and provides access to embedding model implementations based on their corresponding EmbedderName enumeration values. It extends the base Registry class to provide type-safe access to embedders. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, which is EmbedderName enum in this case. Source code in src/embedding/embedders/registry.py 7 8 9 10 11 12 13 14 15 16 17 18 19 class EmbedderRegistry ( Registry ): \"\"\"Registry for managing embedding models. This registry stores and provides access to embedding model implementations based on their corresponding EmbedderName enumeration values. It extends the base Registry class to provide type-safe access to embedders. Attributes: _key_class (Type): The class type used as keys in the registry, which is EmbedderName enum in this case. \"\"\" _key_class : Type = EmbedderName","title":"EmbedderRegistry"},{"location":"src/embedding/embedders/basic/embedder/","text":"Embedder This module contains functionality related to the the embedder module for embedding.embedders.basic . Embedder BasicEmbedder Bases: BaseEmbedder Implementation of text node embedding operations. Handles batch embedding generation and vector store persistence for text nodes. Source code in src/embedding/embedders/basic/embedder.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class BasicEmbedder ( BaseEmbedder ): \"\"\"Implementation of text node embedding operations. Handles batch embedding generation and vector store persistence for text nodes. \"\"\" def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize BasicEmbedder with model and storage. Args: configuration: Configuration for embedding process embedding_model: Model to generate embeddings vector_store: Storage for embedding vectors logger: Logger instance for tracking operations \"\"\" super () . __init__ ( configuration , embedding_model , vector_store ) self . logger = logger self . batch_size = configuration . embedding . embedding_model . batch_size self . current_nodes_batch = [] def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Args: nodes: Collection of text nodes to embed Note: Modifies nodes in-place by setting embedding attribute \"\"\" self . current_nodes_batch . extend ( nodes ) while len ( self . current_nodes_batch ) >= self . batch_size : batch = self . current_nodes_batch [: self . batch_size ] self . _embed_nodes_batch ( batch ) self . _save_nodes_batch ( batch ) self . current_nodes_batch = self . current_nodes_batch [ self . batch_size : ] def embed_flush ( self ) -> None : \"\"\"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. \"\"\" if self . current_nodes_batch : self . _embed_nodes_batch ( self . current_nodes_batch ) self . _save_nodes_batch ( self . current_nodes_batch ) self . current_nodes_batch = [] def _embed_nodes_batch ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for a batch of text nodes. Extracts content from each node, generates embeddings using the embedding model, and assigns the resulting embeddings back to each node. Args: nodes: Batch of nodes to generate embeddings for \"\"\" self . logger . info ( f \"Embedding batch of { len ( nodes ) } nodes.\" ) nodes_contents = [ node . get_content ( metadata_mode = MetadataMode . EMBED ) for node in nodes ] nodes_embeddings = self . embedding_model . get_text_embedding_batch ( nodes_contents , ) for node , node_embedding in zip ( nodes , nodes_embeddings ): node . embedding = node_embedding def _save_nodes_batch ( self , nodes : List [ TextNode ]) -> None : \"\"\"Save batch of text nodes to vector store. Creates a storage context with the configured vector store and stores the nodes with their embeddings using VectorStoreIndex. Args: nodes: Batch of nodes to save to the vector store \"\"\" self . logger . info ( f \"Saving batch of { len ( nodes ) } nodes to vector store.\" ) self . vector_store . add ( nodes ) __init__ ( configuration , embedding_model , vector_store , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize BasicEmbedder with model and storage. Parameters: configuration ( EmbeddingConfiguration ) \u2013 Configuration for embedding process embedding_model ( BaseEmbedding ) \u2013 Model to generate embeddings vector_store ( VectorStore ) \u2013 Storage for embedding vectors logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for tracking operations Source code in src/embedding/embedders/basic/embedder.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize BasicEmbedder with model and storage. Args: configuration: Configuration for embedding process embedding_model: Model to generate embeddings vector_store: Storage for embedding vectors logger: Logger instance for tracking operations \"\"\" super () . __init__ ( configuration , embedding_model , vector_store ) self . logger = logger self . batch_size = configuration . embedding . embedding_model . batch_size self . current_nodes_batch = [] embed ( nodes ) Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Parameters: nodes ( List [ TextNode ] ) \u2013 Collection of text nodes to embed Note Modifies nodes in-place by setting embedding attribute Source code in src/embedding/embedders/basic/embedder.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Args: nodes: Collection of text nodes to embed Note: Modifies nodes in-place by setting embedding attribute \"\"\" self . current_nodes_batch . extend ( nodes ) while len ( self . current_nodes_batch ) >= self . batch_size : batch = self . current_nodes_batch [: self . batch_size ] self . _embed_nodes_batch ( batch ) self . _save_nodes_batch ( batch ) self . current_nodes_batch = self . current_nodes_batch [ self . batch_size : ] embed_flush () Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. Source code in src/embedding/embedders/basic/embedder.py 68 69 70 71 72 73 74 75 76 77 78 79 def embed_flush ( self ) -> None : \"\"\"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. \"\"\" if self . current_nodes_batch : self . _embed_nodes_batch ( self . current_nodes_batch ) self . _save_nodes_batch ( self . current_nodes_batch ) self . current_nodes_batch = [] BasicEmbedderFactory Bases: Factory Source code in src/embedding/embedders/basic/embedder.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class BasicEmbedderFactory ( Factory ): _configuration_class : Type = EmbeddingConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingConfiguration ) -> BasicEmbedder : \"\"\"Creates a configured BasicEmbedder instance. Initializes embedding model and vector store components based on the provided configuration settings. Args: configuration: Settings for embedding process configuration Returns: BasicEmbedder: Configured embedder instance ready for document processing \"\"\" embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) vector_store_config = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_config . name ) . create ( vector_store_config ) return BasicEmbedder ( configuration = configuration , embedding_model = embedding_model , vector_store = vector_store , )","title":"Embedder"},{"location":"src/embedding/embedders/basic/embedder/#embedder","text":"This module contains functionality related to the the embedder module for embedding.embedders.basic .","title":"Embedder"},{"location":"src/embedding/embedders/basic/embedder/#embedder_1","text":"","title":"Embedder"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedder","text":"Bases: BaseEmbedder Implementation of text node embedding operations. Handles batch embedding generation and vector store persistence for text nodes. Source code in src/embedding/embedders/basic/embedder.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class BasicEmbedder ( BaseEmbedder ): \"\"\"Implementation of text node embedding operations. Handles batch embedding generation and vector store persistence for text nodes. \"\"\" def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize BasicEmbedder with model and storage. Args: configuration: Configuration for embedding process embedding_model: Model to generate embeddings vector_store: Storage for embedding vectors logger: Logger instance for tracking operations \"\"\" super () . __init__ ( configuration , embedding_model , vector_store ) self . logger = logger self . batch_size = configuration . embedding . embedding_model . batch_size self . current_nodes_batch = [] def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Args: nodes: Collection of text nodes to embed Note: Modifies nodes in-place by setting embedding attribute \"\"\" self . current_nodes_batch . extend ( nodes ) while len ( self . current_nodes_batch ) >= self . batch_size : batch = self . current_nodes_batch [: self . batch_size ] self . _embed_nodes_batch ( batch ) self . _save_nodes_batch ( batch ) self . current_nodes_batch = self . current_nodes_batch [ self . batch_size : ] def embed_flush ( self ) -> None : \"\"\"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. \"\"\" if self . current_nodes_batch : self . _embed_nodes_batch ( self . current_nodes_batch ) self . _save_nodes_batch ( self . current_nodes_batch ) self . current_nodes_batch = [] def _embed_nodes_batch ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for a batch of text nodes. Extracts content from each node, generates embeddings using the embedding model, and assigns the resulting embeddings back to each node. Args: nodes: Batch of nodes to generate embeddings for \"\"\" self . logger . info ( f \"Embedding batch of { len ( nodes ) } nodes.\" ) nodes_contents = [ node . get_content ( metadata_mode = MetadataMode . EMBED ) for node in nodes ] nodes_embeddings = self . embedding_model . get_text_embedding_batch ( nodes_contents , ) for node , node_embedding in zip ( nodes , nodes_embeddings ): node . embedding = node_embedding def _save_nodes_batch ( self , nodes : List [ TextNode ]) -> None : \"\"\"Save batch of text nodes to vector store. Creates a storage context with the configured vector store and stores the nodes with their embeddings using VectorStoreIndex. Args: nodes: Batch of nodes to save to the vector store \"\"\" self . logger . info ( f \"Saving batch of { len ( nodes ) } nodes to vector store.\" ) self . vector_store . add ( nodes )","title":"BasicEmbedder"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedder.__init__","text":"Initialize BasicEmbedder with model and storage. Parameters: configuration ( EmbeddingConfiguration ) \u2013 Configuration for embedding process embedding_model ( BaseEmbedding ) \u2013 Model to generate embeddings vector_store ( VectorStore ) \u2013 Storage for embedding vectors logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for tracking operations Source code in src/embedding/embedders/basic/embedder.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize BasicEmbedder with model and storage. Args: configuration: Configuration for embedding process embedding_model: Model to generate embeddings vector_store: Storage for embedding vectors logger: Logger instance for tracking operations \"\"\" super () . __init__ ( configuration , embedding_model , vector_store ) self . logger = logger self . batch_size = configuration . embedding . embedding_model . batch_size self . current_nodes_batch = []","title":"__init__"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedder.embed","text":"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Parameters: nodes ( List [ TextNode ] ) \u2013 Collection of text nodes to embed Note Modifies nodes in-place by setting embedding attribute Source code in src/embedding/embedders/basic/embedder.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Args: nodes: Collection of text nodes to embed Note: Modifies nodes in-place by setting embedding attribute \"\"\" self . current_nodes_batch . extend ( nodes ) while len ( self . current_nodes_batch ) >= self . batch_size : batch = self . current_nodes_batch [: self . batch_size ] self . _embed_nodes_batch ( batch ) self . _save_nodes_batch ( batch ) self . current_nodes_batch = self . current_nodes_batch [ self . batch_size : ]","title":"embed"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedder.embed_flush","text":"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. Source code in src/embedding/embedders/basic/embedder.py 68 69 70 71 72 73 74 75 76 77 78 79 def embed_flush ( self ) -> None : \"\"\"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. \"\"\" if self . current_nodes_batch : self . _embed_nodes_batch ( self . current_nodes_batch ) self . _save_nodes_batch ( self . current_nodes_batch ) self . current_nodes_batch = []","title":"embed_flush"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedderFactory","text":"Bases: Factory Source code in src/embedding/embedders/basic/embedder.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class BasicEmbedderFactory ( Factory ): _configuration_class : Type = EmbeddingConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingConfiguration ) -> BasicEmbedder : \"\"\"Creates a configured BasicEmbedder instance. Initializes embedding model and vector store components based on the provided configuration settings. Args: configuration: Settings for embedding process configuration Returns: BasicEmbedder: Configured embedder instance ready for document processing \"\"\" embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) vector_store_config = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_config . name ) . create ( vector_store_config ) return BasicEmbedder ( configuration = configuration , embedding_model = embedding_model , vector_store = vector_store , )","title":"BasicEmbedderFactory"},{"location":"src/embedding/embedding_models/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.embedding_models . Registry EmbeddingModelRegistry Bases: Registry Registry for embedding models that maps provider names to their implementations. This registry uses EmbeddingModelProviderName as keys to store and retrieve embedding model implementations, allowing the system to support multiple embedding model providers while providing a uniform interface for registration and lookup. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingModelProviderName enum. Source code in src/embedding/embedding_models/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class EmbeddingModelRegistry ( Registry ): \"\"\" Registry for embedding models that maps provider names to their implementations. This registry uses EmbeddingModelProviderName as keys to store and retrieve embedding model implementations, allowing the system to support multiple embedding model providers while providing a uniform interface for registration and lookup. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingModelProviderName enum. \"\"\" _key_class : Type = EmbeddingModelProviderName EmbeddingModelTokenizerRegistry Bases: Registry Registry for embedding model tokenizers that maps provider names to their respective tokenizers. This registry uses EmbeddingModelProviderName as keys to store and retrieve tokenizer implementations that are compatible with specific embedding models. Tokenizers are used to preprocess text before embedding generation. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingModelProviderName enum. Source code in src/embedding/embedding_models/registry.py 25 26 27 28 29 30 31 32 33 34 35 36 37 class EmbeddingModelTokenizerRegistry ( Registry ): \"\"\" Registry for embedding model tokenizers that maps provider names to their respective tokenizers. This registry uses EmbeddingModelProviderName as keys to store and retrieve tokenizer implementations that are compatible with specific embedding models. Tokenizers are used to preprocess text before embedding generation. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingModelProviderName enum. \"\"\" _key_class : Type = EmbeddingModelProviderName","title":"Registry"},{"location":"src/embedding/embedding_models/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.embedding_models .","title":"Registry"},{"location":"src/embedding/embedding_models/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/embedding_models/registry/#src.embedding.embedding_models.registry.EmbeddingModelRegistry","text":"Bases: Registry Registry for embedding models that maps provider names to their implementations. This registry uses EmbeddingModelProviderName as keys to store and retrieve embedding model implementations, allowing the system to support multiple embedding model providers while providing a uniform interface for registration and lookup. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingModelProviderName enum. Source code in src/embedding/embedding_models/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class EmbeddingModelRegistry ( Registry ): \"\"\" Registry for embedding models that maps provider names to their implementations. This registry uses EmbeddingModelProviderName as keys to store and retrieve embedding model implementations, allowing the system to support multiple embedding model providers while providing a uniform interface for registration and lookup. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingModelProviderName enum. \"\"\" _key_class : Type = EmbeddingModelProviderName","title":"EmbeddingModelRegistry"},{"location":"src/embedding/embedding_models/registry/#src.embedding.embedding_models.registry.EmbeddingModelTokenizerRegistry","text":"Bases: Registry Registry for embedding model tokenizers that maps provider names to their respective tokenizers. This registry uses EmbeddingModelProviderName as keys to store and retrieve tokenizer implementations that are compatible with specific embedding models. Tokenizers are used to preprocess text before embedding generation. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingModelProviderName enum. Source code in src/embedding/embedding_models/registry.py 25 26 27 28 29 30 31 32 33 34 35 36 37 class EmbeddingModelTokenizerRegistry ( Registry ): \"\"\" Registry for embedding model tokenizers that maps provider names to their respective tokenizers. This registry uses EmbeddingModelProviderName as keys to store and retrieve tokenizer implementations that are compatible with specific embedding models. Tokenizers are used to preprocess text before embedding generation. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingModelProviderName enum. \"\"\" _key_class : Type = EmbeddingModelProviderName","title":"EmbeddingModelTokenizerRegistry"},{"location":"src/embedding/embedding_models/hugging_face/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.embedding_models.hugging_face . Configuration HuggingFaceEmbeddingModelConfiguration Bases: EmbeddingModelConfiguration Configuration class for Hugging Face embedding models. This class extends the base EmbeddingModelConfiguration to provide specific configuration options for Hugging Face embedding models. Source code in src/embedding/embedding_models/hugging_face/configuration.py 11 12 13 14 15 16 17 18 19 20 21 class HuggingFaceEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\" Configuration class for Hugging Face embedding models. This class extends the base EmbeddingModelConfiguration to provide specific configuration options for Hugging Face embedding models. \"\"\" provider : Literal [ EmbeddingModelProviderName . HUGGING_FACE ] = Field ( ... , description = \"The provider of the embedding model.\" )","title":"Configuration"},{"location":"src/embedding/embedding_models/hugging_face/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.embedding_models.hugging_face .","title":"Configuration"},{"location":"src/embedding/embedding_models/hugging_face/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/embedding_models/hugging_face/configuration/#src.embedding.embedding_models.hugging_face.configuration.HuggingFaceEmbeddingModelConfiguration","text":"Bases: EmbeddingModelConfiguration Configuration class for Hugging Face embedding models. This class extends the base EmbeddingModelConfiguration to provide specific configuration options for Hugging Face embedding models. Source code in src/embedding/embedding_models/hugging_face/configuration.py 11 12 13 14 15 16 17 18 19 20 21 class HuggingFaceEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\" Configuration class for Hugging Face embedding models. This class extends the base EmbeddingModelConfiguration to provide specific configuration options for Hugging Face embedding models. \"\"\" provider : Literal [ EmbeddingModelProviderName . HUGGING_FACE ] = Field ( ... , description = \"The provider of the embedding model.\" )","title":"HuggingFaceEmbeddingModelConfiguration"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/","text":"Embedding_model This module contains functionality related to the the embedding_model module for embedding.embedding_models.hugging_face . Embedding_model HuggingFaceEmbeddingModelFactory Bases: SingletonFactory Factory for creating configured HuggingFace embedding models. This singleton factory creates and configures HuggingFaceEmbedding instances based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating instances. Source code in src/embedding/embedding_models/hugging_face/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class HuggingFaceEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured HuggingFace embedding models. This singleton factory creates and configures HuggingFaceEmbedding instances based on the provided configuration. Attributes: _configuration_class (Type): The configuration class used for creating instances. \"\"\" _configuration_class : Type = HuggingFaceEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : HuggingFaceEmbeddingModelConfiguration ) -> HuggingFaceEmbedding : \"\"\"Creates a HuggingFaceEmbedding instance based on provided configuration. Args: configuration: HuggingFace embedding model configuration. Returns: HuggingFaceEmbedding: Configured embedding model instance. \"\"\" return HuggingFaceEmbedding ( model_name = configuration . name , embed_batch_size = configuration . batch_size , ) HuggingFaceEmbeddingModelTokenizerFactory Bases: SingletonFactory Factory for creating HuggingFace tokenizer functions. This singleton factory creates and configures tokenizer functions for HuggingFace models based on the provided configuration. Source code in src/embedding/embedding_models/hugging_face/embedding_model.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class HuggingFaceEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating HuggingFace tokenizer functions. This singleton factory creates and configures tokenizer functions for HuggingFace models based on the provided configuration. \"\"\" _configuration_class : Type = HuggingFaceEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : HuggingFaceEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function based on provided configuration. Args: configuration: HuggingFace embedding model configuration. Returns: Callable: A tokenize function from the configured tokenizer. \"\"\" return AutoTokenizer . from_pretrained ( configuration . tokenizer_name ) . tokenize","title":"Embedding_model"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/#embedding_model","text":"This module contains functionality related to the the embedding_model module for embedding.embedding_models.hugging_face .","title":"Embedding_model"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/#embedding_model_1","text":"","title":"Embedding_model"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/#src.embedding.embedding_models.hugging_face.embedding_model.HuggingFaceEmbeddingModelFactory","text":"Bases: SingletonFactory Factory for creating configured HuggingFace embedding models. This singleton factory creates and configures HuggingFaceEmbedding instances based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating instances. Source code in src/embedding/embedding_models/hugging_face/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class HuggingFaceEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured HuggingFace embedding models. This singleton factory creates and configures HuggingFaceEmbedding instances based on the provided configuration. Attributes: _configuration_class (Type): The configuration class used for creating instances. \"\"\" _configuration_class : Type = HuggingFaceEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : HuggingFaceEmbeddingModelConfiguration ) -> HuggingFaceEmbedding : \"\"\"Creates a HuggingFaceEmbedding instance based on provided configuration. Args: configuration: HuggingFace embedding model configuration. Returns: HuggingFaceEmbedding: Configured embedding model instance. \"\"\" return HuggingFaceEmbedding ( model_name = configuration . name , embed_batch_size = configuration . batch_size , )","title":"HuggingFaceEmbeddingModelFactory"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/#src.embedding.embedding_models.hugging_face.embedding_model.HuggingFaceEmbeddingModelTokenizerFactory","text":"Bases: SingletonFactory Factory for creating HuggingFace tokenizer functions. This singleton factory creates and configures tokenizer functions for HuggingFace models based on the provided configuration. Source code in src/embedding/embedding_models/hugging_face/embedding_model.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class HuggingFaceEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating HuggingFace tokenizer functions. This singleton factory creates and configures tokenizer functions for HuggingFace models based on the provided configuration. \"\"\" _configuration_class : Type = HuggingFaceEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : HuggingFaceEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function based on provided configuration. Args: configuration: HuggingFace embedding model configuration. Returns: Callable: A tokenize function from the configured tokenizer. \"\"\" return AutoTokenizer . from_pretrained ( configuration . tokenizer_name ) . tokenize","title":"HuggingFaceEmbeddingModelTokenizerFactory"},{"location":"src/embedding/embedding_models/openai/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.embedding_models.openai . Configuration OpenAIEmbeddingModelConfiguration Bases: EmbeddingModelConfiguration Configuration for OpenAI embedding models. This class defines the configuration parameters needed to use OpenAI embedding models, including API credentials, model parameters, and request size limitations. Source code in src/embedding/embedding_models/openai/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class OpenAIEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\" Configuration for OpenAI embedding models. This class defines the configuration parameters needed to use OpenAI embedding models, including API credentials, model parameters, and request size limitations. \"\"\" class Secrets ( BaseSecrets ): \"\"\" Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__OPEN_AI__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) provider : Literal [ EmbeddingModelProviderName . OPENAI ] = Field ( ... , description = \"The provider of the embedding model.\" ) max_request_size_in_tokens : int = Field ( 8191 , description = \"Maximum size of the request in tokens.\" , ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) def model_post_init ( self , __context : dict ): \"\"\" Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Args: __context: Context information provided by Pydantic during initialization \"\"\" super () . model_post_init ( __context ) if self . splitter : self . batch_size = ( self . max_request_size_in_tokens // self . splitter . chunk_size_in_tokens ) Secrets Bases: BaseSecrets Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. Source code in src/embedding/embedding_models/openai/configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Secrets ( BaseSecrets ): \"\"\" Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__OPEN_AI__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) model_post_init ( __context ) Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Parameters: __context ( dict ) \u2013 Context information provided by Pydantic during initialization Source code in src/embedding/embedding_models/openai/configuration.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def model_post_init ( self , __context : dict ): \"\"\" Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Args: __context: Context information provided by Pydantic during initialization \"\"\" super () . model_post_init ( __context ) if self . splitter : self . batch_size = ( self . max_request_size_in_tokens // self . splitter . chunk_size_in_tokens )","title":"Configuration"},{"location":"src/embedding/embedding_models/openai/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.embedding_models.openai .","title":"Configuration"},{"location":"src/embedding/embedding_models/openai/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/embedding_models/openai/configuration/#src.embedding.embedding_models.openai.configuration.OpenAIEmbeddingModelConfiguration","text":"Bases: EmbeddingModelConfiguration Configuration for OpenAI embedding models. This class defines the configuration parameters needed to use OpenAI embedding models, including API credentials, model parameters, and request size limitations. Source code in src/embedding/embedding_models/openai/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class OpenAIEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\" Configuration for OpenAI embedding models. This class defines the configuration parameters needed to use OpenAI embedding models, including API credentials, model parameters, and request size limitations. \"\"\" class Secrets ( BaseSecrets ): \"\"\" Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__OPEN_AI__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) provider : Literal [ EmbeddingModelProviderName . OPENAI ] = Field ( ... , description = \"The provider of the embedding model.\" ) max_request_size_in_tokens : int = Field ( 8191 , description = \"Maximum size of the request in tokens.\" , ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) def model_post_init ( self , __context : dict ): \"\"\" Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Args: __context: Context information provided by Pydantic during initialization \"\"\" super () . model_post_init ( __context ) if self . splitter : self . batch_size = ( self . max_request_size_in_tokens // self . splitter . chunk_size_in_tokens )","title":"OpenAIEmbeddingModelConfiguration"},{"location":"src/embedding/embedding_models/openai/configuration/#src.embedding.embedding_models.openai.configuration.OpenAIEmbeddingModelConfiguration.Secrets","text":"Bases: BaseSecrets Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. Source code in src/embedding/embedding_models/openai/configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Secrets ( BaseSecrets ): \"\"\" Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__OPEN_AI__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" )","title":"Secrets"},{"location":"src/embedding/embedding_models/openai/configuration/#src.embedding.embedding_models.openai.configuration.OpenAIEmbeddingModelConfiguration.model_post_init","text":"Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Parameters: __context ( dict ) \u2013 Context information provided by Pydantic during initialization Source code in src/embedding/embedding_models/openai/configuration.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def model_post_init ( self , __context : dict ): \"\"\" Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Args: __context: Context information provided by Pydantic during initialization \"\"\" super () . model_post_init ( __context ) if self . splitter : self . batch_size = ( self . max_request_size_in_tokens // self . splitter . chunk_size_in_tokens )","title":"model_post_init"},{"location":"src/embedding/embedding_models/openai/embedding_model/","text":"Embedding_model This module contains functionality related to the the embedding_model module for embedding.embedding_models.openai . Embedding_model OpenAIEmbeddingModelFactory Bases: SingletonFactory Factory for creating configured OpenAI embedding models. This factory creates singleton instances of OpenAI embedding models based on the provided configuration. Source code in src/embedding/embedding_models/openai/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class OpenAIEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured OpenAI embedding models. This factory creates singleton instances of OpenAI embedding models based on the provided configuration. \"\"\" _configuration_class : Type = OpenAIEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : OpenAIEmbeddingModelConfiguration ) -> OpenAIEmbedding : \"\"\"Creates an OpenAI embedding model based on provided configuration. Args: configuration: OpenAI embedding model configuration. Returns: OpenAIEmbedding: Configured OpenAI embedding model instance. \"\"\" return OpenAIEmbedding ( api_key = configuration . secrets . api_key . get_secret_value (), model_name = configuration . name , embed_batch_size = configuration . batch_size , ) OpenAIEmbeddingModelTokenizerFactory Bases: SingletonFactory Factory for creating OpenAI tokenizer functions. This factory creates singleton instances of OpenAI tokenizer functions based on the provided configuration. Source code in src/embedding/embedding_models/openai/embedding_model.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class OpenAIEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating OpenAI tokenizer functions. This factory creates singleton instances of OpenAI tokenizer functions based on the provided configuration. \"\"\" _configuration_class : Type = OpenAIEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : OpenAIEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function for OpenAI models based on provided configuration. Args: configuration: OpenAI embedding model configuration. Returns: Callable: A tokenizer function that converts text to token IDs. \"\"\" return tiktoken . encoding_for_model ( configuration . tokenizer_name ) . encode","title":"Embedding_model"},{"location":"src/embedding/embedding_models/openai/embedding_model/#embedding_model","text":"This module contains functionality related to the the embedding_model module for embedding.embedding_models.openai .","title":"Embedding_model"},{"location":"src/embedding/embedding_models/openai/embedding_model/#embedding_model_1","text":"","title":"Embedding_model"},{"location":"src/embedding/embedding_models/openai/embedding_model/#src.embedding.embedding_models.openai.embedding_model.OpenAIEmbeddingModelFactory","text":"Bases: SingletonFactory Factory for creating configured OpenAI embedding models. This factory creates singleton instances of OpenAI embedding models based on the provided configuration. Source code in src/embedding/embedding_models/openai/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class OpenAIEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured OpenAI embedding models. This factory creates singleton instances of OpenAI embedding models based on the provided configuration. \"\"\" _configuration_class : Type = OpenAIEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : OpenAIEmbeddingModelConfiguration ) -> OpenAIEmbedding : \"\"\"Creates an OpenAI embedding model based on provided configuration. Args: configuration: OpenAI embedding model configuration. Returns: OpenAIEmbedding: Configured OpenAI embedding model instance. \"\"\" return OpenAIEmbedding ( api_key = configuration . secrets . api_key . get_secret_value (), model_name = configuration . name , embed_batch_size = configuration . batch_size , )","title":"OpenAIEmbeddingModelFactory"},{"location":"src/embedding/embedding_models/openai/embedding_model/#src.embedding.embedding_models.openai.embedding_model.OpenAIEmbeddingModelTokenizerFactory","text":"Bases: SingletonFactory Factory for creating OpenAI tokenizer functions. This factory creates singleton instances of OpenAI tokenizer functions based on the provided configuration. Source code in src/embedding/embedding_models/openai/embedding_model.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class OpenAIEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating OpenAI tokenizer functions. This factory creates singleton instances of OpenAI tokenizer functions based on the provided configuration. \"\"\" _configuration_class : Type = OpenAIEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : OpenAIEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function for OpenAI models based on provided configuration. Args: configuration: OpenAI embedding model configuration. Returns: Callable: A tokenizer function that converts text to token IDs. \"\"\" return tiktoken . encoding_for_model ( configuration . tokenizer_name ) . encode","title":"OpenAIEmbeddingModelTokenizerFactory"},{"location":"src/embedding/embedding_models/voyage/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.embedding_models.voyage . Configuration VoyageEmbeddingModelConfiguration Bases: EmbeddingModelConfiguration Configuration for Voyage embedding models. This class represents the configuration needed for using Voyage AI embedding models, including authentication credentials and model settings. Source code in src/embedding/embedding_models/voyage/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VoyageEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\"Configuration for Voyage embedding models. This class represents the configuration needed for using Voyage AI embedding models, including authentication credentials and model settings. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__VOYAGE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) provider : Literal [ EmbeddingModelProviderName . VOYAGE ] = Field ( ... , description = \"The provider of the embedding model.\" ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) Secrets Bases: BaseSecrets Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. Source code in src/embedding/embedding_models/voyage/configuration.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Secrets ( BaseSecrets ): \"\"\"Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__VOYAGE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" )","title":"Configuration"},{"location":"src/embedding/embedding_models/voyage/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.embedding_models.voyage .","title":"Configuration"},{"location":"src/embedding/embedding_models/voyage/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/embedding_models/voyage/configuration/#src.embedding.embedding_models.voyage.configuration.VoyageEmbeddingModelConfiguration","text":"Bases: EmbeddingModelConfiguration Configuration for Voyage embedding models. This class represents the configuration needed for using Voyage AI embedding models, including authentication credentials and model settings. Source code in src/embedding/embedding_models/voyage/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VoyageEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\"Configuration for Voyage embedding models. This class represents the configuration needed for using Voyage AI embedding models, including authentication credentials and model settings. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__VOYAGE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) provider : Literal [ EmbeddingModelProviderName . VOYAGE ] = Field ( ... , description = \"The provider of the embedding model.\" ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" )","title":"VoyageEmbeddingModelConfiguration"},{"location":"src/embedding/embedding_models/voyage/configuration/#src.embedding.embedding_models.voyage.configuration.VoyageEmbeddingModelConfiguration.Secrets","text":"Bases: BaseSecrets Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. Source code in src/embedding/embedding_models/voyage/configuration.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Secrets ( BaseSecrets ): \"\"\"Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__VOYAGE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" )","title":"Secrets"},{"location":"src/embedding/embedding_models/voyage/embedding_model/","text":"Embedding_model This module contains functionality related to the the embedding_model module for embedding.embedding_models.voyage . Embedding_model VoyageEmbeddingModelFactory Bases: SingletonFactory Factory for creating configured Voyage embedding models. This factory ensures only one instance of a Voyage embedding model is created for each configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the Voyage embedding model. Source code in src/embedding/embedding_models/voyage/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VoyageEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured Voyage embedding models. This factory ensures only one instance of a Voyage embedding model is created for each configuration. Attributes: _configuration_class (Type): The configuration class for the Voyage embedding model. \"\"\" _configuration_class : Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : VoyageEmbeddingModelConfiguration ) -> VoyageEmbedding : \"\"\"Creates a Voyage embedding model based on provided configuration. Args: configuration: Voyage embedding model configuration with API key and settings. Returns: VoyageEmbedding: Configured Voyage embedding model instance. \"\"\" return VoyageEmbedding ( voyage_api_key = configuration . secrets . api_key . get_secret_value (), model_name = configuration . name , embed_batch_size = configuration . batch_size , ) VoyageEmbeddingModelTokenizerFactory Bases: SingletonFactory Factory for creating tokenizers for Voyage embedding models. Provides a singleton tokenizer function based on the configuration. Source code in src/embedding/embedding_models/voyage/embedding_model.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class VoyageEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating tokenizers for Voyage embedding models. Provides a singleton tokenizer function based on the configuration. \"\"\" _configuration_class : Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : VoyageEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function based on provided configuration. Args: configuration: Voyage embedding model configuration containing tokenizer name. Returns: Callable: A tokenizer function that can be used to tokenize input text. \"\"\" return AutoTokenizer . from_pretrained ( configuration . tokenizer_name ) . tokenize","title":"Embedding_model"},{"location":"src/embedding/embedding_models/voyage/embedding_model/#embedding_model","text":"This module contains functionality related to the the embedding_model module for embedding.embedding_models.voyage .","title":"Embedding_model"},{"location":"src/embedding/embedding_models/voyage/embedding_model/#embedding_model_1","text":"","title":"Embedding_model"},{"location":"src/embedding/embedding_models/voyage/embedding_model/#src.embedding.embedding_models.voyage.embedding_model.VoyageEmbeddingModelFactory","text":"Bases: SingletonFactory Factory for creating configured Voyage embedding models. This factory ensures only one instance of a Voyage embedding model is created for each configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the Voyage embedding model. Source code in src/embedding/embedding_models/voyage/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VoyageEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured Voyage embedding models. This factory ensures only one instance of a Voyage embedding model is created for each configuration. Attributes: _configuration_class (Type): The configuration class for the Voyage embedding model. \"\"\" _configuration_class : Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : VoyageEmbeddingModelConfiguration ) -> VoyageEmbedding : \"\"\"Creates a Voyage embedding model based on provided configuration. Args: configuration: Voyage embedding model configuration with API key and settings. Returns: VoyageEmbedding: Configured Voyage embedding model instance. \"\"\" return VoyageEmbedding ( voyage_api_key = configuration . secrets . api_key . get_secret_value (), model_name = configuration . name , embed_batch_size = configuration . batch_size , )","title":"VoyageEmbeddingModelFactory"},{"location":"src/embedding/embedding_models/voyage/embedding_model/#src.embedding.embedding_models.voyage.embedding_model.VoyageEmbeddingModelTokenizerFactory","text":"Bases: SingletonFactory Factory for creating tokenizers for Voyage embedding models. Provides a singleton tokenizer function based on the configuration. Source code in src/embedding/embedding_models/voyage/embedding_model.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class VoyageEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating tokenizers for Voyage embedding models. Provides a singleton tokenizer function based on the configuration. \"\"\" _configuration_class : Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : VoyageEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function based on provided configuration. Args: configuration: Voyage embedding model configuration containing tokenizer name. Returns: Callable: A tokenizer function that can be used to tokenize input text. \"\"\" return AutoTokenizer . from_pretrained ( configuration . tokenizer_name ) . tokenize","title":"VoyageEmbeddingModelTokenizerFactory"},{"location":"src/embedding/orchestrators/base_orchestrator/","text":"Base_orchestrator This module contains functionality related to the the base_orchestrator module for embedding.orchestrators . Base_orchestrator BaseEmbeddingOrchestrator Bases: ABC Abstract base class for embedding orchestration. This class defines the interface for embedding orchestrators that coordinate data extraction, splitting, and embedding processes. Source code in src/embedding/orchestrators/base_orchestrator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class BaseEmbeddingOrchestrator ( ABC ): \"\"\" Abstract base class for embedding orchestration. This class defines the interface for embedding orchestrators that coordinate data extraction, splitting, and embedding processes. \"\"\" def __init__ ( self , datasource_orchestrator : BaseDatasourceOrchestrator , splitter : BaseSplitter , embedder : BaseEmbedder , ) -> None : \"\"\" Initialize a new embedding orchestrator. Args: datasource_orchestrator: Orchestrator for extracting data from sources splitter: Component responsible for splitting documents into nodes embedder: Component that generates embeddings for nodes \"\"\" self . datasource_orchestrator = datasource_orchestrator self . splitter = splitter self . embedder = embedder @abstractmethod async def embed ( self ) -> None : \"\"\" Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. \"\"\" pass __init__ ( datasource_orchestrator , splitter , embedder ) Initialize a new embedding orchestrator. Parameters: datasource_orchestrator ( BaseDatasourceOrchestrator ) \u2013 Orchestrator for extracting data from sources splitter ( BaseSplitter ) \u2013 Component responsible for splitting documents into nodes embedder ( BaseEmbedder ) \u2013 Component that generates embeddings for nodes Source code in src/embedding/orchestrators/base_orchestrator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , datasource_orchestrator : BaseDatasourceOrchestrator , splitter : BaseSplitter , embedder : BaseEmbedder , ) -> None : \"\"\" Initialize a new embedding orchestrator. Args: datasource_orchestrator: Orchestrator for extracting data from sources splitter: Component responsible for splitting documents into nodes embedder: Component that generates embeddings for nodes \"\"\" self . datasource_orchestrator = datasource_orchestrator self . splitter = splitter self . embedder = embedder embed () abstractmethod async Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. Source code in src/embedding/orchestrators/base_orchestrator.py 34 35 36 37 38 39 40 41 42 @abstractmethod async def embed ( self ) -> None : \"\"\" Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. \"\"\" pass BasicEmbeddingOrchestrator Bases: BaseEmbeddingOrchestrator Basic implementation of embedding orchestration. This orchestrator implements a simple workflow that: 1. Retrieves documents from the datasource 2. Splits each document into nodes 3. Embeds the nodes 4. Flushes any remaining embeddings Source code in src/embedding/orchestrators/base_orchestrator.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class BasicEmbeddingOrchestrator ( BaseEmbeddingOrchestrator ): \"\"\" Basic implementation of embedding orchestration. This orchestrator implements a simple workflow that: 1. Retrieves documents from the datasource 2. Splits each document into nodes 3. Embeds the nodes 4. Flushes any remaining embeddings \"\"\" async def embed ( self ) -> None : \"\"\" Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush () embed () async Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. Source code in src/embedding/orchestrators/base_orchestrator.py 56 57 58 59 60 61 62 63 64 65 66 67 async def embed ( self ) -> None : \"\"\" Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"Base_orchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#base_orchestrator","text":"This module contains functionality related to the the base_orchestrator module for embedding.orchestrators .","title":"Base_orchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#base_orchestrator_1","text":"","title":"Base_orchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BaseEmbeddingOrchestrator","text":"Bases: ABC Abstract base class for embedding orchestration. This class defines the interface for embedding orchestrators that coordinate data extraction, splitting, and embedding processes. Source code in src/embedding/orchestrators/base_orchestrator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class BaseEmbeddingOrchestrator ( ABC ): \"\"\" Abstract base class for embedding orchestration. This class defines the interface for embedding orchestrators that coordinate data extraction, splitting, and embedding processes. \"\"\" def __init__ ( self , datasource_orchestrator : BaseDatasourceOrchestrator , splitter : BaseSplitter , embedder : BaseEmbedder , ) -> None : \"\"\" Initialize a new embedding orchestrator. Args: datasource_orchestrator: Orchestrator for extracting data from sources splitter: Component responsible for splitting documents into nodes embedder: Component that generates embeddings for nodes \"\"\" self . datasource_orchestrator = datasource_orchestrator self . splitter = splitter self . embedder = embedder @abstractmethod async def embed ( self ) -> None : \"\"\" Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. \"\"\" pass","title":"BaseEmbeddingOrchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BaseEmbeddingOrchestrator.__init__","text":"Initialize a new embedding orchestrator. Parameters: datasource_orchestrator ( BaseDatasourceOrchestrator ) \u2013 Orchestrator for extracting data from sources splitter ( BaseSplitter ) \u2013 Component responsible for splitting documents into nodes embedder ( BaseEmbedder ) \u2013 Component that generates embeddings for nodes Source code in src/embedding/orchestrators/base_orchestrator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , datasource_orchestrator : BaseDatasourceOrchestrator , splitter : BaseSplitter , embedder : BaseEmbedder , ) -> None : \"\"\" Initialize a new embedding orchestrator. Args: datasource_orchestrator: Orchestrator for extracting data from sources splitter: Component responsible for splitting documents into nodes embedder: Component that generates embeddings for nodes \"\"\" self . datasource_orchestrator = datasource_orchestrator self . splitter = splitter self . embedder = embedder","title":"__init__"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BaseEmbeddingOrchestrator.embed","text":"Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. Source code in src/embedding/orchestrators/base_orchestrator.py 34 35 36 37 38 39 40 41 42 @abstractmethod async def embed ( self ) -> None : \"\"\" Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. \"\"\" pass","title":"embed"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BasicEmbeddingOrchestrator","text":"Bases: BaseEmbeddingOrchestrator Basic implementation of embedding orchestration. This orchestrator implements a simple workflow that: 1. Retrieves documents from the datasource 2. Splits each document into nodes 3. Embeds the nodes 4. Flushes any remaining embeddings Source code in src/embedding/orchestrators/base_orchestrator.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class BasicEmbeddingOrchestrator ( BaseEmbeddingOrchestrator ): \"\"\" Basic implementation of embedding orchestration. This orchestrator implements a simple workflow that: 1. Retrieves documents from the datasource 2. Splits each document into nodes 3. Embeds the nodes 4. Flushes any remaining embeddings \"\"\" async def embed ( self ) -> None : \"\"\" Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"BasicEmbeddingOrchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BasicEmbeddingOrchestrator.embed","text":"Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. Source code in src/embedding/orchestrators/base_orchestrator.py 56 57 58 59 60 61 62 63 64 65 66 67 async def embed ( self ) -> None : \"\"\" Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"embed"},{"location":"src/embedding/orchestrators/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.orchestrators . Registry EmbeddingOrchestratorRegistry Bases: Registry Registry for managing embedding orchestrators. This registry uses EmbeddingOrchestratorName as keys to store and retrieve orchestrator implementations. It extends the base Registry class to provide specialized functionality for embedding orchestration components. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingOrchestratorName enum. Source code in src/embedding/orchestrators/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 class EmbeddingOrchestratorRegistry ( Registry ): \"\"\"Registry for managing embedding orchestrators. This registry uses EmbeddingOrchestratorName as keys to store and retrieve orchestrator implementations. It extends the base Registry class to provide specialized functionality for embedding orchestration components. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingOrchestratorName enum. \"\"\" _key_class : Type = EmbeddingOrchestratorName","title":"Registry"},{"location":"src/embedding/orchestrators/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.orchestrators .","title":"Registry"},{"location":"src/embedding/orchestrators/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/orchestrators/registry/#src.embedding.orchestrators.registry.EmbeddingOrchestratorRegistry","text":"Bases: Registry Registry for managing embedding orchestrators. This registry uses EmbeddingOrchestratorName as keys to store and retrieve orchestrator implementations. It extends the base Registry class to provide specialized functionality for embedding orchestration components. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingOrchestratorName enum. Source code in src/embedding/orchestrators/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 class EmbeddingOrchestratorRegistry ( Registry ): \"\"\"Registry for managing embedding orchestrators. This registry uses EmbeddingOrchestratorName as keys to store and retrieve orchestrator implementations. It extends the base Registry class to provide specialized functionality for embedding orchestration components. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingOrchestratorName enum. \"\"\" _key_class : Type = EmbeddingOrchestratorName","title":"EmbeddingOrchestratorRegistry"},{"location":"src/embedding/orchestrators/basic/orchestrator/","text":"Orchestrator This module contains functionality related to the the orchestrator module for embedding.orchestrators.basic . Orchestrator BasicEmbeddingOrchestrator Bases: BaseEmbeddingOrchestrator A basic orchestrator for embedding pipeline processing. This orchestrator implements a straightforward process that: 1. Fetches documents from a datasource 2. Splits documents into nodes 3. Embeds those nodes Source code in src/embedding/orchestrators/basic/orchestrator.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class BasicEmbeddingOrchestrator ( BaseEmbeddingOrchestrator ): \"\"\" A basic orchestrator for embedding pipeline processing. This orchestrator implements a straightforward process that: 1. Fetches documents from a datasource 2. Splits documents into nodes 3. Embeds those nodes \"\"\" async def embed ( self ) -> None : \"\"\" Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush () embed () async Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. Source code in src/embedding/orchestrators/basic/orchestrator.py 23 24 25 26 27 28 29 30 31 32 33 34 35 async def embed ( self ) -> None : \"\"\" Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush () BasicEmbeddingOrchestratorFactory Bases: Factory Factory for creating BasicEmbeddingOrchestrator instances. Creates and configures orchestrators with appropriate datasources, splitters, and embedders based on the provided configuration. Source code in src/embedding/orchestrators/basic/orchestrator.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class BasicEmbeddingOrchestratorFactory ( Factory ): \"\"\" Factory for creating BasicEmbeddingOrchestrator instances. Creates and configures orchestrators with appropriate datasources, splitters, and embedders based on the provided configuration. \"\"\" _configuration_class : Type = EmbeddingConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingConfiguration ) -> BasicEmbeddingOrchestrator : \"\"\" Create a configured BasicEmbeddingOrchestrator instance. Args: configuration: Complete embedding configuration containing datasource, splitter, and embedder specifications Returns: A configured BasicEmbeddingOrchestrator ready for use Raises: ValueError: If splitter configuration is missing \"\"\" datasource_orchestrator = DatasourceOrchestratorRegistry . get ( configuration . extraction . orchestrator_name ) . create ( configuration ) embedding_model_configuration = configuration . embedding . embedding_model splitter_configuration = embedding_model_configuration . splitter if not splitter_configuration : raise ValueError ( \"Splitter configuration is required for embedding process.\" ) splitter = SplitterRegistry . get ( splitter_configuration . name ) . create ( embedding_model_configuration ) embedder = EmbedderRegistry . get ( configuration . embedding . embedder_name ) . create ( configuration ) return BasicEmbeddingOrchestrator ( datasource_orchestrator = datasource_orchestrator , splitter = splitter , embedder = embedder , )","title":"Orchestrator"},{"location":"src/embedding/orchestrators/basic/orchestrator/#orchestrator","text":"This module contains functionality related to the the orchestrator module for embedding.orchestrators.basic .","title":"Orchestrator"},{"location":"src/embedding/orchestrators/basic/orchestrator/#orchestrator_1","text":"","title":"Orchestrator"},{"location":"src/embedding/orchestrators/basic/orchestrator/#src.embedding.orchestrators.basic.orchestrator.BasicEmbeddingOrchestrator","text":"Bases: BaseEmbeddingOrchestrator A basic orchestrator for embedding pipeline processing. This orchestrator implements a straightforward process that: 1. Fetches documents from a datasource 2. Splits documents into nodes 3. Embeds those nodes Source code in src/embedding/orchestrators/basic/orchestrator.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class BasicEmbeddingOrchestrator ( BaseEmbeddingOrchestrator ): \"\"\" A basic orchestrator for embedding pipeline processing. This orchestrator implements a straightforward process that: 1. Fetches documents from a datasource 2. Splits documents into nodes 3. Embeds those nodes \"\"\" async def embed ( self ) -> None : \"\"\" Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"BasicEmbeddingOrchestrator"},{"location":"src/embedding/orchestrators/basic/orchestrator/#src.embedding.orchestrators.basic.orchestrator.BasicEmbeddingOrchestrator.embed","text":"Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. Source code in src/embedding/orchestrators/basic/orchestrator.py 23 24 25 26 27 28 29 30 31 32 33 34 35 async def embed ( self ) -> None : \"\"\" Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"embed"},{"location":"src/embedding/orchestrators/basic/orchestrator/#src.embedding.orchestrators.basic.orchestrator.BasicEmbeddingOrchestratorFactory","text":"Bases: Factory Factory for creating BasicEmbeddingOrchestrator instances. Creates and configures orchestrators with appropriate datasources, splitters, and embedders based on the provided configuration. Source code in src/embedding/orchestrators/basic/orchestrator.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class BasicEmbeddingOrchestratorFactory ( Factory ): \"\"\" Factory for creating BasicEmbeddingOrchestrator instances. Creates and configures orchestrators with appropriate datasources, splitters, and embedders based on the provided configuration. \"\"\" _configuration_class : Type = EmbeddingConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingConfiguration ) -> BasicEmbeddingOrchestrator : \"\"\" Create a configured BasicEmbeddingOrchestrator instance. Args: configuration: Complete embedding configuration containing datasource, splitter, and embedder specifications Returns: A configured BasicEmbeddingOrchestrator ready for use Raises: ValueError: If splitter configuration is missing \"\"\" datasource_orchestrator = DatasourceOrchestratorRegistry . get ( configuration . extraction . orchestrator_name ) . create ( configuration ) embedding_model_configuration = configuration . embedding . embedding_model splitter_configuration = embedding_model_configuration . splitter if not splitter_configuration : raise ValueError ( \"Splitter configuration is required for embedding process.\" ) splitter = SplitterRegistry . get ( splitter_configuration . name ) . create ( embedding_model_configuration ) embedder = EmbedderRegistry . get ( configuration . embedding . embedder_name ) . create ( configuration ) return BasicEmbeddingOrchestrator ( datasource_orchestrator = datasource_orchestrator , splitter = splitter , embedder = embedder , )","title":"BasicEmbeddingOrchestratorFactory"},{"location":"src/embedding/splitters/base_splitter/","text":"Base_splitter This module contains functionality related to the the base_splitter module for embedding.splitters . Base_splitter BaseSplitter Bases: ABC , Generic [ DocType ] Abstract base class for document splitter. This class defines a common interface for document splitters that transform various document types into text nodes for further processing. It leverages generic typing to support different document formats while maintaining type safety. Implementations should handle the specific logic required to parse and split different document types into meaningful text chunks. Source code in src/embedding/splitters/base_splitter.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class BaseSplitter ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document splitter. This class defines a common interface for document splitters that transform various document types into text nodes for further processing. It leverages generic typing to support different document formats while maintaining type safety. Implementations should handle the specific logic required to parse and split different document types into meaningful text chunks. \"\"\" @abstractmethod def split ( self , document : DocType ) -> TextNode : \"\"\"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Args: document: The document to split or process Returns: TextNode: The processed text node generated from the document \"\"\" pass split ( document ) abstractmethod Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Parameters: document ( DocType ) \u2013 The document to split or process Returns: TextNode ( TextNode ) \u2013 The processed text node generated from the document Source code in src/embedding/splitters/base_splitter.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @abstractmethod def split ( self , document : DocType ) -> TextNode : \"\"\"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Args: document: The document to split or process Returns: TextNode: The processed text node generated from the document \"\"\" pass","title":"Base_splitter"},{"location":"src/embedding/splitters/base_splitter/#base_splitter","text":"This module contains functionality related to the the base_splitter module for embedding.splitters .","title":"Base_splitter"},{"location":"src/embedding/splitters/base_splitter/#base_splitter_1","text":"","title":"Base_splitter"},{"location":"src/embedding/splitters/base_splitter/#src.embedding.splitters.base_splitter.BaseSplitter","text":"Bases: ABC , Generic [ DocType ] Abstract base class for document splitter. This class defines a common interface for document splitters that transform various document types into text nodes for further processing. It leverages generic typing to support different document formats while maintaining type safety. Implementations should handle the specific logic required to parse and split different document types into meaningful text chunks. Source code in src/embedding/splitters/base_splitter.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class BaseSplitter ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document splitter. This class defines a common interface for document splitters that transform various document types into text nodes for further processing. It leverages generic typing to support different document formats while maintaining type safety. Implementations should handle the specific logic required to parse and split different document types into meaningful text chunks. \"\"\" @abstractmethod def split ( self , document : DocType ) -> TextNode : \"\"\"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Args: document: The document to split or process Returns: TextNode: The processed text node generated from the document \"\"\" pass","title":"BaseSplitter"},{"location":"src/embedding/splitters/base_splitter/#src.embedding.splitters.base_splitter.BaseSplitter.split","text":"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Parameters: document ( DocType ) \u2013 The document to split or process Returns: TextNode ( TextNode ) \u2013 The processed text node generated from the document Source code in src/embedding/splitters/base_splitter.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @abstractmethod def split ( self , document : DocType ) -> TextNode : \"\"\"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Args: document: The document to split or process Returns: TextNode: The processed text node generated from the document \"\"\" pass","title":"split"},{"location":"src/embedding/splitters/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.splitters . Registry SplitterRegistry Bases: Registry Registry for document splitters that manages the registration and retrieval of different splitter implementations. This registry maps SplitterName enum values to their corresponding splitter implementations, allowing for a centralized way to access different text splitting strategies throughout the application. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to SplitterName enum. Source code in src/embedding/splitters/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 class SplitterRegistry ( Registry ): \"\"\" Registry for document splitters that manages the registration and retrieval of different splitter implementations. This registry maps SplitterName enum values to their corresponding splitter implementations, allowing for a centralized way to access different text splitting strategies throughout the application. Attributes: _key_class: The class type used as keys in the registry, set to SplitterName enum. \"\"\" _key_class : Type = SplitterName","title":"Registry"},{"location":"src/embedding/splitters/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.splitters .","title":"Registry"},{"location":"src/embedding/splitters/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/splitters/registry/#src.embedding.splitters.registry.SplitterRegistry","text":"Bases: Registry Registry for document splitters that manages the registration and retrieval of different splitter implementations. This registry maps SplitterName enum values to their corresponding splitter implementations, allowing for a centralized way to access different text splitting strategies throughout the application. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to SplitterName enum. Source code in src/embedding/splitters/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 class SplitterRegistry ( Registry ): \"\"\" Registry for document splitters that manages the registration and retrieval of different splitter implementations. This registry maps SplitterName enum values to their corresponding splitter implementations, allowing for a centralized way to access different text splitting strategies throughout the application. Attributes: _key_class: The class type used as keys in the registry, set to SplitterName enum. \"\"\" _key_class : Type = SplitterName","title":"SplitterRegistry"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/","text":"Basic_markdown_splitter This module contains functionality related to the the basic_markdown_splitter module for embedding.splitters.basic_markdown . Basic_markdown_splitter BasicMarkdownSplitter Bases: BaseSplitter , Generic [ DocType ] Splitter for markdown documents with token-based chunking. Splits markdown content into nodes based on document structure and token limits. Supports node merging and splitting to maintain consistent chunk sizes. Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class BasicMarkdownSplitter ( BaseSplitter , Generic [ DocType ]): \"\"\"Splitter for markdown documents with token-based chunking. Splits markdown content into nodes based on document structure and token limits. Supports node merging and splitting to maintain consistent chunk sizes. \"\"\" def __init__ ( self , chunk_size_in_tokens : int , chunk_overlap_in_tokens : int , tokenize_func : Callable , ): \"\"\"Initialize markdown splitter. Args: chunk_size_in_tokens: Maximum tokens per chunk chunk_overlap_in_tokens: Token overlap between chunks tokenize_func: Function to tokenize text for token counting \"\"\" self . chunk_size_in_tokens = chunk_size_in_tokens self . tokenize_func = tokenize_func self . markdown_node_parser = MarkdownNodeParser () self . sentence_splitter = SentenceSplitter ( chunk_size = chunk_size_in_tokens , chunk_overlap = chunk_overlap_in_tokens , tokenizer = tokenize_func , ) def split ( self , document : DocType ) -> TextNode : \"\"\"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Args: document: Markdown document to be processed Returns: List[TextNode]: Collection of processed text nodes with optimized sizes \"\"\" document_nodes = self . markdown_node_parser . get_nodes_from_documents ( [ document ] ) document_nodes = self . _split_big_nodes ( document_nodes ) document_nodes = self . _merge_small_nodes ( document_nodes ) return document_nodes def _split_big_nodes ( self , document_nodes : List [ TextNode ] ) -> List [ TextNode ]: \"\"\"Split oversized nodes into smaller chunks. Identifies nodes exceeding the token limit and processes them through the sentence splitter to create smaller, semantically coherent chunks. Args: document_nodes: Collection of nodes to process Returns: List[TextNode]: Processed nodes within token size limits \"\"\" new_document_nodes = [] for document_node in document_nodes : text = document_node . text document_node_size = len ( self . tokenize_func ( text )) if document_node_size > self . chunk_size_in_tokens : document_sub_nodes = self . _split_big_node ( document_node ) new_document_nodes . extend ( document_sub_nodes ) else : new_document_nodes . append ( document_node ) return new_document_nodes def _split_big_node ( self , document_node : TextNode ) -> List [ TextNode ]: \"\"\"Split single oversized node into smaller nodes. Uses sentence boundary detection to create semantically meaningful smaller chunks from a large node, preserving metadata from the original node. Args: document_node: Node exceeding token size limit Returns: List[TextNode]: Collection of smaller nodes derived from original \"\"\" text = document_node . text sub_texts = self . sentence_splitter . split_text ( text ) sub_nodes = [] for sub_text in sub_texts : sub_node = document_node . model_copy () sub_node . id_ = str ( uuid . uuid4 ()) sub_node . text = sub_text sub_nodes . append ( sub_node ) return sub_nodes def _merge_small_nodes ( self , document_nodes : List [ TextNode ] ) -> List [ TextNode ]: \"\"\"Merge adjacent small nodes into larger chunks. Combines consecutive nodes when their combined token count remains under the maximum limit, optimizing for fewer, larger chunks while respecting token boundaries. Args: document_nodes: Collection of nodes to potentially merge Returns: List[TextNode]: Optimized collection with merged nodes \"\"\" new_document_nodes = [] current_node = document_nodes [ 0 ] for node in document_nodes [ 1 :]: current_text = current_node . text current_node_size = len ( self . tokenize_func ( current_text )) node_text = node . text node_size = len ( self . tokenize_func ( node_text )) if current_node_size + node_size <= self . chunk_size_in_tokens : current_node . text += node . text else : new_document_nodes . append ( current_node ) current_node = node new_document_nodes . append ( current_node ) return new_document_nodes __init__ ( chunk_size_in_tokens , chunk_overlap_in_tokens , tokenize_func ) Initialize markdown splitter. Parameters: chunk_size_in_tokens ( int ) \u2013 Maximum tokens per chunk chunk_overlap_in_tokens ( int ) \u2013 Token overlap between chunks tokenize_func ( Callable ) \u2013 Function to tokenize text for token counting Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , chunk_size_in_tokens : int , chunk_overlap_in_tokens : int , tokenize_func : Callable , ): \"\"\"Initialize markdown splitter. Args: chunk_size_in_tokens: Maximum tokens per chunk chunk_overlap_in_tokens: Token overlap between chunks tokenize_func: Function to tokenize text for token counting \"\"\" self . chunk_size_in_tokens = chunk_size_in_tokens self . tokenize_func = tokenize_func self . markdown_node_parser = MarkdownNodeParser () self . sentence_splitter = SentenceSplitter ( chunk_size = chunk_size_in_tokens , chunk_overlap = chunk_overlap_in_tokens , tokenizer = tokenize_func , ) split ( document ) Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Parameters: document ( DocType ) \u2013 Markdown document to be processed Returns: TextNode \u2013 List[TextNode]: Collection of processed text nodes with optimized sizes Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def split ( self , document : DocType ) -> TextNode : \"\"\"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Args: document: Markdown document to be processed Returns: List[TextNode]: Collection of processed text nodes with optimized sizes \"\"\" document_nodes = self . markdown_node_parser . get_nodes_from_documents ( [ document ] ) document_nodes = self . _split_big_nodes ( document_nodes ) document_nodes = self . _merge_small_nodes ( document_nodes ) return document_nodes BasicMarkdownSplitterFactory Bases: Factory Factory for creating BasicMarkdownSplitter instances. Creates splitter instances configured according to the provided embedding model configuration. Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class BasicMarkdownSplitterFactory ( Factory ): \"\"\"Factory for creating BasicMarkdownSplitter instances. Creates splitter instances configured according to the provided embedding model configuration. \"\"\" _configuration_class : Type = EmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingModelConfiguration ) -> BasicMarkdownSplitter : \"\"\"Create a BasicMarkdownSplitter instance from configuration. Validates the configuration, retrieves the appropriate tokenizer, and instantiates a properly configured splitter. Args: configuration: Embedding model configuration containing splitter settings Returns: BasicMarkdownSplitter: Configured splitter instance Raises: ValueError: If the configuration lacks proper splitter settings \"\"\" if not configuration . splitter or not isinstance ( configuration . splitter , BasicMarkdownSplitterConfiguration ): raise ValueError ( \"`BasicMarkdownSplitterConfiguration` configuration is required for `BasicMarkdownSplitter`.\" ) tokenizer_func = EmbeddingModelTokenizerRegistry . get ( configuration . provider ) . create ( configuration ) return BasicMarkdownSplitter ( chunk_size_in_tokens = configuration . splitter . chunk_size_in_tokens , chunk_overlap_in_tokens = configuration . splitter . chunk_overlap_in_tokens , tokenize_func = tokenizer_func , )","title":"Basic_markdown_splitter"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#basic_markdown_splitter","text":"This module contains functionality related to the the basic_markdown_splitter module for embedding.splitters.basic_markdown .","title":"Basic_markdown_splitter"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#basic_markdown_splitter_1","text":"","title":"Basic_markdown_splitter"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#src.embedding.splitters.basic_markdown.basic_markdown_splitter.BasicMarkdownSplitter","text":"Bases: BaseSplitter , Generic [ DocType ] Splitter for markdown documents with token-based chunking. Splits markdown content into nodes based on document structure and token limits. Supports node merging and splitting to maintain consistent chunk sizes. Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class BasicMarkdownSplitter ( BaseSplitter , Generic [ DocType ]): \"\"\"Splitter for markdown documents with token-based chunking. Splits markdown content into nodes based on document structure and token limits. Supports node merging and splitting to maintain consistent chunk sizes. \"\"\" def __init__ ( self , chunk_size_in_tokens : int , chunk_overlap_in_tokens : int , tokenize_func : Callable , ): \"\"\"Initialize markdown splitter. Args: chunk_size_in_tokens: Maximum tokens per chunk chunk_overlap_in_tokens: Token overlap between chunks tokenize_func: Function to tokenize text for token counting \"\"\" self . chunk_size_in_tokens = chunk_size_in_tokens self . tokenize_func = tokenize_func self . markdown_node_parser = MarkdownNodeParser () self . sentence_splitter = SentenceSplitter ( chunk_size = chunk_size_in_tokens , chunk_overlap = chunk_overlap_in_tokens , tokenizer = tokenize_func , ) def split ( self , document : DocType ) -> TextNode : \"\"\"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Args: document: Markdown document to be processed Returns: List[TextNode]: Collection of processed text nodes with optimized sizes \"\"\" document_nodes = self . markdown_node_parser . get_nodes_from_documents ( [ document ] ) document_nodes = self . _split_big_nodes ( document_nodes ) document_nodes = self . _merge_small_nodes ( document_nodes ) return document_nodes def _split_big_nodes ( self , document_nodes : List [ TextNode ] ) -> List [ TextNode ]: \"\"\"Split oversized nodes into smaller chunks. Identifies nodes exceeding the token limit and processes them through the sentence splitter to create smaller, semantically coherent chunks. Args: document_nodes: Collection of nodes to process Returns: List[TextNode]: Processed nodes within token size limits \"\"\" new_document_nodes = [] for document_node in document_nodes : text = document_node . text document_node_size = len ( self . tokenize_func ( text )) if document_node_size > self . chunk_size_in_tokens : document_sub_nodes = self . _split_big_node ( document_node ) new_document_nodes . extend ( document_sub_nodes ) else : new_document_nodes . append ( document_node ) return new_document_nodes def _split_big_node ( self , document_node : TextNode ) -> List [ TextNode ]: \"\"\"Split single oversized node into smaller nodes. Uses sentence boundary detection to create semantically meaningful smaller chunks from a large node, preserving metadata from the original node. Args: document_node: Node exceeding token size limit Returns: List[TextNode]: Collection of smaller nodes derived from original \"\"\" text = document_node . text sub_texts = self . sentence_splitter . split_text ( text ) sub_nodes = [] for sub_text in sub_texts : sub_node = document_node . model_copy () sub_node . id_ = str ( uuid . uuid4 ()) sub_node . text = sub_text sub_nodes . append ( sub_node ) return sub_nodes def _merge_small_nodes ( self , document_nodes : List [ TextNode ] ) -> List [ TextNode ]: \"\"\"Merge adjacent small nodes into larger chunks. Combines consecutive nodes when their combined token count remains under the maximum limit, optimizing for fewer, larger chunks while respecting token boundaries. Args: document_nodes: Collection of nodes to potentially merge Returns: List[TextNode]: Optimized collection with merged nodes \"\"\" new_document_nodes = [] current_node = document_nodes [ 0 ] for node in document_nodes [ 1 :]: current_text = current_node . text current_node_size = len ( self . tokenize_func ( current_text )) node_text = node . text node_size = len ( self . tokenize_func ( node_text )) if current_node_size + node_size <= self . chunk_size_in_tokens : current_node . text += node . text else : new_document_nodes . append ( current_node ) current_node = node new_document_nodes . append ( current_node ) return new_document_nodes","title":"BasicMarkdownSplitter"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#src.embedding.splitters.basic_markdown.basic_markdown_splitter.BasicMarkdownSplitter.__init__","text":"Initialize markdown splitter. Parameters: chunk_size_in_tokens ( int ) \u2013 Maximum tokens per chunk chunk_overlap_in_tokens ( int ) \u2013 Token overlap between chunks tokenize_func ( Callable ) \u2013 Function to tokenize text for token counting Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , chunk_size_in_tokens : int , chunk_overlap_in_tokens : int , tokenize_func : Callable , ): \"\"\"Initialize markdown splitter. Args: chunk_size_in_tokens: Maximum tokens per chunk chunk_overlap_in_tokens: Token overlap between chunks tokenize_func: Function to tokenize text for token counting \"\"\" self . chunk_size_in_tokens = chunk_size_in_tokens self . tokenize_func = tokenize_func self . markdown_node_parser = MarkdownNodeParser () self . sentence_splitter = SentenceSplitter ( chunk_size = chunk_size_in_tokens , chunk_overlap = chunk_overlap_in_tokens , tokenizer = tokenize_func , )","title":"__init__"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#src.embedding.splitters.basic_markdown.basic_markdown_splitter.BasicMarkdownSplitter.split","text":"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Parameters: document ( DocType ) \u2013 Markdown document to be processed Returns: TextNode \u2013 List[TextNode]: Collection of processed text nodes with optimized sizes Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def split ( self , document : DocType ) -> TextNode : \"\"\"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Args: document: Markdown document to be processed Returns: List[TextNode]: Collection of processed text nodes with optimized sizes \"\"\" document_nodes = self . markdown_node_parser . get_nodes_from_documents ( [ document ] ) document_nodes = self . _split_big_nodes ( document_nodes ) document_nodes = self . _merge_small_nodes ( document_nodes ) return document_nodes","title":"split"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#src.embedding.splitters.basic_markdown.basic_markdown_splitter.BasicMarkdownSplitterFactory","text":"Bases: Factory Factory for creating BasicMarkdownSplitter instances. Creates splitter instances configured according to the provided embedding model configuration. Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class BasicMarkdownSplitterFactory ( Factory ): \"\"\"Factory for creating BasicMarkdownSplitter instances. Creates splitter instances configured according to the provided embedding model configuration. \"\"\" _configuration_class : Type = EmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingModelConfiguration ) -> BasicMarkdownSplitter : \"\"\"Create a BasicMarkdownSplitter instance from configuration. Validates the configuration, retrieves the appropriate tokenizer, and instantiates a properly configured splitter. Args: configuration: Embedding model configuration containing splitter settings Returns: BasicMarkdownSplitter: Configured splitter instance Raises: ValueError: If the configuration lacks proper splitter settings \"\"\" if not configuration . splitter or not isinstance ( configuration . splitter , BasicMarkdownSplitterConfiguration ): raise ValueError ( \"`BasicMarkdownSplitterConfiguration` configuration is required for `BasicMarkdownSplitter`.\" ) tokenizer_func = EmbeddingModelTokenizerRegistry . get ( configuration . provider ) . create ( configuration ) return BasicMarkdownSplitter ( chunk_size_in_tokens = configuration . splitter . chunk_size_in_tokens , chunk_overlap_in_tokens = configuration . splitter . chunk_overlap_in_tokens , tokenize_func = tokenizer_func , )","title":"BasicMarkdownSplitterFactory"},{"location":"src/embedding/splitters/basic_markdown/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.splitters.basic_markdown . Configuration BasicMarkdownSplitterConfiguration Bases: SplitterConfiguration Configuration for the BasicMarkdownSplitter. This class defines the parameters needed to split markdown documents into chunks with specific token sizes and overlaps. Source code in src/embedding/splitters/basic_markdown/configuration.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class BasicMarkdownSplitterConfiguration ( SplitterConfiguration ): \"\"\" Configuration for the BasicMarkdownSplitter. This class defines the parameters needed to split markdown documents into chunks with specific token sizes and overlaps. \"\"\" chunk_overlap_in_tokens : int = Field ( ... , description = \"The number of tokens that overlap between chunks.\" ) chunk_size_in_tokens : int = Field ( ... , description = \"The size of each chunk in tokens.\" ) name : SplitterName = Field ( SplitterName . BASIC_MARKDOWN , description = \"The name of the splitter.\" )","title":"Configuration"},{"location":"src/embedding/splitters/basic_markdown/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.splitters.basic_markdown .","title":"Configuration"},{"location":"src/embedding/splitters/basic_markdown/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/splitters/basic_markdown/configuration/#src.embedding.splitters.basic_markdown.configuration.BasicMarkdownSplitterConfiguration","text":"Bases: SplitterConfiguration Configuration for the BasicMarkdownSplitter. This class defines the parameters needed to split markdown documents into chunks with specific token sizes and overlaps. Source code in src/embedding/splitters/basic_markdown/configuration.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class BasicMarkdownSplitterConfiguration ( SplitterConfiguration ): \"\"\" Configuration for the BasicMarkdownSplitter. This class defines the parameters needed to split markdown documents into chunks with specific token sizes and overlaps. \"\"\" chunk_overlap_in_tokens : int = Field ( ... , description = \"The number of tokens that overlap between chunks.\" ) chunk_size_in_tokens : int = Field ( ... , description = \"The size of each chunk in tokens.\" ) name : SplitterName = Field ( SplitterName . BASIC_MARKDOWN , description = \"The name of the splitter.\" )","title":"BasicMarkdownSplitterConfiguration"},{"location":"src/embedding/vector_stores/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.vector_stores . Registry VectorStoreRegistry Bases: Registry Registry for vector store implementations. Maps VectorStoreName values to vector store implementation classes. Used for registering and retrieving different vector store backends that can be configured in the system. Source code in src/embedding/vector_stores/registry.py 9 10 11 12 13 14 15 16 17 18 class VectorStoreRegistry ( Registry ): \"\"\" Registry for vector store implementations. Maps VectorStoreName values to vector store implementation classes. Used for registering and retrieving different vector store backends that can be configured in the system. \"\"\" _key_class : Type = VectorStoreName VectorStoreValidatorRegistry Bases: Registry Registry for vector store validator implementations. Maps VectorStoreName values to validator classes responsible for validating vector store configurations before use. Validators ensure proper setup and connection parameters. Source code in src/embedding/vector_stores/registry.py 21 22 23 24 25 26 27 28 29 30 class VectorStoreValidatorRegistry ( Registry ): \"\"\" Registry for vector store validator implementations. Maps VectorStoreName values to validator classes responsible for validating vector store configurations before use. Validators ensure proper setup and connection parameters. \"\"\" _key_class : Type = VectorStoreName","title":"Registry"},{"location":"src/embedding/vector_stores/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.vector_stores .","title":"Registry"},{"location":"src/embedding/vector_stores/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/vector_stores/registry/#src.embedding.vector_stores.registry.VectorStoreRegistry","text":"Bases: Registry Registry for vector store implementations. Maps VectorStoreName values to vector store implementation classes. Used for registering and retrieving different vector store backends that can be configured in the system. Source code in src/embedding/vector_stores/registry.py 9 10 11 12 13 14 15 16 17 18 class VectorStoreRegistry ( Registry ): \"\"\" Registry for vector store implementations. Maps VectorStoreName values to vector store implementation classes. Used for registering and retrieving different vector store backends that can be configured in the system. \"\"\" _key_class : Type = VectorStoreName","title":"VectorStoreRegistry"},{"location":"src/embedding/vector_stores/registry/#src.embedding.vector_stores.registry.VectorStoreValidatorRegistry","text":"Bases: Registry Registry for vector store validator implementations. Maps VectorStoreName values to validator classes responsible for validating vector store configurations before use. Validators ensure proper setup and connection parameters. Source code in src/embedding/vector_stores/registry.py 21 22 23 24 25 26 27 28 29 30 class VectorStoreValidatorRegistry ( Registry ): \"\"\" Registry for vector store validator implementations. Maps VectorStoreName values to validator classes responsible for validating vector store configurations before use. Validators ensure proper setup and connection parameters. \"\"\" _key_class : Type = VectorStoreName","title":"VectorStoreValidatorRegistry"},{"location":"src/embedding/vector_stores/chroma/client/","text":"Client This module contains functionality related to the the client module for embedding.vector_stores.chroma . Client ChromaVectorStoreClientFactory Bases: SingletonFactory Factory for creating and managing Chroma vector store client instances. This factory implements the Singleton pattern, ensuring only one client instance exists per unique configuration. It creates HTTP clients for connecting to Chroma DB vector store services. Source code in src/embedding/vector_stores/chroma/client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ChromaVectorStoreClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Chroma vector store client instances. This factory implements the Singleton pattern, ensuring only one client instance exists per unique configuration. It creates HTTP clients for connecting to Chroma DB vector store services. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaClient : \"\"\" Create a new Chroma client instance based on the provided configuration. Args: configuration (ChromaVectorStoreConfiguration): Configuration containing connection details for the Chroma vector store. Returns: ChromaClient: An HTTP client connected to the specified Chroma service. \"\"\" return ChromaHttpClient ( host = configuration . host , port = configuration . port , )","title":"Client"},{"location":"src/embedding/vector_stores/chroma/client/#client","text":"This module contains functionality related to the the client module for embedding.vector_stores.chroma .","title":"Client"},{"location":"src/embedding/vector_stores/chroma/client/#client_1","text":"","title":"Client"},{"location":"src/embedding/vector_stores/chroma/client/#src.embedding.vector_stores.chroma.client.ChromaVectorStoreClientFactory","text":"Bases: SingletonFactory Factory for creating and managing Chroma vector store client instances. This factory implements the Singleton pattern, ensuring only one client instance exists per unique configuration. It creates HTTP clients for connecting to Chroma DB vector store services. Source code in src/embedding/vector_stores/chroma/client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ChromaVectorStoreClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Chroma vector store client instances. This factory implements the Singleton pattern, ensuring only one client instance exists per unique configuration. It creates HTTP clients for connecting to Chroma DB vector store services. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaClient : \"\"\" Create a new Chroma client instance based on the provided configuration. Args: configuration (ChromaVectorStoreConfiguration): Configuration containing connection details for the Chroma vector store. Returns: ChromaClient: An HTTP client connected to the specified Chroma service. \"\"\" return ChromaHttpClient ( host = configuration . host , port = configuration . port , )","title":"ChromaVectorStoreClientFactory"},{"location":"src/embedding/vector_stores/chroma/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.vector_stores.chroma . Configuration ChromaVectorStoreConfiguration Bases: VectorStoreConfiguration Configuration for the ChromaDB vector store. Source code in src/embedding/vector_stores/chroma/configuration.py 11 12 13 14 15 16 class ChromaVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\"Configuration for the ChromaDB vector store.\"\"\" name : Literal [ VectorStoreName . CHROMA ] = Field ( ... , description = \"The name of the vector store.\" )","title":"Configuration"},{"location":"src/embedding/vector_stores/chroma/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.vector_stores.chroma .","title":"Configuration"},{"location":"src/embedding/vector_stores/chroma/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/vector_stores/chroma/configuration/#src.embedding.vector_stores.chroma.configuration.ChromaVectorStoreConfiguration","text":"Bases: VectorStoreConfiguration Configuration for the ChromaDB vector store. Source code in src/embedding/vector_stores/chroma/configuration.py 11 12 13 14 15 16 class ChromaVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\"Configuration for the ChromaDB vector store.\"\"\" name : Literal [ VectorStoreName . CHROMA ] = Field ( ... , description = \"The name of the vector store.\" )","title":"ChromaVectorStoreConfiguration"},{"location":"src/embedding/vector_stores/chroma/validator/","text":"Validator This module contains functionality related to the the validator module for embedding.vector_stores.chroma . Validator ChromaVectorStoreValidator Bases: BaseVectorStoreValidator Validator for Chroma vector store configuration. Validates collection settings and existence for Chroma vector store backend. Ensures proper configuration before operations are performed against the vector store. Source code in src/embedding/vector_stores/chroma/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class ChromaVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Chroma vector store configuration. Validates collection settings and existence for Chroma vector store backend. Ensures proper configuration before operations are performed against the vector store. \"\"\" def __init__ ( self , configuration : ChromaVectorStoreConfiguration , client : ChromaClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Chroma vector store settings client: Client for Chroma operations \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException: If collection already exists \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException: If collection already exists \"\"\" collection_name = self . configuration . collection_name if collection_name in self . client . list_collections (): raise CollectionExistsException ( collection_name ) __init__ ( configuration , client ) Initialize validator with configuration and client. Parameters: configuration ( ChromaVectorStoreConfiguration ) \u2013 Chroma vector store settings client ( ClientAPI ) \u2013 Client for Chroma operations Source code in src/embedding/vector_stores/chroma/validator.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration : ChromaVectorStoreConfiguration , client : ChromaClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Chroma vector store settings client: Client for Chroma operations \"\"\" self . configuration = configuration self . client = client validate () Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException \u2013 If collection already exists Source code in src/embedding/vector_stores/chroma/validator.py 36 37 38 39 40 41 42 43 44 45 def validate ( self ) -> None : \"\"\"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException: If collection already exists \"\"\" self . validate_collection () validate_collection () Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException \u2013 If collection already exists Source code in src/embedding/vector_stores/chroma/validator.py 47 48 49 50 51 52 53 54 55 56 57 58 def validate_collection ( self ) -> None : \"\"\"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException: If collection already exists \"\"\" collection_name = self . configuration . collection_name if collection_name in self . client . list_collections (): raise CollectionExistsException ( collection_name ) ChromaVectorStoreValidatorFactory Bases: SingletonFactory Factory for creating configured Chroma validator instances. Manages the creation and caching of ChromaVectorStoreValidator instances based on provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for Chroma vector store. Source code in src/embedding/vector_stores/chroma/validator.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class ChromaVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating configured Chroma validator instances. Manages the creation and caching of ChromaVectorStoreValidator instances based on provided configuration. Attributes: _configuration_class (Type): The configuration class for Chroma vector store. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaVectorStoreValidator : \"\"\"Creates a Chroma validator based on provided configuration. Args: configuration: Chroma connection configuration. Returns: ChromaVectorStoreValidator: Configured validator instance. \"\"\" client = ChromaVectorStoreClientFactory . create ( configuration ) return ChromaVectorStoreValidator ( configuration = configuration , client = client )","title":"Validator"},{"location":"src/embedding/vector_stores/chroma/validator/#validator","text":"This module contains functionality related to the the validator module for embedding.vector_stores.chroma .","title":"Validator"},{"location":"src/embedding/vector_stores/chroma/validator/#validator_1","text":"","title":"Validator"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidator","text":"Bases: BaseVectorStoreValidator Validator for Chroma vector store configuration. Validates collection settings and existence for Chroma vector store backend. Ensures proper configuration before operations are performed against the vector store. Source code in src/embedding/vector_stores/chroma/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class ChromaVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Chroma vector store configuration. Validates collection settings and existence for Chroma vector store backend. Ensures proper configuration before operations are performed against the vector store. \"\"\" def __init__ ( self , configuration : ChromaVectorStoreConfiguration , client : ChromaClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Chroma vector store settings client: Client for Chroma operations \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException: If collection already exists \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException: If collection already exists \"\"\" collection_name = self . configuration . collection_name if collection_name in self . client . list_collections (): raise CollectionExistsException ( collection_name )","title":"ChromaVectorStoreValidator"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidator.__init__","text":"Initialize validator with configuration and client. Parameters: configuration ( ChromaVectorStoreConfiguration ) \u2013 Chroma vector store settings client ( ClientAPI ) \u2013 Client for Chroma operations Source code in src/embedding/vector_stores/chroma/validator.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration : ChromaVectorStoreConfiguration , client : ChromaClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Chroma vector store settings client: Client for Chroma operations \"\"\" self . configuration = configuration self . client = client","title":"__init__"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidator.validate","text":"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException \u2013 If collection already exists Source code in src/embedding/vector_stores/chroma/validator.py 36 37 38 39 40 41 42 43 44 45 def validate ( self ) -> None : \"\"\"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException: If collection already exists \"\"\" self . validate_collection ()","title":"validate"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidator.validate_collection","text":"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException \u2013 If collection already exists Source code in src/embedding/vector_stores/chroma/validator.py 47 48 49 50 51 52 53 54 55 56 57 58 def validate_collection ( self ) -> None : \"\"\"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException: If collection already exists \"\"\" collection_name = self . configuration . collection_name if collection_name in self . client . list_collections (): raise CollectionExistsException ( collection_name )","title":"validate_collection"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidatorFactory","text":"Bases: SingletonFactory Factory for creating configured Chroma validator instances. Manages the creation and caching of ChromaVectorStoreValidator instances based on provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for Chroma vector store. Source code in src/embedding/vector_stores/chroma/validator.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class ChromaVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating configured Chroma validator instances. Manages the creation and caching of ChromaVectorStoreValidator instances based on provided configuration. Attributes: _configuration_class (Type): The configuration class for Chroma vector store. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaVectorStoreValidator : \"\"\"Creates a Chroma validator based on provided configuration. Args: configuration: Chroma connection configuration. Returns: ChromaVectorStoreValidator: Configured validator instance. \"\"\" client = ChromaVectorStoreClientFactory . create ( configuration ) return ChromaVectorStoreValidator ( configuration = configuration , client = client )","title":"ChromaVectorStoreValidatorFactory"},{"location":"src/embedding/vector_stores/chroma/vector_store/","text":"Vector_store This module contains functionality related to the the vector_store module for embedding.vector_stores.chroma . Vector_store ChromaVectorStoreFactory Bases: SingletonFactory Factory for creating configured Chroma vector stores. This singleton factory creates and manages ChromaVectorStore instances based on the provided configuration. It ensures that only one instance is created for each unique configuration. Source code in src/embedding/vector_stores/chroma/vector_store.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class ChromaVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured Chroma vector stores. This singleton factory creates and manages ChromaVectorStore instances based on the provided configuration. It ensures that only one instance is created for each unique configuration. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaVectorStore : \"\"\"Creates a Chroma vector store based on provided configuration. Args: configuration: Chroma vector store connection configuration containing host, port, and collection name. Returns: ChromaVectorStore: Configured Chroma vector store instance. \"\"\" return ChromaVectorStore ( host = configuration . host , port = str ( configuration . port ), collection_name = configuration . collection_name , )","title":"Vector_store"},{"location":"src/embedding/vector_stores/chroma/vector_store/#vector_store","text":"This module contains functionality related to the the vector_store module for embedding.vector_stores.chroma .","title":"Vector_store"},{"location":"src/embedding/vector_stores/chroma/vector_store/#vector_store_1","text":"","title":"Vector_store"},{"location":"src/embedding/vector_stores/chroma/vector_store/#src.embedding.vector_stores.chroma.vector_store.ChromaVectorStoreFactory","text":"Bases: SingletonFactory Factory for creating configured Chroma vector stores. This singleton factory creates and manages ChromaVectorStore instances based on the provided configuration. It ensures that only one instance is created for each unique configuration. Source code in src/embedding/vector_stores/chroma/vector_store.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class ChromaVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured Chroma vector stores. This singleton factory creates and manages ChromaVectorStore instances based on the provided configuration. It ensures that only one instance is created for each unique configuration. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaVectorStore : \"\"\"Creates a Chroma vector store based on provided configuration. Args: configuration: Chroma vector store connection configuration containing host, port, and collection name. Returns: ChromaVectorStore: Configured Chroma vector store instance. \"\"\" return ChromaVectorStore ( host = configuration . host , port = str ( configuration . port ), collection_name = configuration . collection_name , )","title":"ChromaVectorStoreFactory"},{"location":"src/embedding/vector_stores/core/exceptions/","text":"Exceptions This module contains functionality related to the the exceptions module for embedding.vector_stores.core . Exceptions CollectionExistsException Bases: Exception Exception raised when attempting to create a vector store collection that already exists. This exception helps prevent accidental overwriting of existing collections and provides clear error messaging about the conflicting collection name. Source code in src/embedding/vector_stores/core/exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class CollectionExistsException ( Exception ): \"\"\"Exception raised when attempting to create a vector store collection that already exists. This exception helps prevent accidental overwriting of existing collections and provides clear error messaging about the conflicting collection name. \"\"\" def __init__ ( self , collection_name : str ) -> None : \"\"\"Initialize the CollectionExistsException with the conflicting collection name. Args: collection_name: Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. \"\"\" self . collection_name = collection_name self . message = f \"Collection with name { collection_name } already exists\" super () . __init__ ( self . message ) __init__ ( collection_name ) Initialize the CollectionExistsException with the conflicting collection name. Parameters: collection_name ( str ) \u2013 Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. Source code in src/embedding/vector_stores/core/exceptions.py 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , collection_name : str ) -> None : \"\"\"Initialize the CollectionExistsException with the conflicting collection name. Args: collection_name: Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. \"\"\" self . collection_name = collection_name self . message = f \"Collection with name { collection_name } already exists\" super () . __init__ ( self . message )","title":"Exceptions"},{"location":"src/embedding/vector_stores/core/exceptions/#exceptions","text":"This module contains functionality related to the the exceptions module for embedding.vector_stores.core .","title":"Exceptions"},{"location":"src/embedding/vector_stores/core/exceptions/#exceptions_1","text":"","title":"Exceptions"},{"location":"src/embedding/vector_stores/core/exceptions/#src.embedding.vector_stores.core.exceptions.CollectionExistsException","text":"Bases: Exception Exception raised when attempting to create a vector store collection that already exists. This exception helps prevent accidental overwriting of existing collections and provides clear error messaging about the conflicting collection name. Source code in src/embedding/vector_stores/core/exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class CollectionExistsException ( Exception ): \"\"\"Exception raised when attempting to create a vector store collection that already exists. This exception helps prevent accidental overwriting of existing collections and provides clear error messaging about the conflicting collection name. \"\"\" def __init__ ( self , collection_name : str ) -> None : \"\"\"Initialize the CollectionExistsException with the conflicting collection name. Args: collection_name: Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. \"\"\" self . collection_name = collection_name self . message = f \"Collection with name { collection_name } already exists\" super () . __init__ ( self . message )","title":"CollectionExistsException"},{"location":"src/embedding/vector_stores/core/exceptions/#src.embedding.vector_stores.core.exceptions.CollectionExistsException.__init__","text":"Initialize the CollectionExistsException with the conflicting collection name. Parameters: collection_name ( str ) \u2013 Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. Source code in src/embedding/vector_stores/core/exceptions.py 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , collection_name : str ) -> None : \"\"\"Initialize the CollectionExistsException with the conflicting collection name. Args: collection_name: Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. \"\"\" self . collection_name = collection_name self . message = f \"Collection with name { collection_name } already exists\" super () . __init__ ( self . message )","title":"__init__"},{"location":"src/embedding/vector_stores/core/validator/","text":"Validator This module contains functionality related to the the validator module for embedding.vector_stores.core . Validator BaseVectorStoreValidator Bases: ABC Abstract base class for vector store validators. This class defines the interface for vector store validators which are responsible for verifying that vector store configurations are valid before attempting operations. All vector store validator implementations should inherit from this class and implement the validate method. Source code in src/embedding/vector_stores/core/validator.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class BaseVectorStoreValidator ( ABC ): \"\"\" Abstract base class for vector store validators. This class defines the interface for vector store validators which are responsible for verifying that vector store configurations are valid before attempting operations. All vector store validator implementations should inherit from this class and implement the validate method. \"\"\" @abstractmethod def validate ( self ) -> None : \"\"\" Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception: If validation fails, implementations should raise appropriate exceptions with descriptive messages. \"\"\" pass validate () abstractmethod Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception \u2013 If validation fails, implementations should raise appropriate exceptions with descriptive messages. Source code in src/embedding/vector_stores/core/validator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @abstractmethod def validate ( self ) -> None : \"\"\" Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception: If validation fails, implementations should raise appropriate exceptions with descriptive messages. \"\"\" pass","title":"Validator"},{"location":"src/embedding/vector_stores/core/validator/#validator","text":"This module contains functionality related to the the validator module for embedding.vector_stores.core .","title":"Validator"},{"location":"src/embedding/vector_stores/core/validator/#validator_1","text":"","title":"Validator"},{"location":"src/embedding/vector_stores/core/validator/#src.embedding.vector_stores.core.validator.BaseVectorStoreValidator","text":"Bases: ABC Abstract base class for vector store validators. This class defines the interface for vector store validators which are responsible for verifying that vector store configurations are valid before attempting operations. All vector store validator implementations should inherit from this class and implement the validate method. Source code in src/embedding/vector_stores/core/validator.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class BaseVectorStoreValidator ( ABC ): \"\"\" Abstract base class for vector store validators. This class defines the interface for vector store validators which are responsible for verifying that vector store configurations are valid before attempting operations. All vector store validator implementations should inherit from this class and implement the validate method. \"\"\" @abstractmethod def validate ( self ) -> None : \"\"\" Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception: If validation fails, implementations should raise appropriate exceptions with descriptive messages. \"\"\" pass","title":"BaseVectorStoreValidator"},{"location":"src/embedding/vector_stores/core/validator/#src.embedding.vector_stores.core.validator.BaseVectorStoreValidator.validate","text":"Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception \u2013 If validation fails, implementations should raise appropriate exceptions with descriptive messages. Source code in src/embedding/vector_stores/core/validator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @abstractmethod def validate ( self ) -> None : \"\"\" Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception: If validation fails, implementations should raise appropriate exceptions with descriptive messages. \"\"\" pass","title":"validate"},{"location":"src/embedding/vector_stores/pgvector/client/","text":"Client This module contains functionality related to the the client module for embedding.vector_stores.pgvector . Client PGVectorStoreClientFactory Bases: SingletonFactory Factory class for creating and managing a singleton instance of the PGVector database client. This factory ensures that only one connection to the PGVector database is maintained throughout the application lifecycle, following the Singleton pattern. Source code in src/embedding/vector_stores/pgvector/client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class PGVectorStoreClientFactory ( SingletonFactory ): \"\"\" Factory class for creating and managing a singleton instance of the PGVector database client. This factory ensures that only one connection to the PGVector database is maintained throughout the application lifecycle, following the Singleton pattern. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorClient : \"\"\" Creates a new connection to the PGVector database using the provided configuration. Args: configuration: Configuration object containing connection details and credentials for the PGVector database. Returns: A connection object to the PGVector database that can be used for database operations. \"\"\" return connect ( host = configuration . host , port = configuration . port , database = configuration . database_name , user = configuration . secrets . username . get_secret_value (), password = configuration . secrets . password . get_secret_value (), )","title":"Client"},{"location":"src/embedding/vector_stores/pgvector/client/#client","text":"This module contains functionality related to the the client module for embedding.vector_stores.pgvector .","title":"Client"},{"location":"src/embedding/vector_stores/pgvector/client/#client_1","text":"","title":"Client"},{"location":"src/embedding/vector_stores/pgvector/client/#src.embedding.vector_stores.pgvector.client.PGVectorStoreClientFactory","text":"Bases: SingletonFactory Factory class for creating and managing a singleton instance of the PGVector database client. This factory ensures that only one connection to the PGVector database is maintained throughout the application lifecycle, following the Singleton pattern. Source code in src/embedding/vector_stores/pgvector/client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class PGVectorStoreClientFactory ( SingletonFactory ): \"\"\" Factory class for creating and managing a singleton instance of the PGVector database client. This factory ensures that only one connection to the PGVector database is maintained throughout the application lifecycle, following the Singleton pattern. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorClient : \"\"\" Creates a new connection to the PGVector database using the provided configuration. Args: configuration: Configuration object containing connection details and credentials for the PGVector database. Returns: A connection object to the PGVector database that can be used for database operations. \"\"\" return connect ( host = configuration . host , port = configuration . port , database = configuration . database_name , user = configuration . secrets . username . get_secret_value (), password = configuration . secrets . password . get_secret_value (), )","title":"PGVectorStoreClientFactory"},{"location":"src/embedding/vector_stores/pgvector/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.vector_stores.pgvector . Configuration PGVectorStoreConfiguration Bases: VectorStoreConfiguration Configuration for PostgreSQL with pgvector extension as a vector store. This class provides all necessary configuration parameters to connect to and operate with a PostgreSQL database using the pgvector extension. Source code in src/embedding/vector_stores/pgvector/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class PGVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\"Configuration for PostgreSQL with pgvector extension as a vector store. This class provides all necessary configuration parameters to connect to and operate with a PostgreSQL database using the pgvector extension. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__VECTOR_STORE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username for PostgreSQL database authentication.\" ) password : SecretStr = Field ( ... , description = \"Password for PostgreSQL database authentication.\" ) name : Literal [ VectorStoreName . PGVECTOR ] = Field ( ... , description = \"The identifier of the vector store, must be PGVECTOR.\" ) database_name : str = Field ( ... , description = \"Name of the PostgreSQL database to connect to.\" ) embed_dim : int = Field ( 384 , description = \"Dimension of the vector embeddings stored in pgvector.\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials for the PostgreSQL database.\" , ) Secrets Bases: BaseSecrets Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. Source code in src/embedding/vector_stores/pgvector/configuration.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Secrets ( BaseSecrets ): \"\"\"Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__VECTOR_STORE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username for PostgreSQL database authentication.\" ) password : SecretStr = Field ( ... , description = \"Password for PostgreSQL database authentication.\" )","title":"Configuration"},{"location":"src/embedding/vector_stores/pgvector/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.vector_stores.pgvector .","title":"Configuration"},{"location":"src/embedding/vector_stores/pgvector/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/vector_stores/pgvector/configuration/#src.embedding.vector_stores.pgvector.configuration.PGVectorStoreConfiguration","text":"Bases: VectorStoreConfiguration Configuration for PostgreSQL with pgvector extension as a vector store. This class provides all necessary configuration parameters to connect to and operate with a PostgreSQL database using the pgvector extension. Source code in src/embedding/vector_stores/pgvector/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class PGVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\"Configuration for PostgreSQL with pgvector extension as a vector store. This class provides all necessary configuration parameters to connect to and operate with a PostgreSQL database using the pgvector extension. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__VECTOR_STORE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username for PostgreSQL database authentication.\" ) password : SecretStr = Field ( ... , description = \"Password for PostgreSQL database authentication.\" ) name : Literal [ VectorStoreName . PGVECTOR ] = Field ( ... , description = \"The identifier of the vector store, must be PGVECTOR.\" ) database_name : str = Field ( ... , description = \"Name of the PostgreSQL database to connect to.\" ) embed_dim : int = Field ( 384 , description = \"Dimension of the vector embeddings stored in pgvector.\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials for the PostgreSQL database.\" , )","title":"PGVectorStoreConfiguration"},{"location":"src/embedding/vector_stores/pgvector/configuration/#src.embedding.vector_stores.pgvector.configuration.PGVectorStoreConfiguration.Secrets","text":"Bases: BaseSecrets Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. Source code in src/embedding/vector_stores/pgvector/configuration.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Secrets ( BaseSecrets ): \"\"\"Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__VECTOR_STORE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username for PostgreSQL database authentication.\" ) password : SecretStr = Field ( ... , description = \"Password for PostgreSQL database authentication.\" )","title":"Secrets"},{"location":"src/embedding/vector_stores/pgvector/validator/","text":"Validator This module contains functionality related to the the validator module for embedding.vector_stores.pgvector . Validator PGVectorStoreValidator Bases: BaseVectorStoreValidator Validator for Postgres vector store configuration. Validates the existence of a table (treated as a collection) in the Postgres vector store backend. This validator ensures we don't create duplicate tables in the database. Source code in src/embedding/vector_stores/pgvector/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class PGVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Postgres vector store configuration. Validates the existence of a table (treated as a collection) in the Postgres vector store backend. This validator ensures we don't create duplicate tables in the database. \"\"\" def __init__ ( self , configuration : PGVectorStoreConfiguration , client : PGVectorClient , ): \"\"\"Initialize the PGVector validator with configuration and client. Args: configuration: Configuration for the PGVector store client: PostgreSQL connection client \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException: If the collection already exists in database \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException: If the table (collection) already exists. \"\"\" collection_name = self . configuration . collection_name with self . client . cursor () as cursor : query = f \"SELECT to_regclass('data_ { collection_name } ');\" cursor . execute ( query ) result = cursor . fetchone ()[ 0 ] if result is not None : raise CollectionExistsException ( collection_name ) __init__ ( configuration , client ) Initialize the PGVector validator with configuration and client. Parameters: configuration ( PGVectorStoreConfiguration ) \u2013 Configuration for the PGVector store client ( connection ) \u2013 PostgreSQL connection client Source code in src/embedding/vector_stores/pgvector/validator.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration : PGVectorStoreConfiguration , client : PGVectorClient , ): \"\"\"Initialize the PGVector validator with configuration and client. Args: configuration: Configuration for the PGVector store client: PostgreSQL connection client \"\"\" self . configuration = configuration self . client = client validate () Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException \u2013 If the collection already exists in database Source code in src/embedding/vector_stores/pgvector/validator.py 36 37 38 39 40 41 42 43 44 def validate ( self ) -> None : \"\"\"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException: If the collection already exists in database \"\"\" self . validate_collection () validate_collection () Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException \u2013 If the table (collection) already exists. Source code in src/embedding/vector_stores/pgvector/validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def validate_collection ( self ) -> None : \"\"\"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException: If the table (collection) already exists. \"\"\" collection_name = self . configuration . collection_name with self . client . cursor () as cursor : query = f \"SELECT to_regclass('data_ { collection_name } ');\" cursor . execute ( query ) result = cursor . fetchone ()[ 0 ] if result is not None : raise CollectionExistsException ( collection_name ) PGVectorStoreValidatorFactory Bases: SingletonFactory Factory for creating configured PGVector store validators. Creates and manages singleton instances of validators for PostgreSQL vector store backends. Attributes: _configuration_class ( Type ) \u2013 Type of the configuration class used for validation. Source code in src/embedding/vector_stores/pgvector/validator.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class PGVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating configured PGVector store validators. Creates and manages singleton instances of validators for PostgreSQL vector store backends. Attributes: _configuration_class: Type of the configuration class used for validation. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorStoreValidator : \"\"\"Creates a PGVector validator based on provided configuration. Args: configuration: PostgreSQL vector store connection configuration. Returns: PGVectorStoreValidator: Configured validator instance. \"\"\" client = PGVectorStoreClientFactory . create ( configuration ) return PGVectorStoreValidator ( configuration = configuration , client = client )","title":"Validator"},{"location":"src/embedding/vector_stores/pgvector/validator/#validator","text":"This module contains functionality related to the the validator module for embedding.vector_stores.pgvector .","title":"Validator"},{"location":"src/embedding/vector_stores/pgvector/validator/#validator_1","text":"","title":"Validator"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidator","text":"Bases: BaseVectorStoreValidator Validator for Postgres vector store configuration. Validates the existence of a table (treated as a collection) in the Postgres vector store backend. This validator ensures we don't create duplicate tables in the database. Source code in src/embedding/vector_stores/pgvector/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class PGVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Postgres vector store configuration. Validates the existence of a table (treated as a collection) in the Postgres vector store backend. This validator ensures we don't create duplicate tables in the database. \"\"\" def __init__ ( self , configuration : PGVectorStoreConfiguration , client : PGVectorClient , ): \"\"\"Initialize the PGVector validator with configuration and client. Args: configuration: Configuration for the PGVector store client: PostgreSQL connection client \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException: If the collection already exists in database \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException: If the table (collection) already exists. \"\"\" collection_name = self . configuration . collection_name with self . client . cursor () as cursor : query = f \"SELECT to_regclass('data_ { collection_name } ');\" cursor . execute ( query ) result = cursor . fetchone ()[ 0 ] if result is not None : raise CollectionExistsException ( collection_name )","title":"PGVectorStoreValidator"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidator.__init__","text":"Initialize the PGVector validator with configuration and client. Parameters: configuration ( PGVectorStoreConfiguration ) \u2013 Configuration for the PGVector store client ( connection ) \u2013 PostgreSQL connection client Source code in src/embedding/vector_stores/pgvector/validator.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration : PGVectorStoreConfiguration , client : PGVectorClient , ): \"\"\"Initialize the PGVector validator with configuration and client. Args: configuration: Configuration for the PGVector store client: PostgreSQL connection client \"\"\" self . configuration = configuration self . client = client","title":"__init__"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidator.validate","text":"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException \u2013 If the collection already exists in database Source code in src/embedding/vector_stores/pgvector/validator.py 36 37 38 39 40 41 42 43 44 def validate ( self ) -> None : \"\"\"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException: If the collection already exists in database \"\"\" self . validate_collection ()","title":"validate"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidator.validate_collection","text":"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException \u2013 If the table (collection) already exists. Source code in src/embedding/vector_stores/pgvector/validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def validate_collection ( self ) -> None : \"\"\"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException: If the table (collection) already exists. \"\"\" collection_name = self . configuration . collection_name with self . client . cursor () as cursor : query = f \"SELECT to_regclass('data_ { collection_name } ');\" cursor . execute ( query ) result = cursor . fetchone ()[ 0 ] if result is not None : raise CollectionExistsException ( collection_name )","title":"validate_collection"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidatorFactory","text":"Bases: SingletonFactory Factory for creating configured PGVector store validators. Creates and manages singleton instances of validators for PostgreSQL vector store backends. Attributes: _configuration_class ( Type ) \u2013 Type of the configuration class used for validation. Source code in src/embedding/vector_stores/pgvector/validator.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class PGVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating configured PGVector store validators. Creates and manages singleton instances of validators for PostgreSQL vector store backends. Attributes: _configuration_class: Type of the configuration class used for validation. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorStoreValidator : \"\"\"Creates a PGVector validator based on provided configuration. Args: configuration: PostgreSQL vector store connection configuration. Returns: PGVectorStoreValidator: Configured validator instance. \"\"\" client = PGVectorStoreClientFactory . create ( configuration ) return PGVectorStoreValidator ( configuration = configuration , client = client )","title":"PGVectorStoreValidatorFactory"},{"location":"src/embedding/vector_stores/pgvector/vector_store/","text":"Vector_store This module contains functionality related to the the vector_store module for embedding.vector_stores.pgvector . Vector_store PGVectorStoreFactory Bases: SingletonFactory Factory for creating configured PostgreSQL vector store clients. This factory creates and manages a singleton instance of PGVectorStore for vector similarity search using the pgvector extension. Source code in src/embedding/vector_stores/pgvector/vector_store.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class PGVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured PostgreSQL vector store clients. This factory creates and manages a singleton instance of PGVectorStore for vector similarity search using the pgvector extension. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorStore : \"\"\"Creates a PostgreSQL vector store client based on provided configuration. Args: configuration: PostgreSQL vector store connection configuration. Returns: PGVectorStore: Configured PostgreSQL vector store instance. \"\"\" return PGVectorStore . from_params ( database = configuration . database_name , host = configuration . host , password = configuration . secrets . password . get_secret_value (), port = configuration . port , user = configuration . secrets . username . get_secret_value (), table_name = configuration . collection_name , embed_dim = configuration . embed_dim , )","title":"Vector_store"},{"location":"src/embedding/vector_stores/pgvector/vector_store/#vector_store","text":"This module contains functionality related to the the vector_store module for embedding.vector_stores.pgvector .","title":"Vector_store"},{"location":"src/embedding/vector_stores/pgvector/vector_store/#vector_store_1","text":"","title":"Vector_store"},{"location":"src/embedding/vector_stores/pgvector/vector_store/#src.embedding.vector_stores.pgvector.vector_store.PGVectorStoreFactory","text":"Bases: SingletonFactory Factory for creating configured PostgreSQL vector store clients. This factory creates and manages a singleton instance of PGVectorStore for vector similarity search using the pgvector extension. Source code in src/embedding/vector_stores/pgvector/vector_store.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class PGVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured PostgreSQL vector store clients. This factory creates and manages a singleton instance of PGVectorStore for vector similarity search using the pgvector extension. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorStore : \"\"\"Creates a PostgreSQL vector store client based on provided configuration. Args: configuration: PostgreSQL vector store connection configuration. Returns: PGVectorStore: Configured PostgreSQL vector store instance. \"\"\" return PGVectorStore . from_params ( database = configuration . database_name , host = configuration . host , password = configuration . secrets . password . get_secret_value (), port = configuration . port , user = configuration . secrets . username . get_secret_value (), table_name = configuration . collection_name , embed_dim = configuration . embed_dim , )","title":"PGVectorStoreFactory"},{"location":"src/embedding/vector_stores/qdrant/client/","text":"Client This module contains functionality related to the the client module for embedding.vector_stores.qdrant . Client QdrantClientFactory Bases: SingletonFactory Singleton factory for creating and managing Qdrant client instances. This factory ensures that only one Qdrant client instance is created for each unique configuration, promoting resource efficiency. Source code in src/embedding/vector_stores/qdrant/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class QdrantClientFactory ( SingletonFactory ): \"\"\" Singleton factory for creating and managing Qdrant client instances. This factory ensures that only one Qdrant client instance is created for each unique configuration, promoting resource efficiency. \"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantClient : \"\"\" Create a new QdrantClient instance based on the provided configuration. Args: configuration (QDrantVectorStoreConfiguration): Configuration containing connection parameters for the Qdrant server. Returns: QdrantClient: A configured client instance for interacting with the Qdrant vector database. \"\"\" return QdrantClient ( url = configuration . url , port = configuration . port , check_compatibility = False , )","title":"Client"},{"location":"src/embedding/vector_stores/qdrant/client/#client","text":"This module contains functionality related to the the client module for embedding.vector_stores.qdrant .","title":"Client"},{"location":"src/embedding/vector_stores/qdrant/client/#client_1","text":"","title":"Client"},{"location":"src/embedding/vector_stores/qdrant/client/#src.embedding.vector_stores.qdrant.client.QdrantClientFactory","text":"Bases: SingletonFactory Singleton factory for creating and managing Qdrant client instances. This factory ensures that only one Qdrant client instance is created for each unique configuration, promoting resource efficiency. Source code in src/embedding/vector_stores/qdrant/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class QdrantClientFactory ( SingletonFactory ): \"\"\" Singleton factory for creating and managing Qdrant client instances. This factory ensures that only one Qdrant client instance is created for each unique configuration, promoting resource efficiency. \"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantClient : \"\"\" Create a new QdrantClient instance based on the provided configuration. Args: configuration (QDrantVectorStoreConfiguration): Configuration containing connection parameters for the Qdrant server. Returns: QdrantClient: A configured client instance for interacting with the Qdrant vector database. \"\"\" return QdrantClient ( url = configuration . url , port = configuration . port , check_compatibility = False , )","title":"QdrantClientFactory"},{"location":"src/embedding/vector_stores/qdrant/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.vector_stores.qdrant . Configuration QDrantVectorStoreConfiguration Bases: VectorStoreConfiguration Configuration settings specific to QDrant vector store. Extends the base VectorStoreConfiguration with QDrant-specific properties and functionality. Source code in src/embedding/vector_stores/qdrant/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class QDrantVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\" Configuration settings specific to QDrant vector store. Extends the base VectorStoreConfiguration with QDrant-specific properties and functionality. \"\"\" name : Literal [ VectorStoreName . QDRANT ] = Field ( ... , description = \"The name of the vector store.\" ) @property def url ( self ) -> str : \"\"\" Constructs the full URL for connecting to the QDrant service. Returns: str: The complete URL with protocol, host, and port. \"\"\" return f \" { self . protocol } :// { self . host } : { self . port } \" url property Constructs the full URL for connecting to the QDrant service. Returns: str ( str ) \u2013 The complete URL with protocol, host, and port.","title":"Configuration"},{"location":"src/embedding/vector_stores/qdrant/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.vector_stores.qdrant .","title":"Configuration"},{"location":"src/embedding/vector_stores/qdrant/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/vector_stores/qdrant/configuration/#src.embedding.vector_stores.qdrant.configuration.QDrantVectorStoreConfiguration","text":"Bases: VectorStoreConfiguration Configuration settings specific to QDrant vector store. Extends the base VectorStoreConfiguration with QDrant-specific properties and functionality. Source code in src/embedding/vector_stores/qdrant/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class QDrantVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\" Configuration settings specific to QDrant vector store. Extends the base VectorStoreConfiguration with QDrant-specific properties and functionality. \"\"\" name : Literal [ VectorStoreName . QDRANT ] = Field ( ... , description = \"The name of the vector store.\" ) @property def url ( self ) -> str : \"\"\" Constructs the full URL for connecting to the QDrant service. Returns: str: The complete URL with protocol, host, and port. \"\"\" return f \" { self . protocol } :// { self . host } : { self . port } \"","title":"QDrantVectorStoreConfiguration"},{"location":"src/embedding/vector_stores/qdrant/configuration/#src.embedding.vector_stores.qdrant.configuration.QDrantVectorStoreConfiguration.url","text":"Constructs the full URL for connecting to the QDrant service. Returns: str ( str ) \u2013 The complete URL with protocol, host, and port.","title":"url"},{"location":"src/embedding/vector_stores/qdrant/validator/","text":"Validator This module contains functionality related to the the validator module for embedding.vector_stores.qdrant . Validator QdrantVectorStoreValidator Bases: BaseVectorStoreValidator Validator for Qdrant vector store configuration. Validates collection settings and existence for Qdrant vector store backend. Attributes: configuration \u2013 Settings for vector store client \u2013 Client for Qdrant interactions Source code in src/embedding/vector_stores/qdrant/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class QdrantVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Qdrant vector store configuration. Validates collection settings and existence for Qdrant vector store backend. Attributes: configuration: Settings for vector store client: Client for Qdrant interactions \"\"\" def __init__ ( self , configuration : QDrantVectorStoreConfiguration , client : QdrantClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Qdrant vector store settings client: Client for Qdrant operations \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException: If collection already exists in the Qdrant database with the specified name \"\"\" collection_name = self . configuration . collection_name if self . client . collection_exists ( collection_name ): raise CollectionExistsException ( collection_name ) __init__ ( configuration , client ) Initialize validator with configuration and client. Parameters: configuration ( QDrantVectorStoreConfiguration ) \u2013 Qdrant vector store settings client ( QdrantClient ) \u2013 Client for Qdrant operations Source code in src/embedding/vector_stores/qdrant/validator.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , configuration : QDrantVectorStoreConfiguration , client : QdrantClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Qdrant vector store settings client: Client for Qdrant operations \"\"\" self . configuration = configuration self . client = client validate () Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. Source code in src/embedding/vector_stores/qdrant/validator.py 39 40 41 42 43 44 45 def validate ( self ) -> None : \"\"\"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. \"\"\" self . validate_collection () validate_collection () Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException \u2013 If collection already exists in the Qdrant database with the specified name Source code in src/embedding/vector_stores/qdrant/validator.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def validate_collection ( self ) -> None : \"\"\"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException: If collection already exists in the Qdrant database with the specified name \"\"\" collection_name = self . configuration . collection_name if self . client . collection_exists ( collection_name ): raise CollectionExistsException ( collection_name ) QdrantVectorStoreValidatorFactory Bases: SingletonFactory Factory for creating Qdrant vector store validators. Implements the singleton pattern to ensure only one validator instance exists for each unique configuration. Source code in src/embedding/vector_stores/qdrant/validator.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class QdrantVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating Qdrant vector store validators. Implements the singleton pattern to ensure only one validator instance exists for each unique configuration. \"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantVectorStoreValidator : \"\"\"Creates a Qdrant validator based on provided configuration. Instantiates a new validator with the appropriate client for the given configuration. Args: configuration: QDrant connection configuration. Returns: QdrantVectorStoreValidator: Configured validator instance. \"\"\" client = QdrantClientFactory . create ( configuration ) return QdrantVectorStoreValidator ( configuration = configuration , client = client )","title":"Validator"},{"location":"src/embedding/vector_stores/qdrant/validator/#validator","text":"This module contains functionality related to the the validator module for embedding.vector_stores.qdrant .","title":"Validator"},{"location":"src/embedding/vector_stores/qdrant/validator/#validator_1","text":"","title":"Validator"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidator","text":"Bases: BaseVectorStoreValidator Validator for Qdrant vector store configuration. Validates collection settings and existence for Qdrant vector store backend. Attributes: configuration \u2013 Settings for vector store client \u2013 Client for Qdrant interactions Source code in src/embedding/vector_stores/qdrant/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class QdrantVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Qdrant vector store configuration. Validates collection settings and existence for Qdrant vector store backend. Attributes: configuration: Settings for vector store client: Client for Qdrant interactions \"\"\" def __init__ ( self , configuration : QDrantVectorStoreConfiguration , client : QdrantClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Qdrant vector store settings client: Client for Qdrant operations \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException: If collection already exists in the Qdrant database with the specified name \"\"\" collection_name = self . configuration . collection_name if self . client . collection_exists ( collection_name ): raise CollectionExistsException ( collection_name )","title":"QdrantVectorStoreValidator"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidator.__init__","text":"Initialize validator with configuration and client. Parameters: configuration ( QDrantVectorStoreConfiguration ) \u2013 Qdrant vector store settings client ( QdrantClient ) \u2013 Client for Qdrant operations Source code in src/embedding/vector_stores/qdrant/validator.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , configuration : QDrantVectorStoreConfiguration , client : QdrantClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Qdrant vector store settings client: Client for Qdrant operations \"\"\" self . configuration = configuration self . client = client","title":"__init__"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidator.validate","text":"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. Source code in src/embedding/vector_stores/qdrant/validator.py 39 40 41 42 43 44 45 def validate ( self ) -> None : \"\"\"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. \"\"\" self . validate_collection ()","title":"validate"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidator.validate_collection","text":"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException \u2013 If collection already exists in the Qdrant database with the specified name Source code in src/embedding/vector_stores/qdrant/validator.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def validate_collection ( self ) -> None : \"\"\"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException: If collection already exists in the Qdrant database with the specified name \"\"\" collection_name = self . configuration . collection_name if self . client . collection_exists ( collection_name ): raise CollectionExistsException ( collection_name )","title":"validate_collection"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidatorFactory","text":"Bases: SingletonFactory Factory for creating Qdrant vector store validators. Implements the singleton pattern to ensure only one validator instance exists for each unique configuration. Source code in src/embedding/vector_stores/qdrant/validator.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class QdrantVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating Qdrant vector store validators. Implements the singleton pattern to ensure only one validator instance exists for each unique configuration. \"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantVectorStoreValidator : \"\"\"Creates a Qdrant validator based on provided configuration. Instantiates a new validator with the appropriate client for the given configuration. Args: configuration: QDrant connection configuration. Returns: QdrantVectorStoreValidator: Configured validator instance. \"\"\" client = QdrantClientFactory . create ( configuration ) return QdrantVectorStoreValidator ( configuration = configuration , client = client )","title":"QdrantVectorStoreValidatorFactory"},{"location":"src/embedding/vector_stores/qdrant/vector_store/","text":"Vector_store This module contains functionality related to the the vector_store module for embedding.vector_stores.qdrant . Vector_store QdrantVectorStoreFactory Bases: SingletonFactory Factory for creating configured Qdrant vector store instances using the Singleton pattern. Source code in src/embedding/vector_stores/qdrant/vector_store.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class QdrantVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured Qdrant vector store instances using the Singleton pattern.\"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantVectorStore : \"\"\"Creates a Qdrant vector store based on provided configuration. This method instantiates a Qdrant client using the QdrantClientFactory and uses it to create a QdrantVectorStore instance with the specified collection name from the configuration. Args: configuration: QDrant connection configuration containing connection parameters and collection name. Returns: QdrantVectorStore: Configured vector store instance ready for embedding storage and retrieval operations. \"\"\" client = QdrantClientFactory . create ( configuration ) return QdrantVectorStore ( client = client , collection_name = configuration . collection_name )","title":"Vector_store"},{"location":"src/embedding/vector_stores/qdrant/vector_store/#vector_store","text":"This module contains functionality related to the the vector_store module for embedding.vector_stores.qdrant .","title":"Vector_store"},{"location":"src/embedding/vector_stores/qdrant/vector_store/#vector_store_1","text":"","title":"Vector_store"},{"location":"src/embedding/vector_stores/qdrant/vector_store/#src.embedding.vector_stores.qdrant.vector_store.QdrantVectorStoreFactory","text":"Bases: SingletonFactory Factory for creating configured Qdrant vector store instances using the Singleton pattern. Source code in src/embedding/vector_stores/qdrant/vector_store.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class QdrantVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured Qdrant vector store instances using the Singleton pattern.\"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantVectorStore : \"\"\"Creates a Qdrant vector store based on provided configuration. This method instantiates a Qdrant client using the QdrantClientFactory and uses it to create a QdrantVectorStore instance with the specified collection name from the configuration. Args: configuration: QDrant connection configuration containing connection parameters and collection name. Returns: QdrantVectorStore: Configured vector store instance ready for embedding storage and retrieval operations. \"\"\" client = QdrantClientFactory . create ( configuration ) return QdrantVectorStore ( client = client , collection_name = configuration . collection_name )","title":"QdrantVectorStoreFactory"},{"location":"src/evaluation/bootstrap/initializer/","text":"Initializer This module contains functionality related to the the initializer module for evaluation.bootstrap . Initializer EvaluationInitializer Bases: AugmentationInitializer Initializer for evaluation processes that extends augmentation capabilities. This initializer inherits from AugmentationInitializer to reuse augmentation functionality while setting up evaluation-specific components. It handles the configuration binding and initialization of services required for evaluating RAG systems. The class bridges augmentation and evaluation processes by providing a consistent initialization pattern across the application. It's responsible for: - Loading evaluation configuration - Setting up evaluation metrics and benchmarks - Binding evaluation-specific dependencies Source code in src/evaluation/bootstrap/initializer.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EvaluationInitializer ( AugmentationInitializer ): \"\"\"Initializer for evaluation processes that extends augmentation capabilities. This initializer inherits from AugmentationInitializer to reuse augmentation functionality while setting up evaluation-specific components. It handles the configuration binding and initialization of services required for evaluating RAG systems. The class bridges augmentation and evaluation processes by providing a consistent initialization pattern across the application. It's responsible for: - Loading evaluation configuration - Setting up evaluation metrics and benchmarks - Binding evaluation-specific dependencies \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EvaluationConfiguration , ): \"\"\"Initialize the evaluation components with the specified configuration. Args: configuration_class: Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. \"\"\" super () . __init__ ( configuration_class ) __init__ ( configuration_class = EvaluationConfiguration ) Initialize the evaluation components with the specified configuration. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: EvaluationConfiguration ) \u2013 Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. Source code in src/evaluation/bootstrap/initializer.py 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EvaluationConfiguration , ): \"\"\"Initialize the evaluation components with the specified configuration. Args: configuration_class: Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. \"\"\" super () . __init__ ( configuration_class )","title":"Initializer"},{"location":"src/evaluation/bootstrap/initializer/#initializer","text":"This module contains functionality related to the the initializer module for evaluation.bootstrap .","title":"Initializer"},{"location":"src/evaluation/bootstrap/initializer/#initializer_1","text":"","title":"Initializer"},{"location":"src/evaluation/bootstrap/initializer/#src.evaluation.bootstrap.initializer.EvaluationInitializer","text":"Bases: AugmentationInitializer Initializer for evaluation processes that extends augmentation capabilities. This initializer inherits from AugmentationInitializer to reuse augmentation functionality while setting up evaluation-specific components. It handles the configuration binding and initialization of services required for evaluating RAG systems. The class bridges augmentation and evaluation processes by providing a consistent initialization pattern across the application. It's responsible for: - Loading evaluation configuration - Setting up evaluation metrics and benchmarks - Binding evaluation-specific dependencies Source code in src/evaluation/bootstrap/initializer.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EvaluationInitializer ( AugmentationInitializer ): \"\"\"Initializer for evaluation processes that extends augmentation capabilities. This initializer inherits from AugmentationInitializer to reuse augmentation functionality while setting up evaluation-specific components. It handles the configuration binding and initialization of services required for evaluating RAG systems. The class bridges augmentation and evaluation processes by providing a consistent initialization pattern across the application. It's responsible for: - Loading evaluation configuration - Setting up evaluation metrics and benchmarks - Binding evaluation-specific dependencies \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EvaluationConfiguration , ): \"\"\"Initialize the evaluation components with the specified configuration. Args: configuration_class: Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. \"\"\" super () . __init__ ( configuration_class )","title":"EvaluationInitializer"},{"location":"src/evaluation/bootstrap/initializer/#src.evaluation.bootstrap.initializer.EvaluationInitializer.__init__","text":"Initialize the evaluation components with the specified configuration. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: EvaluationConfiguration ) \u2013 Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. Source code in src/evaluation/bootstrap/initializer.py 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EvaluationConfiguration , ): \"\"\"Initialize the evaluation components with the specified configuration. Args: configuration_class: Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. \"\"\" super () . __init__ ( configuration_class )","title":"__init__"},{"location":"src/evaluation/bootstrap/configuration/configuration/","text":"Configuration This module contains functionality related to the the configuration module for evaluation.bootstrap.configuration . Configuration EvaluationConfiguration Bases: AugmentationConfiguration Main configuration class for the evaluation process. This class extends the AugmentationConfiguration to include evaluation-specific settings, allowing the evaluation pipeline to be properly configured. Source code in src/evaluation/bootstrap/configuration/configuration.py 75 76 77 78 79 80 81 82 83 84 85 86 class EvaluationConfiguration ( AugmentationConfiguration ): \"\"\" Main configuration class for the evaluation process. This class extends the AugmentationConfiguration to include evaluation-specific settings, allowing the evaluation pipeline to be properly configured. \"\"\" evaluation : _EvaluationConfiguration = Field ( ... , description = \"Configuration of the augmentation process.\" )","title":"Configuration"},{"location":"src/evaluation/bootstrap/configuration/configuration/#configuration","text":"This module contains functionality related to the the configuration module for evaluation.bootstrap.configuration .","title":"Configuration"},{"location":"src/evaluation/bootstrap/configuration/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/evaluation/bootstrap/configuration/configuration/#src.evaluation.bootstrap.configuration.configuration.EvaluationConfiguration","text":"Bases: AugmentationConfiguration Main configuration class for the evaluation process. This class extends the AugmentationConfiguration to include evaluation-specific settings, allowing the evaluation pipeline to be properly configured. Source code in src/evaluation/bootstrap/configuration/configuration.py 75 76 77 78 79 80 81 82 83 84 85 86 class EvaluationConfiguration ( AugmentationConfiguration ): \"\"\" Main configuration class for the evaluation process. This class extends the AugmentationConfiguration to include evaluation-specific settings, allowing the evaluation pipeline to be properly configured. \"\"\" evaluation : _EvaluationConfiguration = Field ( ... , description = \"Configuration of the augmentation process.\" )","title":"EvaluationConfiguration"},{"location":"src/evaluation/evaluators/langfuse/","text":"Langfuse This module contains functionality related to the the langfuse module for evaluation.evaluators . Langfuse LangfuseEvaluator Evaluator that tracks RAG performance metrics in Langfuse. Integrates chat engine execution with RAGAS evaluation and publishes quality metrics to Langfuse for monitoring and analysis. Source code in src/evaluation/evaluators/langfuse.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class LangfuseEvaluator : \"\"\"Evaluator that tracks RAG performance metrics in Langfuse. Integrates chat engine execution with RAGAS evaluation and publishes quality metrics to Langfuse for monitoring and analysis. \"\"\" def __init__ ( self , chat_engine : LangfuseChatEngine , langfuse_dataset_service : LangfuseDatasetService , ragas_evaluator : RagasEvaluator , run_metadata : dict , ) -> None : \"\"\"Initialize the Langfuse evaluator with required components. Args: chat_engine: The chat engine that will generate responses langfuse_dataset_service: Service to retrieve evaluation datasets ragas_evaluator: Component to calculate quality metrics run_metadata: Dictionary containing metadata about the evaluation run \"\"\" self . chat_engine = chat_engine self . ragas_evaluator = ragas_evaluator self . langfuse_dataset_service = langfuse_dataset_service self . run_name = run_metadata [ \"build_name\" ] self . run_metadata = run_metadata def evaluate ( self , dataset_name : str ) -> None : \"\"\"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Args: dataset_name: Identifier of the dataset to evaluate Note: Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). \"\"\" langfuse_dataset = self . langfuse_dataset_service . get_dataset ( dataset_name ) for item in langfuse_dataset . items : if item . status == DatasetStatus . ARCHIVED : continue response = self . chat_engine . chat ( message = item . input [ \"query_str\" ], chat_history = [], chainlit_message_id = None , source_process = SourceProcess . DEPLOYMENT_EVALUATION , ) scores = self . ragas_evaluator . evaluate ( response = response , item = item ) trace = self . chat_engine . get_current_langfuse_trace () trace . update ( output = response . response ) item . link ( trace_or_observation = trace , run_name = self . run_name , run_description = \"Deployment evaluation\" , run_metadata = self . run_metadata , ) # TODO: How to handle NaNs? if not isnan ( scores [ \"answer_relevancy\" ]): trace . score ( name = \"Answer Relevancy\" , value = scores [ \"answer_relevancy\" ] ) if not isnan ( scores [ \"context_recall\" ]): trace . score ( name = \"Context Recall\" , value = scores [ \"context_recall\" ] ) if not isnan ( scores [ \"faithfulness\" ]): trace . score ( name = \"Faithfulness\" , value = scores [ \"faithfulness\" ]) if not isnan ( scores [ \"harmfulness\" ]): trace . score ( name = \"Harmfulness\" , value = scores [ \"harmfulness\" ]) __init__ ( chat_engine , langfuse_dataset_service , ragas_evaluator , run_metadata ) Initialize the Langfuse evaluator with required components. Parameters: chat_engine ( LangfuseChatEngine ) \u2013 The chat engine that will generate responses langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service to retrieve evaluation datasets ragas_evaluator ( RagasEvaluator ) \u2013 Component to calculate quality metrics run_metadata ( dict ) \u2013 Dictionary containing metadata about the evaluation run Source code in src/evaluation/evaluators/langfuse.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , chat_engine : LangfuseChatEngine , langfuse_dataset_service : LangfuseDatasetService , ragas_evaluator : RagasEvaluator , run_metadata : dict , ) -> None : \"\"\"Initialize the Langfuse evaluator with required components. Args: chat_engine: The chat engine that will generate responses langfuse_dataset_service: Service to retrieve evaluation datasets ragas_evaluator: Component to calculate quality metrics run_metadata: Dictionary containing metadata about the evaluation run \"\"\" self . chat_engine = chat_engine self . ragas_evaluator = ragas_evaluator self . langfuse_dataset_service = langfuse_dataset_service self . run_name = run_metadata [ \"build_name\" ] self . run_metadata = run_metadata evaluate ( dataset_name ) Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Parameters: dataset_name ( str ) \u2013 Identifier of the dataset to evaluate Note Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). Source code in src/evaluation/evaluators/langfuse.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def evaluate ( self , dataset_name : str ) -> None : \"\"\"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Args: dataset_name: Identifier of the dataset to evaluate Note: Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). \"\"\" langfuse_dataset = self . langfuse_dataset_service . get_dataset ( dataset_name ) for item in langfuse_dataset . items : if item . status == DatasetStatus . ARCHIVED : continue response = self . chat_engine . chat ( message = item . input [ \"query_str\" ], chat_history = [], chainlit_message_id = None , source_process = SourceProcess . DEPLOYMENT_EVALUATION , ) scores = self . ragas_evaluator . evaluate ( response = response , item = item ) trace = self . chat_engine . get_current_langfuse_trace () trace . update ( output = response . response ) item . link ( trace_or_observation = trace , run_name = self . run_name , run_description = \"Deployment evaluation\" , run_metadata = self . run_metadata , ) # TODO: How to handle NaNs? if not isnan ( scores [ \"answer_relevancy\" ]): trace . score ( name = \"Answer Relevancy\" , value = scores [ \"answer_relevancy\" ] ) if not isnan ( scores [ \"context_recall\" ]): trace . score ( name = \"Context Recall\" , value = scores [ \"context_recall\" ] ) if not isnan ( scores [ \"faithfulness\" ]): trace . score ( name = \"Faithfulness\" , value = scores [ \"faithfulness\" ]) if not isnan ( scores [ \"harmfulness\" ]): trace . score ( name = \"Harmfulness\" , value = scores [ \"harmfulness\" ]) LangfuseEvaluatorFactory Bases: Factory Factory for creating LangfuseEvaluator instances. Creates properly configured evaluators based on the provided configuration. Source code in src/evaluation/evaluators/langfuse.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class LangfuseEvaluatorFactory ( Factory ): \"\"\"Factory for creating LangfuseEvaluator instances. Creates properly configured evaluators based on the provided configuration. \"\"\" _configuration_class : Type = EvaluationConfiguration @classmethod def _create_instance ( cls , configuration : EvaluationConfiguration ) -> LangfuseEvaluator : \"\"\"Create a new LangfuseEvaluator instance. Args: configuration: Complete evaluation configuration containing settings for the chat engine, datasets, and metrics Returns: A fully configured LangfuseEvaluator instance ready for evaluation \"\"\" chat_engine = ChatEngineRegistry . get ( configuration . augmentation . chat_engine . name ) . create ( configuration ) langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . augmentation . langfuse ) ragas_evaluator = RagasEvaluatorFactory . create ( configuration . evaluation ) return LangfuseEvaluator ( chat_engine = chat_engine , langfuse_dataset_service = langfuse_dataset_service , ragas_evaluator = ragas_evaluator , run_metadata = { \"build_name\" : configuration . metadata . build_name , \"llm_configuration\" : configuration . augmentation . chat_engine . llm . name , \"judge_llm_configuration\" : configuration . evaluation . judge_llm . name , }, )","title":"Langfuse"},{"location":"src/evaluation/evaluators/langfuse/#langfuse","text":"This module contains functionality related to the the langfuse module for evaluation.evaluators .","title":"Langfuse"},{"location":"src/evaluation/evaluators/langfuse/#langfuse_1","text":"","title":"Langfuse"},{"location":"src/evaluation/evaluators/langfuse/#src.evaluation.evaluators.langfuse.LangfuseEvaluator","text":"Evaluator that tracks RAG performance metrics in Langfuse. Integrates chat engine execution with RAGAS evaluation and publishes quality metrics to Langfuse for monitoring and analysis. Source code in src/evaluation/evaluators/langfuse.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class LangfuseEvaluator : \"\"\"Evaluator that tracks RAG performance metrics in Langfuse. Integrates chat engine execution with RAGAS evaluation and publishes quality metrics to Langfuse for monitoring and analysis. \"\"\" def __init__ ( self , chat_engine : LangfuseChatEngine , langfuse_dataset_service : LangfuseDatasetService , ragas_evaluator : RagasEvaluator , run_metadata : dict , ) -> None : \"\"\"Initialize the Langfuse evaluator with required components. Args: chat_engine: The chat engine that will generate responses langfuse_dataset_service: Service to retrieve evaluation datasets ragas_evaluator: Component to calculate quality metrics run_metadata: Dictionary containing metadata about the evaluation run \"\"\" self . chat_engine = chat_engine self . ragas_evaluator = ragas_evaluator self . langfuse_dataset_service = langfuse_dataset_service self . run_name = run_metadata [ \"build_name\" ] self . run_metadata = run_metadata def evaluate ( self , dataset_name : str ) -> None : \"\"\"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Args: dataset_name: Identifier of the dataset to evaluate Note: Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). \"\"\" langfuse_dataset = self . langfuse_dataset_service . get_dataset ( dataset_name ) for item in langfuse_dataset . items : if item . status == DatasetStatus . ARCHIVED : continue response = self . chat_engine . chat ( message = item . input [ \"query_str\" ], chat_history = [], chainlit_message_id = None , source_process = SourceProcess . DEPLOYMENT_EVALUATION , ) scores = self . ragas_evaluator . evaluate ( response = response , item = item ) trace = self . chat_engine . get_current_langfuse_trace () trace . update ( output = response . response ) item . link ( trace_or_observation = trace , run_name = self . run_name , run_description = \"Deployment evaluation\" , run_metadata = self . run_metadata , ) # TODO: How to handle NaNs? if not isnan ( scores [ \"answer_relevancy\" ]): trace . score ( name = \"Answer Relevancy\" , value = scores [ \"answer_relevancy\" ] ) if not isnan ( scores [ \"context_recall\" ]): trace . score ( name = \"Context Recall\" , value = scores [ \"context_recall\" ] ) if not isnan ( scores [ \"faithfulness\" ]): trace . score ( name = \"Faithfulness\" , value = scores [ \"faithfulness\" ]) if not isnan ( scores [ \"harmfulness\" ]): trace . score ( name = \"Harmfulness\" , value = scores [ \"harmfulness\" ])","title":"LangfuseEvaluator"},{"location":"src/evaluation/evaluators/langfuse/#src.evaluation.evaluators.langfuse.LangfuseEvaluator.__init__","text":"Initialize the Langfuse evaluator with required components. Parameters: chat_engine ( LangfuseChatEngine ) \u2013 The chat engine that will generate responses langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service to retrieve evaluation datasets ragas_evaluator ( RagasEvaluator ) \u2013 Component to calculate quality metrics run_metadata ( dict ) \u2013 Dictionary containing metadata about the evaluation run Source code in src/evaluation/evaluators/langfuse.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , chat_engine : LangfuseChatEngine , langfuse_dataset_service : LangfuseDatasetService , ragas_evaluator : RagasEvaluator , run_metadata : dict , ) -> None : \"\"\"Initialize the Langfuse evaluator with required components. Args: chat_engine: The chat engine that will generate responses langfuse_dataset_service: Service to retrieve evaluation datasets ragas_evaluator: Component to calculate quality metrics run_metadata: Dictionary containing metadata about the evaluation run \"\"\" self . chat_engine = chat_engine self . ragas_evaluator = ragas_evaluator self . langfuse_dataset_service = langfuse_dataset_service self . run_name = run_metadata [ \"build_name\" ] self . run_metadata = run_metadata","title":"__init__"},{"location":"src/evaluation/evaluators/langfuse/#src.evaluation.evaluators.langfuse.LangfuseEvaluator.evaluate","text":"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Parameters: dataset_name ( str ) \u2013 Identifier of the dataset to evaluate Note Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). Source code in src/evaluation/evaluators/langfuse.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def evaluate ( self , dataset_name : str ) -> None : \"\"\"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Args: dataset_name: Identifier of the dataset to evaluate Note: Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). \"\"\" langfuse_dataset = self . langfuse_dataset_service . get_dataset ( dataset_name ) for item in langfuse_dataset . items : if item . status == DatasetStatus . ARCHIVED : continue response = self . chat_engine . chat ( message = item . input [ \"query_str\" ], chat_history = [], chainlit_message_id = None , source_process = SourceProcess . DEPLOYMENT_EVALUATION , ) scores = self . ragas_evaluator . evaluate ( response = response , item = item ) trace = self . chat_engine . get_current_langfuse_trace () trace . update ( output = response . response ) item . link ( trace_or_observation = trace , run_name = self . run_name , run_description = \"Deployment evaluation\" , run_metadata = self . run_metadata , ) # TODO: How to handle NaNs? if not isnan ( scores [ \"answer_relevancy\" ]): trace . score ( name = \"Answer Relevancy\" , value = scores [ \"answer_relevancy\" ] ) if not isnan ( scores [ \"context_recall\" ]): trace . score ( name = \"Context Recall\" , value = scores [ \"context_recall\" ] ) if not isnan ( scores [ \"faithfulness\" ]): trace . score ( name = \"Faithfulness\" , value = scores [ \"faithfulness\" ]) if not isnan ( scores [ \"harmfulness\" ]): trace . score ( name = \"Harmfulness\" , value = scores [ \"harmfulness\" ])","title":"evaluate"},{"location":"src/evaluation/evaluators/langfuse/#src.evaluation.evaluators.langfuse.LangfuseEvaluatorFactory","text":"Bases: Factory Factory for creating LangfuseEvaluator instances. Creates properly configured evaluators based on the provided configuration. Source code in src/evaluation/evaluators/langfuse.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class LangfuseEvaluatorFactory ( Factory ): \"\"\"Factory for creating LangfuseEvaluator instances. Creates properly configured evaluators based on the provided configuration. \"\"\" _configuration_class : Type = EvaluationConfiguration @classmethod def _create_instance ( cls , configuration : EvaluationConfiguration ) -> LangfuseEvaluator : \"\"\"Create a new LangfuseEvaluator instance. Args: configuration: Complete evaluation configuration containing settings for the chat engine, datasets, and metrics Returns: A fully configured LangfuseEvaluator instance ready for evaluation \"\"\" chat_engine = ChatEngineRegistry . get ( configuration . augmentation . chat_engine . name ) . create ( configuration ) langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . augmentation . langfuse ) ragas_evaluator = RagasEvaluatorFactory . create ( configuration . evaluation ) return LangfuseEvaluator ( chat_engine = chat_engine , langfuse_dataset_service = langfuse_dataset_service , ragas_evaluator = ragas_evaluator , run_metadata = { \"build_name\" : configuration . metadata . build_name , \"llm_configuration\" : configuration . augmentation . chat_engine . llm . name , \"judge_llm_configuration\" : configuration . evaluation . judge_llm . name , }, )","title":"LangfuseEvaluatorFactory"},{"location":"src/evaluation/evaluators/ragas/","text":"Ragas This module contains functionality related to the the ragas module for evaluation.evaluators . Ragas RagasEvaluator Evaluator for RAG system quality using RAGAS framework. Provides automatic evaluation of RAG pipeline quality using multiple metrics from the RAGAS evaluation framework. Evaluates answer relevancy, factual consistency (faithfulness), harmfulness, and source context recall. Source code in src/evaluation/evaluators/ragas.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class RagasEvaluator : \"\"\"Evaluator for RAG system quality using RAGAS framework. Provides automatic evaluation of RAG pipeline quality using multiple metrics from the RAGAS evaluation framework. Evaluates answer relevancy, factual consistency (faithfulness), harmfulness, and source context recall. \"\"\" def __init__ ( self , judge_llm : BaseLLM , judge_embedding_model : BaseEmbedding , evaluator_function : Callable = ragas_evaluate , ) -> None : \"\"\"Initialize RAGAS evaluator with required models and configuration. Args: judge_llm: LlamaIndex LLM used to evaluate response quality judge_embedding_model: Embedding model for semantic comparisons evaluator_function: Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function \"\"\" self . judge_llm = LlamaIndexLLMWrapper ( judge_llm ) self . judge_embedding_model = LlamaIndexEmbeddingsWrapper ( judge_embedding_model ) self . evaluator_function = evaluator_function self . metrics = [ answer_relevancy , faithfulness , harmfulness , context_recall , ] def evaluate ( self , response : Response , item : DatasetItemClient ) -> Series : \"\"\"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Args: response: LlamaIndex response object containing the generated answer and source nodes used for retrieval item: Langfuse dataset item containing the original query and expected ground truth answer Returns: Series: Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) \"\"\" dataset = Dataset . from_dict ( { \"question\" : [ item . input [ \"query_str\" ]], \"contexts\" : [[ n . node . text for n in response . source_nodes ]], \"answer\" : [ response . response ], \"ground_truth\" : [ item . expected_output [ \"result\" ]], } ) return ( self . evaluator_function ( metrics = self . metrics , dataset = dataset , llm = self . judge_llm , embeddings = self . judge_embedding_model , ) . to_pandas () . iloc [ 0 ] ) __init__ ( judge_llm , judge_embedding_model , evaluator_function = ragas_evaluate ) Initialize RAGAS evaluator with required models and configuration. Parameters: judge_llm ( BaseLLM ) \u2013 LlamaIndex LLM used to evaluate response quality judge_embedding_model ( BaseEmbedding ) \u2013 Embedding model for semantic comparisons evaluator_function ( Callable , default: evaluate ) \u2013 Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function Source code in src/evaluation/evaluators/ragas.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , judge_llm : BaseLLM , judge_embedding_model : BaseEmbedding , evaluator_function : Callable = ragas_evaluate , ) -> None : \"\"\"Initialize RAGAS evaluator with required models and configuration. Args: judge_llm: LlamaIndex LLM used to evaluate response quality judge_embedding_model: Embedding model for semantic comparisons evaluator_function: Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function \"\"\" self . judge_llm = LlamaIndexLLMWrapper ( judge_llm ) self . judge_embedding_model = LlamaIndexEmbeddingsWrapper ( judge_embedding_model ) self . evaluator_function = evaluator_function self . metrics = [ answer_relevancy , faithfulness , harmfulness , context_recall , ] evaluate ( response , item ) Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Parameters: response ( Response ) \u2013 LlamaIndex response object containing the generated answer and source nodes used for retrieval item ( DatasetItemClient ) \u2013 Langfuse dataset item containing the original query and expected ground truth answer Returns: Series ( Series ) \u2013 Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) Source code in src/evaluation/evaluators/ragas.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def evaluate ( self , response : Response , item : DatasetItemClient ) -> Series : \"\"\"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Args: response: LlamaIndex response object containing the generated answer and source nodes used for retrieval item: Langfuse dataset item containing the original query and expected ground truth answer Returns: Series: Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) \"\"\" dataset = Dataset . from_dict ( { \"question\" : [ item . input [ \"query_str\" ]], \"contexts\" : [[ n . node . text for n in response . source_nodes ]], \"answer\" : [ response . response ], \"ground_truth\" : [ item . expected_output [ \"result\" ]], } ) return ( self . evaluator_function ( metrics = self . metrics , dataset = dataset , llm = self . judge_llm , embeddings = self . judge_embedding_model , ) . to_pandas () . iloc [ 0 ] ) RagasEvaluatorFactory Bases: Factory Factory for creating configured RagasEvaluator instances. Creates RagasEvaluator objects based on provided configuration, handling the initialization of required LLM and embedding models. Attributes: _configuration_class ( Type ) \u2013 Configuration class type used for validation Source code in src/evaluation/evaluators/ragas.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class RagasEvaluatorFactory ( Factory ): \"\"\"Factory for creating configured RagasEvaluator instances. Creates RagasEvaluator objects based on provided configuration, handling the initialization of required LLM and embedding models. Attributes: _configuration_class: Configuration class type used for validation \"\"\" _configuration_class : Type = _EvaluationConfiguration @classmethod def _create_instance ( cls , configuration : _EvaluationConfiguration ) -> RagasEvaluator : \"\"\"Create and initialize a RagasEvaluator with configured models. Initializes judge LLM and embedding models according to configuration specifications, then creates a RagasEvaluator with those models. Args: configuration: Evaluation configuration object containing model specifications for the judge LLM and embedding model Returns: RagasEvaluator: Fully initialized evaluator ready to assess RAG responses \"\"\" judge_llm = LLMRegistry . get ( configuration . judge_llm . provider ) . create ( configuration . judge_llm ) judge_embedding_model = EmbeddingModelRegistry . get ( configuration . judge_embedding_model . provider ) . create ( configuration . judge_embedding_model ) return RagasEvaluator ( judge_llm = judge_llm , judge_embedding_model = judge_embedding_model )","title":"Ragas"},{"location":"src/evaluation/evaluators/ragas/#ragas","text":"This module contains functionality related to the the ragas module for evaluation.evaluators .","title":"Ragas"},{"location":"src/evaluation/evaluators/ragas/#ragas_1","text":"","title":"Ragas"},{"location":"src/evaluation/evaluators/ragas/#src.evaluation.evaluators.ragas.RagasEvaluator","text":"Evaluator for RAG system quality using RAGAS framework. Provides automatic evaluation of RAG pipeline quality using multiple metrics from the RAGAS evaluation framework. Evaluates answer relevancy, factual consistency (faithfulness), harmfulness, and source context recall. Source code in src/evaluation/evaluators/ragas.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class RagasEvaluator : \"\"\"Evaluator for RAG system quality using RAGAS framework. Provides automatic evaluation of RAG pipeline quality using multiple metrics from the RAGAS evaluation framework. Evaluates answer relevancy, factual consistency (faithfulness), harmfulness, and source context recall. \"\"\" def __init__ ( self , judge_llm : BaseLLM , judge_embedding_model : BaseEmbedding , evaluator_function : Callable = ragas_evaluate , ) -> None : \"\"\"Initialize RAGAS evaluator with required models and configuration. Args: judge_llm: LlamaIndex LLM used to evaluate response quality judge_embedding_model: Embedding model for semantic comparisons evaluator_function: Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function \"\"\" self . judge_llm = LlamaIndexLLMWrapper ( judge_llm ) self . judge_embedding_model = LlamaIndexEmbeddingsWrapper ( judge_embedding_model ) self . evaluator_function = evaluator_function self . metrics = [ answer_relevancy , faithfulness , harmfulness , context_recall , ] def evaluate ( self , response : Response , item : DatasetItemClient ) -> Series : \"\"\"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Args: response: LlamaIndex response object containing the generated answer and source nodes used for retrieval item: Langfuse dataset item containing the original query and expected ground truth answer Returns: Series: Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) \"\"\" dataset = Dataset . from_dict ( { \"question\" : [ item . input [ \"query_str\" ]], \"contexts\" : [[ n . node . text for n in response . source_nodes ]], \"answer\" : [ response . response ], \"ground_truth\" : [ item . expected_output [ \"result\" ]], } ) return ( self . evaluator_function ( metrics = self . metrics , dataset = dataset , llm = self . judge_llm , embeddings = self . judge_embedding_model , ) . to_pandas () . iloc [ 0 ] )","title":"RagasEvaluator"},{"location":"src/evaluation/evaluators/ragas/#src.evaluation.evaluators.ragas.RagasEvaluator.__init__","text":"Initialize RAGAS evaluator with required models and configuration. Parameters: judge_llm ( BaseLLM ) \u2013 LlamaIndex LLM used to evaluate response quality judge_embedding_model ( BaseEmbedding ) \u2013 Embedding model for semantic comparisons evaluator_function ( Callable , default: evaluate ) \u2013 Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function Source code in src/evaluation/evaluators/ragas.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , judge_llm : BaseLLM , judge_embedding_model : BaseEmbedding , evaluator_function : Callable = ragas_evaluate , ) -> None : \"\"\"Initialize RAGAS evaluator with required models and configuration. Args: judge_llm: LlamaIndex LLM used to evaluate response quality judge_embedding_model: Embedding model for semantic comparisons evaluator_function: Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function \"\"\" self . judge_llm = LlamaIndexLLMWrapper ( judge_llm ) self . judge_embedding_model = LlamaIndexEmbeddingsWrapper ( judge_embedding_model ) self . evaluator_function = evaluator_function self . metrics = [ answer_relevancy , faithfulness , harmfulness , context_recall , ]","title":"__init__"},{"location":"src/evaluation/evaluators/ragas/#src.evaluation.evaluators.ragas.RagasEvaluator.evaluate","text":"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Parameters: response ( Response ) \u2013 LlamaIndex response object containing the generated answer and source nodes used for retrieval item ( DatasetItemClient ) \u2013 Langfuse dataset item containing the original query and expected ground truth answer Returns: Series ( Series ) \u2013 Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) Source code in src/evaluation/evaluators/ragas.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def evaluate ( self , response : Response , item : DatasetItemClient ) -> Series : \"\"\"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Args: response: LlamaIndex response object containing the generated answer and source nodes used for retrieval item: Langfuse dataset item containing the original query and expected ground truth answer Returns: Series: Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) \"\"\" dataset = Dataset . from_dict ( { \"question\" : [ item . input [ \"query_str\" ]], \"contexts\" : [[ n . node . text for n in response . source_nodes ]], \"answer\" : [ response . response ], \"ground_truth\" : [ item . expected_output [ \"result\" ]], } ) return ( self . evaluator_function ( metrics = self . metrics , dataset = dataset , llm = self . judge_llm , embeddings = self . judge_embedding_model , ) . to_pandas () . iloc [ 0 ] )","title":"evaluate"},{"location":"src/evaluation/evaluators/ragas/#src.evaluation.evaluators.ragas.RagasEvaluatorFactory","text":"Bases: Factory Factory for creating configured RagasEvaluator instances. Creates RagasEvaluator objects based on provided configuration, handling the initialization of required LLM and embedding models. Attributes: _configuration_class ( Type ) \u2013 Configuration class type used for validation Source code in src/evaluation/evaluators/ragas.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class RagasEvaluatorFactory ( Factory ): \"\"\"Factory for creating configured RagasEvaluator instances. Creates RagasEvaluator objects based on provided configuration, handling the initialization of required LLM and embedding models. Attributes: _configuration_class: Configuration class type used for validation \"\"\" _configuration_class : Type = _EvaluationConfiguration @classmethod def _create_instance ( cls , configuration : _EvaluationConfiguration ) -> RagasEvaluator : \"\"\"Create and initialize a RagasEvaluator with configured models. Initializes judge LLM and embedding models according to configuration specifications, then creates a RagasEvaluator with those models. Args: configuration: Evaluation configuration object containing model specifications for the judge LLM and embedding model Returns: RagasEvaluator: Fully initialized evaluator ready to assess RAG responses \"\"\" judge_llm = LLMRegistry . get ( configuration . judge_llm . provider ) . create ( configuration . judge_llm ) judge_embedding_model = EmbeddingModelRegistry . get ( configuration . judge_embedding_model . provider ) . create ( configuration . judge_embedding_model ) return RagasEvaluator ( judge_llm = judge_llm , judge_embedding_model = judge_embedding_model )","title":"RagasEvaluatorFactory"},{"location":"src/extraction/bootstrap/initializer/","text":"Initializer This module contains functionality related to the the initializer module for extraction.bootstrap . Initializer ExtractionInitializer Bases: BasicInitializer Initializer for the extraction module. Handles the initialization of extraction components including configuration and required packages. Source code in src/extraction/bootstrap/initializer.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class ExtractionInitializer ( BasicInitializer ): \"\"\" Initializer for the extraction module. Handles the initialization of extraction components including configuration and required packages. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = ExtractionConfiguration , package_loader : BasePackageLoader = ExtractionPackageLoader (), ): \"\"\" Initialize the extraction module. Args: configuration_class: Configuration class to use for extraction package_loader: Loader for required extraction packages \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) __init__ ( configuration_class = ExtractionConfiguration , package_loader = ExtractionPackageLoader ()) Initialize the extraction module. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: ExtractionConfiguration ) \u2013 Configuration class to use for extraction package_loader ( BasePackageLoader , default: ExtractionPackageLoader () ) \u2013 Loader for required extraction packages Source code in src/extraction/bootstrap/initializer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = ExtractionConfiguration , package_loader : BasePackageLoader = ExtractionPackageLoader (), ): \"\"\" Initialize the extraction module. Args: configuration_class: Configuration class to use for extraction package_loader: Loader for required extraction packages \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) ExtractionPackageLoader Bases: BasePackageLoader Loader for extraction-related packages. Handles loading of extraction datasources and orchestrators packages. Source code in src/extraction/bootstrap/initializer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ExtractionPackageLoader ( BasePackageLoader ): \"\"\" Loader for extraction-related packages. Handles loading of extraction datasources and orchestrators packages. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initialize the extraction package loader. Args: logger: Logger instance for logging information \"\"\" super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\" Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.extraction.datasources\" , \"src.extraction.orchestrators\" ] ) __init__ ( logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the extraction package loader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information Source code in src/extraction/bootstrap/initializer.py 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initialize the extraction package loader. Args: logger: Logger instance for logging information \"\"\" super () . __init__ ( logger ) load_packages () Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. Source code in src/extraction/bootstrap/initializer.py 31 32 33 34 35 36 37 38 39 40 def load_packages ( self ) -> None : \"\"\" Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.extraction.datasources\" , \"src.extraction.orchestrators\" ] )","title":"Initializer"},{"location":"src/extraction/bootstrap/initializer/#initializer","text":"This module contains functionality related to the the initializer module for extraction.bootstrap .","title":"Initializer"},{"location":"src/extraction/bootstrap/initializer/#initializer_1","text":"","title":"Initializer"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionInitializer","text":"Bases: BasicInitializer Initializer for the extraction module. Handles the initialization of extraction components including configuration and required packages. Source code in src/extraction/bootstrap/initializer.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class ExtractionInitializer ( BasicInitializer ): \"\"\" Initializer for the extraction module. Handles the initialization of extraction components including configuration and required packages. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = ExtractionConfiguration , package_loader : BasePackageLoader = ExtractionPackageLoader (), ): \"\"\" Initialize the extraction module. Args: configuration_class: Configuration class to use for extraction package_loader: Loader for required extraction packages \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , )","title":"ExtractionInitializer"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionInitializer.__init__","text":"Initialize the extraction module. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: ExtractionConfiguration ) \u2013 Configuration class to use for extraction package_loader ( BasePackageLoader , default: ExtractionPackageLoader () ) \u2013 Loader for required extraction packages Source code in src/extraction/bootstrap/initializer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = ExtractionConfiguration , package_loader : BasePackageLoader = ExtractionPackageLoader (), ): \"\"\" Initialize the extraction module. Args: configuration_class: Configuration class to use for extraction package_loader: Loader for required extraction packages \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , )","title":"__init__"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionPackageLoader","text":"Bases: BasePackageLoader Loader for extraction-related packages. Handles loading of extraction datasources and orchestrators packages. Source code in src/extraction/bootstrap/initializer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ExtractionPackageLoader ( BasePackageLoader ): \"\"\" Loader for extraction-related packages. Handles loading of extraction datasources and orchestrators packages. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initialize the extraction package loader. Args: logger: Logger instance for logging information \"\"\" super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\" Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.extraction.datasources\" , \"src.extraction.orchestrators\" ] )","title":"ExtractionPackageLoader"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionPackageLoader.__init__","text":"Initialize the extraction package loader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information Source code in src/extraction/bootstrap/initializer.py 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initialize the extraction package loader. Args: logger: Logger instance for logging information \"\"\" super () . __init__ ( logger )","title":"__init__"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionPackageLoader.load_packages","text":"Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. Source code in src/extraction/bootstrap/initializer.py 31 32 33 34 35 36 37 38 39 40 def load_packages ( self ) -> None : \"\"\" Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.extraction.datasources\" , \"src.extraction.orchestrators\" ] )","title":"load_packages"},{"location":"src/extraction/bootstrap/configuration/configuration/","text":"Configuration This module contains functionality related to the the configuration module for extraction.bootstrap.configuration . Configuration ExtractionConfiguration Bases: BasicConfiguration Main configuration class for the extraction system. Extends the BasicConfiguration with extraction-specific settings. Source code in src/extraction/bootstrap/configuration/configuration.py 56 57 58 59 60 61 62 63 64 class ExtractionConfiguration ( BasicConfiguration ): \"\"\" Main configuration class for the extraction system. Extends the BasicConfiguration with extraction-specific settings. \"\"\" extraction : _ExtractionConfiguration = Field ( ... , description = \"Extraction configuration.\" ) OrchestratorName Bases: str , Enum Enum representing the available orchestrators for extraction. Source code in src/extraction/bootstrap/configuration/configuration.py 12 13 14 15 16 17 class OrchestratorName ( str , Enum ): \"\"\" Enum representing the available orchestrators for extraction. \"\"\" BASIC = \"basic\"","title":"Configuration"},{"location":"src/extraction/bootstrap/configuration/configuration/#configuration","text":"This module contains functionality related to the the configuration module for extraction.bootstrap.configuration .","title":"Configuration"},{"location":"src/extraction/bootstrap/configuration/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/extraction/bootstrap/configuration/configuration/#src.extraction.bootstrap.configuration.configuration.ExtractionConfiguration","text":"Bases: BasicConfiguration Main configuration class for the extraction system. Extends the BasicConfiguration with extraction-specific settings. Source code in src/extraction/bootstrap/configuration/configuration.py 56 57 58 59 60 61 62 63 64 class ExtractionConfiguration ( BasicConfiguration ): \"\"\" Main configuration class for the extraction system. Extends the BasicConfiguration with extraction-specific settings. \"\"\" extraction : _ExtractionConfiguration = Field ( ... , description = \"Extraction configuration.\" )","title":"ExtractionConfiguration"},{"location":"src/extraction/bootstrap/configuration/configuration/#src.extraction.bootstrap.configuration.configuration.OrchestratorName","text":"Bases: str , Enum Enum representing the available orchestrators for extraction. Source code in src/extraction/bootstrap/configuration/configuration.py 12 13 14 15 16 17 class OrchestratorName ( str , Enum ): \"\"\" Enum representing the available orchestrators for extraction. \"\"\" BASIC = \"basic\"","title":"OrchestratorName"},{"location":"src/extraction/bootstrap/configuration/datasources/","text":"Datasources This module contains functionality related to the the datasources module for extraction.bootstrap.configuration . Datasources DatasourceConfiguration Bases: BaseConfigurationWithSecrets , ABC Abstract base class for all data source configurations. This class serves as the foundation for specific data source implementations, providing common configuration parameters. All concrete datasource configurations should inherit from this class. Source code in src/extraction/bootstrap/configuration/datasources.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class DatasourceConfiguration ( BaseConfigurationWithSecrets , ABC ): \"\"\" Abstract base class for all data source configurations. This class serves as the foundation for specific data source implementations, providing common configuration parameters. All concrete datasource configurations should inherit from this class. \"\"\" name : DatasourceName = Field ( ... , description = \"The name of the data source.\" ) export_limit : Optional [ int ] = Field ( None , description = \"The export limit for the data source.\" ) DatasourceConfigurationRegistry Bases: ConfigurationRegistry Registry for datasource configurations. This registry manages all available datasource configurations and provides methods to access them based on their type. Source code in src/extraction/bootstrap/configuration/datasources.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class DatasourceConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for datasource configurations. This registry manages all available datasource configurations and provides methods to access them based on their type. \"\"\" _key_class = DatasourceName @classmethod def get_union_type ( self ) -> List [ DatasourceConfiguration ]: \"\"\" Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List[DatasourceConfiguration]: A type representing a list of all registered datasource configurations. \"\"\" return List [ super () . get_union_type ()] get_union_type () classmethod Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List [ DatasourceConfiguration ] \u2013 List[DatasourceConfiguration]: A type representing a list of all List [ DatasourceConfiguration ] \u2013 registered datasource configurations. Source code in src/extraction/bootstrap/configuration/datasources.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @classmethod def get_union_type ( self ) -> List [ DatasourceConfiguration ]: \"\"\" Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List[DatasourceConfiguration]: A type representing a list of all registered datasource configurations. \"\"\" return List [ super () . get_union_type ()] DatasourceName Bases: str , Enum List of all available datasources. Defines the supported data sources that can be used for extraction: - CONFLUENCE: Atlassian Confluence wiki pages and spaces - NOTION: Notion databases, pages, and blocks - PDF: PDF documents from file system or URLs Source code in src/extraction/bootstrap/configuration/datasources.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class DatasourceName ( str , Enum ): \"\"\" List of all available datasources. Defines the supported data sources that can be used for extraction: - CONFLUENCE: Atlassian Confluence wiki pages and spaces - NOTION: Notion databases, pages, and blocks - PDF: PDF documents from file system or URLs \"\"\" CONFLUENCE = \"confluence\" NOTION = \"notion\" PDF = \"pdf\" BUNDESTAG = \"bundestag\"","title":"Datasources"},{"location":"src/extraction/bootstrap/configuration/datasources/#datasources","text":"This module contains functionality related to the the datasources module for extraction.bootstrap.configuration .","title":"Datasources"},{"location":"src/extraction/bootstrap/configuration/datasources/#datasources_1","text":"","title":"Datasources"},{"location":"src/extraction/bootstrap/configuration/datasources/#src.extraction.bootstrap.configuration.datasources.DatasourceConfiguration","text":"Bases: BaseConfigurationWithSecrets , ABC Abstract base class for all data source configurations. This class serves as the foundation for specific data source implementations, providing common configuration parameters. All concrete datasource configurations should inherit from this class. Source code in src/extraction/bootstrap/configuration/datasources.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class DatasourceConfiguration ( BaseConfigurationWithSecrets , ABC ): \"\"\" Abstract base class for all data source configurations. This class serves as the foundation for specific data source implementations, providing common configuration parameters. All concrete datasource configurations should inherit from this class. \"\"\" name : DatasourceName = Field ( ... , description = \"The name of the data source.\" ) export_limit : Optional [ int ] = Field ( None , description = \"The export limit for the data source.\" )","title":"DatasourceConfiguration"},{"location":"src/extraction/bootstrap/configuration/datasources/#src.extraction.bootstrap.configuration.datasources.DatasourceConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for datasource configurations. This registry manages all available datasource configurations and provides methods to access them based on their type. Source code in src/extraction/bootstrap/configuration/datasources.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class DatasourceConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for datasource configurations. This registry manages all available datasource configurations and provides methods to access them based on their type. \"\"\" _key_class = DatasourceName @classmethod def get_union_type ( self ) -> List [ DatasourceConfiguration ]: \"\"\" Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List[DatasourceConfiguration]: A type representing a list of all registered datasource configurations. \"\"\" return List [ super () . get_union_type ()]","title":"DatasourceConfigurationRegistry"},{"location":"src/extraction/bootstrap/configuration/datasources/#src.extraction.bootstrap.configuration.datasources.DatasourceConfigurationRegistry.get_union_type","text":"Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List [ DatasourceConfiguration ] \u2013 List[DatasourceConfiguration]: A type representing a list of all List [ DatasourceConfiguration ] \u2013 registered datasource configurations. Source code in src/extraction/bootstrap/configuration/datasources.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @classmethod def get_union_type ( self ) -> List [ DatasourceConfiguration ]: \"\"\" Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List[DatasourceConfiguration]: A type representing a list of all registered datasource configurations. \"\"\" return List [ super () . get_union_type ()]","title":"get_union_type"},{"location":"src/extraction/bootstrap/configuration/datasources/#src.extraction.bootstrap.configuration.datasources.DatasourceName","text":"Bases: str , Enum List of all available datasources. Defines the supported data sources that can be used for extraction: - CONFLUENCE: Atlassian Confluence wiki pages and spaces - NOTION: Notion databases, pages, and blocks - PDF: PDF documents from file system or URLs Source code in src/extraction/bootstrap/configuration/datasources.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class DatasourceName ( str , Enum ): \"\"\" List of all available datasources. Defines the supported data sources that can be used for extraction: - CONFLUENCE: Atlassian Confluence wiki pages and spaces - NOTION: Notion databases, pages, and blocks - PDF: PDF documents from file system or URLs \"\"\" CONFLUENCE = \"confluence\" NOTION = \"notion\" PDF = \"pdf\" BUNDESTAG = \"bundestag\"","title":"DatasourceName"},{"location":"src/extraction/datasources/bundestag/","text":"Bundestag Datasource This module contains functionality related to the Bundestag datasource. Client BundestagMineClient Bases: APIClient API Client for the bundestag-mine.de API. Source code in src/extraction/datasources/bundestag/client.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 class BundestagMineClient ( APIClient ): \"\"\" API Client for the bundestag-mine.de API. \"\"\" BASE_URL = \"https://bundestag-mine.de/api/DashboardController\" logger = LoggerConfiguration . get_logger ( __name__ ) def safe_get ( self , path : str ) -> Optional [ Any ]: \"\"\" Perform a GET request, raise for HTTP errors, parse JSON, check API status. Args: path: endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/<protocol_id>\" Returns: Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError: if HTTP status is not OK or unexpected JSON structure. \"\"\" url = f \" { self . BASE_URL } / { path . lstrip ( '/' ) } \" try : resp = self . get ( url ) except Exception as e : self . logger . warning ( f \"Failed to fetch speeches for { url } : { e } \" ) return None try : resp . raise_for_status () except Exception as e : self . logger . error ( f \"HTTP error for { url } : { e } \" ) return None data = resp . json () if not isinstance ( data , dict ) or data . get ( \"status\" ) != \"200\" : self . logger . error ( f \"Unexpected response for { url } : { data } \" ) return None result = data . get ( \"result\" ) if result is None : self . logger . debug ( f \"No result found for { url } \" ) return None return result def get_protocols ( self ) -> Iterator [ Protocol ]: \"\"\" Fetches the list of all protocols. Returns: Iterator[Protocol]: An iterator of valid protocols as Pydantic models. \"\"\" result = self . safe_get ( \"GetProtocols\" ) if not isinstance ( result , list ): raise ResponseParseError ( f \"Expected list of protocols, got: { result } \" ) for protocol_data in result : try : yield Protocol . model_validate ( protocol_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate protocol: { protocol_data } . Error: { e } \" ) def get_agenda_items ( self , protocol_id : str ) -> Iterator [ AgendaItem ]: \"\"\" Fetches agenda items for a specific protocol ID. Args: protocol_id (str): The ID of the protocol. Returns: Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. \"\"\" result = self . safe_get ( f \"GetAgendaItemsOfProtocol/ { protocol_id } \" ) if result is None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return items = result . get ( \"agendaItems\" ) if \"items\" == None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return if not isinstance ( items , list ): self . logger . error ( f \"Expected list of agendaItems for { protocol_id } , got: { items } \" ) return for item_data in items : try : yield AgendaItem . model_validate ( item_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate agenda item: { item_data } . Error: { e } \" ) def get_speaker_data ( self , speaker_id : str ) -> Optional [ Speaker ]: \"\"\" Fetches speaker data for a specific speaker ID. Args: speaker_id (str): The ID of the speaker. Returns: Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. \"\"\" result = self . safe_get ( f \"GetSpeakerById/ { speaker_id } \" ) if not isinstance ( result , dict ): self . logger . error ( f \"Expected speaker data for { speaker_id } , got: { result } \" ) return None try : return Speaker . model_validate ( result ) except ValidationError as e : self . logger . warning ( f \"Failed to validate speaker data for { speaker_id } : { result } . Error: { e } \" ) return None @retry_request def get_speeches ( self , protocol : Protocol , agenda_item : AgendaItem , ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches speeches for a specific agenda item within a protocol. Args: legislature_period (int): The legislature period. protocol_number (int): The protocol number. agenda_item_number (str): The agenda item number. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" raw = f \" { protocol . legislaturePeriod } , { protocol . number } , { agenda_item . agendaItemNumber } \" encoded = quote ( raw , safe = \"\" ) result = self . safe_get ( f \"GetSpeechesOfAgendaItem/ { encoded } \" ) if result is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return speeches = result . get ( \"speeches\" ) if speeches is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return if not isinstance ( speeches , list ): self . logger . warning ( f \"Expected list of speeches for { raw } , got: { speeches } \" ) return for speech in speeches : try : speech = BundestagSpeech . model_validate ( speech ) speech . protocol = protocol speech . agendaItem = agenda_item yield speech except ValidationError as e : self . logger . warning ( f \"Failed to validate speech: { speech } . Error: { e } \" ) def fetch_all_speeches ( self ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" for protocol in self . get_protocols (): self . logger . info ( f \"Processing protocol { protocol . id } \" ) for agenda_item in self . get_agenda_items ( protocol . id ): for speech in self . get_speeches ( protocol = protocol , agenda_item = agenda_item , ): speaker = self . get_speaker_data ( speech . speakerId ) if speaker : speech . speaker = speaker yield speech fetch_all_speeches () Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator [ BundestagSpeech ] \u2013 Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def fetch_all_speeches ( self ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" for protocol in self . get_protocols (): self . logger . info ( f \"Processing protocol { protocol . id } \" ) for agenda_item in self . get_agenda_items ( protocol . id ): for speech in self . get_speeches ( protocol = protocol , agenda_item = agenda_item , ): speaker = self . get_speaker_data ( speech . speakerId ) if speaker : speech . speaker = speaker yield speech get_agenda_items ( protocol_id ) Fetches agenda items for a specific protocol ID. Parameters: protocol_id ( str ) \u2013 The ID of the protocol. Returns: Iterator [ AgendaItem ] \u2013 Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def get_agenda_items ( self , protocol_id : str ) -> Iterator [ AgendaItem ]: \"\"\" Fetches agenda items for a specific protocol ID. Args: protocol_id (str): The ID of the protocol. Returns: Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. \"\"\" result = self . safe_get ( f \"GetAgendaItemsOfProtocol/ { protocol_id } \" ) if result is None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return items = result . get ( \"agendaItems\" ) if \"items\" == None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return if not isinstance ( items , list ): self . logger . error ( f \"Expected list of agendaItems for { protocol_id } , got: { items } \" ) return for item_data in items : try : yield AgendaItem . model_validate ( item_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate agenda item: { item_data } . Error: { e } \" ) get_protocols () Fetches the list of all protocols. Returns: Iterator [ Protocol ] \u2013 Iterator[Protocol]: An iterator of valid protocols as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def get_protocols ( self ) -> Iterator [ Protocol ]: \"\"\" Fetches the list of all protocols. Returns: Iterator[Protocol]: An iterator of valid protocols as Pydantic models. \"\"\" result = self . safe_get ( \"GetProtocols\" ) if not isinstance ( result , list ): raise ResponseParseError ( f \"Expected list of protocols, got: { result } \" ) for protocol_data in result : try : yield Protocol . model_validate ( protocol_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate protocol: { protocol_data } . Error: { e } \" ) get_speaker_data ( speaker_id ) Fetches speaker data for a specific speaker ID. Parameters: speaker_id ( str ) \u2013 The ID of the speaker. Returns: Optional [ Speaker ] \u2013 Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. Source code in src/extraction/datasources/bundestag/client.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def get_speaker_data ( self , speaker_id : str ) -> Optional [ Speaker ]: \"\"\" Fetches speaker data for a specific speaker ID. Args: speaker_id (str): The ID of the speaker. Returns: Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. \"\"\" result = self . safe_get ( f \"GetSpeakerById/ { speaker_id } \" ) if not isinstance ( result , dict ): self . logger . error ( f \"Expected speaker data for { speaker_id } , got: { result } \" ) return None try : return Speaker . model_validate ( result ) except ValidationError as e : self . logger . warning ( f \"Failed to validate speaker data for { speaker_id } : { result } . Error: { e } \" ) return None get_speeches ( protocol , agenda_item ) Fetches speeches for a specific agenda item within a protocol. Parameters: legislature_period ( int ) \u2013 The legislature period. protocol_number ( int ) \u2013 The protocol number. agenda_item_number ( str ) \u2013 The agenda item number. Returns: Iterator [ BundestagSpeech ] \u2013 Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @retry_request def get_speeches ( self , protocol : Protocol , agenda_item : AgendaItem , ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches speeches for a specific agenda item within a protocol. Args: legislature_period (int): The legislature period. protocol_number (int): The protocol number. agenda_item_number (str): The agenda item number. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" raw = f \" { protocol . legislaturePeriod } , { protocol . number } , { agenda_item . agendaItemNumber } \" encoded = quote ( raw , safe = \"\" ) result = self . safe_get ( f \"GetSpeechesOfAgendaItem/ { encoded } \" ) if result is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return speeches = result . get ( \"speeches\" ) if speeches is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return if not isinstance ( speeches , list ): self . logger . warning ( f \"Expected list of speeches for { raw } , got: { speeches } \" ) return for speech in speeches : try : speech = BundestagSpeech . model_validate ( speech ) speech . protocol = protocol speech . agendaItem = agenda_item yield speech except ValidationError as e : self . logger . warning ( f \"Failed to validate speech: { speech } . Error: { e } \" ) safe_get ( path ) Perform a GET request, raise for HTTP errors, parse JSON, check API status. Parameters: path ( str ) \u2013 endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/ \" Returns: Optional [ Any ] \u2013 Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError \u2013 if HTTP status is not OK or unexpected JSON structure. Source code in src/extraction/datasources/bundestag/client.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def safe_get ( self , path : str ) -> Optional [ Any ]: \"\"\" Perform a GET request, raise for HTTP errors, parse JSON, check API status. Args: path: endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/<protocol_id>\" Returns: Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError: if HTTP status is not OK or unexpected JSON structure. \"\"\" url = f \" { self . BASE_URL } / { path . lstrip ( '/' ) } \" try : resp = self . get ( url ) except Exception as e : self . logger . warning ( f \"Failed to fetch speeches for { url } : { e } \" ) return None try : resp . raise_for_status () except Exception as e : self . logger . error ( f \"HTTP error for { url } : { e } \" ) return None data = resp . json () if not isinstance ( data , dict ) or data . get ( \"status\" ) != \"200\" : self . logger . error ( f \"Unexpected response for { url } : { data } \" ) return None result = data . get ( \"result\" ) if result is None : self . logger . debug ( f \"No result found for { url } \" ) return None return result BundestagMineClientFactory Bases: SingletonFactory Factory for creating and managing Bundestag client instances. This factory ensures only one Bundestag client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. Source code in src/extraction/datasources/bundestag/client.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 class BundestagMineClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Bundestag client instances. This factory ensures only one Bundestag client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineClient : \"\"\" Creates a new BundestagMine client instance using the provided configuration. Args: configuration: Configuration object containing BundestagMine details Returns: A configured BundestagMine client instance ready for API interactions. \"\"\" return BundestagMineClient () BundestagSpeech Bases: BaseModel Represents a speech from BundestagMine API. Source code in src/extraction/datasources/bundestag/client.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class BundestagSpeech ( BaseModel ): \"\"\"Represents a speech from BundestagMine API.\"\"\" id : str speakerId : str text : str speaker : Optional [ Speaker ] = None protocol : Optional [ Protocol ] = None agendaItem : Optional [ AgendaItem ] = None @model_validator ( mode = \"after\" ) def validate_text_not_empty ( self ) -> \"BundestagSpeech\" : if not self . text or self . text . strip () == \"\" : raise ValueError ( \"BundestagSpeech text cannot be empty\" ) return self Client_dip DIP API Client for Bundestag datasource using deutschland package. This client provides access to the DIP (Dokumentations- und Informationssystem f\u00fcr Parlamentsmaterialien) API, which is the official German Bundestag API for parliamentary materials. DIPClient Client for DIP (Dokumentations- und Informationssystem) API. Uses the deutschland Python package for API access, providing access to plenary protocols, printed materials, and proceedings. Source code in src/extraction/datasources/bundestag/client_dip.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class DIPClient : \"\"\" Client for DIP (Dokumentations- und Informationssystem) API. Uses the deutschland Python package for API access, providing access to plenary protocols, printed materials, and proceedings. \"\"\" DEFAULT_API_KEY = \"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" def __init__ ( self , api_key : Optional [ str ] = None , wahlperiode : int = 21 , fetch_sources : Optional [ List [ str ]] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\" Initialize DIP client. Args: api_key: DIP API key. If not provided, uses public test key. wahlperiode: Electoral period number (default: 20). fetch_sources: List of data sources to fetch. Options: \"protocols\", \"drucksachen\", \"proceedings\" logger: Logger instance for logging. \"\"\" self . api_key = api_key or self . DEFAULT_API_KEY self . wahlperiode = wahlperiode self . fetch_sources = fetch_sources or [ \"protocols\" , \"drucksachen\" ] self . logger = logger or logging . getLogger ( __name__ ) # Configure deutschland package self . configuration = dip_bundestag . Configuration ( host = \"https://search.dip.bundestag.de/api/v1\" ) # Use query parameter auth (more reliable) self . configuration . api_key [ \"ApiKeyQuery\" ] = self . api_key self . logger . info ( f \"Initialized DIP client for Wahlperiode { self . wahlperiode } , \" f \"sources: { self . fetch_sources } \" ) def fetch_all ( self ) -> Iterator [ DIPDocument ]: \"\"\" Fetch all enabled data sources. Yields: DIPDocument instances wrapping different content types. \"\"\" with dip_bundestag . ApiClient ( self . configuration ) as api_client : if \"protocols\" in self . fetch_sources : yield from self . _fetch_protocols ( api_client ) if \"drucksachen\" in self . fetch_sources : yield from self . _fetch_drucksachen ( api_client ) if \"proceedings\" in self . fetch_sources : yield from self . _fetch_proceedings ( api_client ) def _fetch_protocols ( self , api_client ) -> Iterator [ DIPDocument ]: \"\"\" Fetch plenary protocols with full text. Args: api_client: deutschland API client instance. Yields: DIPDocument instances for protocols. \"\"\" self . logger . info ( f \"Fetching protocols for Wahlperiode { self . wahlperiode } \" ) protokoll_api = plenarprotokolle_api . PlenarprotokolleApi ( api_client ) try : # Get list of protocols - need to paginate to find Bundestag protocols # The API returns Bundesrat protocols first, so we need to paginate all_bt_protocols = [] cursor = \"*\" max_pages = 10 # Limit pagination to avoid excessive API calls page_count = 0 while cursor and page_count < max_pages : response = protokoll_api . get_plenarprotokoll_list ( f_wahlperiode = self . wahlperiode , format = \"json\" , cursor = cursor ) # Filter for Bundestag protocols only (not Bundesrat) # herausgeber is a Zuordnung object, need to convert to string bt_protocols = [ p for p in response . documents if str ( getattr ( p , \"herausgeber\" , \"\" )) == \"BT\" ] all_bt_protocols . extend ( bt_protocols ) # Check if we have more results cursor = getattr ( response , \"cursor\" , None ) page_count += 1 self . logger . debug ( f \"Page { page_count } : Found { len ( bt_protocols ) } BT protocols, \" f \"total so far: { len ( all_bt_protocols ) } \" ) # Stop if we have enough if len ( all_bt_protocols ) >= 50 : # Reasonable limit for testing break protocol_ids = [ p . id for p in all_bt_protocols ] self . logger . info ( f \"Found { len ( protocol_ids ) } Bundestag protocols \" f \"for Wahlperiode { self . wahlperiode } (across { page_count } pages)\" ) # Fetch full text for each protocol for protocol_id in protocol_ids : try : # API expects integer ID fulltext = protokoll_api . get_plenarprotokoll_text ( id = int ( protocol_id ), format = \"json\" ) # Only yield if we have text if hasattr ( fulltext , \"text\" ) and fulltext . text : # Convert to dict for serialization content_dict = fulltext . to_dict () yield DIPDocument ( source_type = \"protocol\" , content = content_dict ) dokumentnummer = getattr ( fulltext , \"dokumentnummer\" , \"unknown\" ) text_length = len ( fulltext . text ) self . logger . debug ( f \"Fetched protocol { dokumentnummer } \" f \"( { text_length } chars)\" ) except Exception as e : self . logger . warning ( f \"Failed to fetch protocol { protocol_id } : { e } \" ) continue except Exception as e : self . logger . error ( f \"Failed to list protocols: { e } \" , exc_info = True ) def _fetch_drucksachen ( self , api_client ) -> Iterator [ DIPDocument ]: \"\"\" Fetch printed materials (drucksachen) with full text. Args: api_client: deutschland API client instance. Yields: DIPDocument instances for drucksachen. \"\"\" self . logger . info ( f \"Fetching drucksachen for Wahlperiode { self . wahlperiode } \" ) drucksache_api = drucksachen_api . DrucksachenApi ( api_client ) try : cursor = None fetched_count = 0 page = 1 while True : # Fetch page of documents cursor_param = cursor if cursor else \"\" response = drucksache_api . get_drucksache_list ( f_wahlperiode = self . wahlperiode , cursor = cursor_param , format = \"json\" , ) # Try to get full text for each document for doc_meta in response . documents : try : fulltext = drucksache_api . get_drucksache_text ( id = doc_meta . id , format = \"json\" ) # Only yield if we have text if hasattr ( fulltext , \"text\" ) and fulltext . text : # Convert to dict for serialization content_dict = fulltext . to_dict () yield DIPDocument ( source_type = \"drucksache\" , content = content_dict ) fetched_count += 1 except Exception : # Many drucksachen don't have full text, log at debug level self . logger . debug ( f \"No full text for drucksache { doc_meta . id } \" ) continue # Log progress self . logger . info ( f \"Drucksachen: page { page } complete, \" f \" { fetched_count } with full text so far\" ) # Check pagination new_cursor = getattr ( response , \"cursor\" , None ) if not new_cursor or new_cursor == cursor : break cursor = new_cursor page += 1 self . logger . info ( f \"Completed fetching drucksachen. \" f \"Total with full text: { fetched_count } \" ) except Exception as e : self . logger . error ( f \"Failed to fetch drucksachen: { e } \" , exc_info = True ) def _fetch_proceedings ( self , api_client ) -> Iterator [ DIPDocument ]: \"\"\" Fetch proceedings (vorg\u00e4nge) - legislative processes. Args: api_client: deutschland API client instance. Yields: DIPDocument instances for proceedings. \"\"\" self . logger . info ( f \"Fetching proceedings for Wahlperiode { self . wahlperiode } \" ) vorgang_api = vorgnge_api . VorgngeApi ( api_client ) try : cursor = None fetched_count = 0 page = 1 while True : # Fetch page of proceedings cursor_param = cursor if cursor else \"\" response = vorgang_api . get_vorgang_list ( f_wahlperiode = self . wahlperiode , cursor = cursor_param , format = \"json\" , ) for vorgang in response . documents : # Convert to dict for serialization content_dict = vorgang . to_dict () yield DIPDocument ( source_type = \"proceeding\" , content = content_dict ) fetched_count += 1 # Log progress self . logger . info ( f \"Proceedings: page { page } complete, \" f \" { fetched_count } total so far\" ) # Check pagination new_cursor = getattr ( response , \"cursor\" , None ) if not new_cursor or new_cursor == cursor : break cursor = new_cursor page += 1 self . logger . info ( f \"Completed fetching proceedings. \" f \"Total: { fetched_count } \" ) except Exception as e : self . logger . error ( f \"Failed to fetch proceedings: { e } \" , exc_info = True ) __init__ ( api_key = None , wahlperiode = 21 , fetch_sources = None , logger = None ) Initialize DIP client. Parameters: api_key ( Optional [ str ] , default: None ) \u2013 DIP API key. If not provided, uses public test key. wahlperiode ( int , default: 21 ) \u2013 Electoral period number (default: 20). fetch_sources ( Optional [ List [ str ]] , default: None ) \u2013 List of data sources to fetch. Options: \"protocols\", \"drucksachen\", \"proceedings\" logger ( Optional [ Logger ] , default: None ) \u2013 Logger instance for logging. Source code in src/extraction/datasources/bundestag/client_dip.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __init__ ( self , api_key : Optional [ str ] = None , wahlperiode : int = 21 , fetch_sources : Optional [ List [ str ]] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\" Initialize DIP client. Args: api_key: DIP API key. If not provided, uses public test key. wahlperiode: Electoral period number (default: 20). fetch_sources: List of data sources to fetch. Options: \"protocols\", \"drucksachen\", \"proceedings\" logger: Logger instance for logging. \"\"\" self . api_key = api_key or self . DEFAULT_API_KEY self . wahlperiode = wahlperiode self . fetch_sources = fetch_sources or [ \"protocols\" , \"drucksachen\" ] self . logger = logger or logging . getLogger ( __name__ ) # Configure deutschland package self . configuration = dip_bundestag . Configuration ( host = \"https://search.dip.bundestag.de/api/v1\" ) # Use query parameter auth (more reliable) self . configuration . api_key [ \"ApiKeyQuery\" ] = self . api_key self . logger . info ( f \"Initialized DIP client for Wahlperiode { self . wahlperiode } , \" f \"sources: { self . fetch_sources } \" ) fetch_all () Fetch all enabled data sources. Yields: DIPDocument \u2013 DIPDocument instances wrapping different content types. Source code in src/extraction/datasources/bundestag/client_dip.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def fetch_all ( self ) -> Iterator [ DIPDocument ]: \"\"\" Fetch all enabled data sources. Yields: DIPDocument instances wrapping different content types. \"\"\" with dip_bundestag . ApiClient ( self . configuration ) as api_client : if \"protocols\" in self . fetch_sources : yield from self . _fetch_protocols ( api_client ) if \"drucksachen\" in self . fetch_sources : yield from self . _fetch_drucksachen ( api_client ) if \"proceedings\" in self . fetch_sources : yield from self . _fetch_proceedings ( api_client ) DIPDocument Bases: BaseModel Unified model for all DIP data types. This wrapper provides a consistent interface for different document types from the DIP API (protocols, drucksachen, proceedings). Source code in src/extraction/datasources/bundestag/client_dip.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class DIPDocument ( BaseModel ): \"\"\" Unified model for all DIP data types. This wrapper provides a consistent interface for different document types from the DIP API (protocols, drucksachen, proceedings). \"\"\" model_config = ConfigDict ( arbitrary_types_allowed = True ) source_type : str # \"protocol\", \"proceeding\", \"drucksache\" content : Dict [ str , Any ] # Raw content from deutschland package @property def text ( self ) -> str : \"\"\"Extract text based on source type.\"\"\" if self . source_type in [ \"protocol\" , \"drucksache\" ]: return self . content . get ( \"text\" , \"\" ) elif self . source_type == \"proceeding\" : # For proceedings, use abstract as text return self . content . get ( \"abstract\" , \"\" ) return \"\" text property Extract text based on source type. Configuration BundestagMineDatasourceConfiguration Bases: DatasourceConfiguration Configuration for the Bundestag datasource. Supports multiple data sources: - BundestagMine API: Individual speeches from parliamentary sessions - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) Source code in src/extraction/datasources/bundestag/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class BundestagMineDatasourceConfiguration ( DatasourceConfiguration ): \"\"\" Configuration for the Bundestag datasource. Supports multiple data sources: - BundestagMine API: Individual speeches from parliamentary sessions - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) \"\"\" name : Literal [ DatasourceName . BUNDESTAG ] = Field ( ... , description = \"Identifier specifying this configuration is for the Bundestag datasource\" , ) # BundestagMine settings include_bundestag_mine : bool = Field ( default = True , description = \"Fetch speeches from BundestagMine API (bundestag-mine.de)\" , ) # DIP API settings include_dip : bool = Field ( default = True , description = \"Fetch data from DIP (Dokumentations- und Informationssystem) API\" , ) dip_api_key : Optional [ str ] = Field ( default = None , description = \"API key for DIP API. If not provided, uses public test key.\" , ) dip_wahlperiode : int = Field ( default = 21 , description = \"Electoral period (Wahlperiode) for DIP data\" , ) dip_sources : List [ str ] = Field ( default_factory = lambda : [ \"protocols\" ], description = ( \"DIP data sources to fetch. \" \"Options: 'protocols' (plenary transcripts), \" \"'drucksachen' (printed materials), \" \"'proceedings' (legislative processes)\" ), ) Document BundestagMineDocument Bases: BaseDocument Represents a document from the Bundestag datasource. Supports multiple data sources: - BundestagMine: Individual speeches with speaker information - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) Inherits from BaseDocument and includes additional metadata specific to Bundestag documents. Source code in src/extraction/datasources/bundestag/document.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BundestagMineDocument ( BaseDocument ): \"\"\" Represents a document from the Bundestag datasource. Supports multiple data sources: - BundestagMine: Individual speeches with speaker information - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) Inherits from BaseDocument and includes additional metadata specific to Bundestag documents. \"\"\" included_embed_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , \"speaker_party\" , \"speaker\" , \"protocol_number\" , \"legislature_period\" , \"document_type\" , \"document_number\" , \"document_subtype\" , \"agenda_item_number\" , \"source_client\" , \"publisher\" , \"document_art\" , \"document_id\" , \"parliamentary_composition\" , # NEW: Party/fraction composition metadata ] included_llm_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , \"speaker_party\" , \"speaker\" , \"protocol_number\" , \"legislature_period\" , \"document_type\" , \"document_number\" , \"document_subtype\" , \"agenda_item_number\" , \"source_client\" , \"publisher\" , \"document_art\" , \"document_id\" , \"distribution_date\" , \"xml_url\" , \"related_proceedings_count\" , \"parliamentary_composition\" , # NEW: Party/fraction composition metadata ] Manager BundestagMineDatasourceManagerFactory Bases: Factory Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object Source code in src/extraction/datasources/bundestag/manager.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class BundestagMineDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class: Type of configuration object \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create an instance of the BundestagMine datasource manager. This method constructs a BasicDatasourceManager by creating the appropriate reader and parser based on the provided configuration. Args: configuration: Configuration specifying how to set up the BundestagMine datasource manager, reader, and parser. Returns: A configured BasicDatasourceManager instance for handling BundestagMine documents. \"\"\" reader = BundestagMineDatasourceReaderFactory . create ( configuration ) parser = BundestagMineDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration = configuration , reader = reader , parser = parser , ) Parser BundestagMineDatasourceParser Bases: BaseParser [ BundestagMineDocument ] Parser for Bundestag data from multiple sources. Handles parsing of: - BundestagMine speeches (individual speech data) - DIP documents (protocols, drucksachen, proceedings) Source code in src/extraction/datasources/bundestag/parser.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 class BundestagMineDatasourceParser ( BaseParser [ BundestagMineDocument ]): \"\"\"Parser for Bundestag data from multiple sources. Handles parsing of: - BundestagMine speeches (individual speech data) - DIP documents (protocols, drucksachen, proceedings) \"\"\" logger = LoggerConfiguration . get_logger ( __name__ ) def parse ( self , content : Union [ BundestagSpeech , DIPDocument ] ) -> BundestagMineDocument : \"\"\" Parse content into a BundestagMineDocument object. Args: content: Raw content to be parsed (BundestagSpeech or DIPDocument) Returns: Parsed document of type BundestagMineDocument \"\"\" if isinstance ( content , BundestagSpeech ): return self . _parse_bundestag_speech ( content ) elif isinstance ( content , DIPDocument ): return self . _parse_dip_document ( content ) else : raise ValueError ( f \"Unsupported content type: { type ( content ) } \" ) def _parse_bundestag_speech ( self , speech : BundestagSpeech ) -> BundestagMineDocument : \"\"\"Parse a BundestagMine speech into a document. Args: speech: BundestagSpeech object Returns: BundestagMineDocument with speech data \"\"\" metadata = self . _extract_metadata_from_speech ( speech ) return BundestagMineDocument ( text = speech . text , metadata = metadata ) def _parse_dip_document ( self , dip_doc : DIPDocument ) -> BundestagMineDocument : \"\"\"Parse a DIP document into a BundestagMineDocument. Args: dip_doc: DIPDocument object Returns: BundestagMineDocument with DIP data \"\"\" metadata = self . _extract_metadata_from_dip ( dip_doc ) # Filter protocol text to remove non-informative sections text = dip_doc . text if dip_doc . source_type == \"protocol\" : text = self . _filter_protocol_text ( text ) return BundestagMineDocument ( text = text , metadata = metadata ) def _filter_protocol_text ( self , text : str ) -> str : \"\"\"Filter protocol text to remove non-informative sections. Removes: - Anlage sections (attachments with attendance lists, voting records) - Content after \"Anlagen zum Stenografischen Bericht\" marker - Name list sections (lines with >80% proper nouns, no verbs) Args: text: Raw protocol text Returns: Filtered text with only substantive content \"\"\" lines = text . split ( \" \\n \" ) filtered_lines = [] in_anlage_section = False in_name_list = False removed_lines = 0 name_list_start = - 1 for i , line in enumerate ( lines ): stripped = line . strip () # Check for start of Anlagen section (usually near the end) if stripped . startswith ( \"Anlagen zum Stenografischen Bericht\" ): self . logger . debug ( f \"Found 'Anlagen zum Stenografischen Bericht' at line { i } , \" f \"removing remaining { len ( lines ) - i } lines\" ) removed_lines += len ( lines ) - i break # Check for individual Anlage markers followed by minimal content if stripped . startswith ( \"Anlage\" ): # Look ahead to see if this is a low-content section next_lines = lines [ i + 1 : min ( i + 20 , len ( lines ))] non_empty_next = [ l . strip () for l in next_lines if l . strip ()] # If Anlage is followed by very few words or just names/numbers, # it's likely an attachment section if len ( non_empty_next ) <= 3 : in_anlage_section = True self . logger . debug ( f \"Found standalone Anlage at line { i } : { stripped [: 50 ] } \" ) removed_lines += 1 continue # Check if followed by attendance/voting list markers elif any ( marker in \" \" . join ( non_empty_next [: 5 ]) for marker in [ \"Entschuldigte Abgeordnete\" , \"Namensverzeichnis\" , \"Ergebnis und Namensverzeichnis\" , ] ): in_anlage_section = True self . logger . debug ( f \"Found Anlage with attendance/voting list at line { i } \" ) removed_lines += 1 continue # Exit Anlage section when we hit substantive content if in_anlage_section : # Check for speaker pattern (name followed by colon) if ( \":\" in line and len ( stripped ) > 10 and stripped . endswith ( \":\" ) ): in_anlage_section = False self . logger . debug ( f \"Exiting Anlage section at line { i } \" ) # Check for long paragraph (likely substantive content) elif len ( stripped ) > 100 : in_anlage_section = False self . logger . debug ( f \"Exiting Anlage section at line { i } \" ) else : removed_lines += 1 continue # NEW: Detect name list sections (lines with mostly proper nouns, no verbs) if not in_name_list and self . _is_name_list_line ( stripped ): # Look ahead to see if this is start of a name list section next_lines = lines [ i + 1 : min ( i + 10 , len ( lines ))] name_count = sum ( 1 for l in next_lines if l . strip () and self . _is_name_list_line ( l . strip ()) ) # If 5+ consecutive name-like lines, it's a name list section if name_count >= 5 : in_name_list = True name_list_start = i removed_lines += 1 self . logger . debug ( f \"Entering name list section at line { i } \" ) continue # Exit name list when we hit substantive content if in_name_list : # Check for speaker pattern or long substantive text if ( ( \":\" in line and stripped . endswith ( \":\" )) or len ( stripped ) > 150 or self . _has_verbs ( stripped ) ): in_name_list = False list_length = i - name_list_start self . logger . debug ( f \"Exited name list section at line { i } (removed { list_length } lines)\" ) else : removed_lines += 1 continue filtered_lines . append ( line ) filtered_text = \" \\n \" . join ( filtered_lines ) if removed_lines > 0 : self . logger . info ( f \"Filtered protocol: removed { removed_lines } lines \" f \"( { len ( text ) - len ( filtered_text ) } chars) of non-substantive content\" ) return filtered_text def _is_name_list_line ( self , line : str ) -> bool : \"\"\"Check if a line looks like a name list entry. Name list characteristics: - Short line (< 80 chars) - 2-4 words - All words start with capital letter - No verbs or sentence structure Args: line: Line to check Returns: True if line looks like a name entry \"\"\" if not line or len ( line ) > 80 : return False words = line . split () if not ( 2 <= len ( words ) <= 5 ): return False # Check if all words are capitalized (typical for names) # Allow for common German particles: von, van, de, der particles = { \"von\" , \"van\" , \"de\" , \"der\" , \"den\" , \"zu\" } capitalized_count = sum ( 1 for w in words if w [ 0 ] . isupper () or w . lower () in particles ) return ( capitalized_count >= len ( words ) - 1 ) # Allow one non-capitalized word def _has_verbs ( self , text : str ) -> bool : \"\"\"Check if text contains common German verbs (indicates substantive content). Args: text: Text to check Returns: True if text contains verbs \"\"\" # Common German verbs and verb patterns verb_indicators = [ \" ist \" , \" sind \" , \" war \" , \" waren \" , \" hat \" , \" haben \" , \" hatte \" , \" wird \" , \" werden \" , \" wurde \" , \" wurden \" , \" kann \" , \" k\u00f6nnen \" , \" soll \" , \" muss \" , \" m\u00f6chte \" , \" sage \" , \" sagen \" , \" glaube \" , \" denke \" , \" meine \" , \" macht \" , \" machen \" , \" gibt \" , \" geht \" , ] text_lower = f \" { text . lower () } \" return any ( verb in text_lower for verb in verb_indicators ) def _extract_metadata_from_speech ( self , speech : BundestagSpeech ) -> dict : \"\"\" Extract metadata from a BundestagMine speech. Args: speech: BundestagSpeech object Returns: Dictionary containing extracted metadata \"\"\" legislature_period = speech . protocol . legislaturePeriod protocol_number = speech . protocol . number agenda_item_number = speech . agendaItem . agendaItemNumber url = f \"https://dserver.bundestag.de/btp/ { legislature_period } / { legislature_period }{ protocol_number } .pdf\" title = f \"Protocol/Legislature/AgendaItem { protocol_number } / { legislature_period } / { agenda_item_number } \" speaker_name = f \" { speech . speaker . firstName } { speech . speaker . lastName } \" metadata = { \"datasource\" : \"bundestag\" , \"source_client\" : \"bundestag_mine\" , \"language\" : \"de\" , \"url\" : url , \"title\" : title , \"format\" : \"md\" , \"created_time\" : speech . protocol . date , \"last_edited_time\" : speech . protocol . date , \"speaker_party\" : speech . speaker . party , \"speaker\" : speaker_name , \"agenda_item_number\" : agenda_item_number , \"protocol_number\" : protocol_number , \"document_number\" : f \" { legislature_period } / { protocol_number } \" , # Standardized identifier \"legislature_period\" : legislature_period , \"document_type\" : \"speech\" , } # Extract party composition from speaker metadata if speech . speaker and speech . speaker . party : parliamentary_composition = ( PartyExtractor . extract_from_speaker_party ( speech . speaker . party ) ) metadata [ \"parliamentary_composition\" ] = parliamentary_composition return metadata def _extract_metadata_from_dip ( self , dip_doc : DIPDocument ) -> dict : \"\"\" Extract metadata from a DIP document. Args: dip_doc: DIPDocument object Returns: Dictionary containing extracted metadata \"\"\" content = dip_doc . content source_type = dip_doc . source_type # Extract common fields present in all DIP documents metadata = { \"datasource\" : \"bundestag\" , \"source_client\" : \"dip\" , \"language\" : \"de\" , \"format\" : \"md\" , \"document_type\" : source_type , } # Extract common metadata across all document types fundstelle = content . get ( \"fundstelle\" , {}) # Add document ID if available if \"id\" in content : metadata [ \"document_id\" ] = str ( content [ \"id\" ]) # Add publisher (herausgeber) if available if \"herausgeber\" in content : metadata [ \"publisher\" ] = str ( content [ \"herausgeber\" ]) # Add document art (dokumentart) if available if \"dokumentart\" in content : metadata [ \"document_art\" ] = content [ \"dokumentart\" ] # Extract type-specific metadata if source_type == \"protocol\" : # Plenary protocol metadata dokumentnummer = content . get ( \"dokumentnummer\" , \"unknown\" ) wahlperiode = content . get ( \"wahlperiode\" , \"\" ) titel = content . get ( \"titel\" , f \"Plenary Protocol { dokumentnummer } \" ) metadata . update ( { \"title\" : titel , \"protocol_number\" : dokumentnummer , \"document_number\" : dokumentnummer , # Standardized identifier (same as protocol_number for protocols) \"legislature_period\" : wahlperiode , \"url\" : fundstelle . get ( \"pdf_url\" , \"\" ), \"created_time\" : content . get ( \"datum\" , \"\" ), \"last_edited_time\" : content . get ( \"aktualisiert\" , content . get ( \"datum\" , \"\" ) ), } ) # Add fundstelle reference metadata if \"verteildatum\" in fundstelle : metadata [ \"distribution_date\" ] = fundstelle [ \"verteildatum\" ] if \"xml_url\" in fundstelle : metadata [ \"xml_url\" ] = fundstelle [ \"xml_url\" ] # Add vorgangsbezug (proceedings references) count if \"vorgangsbezug_anzahl\" in content : metadata [ \"related_proceedings_count\" ] = content [ \"vorgangsbezug_anzahl\" ] # Extract parliamentary composition from protocol text text = content . get ( \"text\" , \"\" ) parliamentary_composition = ( PartyExtractor . extract_from_protocol_text ( text ) ) metadata [ \"parliamentary_composition\" ] = parliamentary_composition # Log extraction results num_fractions = len ( parliamentary_composition . get ( \"fractions\" , [])) confidence = parliamentary_composition . get ( \"confidence\" , \"unknown\" ) self . logger . info ( f \"Extracted { num_fractions } fractions from protocol { dokumentnummer } \" f \"(confidence: { confidence } )\" ) elif source_type == \"drucksache\" : # Printed material metadata dokumentnummer = content . get ( \"dokumentnummer\" , \"unknown\" ) wahlperiode = content . get ( \"wahlperiode\" , \"\" ) drucksachetyp = content . get ( \"drucksachetyp\" , \"\" ) metadata . update ( { \"title\" : f \"Drucksache { dokumentnummer } \" , \"document_number\" : dokumentnummer , \"document_subtype\" : drucksachetyp , \"legislature_period\" : wahlperiode , \"url\" : fundstelle . get ( \"pdf_url\" , \"\" ), \"created_time\" : content . get ( \"datum\" , \"\" ), \"last_edited_time\" : content . get ( \"aktualisiert\" , content . get ( \"datum\" , \"\" ) ), } ) # Add fundstelle reference metadata if \"verteildatum\" in fundstelle : metadata [ \"distribution_date\" ] = fundstelle [ \"verteildatum\" ] if \"xml_url\" in fundstelle : metadata [ \"xml_url\" ] = fundstelle [ \"xml_url\" ] elif source_type == \"proceeding\" : # Legislative proceeding metadata titel = content . get ( \"titel\" , \"\" ) vorgangsnummer = content . get ( \"vorgangsnummer\" , \"unknown\" ) wahlperiode = content . get ( \"wahlperiode\" , \"\" ) metadata . update ( { \"title\" : titel or f \"Proceeding { vorgangsnummer } \" , \"document_number\" : vorgangsnummer , \"legislature_period\" : wahlperiode , \"url\" : fundstelle . get ( \"url\" , \"\" ), \"created_time\" : content . get ( \"datum\" , \"\" ), \"last_edited_time\" : content . get ( \"aktualisiert\" , content . get ( \"datum\" , \"\" ) ), } ) return metadata parse ( content ) Parse content into a BundestagMineDocument object. Parameters: content ( Union [ BundestagSpeech , DIPDocument ] ) \u2013 Raw content to be parsed (BundestagSpeech or DIPDocument) Returns: BundestagMineDocument \u2013 Parsed document of type BundestagMineDocument Source code in src/extraction/datasources/bundestag/parser.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def parse ( self , content : Union [ BundestagSpeech , DIPDocument ] ) -> BundestagMineDocument : \"\"\" Parse content into a BundestagMineDocument object. Args: content: Raw content to be parsed (BundestagSpeech or DIPDocument) Returns: Parsed document of type BundestagMineDocument \"\"\" if isinstance ( content , BundestagSpeech ): return self . _parse_bundestag_speech ( content ) elif isinstance ( content , DIPDocument ): return self . _parse_dip_document ( content ) else : raise ValueError ( f \"Unsupported content type: { type ( content ) } \" ) BundestagMineDatasourceParserFactory Bases: Factory Factory for creating instances of BundestagMineDatasourceParser. Creates and configures BundestagMineDatasourceParser objects according to the provided configuration. Source code in src/extraction/datasources/bundestag/parser.py 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 class BundestagMineDatasourceParserFactory ( Factory ): \"\"\" Factory for creating instances of BundestagMineDatasourceParser. Creates and configures BundestagMineDatasourceParser objects according to the provided configuration. \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineDatasourceParser : \"\"\" Create an instance of BundestagMineDatasourceParser. Args: configuration: Configuration for the parser (not used in this implementation) Returns: An instance of BundestagMineDatasourceParser \"\"\" return BundestagMineDatasourceParser () Party_extractor Extract parliamentary composition metadata from Bundestag documents. This module uses DYNAMIC extraction without hardcoded party names, making it future-proof for new parties and changing compositions. PartyExtractor Extracts party/fraction composition dynamically from protocol text. Design principle: Extract ALL party mentions using pattern matching and heuristics, without hardcoding specific party names. Source code in src/extraction/datasources/bundestag/party_extractor.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 class PartyExtractor : \"\"\"Extracts party/fraction composition dynamically from protocol text. Design principle: Extract ALL party mentions using pattern matching and heuristics, without hardcoding specific party names. \"\"\" logger = LoggerConfiguration . get_logger ( __name__ ) # Non-party keywords to filter out (roles, locations, organizations, etc.) NON_PARTY_KEYWORDS = { # Governmental roles \"Bundeskanzler\" , \"Bundeskanzlerin\" , \"Bundesminister\" , \"Bundesministerin\" , \"Bundespr\u00e4sident\" , \"Bundespr\u00e4sidentin\" , \"Pr\u00e4sident\" , \"Pr\u00e4sidentin\" , \"Staatsminister\" , \"Staatsministerin\" , \"Staatssekret\u00e4r\" , \"Staatssekret\u00e4rin\" , # Locations \"Berlin\" , \"Bonn\" , # Status \"parteilos\" , \"fraktionslos\" , \"Gast\" , # Guest speakers # Common organizational abbreviations (NOT parties) \"EU\" , \"UN\" , \"NATO\" , \"OSZE\" , \"WHO\" , \"IWF\" , \"EZB\" , \"BMWE\" , \"BMI\" , \"BMF\" , \"BMAS\" , \"BMZ\" , \"BMVI\" , \"BMVg\" , # German ministries \"BT\" , \"BR\" , # Bundestag/Bundesrat abbreviations \"USA\" , \"UK\" , \"FR\" , # Countries # Procedural terms \"TOP\" , \"ZP\" , # Tagesordnungspunkt, Zusatzpunkt } @classmethod def extract_from_protocol_text ( cls , text : str ) -> Dict : \"\"\"Extract party composition from DIP protocol text dynamically. Uses pattern matching and heuristics to identify parties WITHOUT hardcoded party names. Args: text: Full protocol text from DIP API Returns: Parliamentary composition metadata dictionary \"\"\" if not text : return cls . _empty_result () # Pattern: Matches \"Name (PARTY)\" speaker attributions # More flexible pattern that matches various name formats: # - \"Hans M\u00fcller (CDU/CSU)\" # - \"Dr. Maria Schmidt (SPD)\" # - \"Speaker1 (CDU)\" (for tests) # Captures name and content in parentheses pattern = r \"(\\b[A-Z\u00c4\u00d6\u00dca-z\u00e4\u00f6\u00fc\u00df][A-Z\u00c4\u00d6\u00dca-z\u00e4\u00f6\u00fc\u00df0-9\\.\\s]*?)\\s+\\(([^)]+)\\)\" matches = re . findall ( pattern , text ) # Extract candidates: text in parentheses after names candidates = [ match [ 1 ] . strip () for match in matches ] # Filter to likely parties using heuristics party_candidates = [] for candidate in candidates : if cls . _is_likely_party ( candidate ): party_candidates . append ( candidate ) if not party_candidates : cls . logger . debug ( \"No party candidates found in protocol text\" ) return cls . _empty_result () # Count occurrences for confidence scoring party_counts = Counter ( party_candidates ) cls . logger . debug ( f \"Found { len ( party_counts ) } unique party variations \" f \"with { sum ( party_counts . values ()) } total mentions\" ) # CRITICAL FILTER: Remove noise by requiring minimum mentions # Real parties appear throughout the protocol (many mentions) # Noise abbreviations (government agencies, technical terms) appear rarely (1 mention) # Threshold: At least 2 mentions to be considered a party # This filters out single-occurrence noise while catching real parties MIN_MENTIONS = 2 filtered_party_counts = Counter ( { name : count for name , count in party_counts . items () if count >= MIN_MENTIONS } ) if not filtered_party_counts : cls . logger . debug ( f \"After filtering for min { MIN_MENTIONS } mentions, no parties remain\" ) return cls . _empty_result () cls . logger . debug ( f \"After filtering for min { MIN_MENTIONS } mentions: \" f \" { len ( filtered_party_counts ) } candidates remain\" ) # Group related party names (e.g., CDU, CSU, CDU/CSU \u2192 CDU/CSU) party_groups = cls . _group_related_parties ( filtered_party_counts ) cls . logger . info ( f \"Grouped into { len ( party_groups ) } distinct fractions\" ) # Build fractions list fractions = [] for primary_name , related_names in party_groups . items (): total_mentions = sum ( filtered_party_counts [ name ] for name in related_names ) fractions . append ( { \"name\" : primary_name , \"variations\" : sorted ( list ( related_names )), \"type\" : \"fraction\" , \"mention_count\" : total_mentions , } ) # Sort by mention count (most mentioned first) fractions . sort ( key = lambda f : f [ \"mention_count\" ], reverse = True ) confidence = cls . _calculate_confidence ( fractions , filtered_party_counts ) cls . logger . info ( f \"Extracted { len ( fractions ) } fractions with { confidence } confidence: \" f \" { ', ' . join ( f [ 'name' ] for f in fractions ) } \" ) return { \"fractions\" : fractions , \"extraction_source\" : \"protocol_text\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : confidence , } @classmethod def extract_from_speaker_party ( cls , party : str ) -> Dict : \"\"\"Extract party metadata from a single speech's speaker.party field. Stores raw party name without normalization. Args: party: Party abbreviation from speaker metadata Returns: Single-party composition metadata \"\"\" if not party : return cls . _empty_result () # Store raw party name - NO hardcoded normalization fractions = [ { \"name\" : party , \"variations\" : [ party ], \"type\" : \"fraction\" , \"mention_count\" : 1 , } ] return { \"fractions\" : fractions , \"extraction_source\" : \"speaker_metadata\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : \"low\" , # Single speech doesn't show full composition } @classmethod def _is_likely_party ( cls , text : str ) -> bool : \"\"\"Determine if text is likely a party/fraction name using heuristics. CONSERVATIVE approach: Only match text that strongly resembles party names. Heuristics (NO hardcoded party names): 1. Length between 2-20 characters (parties are concise) 2. Not a common non-party phrase (roles, locations, organizations) 3. Matches specific party-name patterns: a) 2-4 char ALL-CAPS abbreviations (SPD, AfD, CDU, CSU, FDP) b) Compound party names with \"/\" (CDU/CSU, B\u00dcNDNIS 90/DIE GR\u00dcNEN) c) Party names starting with \"Die \" followed by capitalized word d) Party names starting with \"B\u00fcndnis\" or \"Bund\" Args: text: Candidate text from parentheses Returns: True if text is likely a party name \"\"\" text_clean = text . strip () # Must have content if not text_clean or len ( text_clean ) < 2 : return False # STRICTER length check (parties are typically 2-25 chars) # Allow up to 25 to accommodate \"B\u00dcNDNIS 90/DIE GR\u00dcNEN\" (23 chars) if len ( text_clean ) > 25 : return False # Exclude known non-party phrases FIRST if any ( keyword in text_clean for keyword in cls . NON_PARTY_KEYWORDS ): return False # Must contain at least one uppercase letter if not any ( c . isupper () for c in text_clean ): return False # Calculate character composition uppercase_count = sum ( 1 for c in text_clean if c . isupper ()) alpha_count = sum ( 1 for c in text_clean if c . isalpha ()) if alpha_count == 0 : return False uppercase_ratio = uppercase_count / alpha_count # PATTERN 1: Short abbreviations (2-6 characters) # Examples: SPD, AfD, CDU, CSU, FDP, BSW, GR\u00dcNE, LINKE if 2 <= len ( text_clean ) <= 6 : # Must be all letters (no numbers) and at least 2 uppercase letters # This allows \"AfD\" (2 uppercase out of 3) but rejects \"EU\", \"UK\" (too international) if alpha_count == len ( text_clean ) and uppercase_count >= 2 : return True return ( False # Reject short strings with numbers or too few uppercase ) # PATTERN 2: Compound names with slash \"/\" # Examples: \"CDU/CSU\", \"B\u00dcNDNIS 90/DIE GR\u00dcNEN\" if \"/\" in text_clean : # Must have at least 50% uppercase if uppercase_ratio >= 0.5 : # Additional check: both sides of \"/\" should have letters parts = text_clean . split ( \"/\" ) if len ( parts ) == 2 and all ( any ( c . isalpha () for c in p ) for p in parts ): return True return False # PATTERN 3: Names starting with \"Die \" or \"DIE \" # Examples: \"Die Linke\", \"DIE LINKE\" if text_clean . startswith ( \"Die \" ) or text_clean . startswith ( \"DIE \" ): # After \"Die \", should have at least one more capitalized word remaining = text_clean [ 4 :] . strip () if remaining and remaining [ 0 ] . isupper (): return True return False # PATTERN 4: Names starting with \"B\u00fcndnis\" or \"Bund\" # Examples: \"B\u00dcNDNIS 90\", \"B\u00fcndnis\" if ( text_clean . startswith ( \"B\u00fcndnis\" ) or text_clean . startswith ( \"B\u00dcNDNIS\" ) or text_clean . startswith ( \"Bund\" ) ): return True # All other patterns rejected return False @classmethod def _group_related_parties ( cls , party_counts : Counter ) -> Dict [ str , Set [ str ]]: \"\"\"Group related party name variations dynamically. Groups variations like: - \"CDU\", \"CSU\", \"CDU/CSU\" \u2192 \"CDU/CSU\" (longest/compound) - \"GR\u00dcNE\", \"DIE GR\u00dcNEN\", \"B\u00dcNDNIS 90/DIE GR\u00dcNEN\" \u2192 longest variant - \"Die Linke\", \"DIE LINKE\" \u2192 most common variant Strategy: Use Union-Find algorithm to merge related parties. Args: party_counts: Counter of party name occurrences Returns: Dict mapping canonical name to set of related names \"\"\" parties = list ( party_counts . keys ()) # Build Union-Find structure # parent[party] = canonical representative of its group parent = { p : p for p in parties } def find ( party ): \"\"\"Find root of party's group.\"\"\" if parent [ party ] != party : parent [ party ] = find ( parent [ party ]) # Path compression return parent [ party ] def union ( party1 , party2 ): \"\"\"Merge groups of party1 and party2.\"\"\" root1 = find ( party1 ) root2 = find ( party2 ) if root1 != root2 : # Merge: prefer party with slash, then longer, then more frequent if ( 1 if \"/\" in root1 else 0 , len ( root1 ), party_counts [ root1 ], ) >= ( 1 if \"/\" in root2 else 0 , len ( root2 ), party_counts [ root2 ], ): parent [ root2 ] = root1 else : parent [ root1 ] = root2 # Find all related pairs and union them for i , party1 in enumerate ( parties ): party1_upper = party1 . upper () for party2 in parties [ i + 1 :]: party2_upper = party2 . upper () # Check if related is_substring = ( party1_upper in party2_upper or party2_upper in party1_upper ) are_related = cls . _are_related_parties ( party1 , party2 ) if is_substring or are_related : union ( party1 , party2 ) # Build groups from Union-Find structure groups : Dict [ str , Set [ str ]] = {} for party in parties : root = find ( party ) if root not in groups : groups [ root ] = set () groups [ root ] . add ( party ) return groups @classmethod def _are_related_parties ( cls , party1 : str , party2 : str ) -> bool : \"\"\"Check if two party names are related variations. Uses word overlap heuristic WITHOUT hardcoded party knowledge. Args: party1: First party name party2: Second party name Returns: True if parties are related \"\"\" p1_upper = party1 . upper () p2_upper = party2 . upper () # Extract significant words (length >= 3, all caps) words1 = { w for w in re . findall ( r \"\\b[A-Z\u00c4\u00d6\u00dc]{3,}\\b\" , p1_upper )} words2 = { w for w in re . findall ( r \"\\b[A-Z\u00c4\u00d6\u00dc]{3,}\\b\" , p2_upper )} # Check for shared significant words # BUT: \"DIE\" and \"LINKE\" are both 3+ chars, so we need to be more careful # Only consider them related if they share a MEANINGFUL word shared_words = words1 & words2 if shared_words : # Exclude common articles/connectors: DIE, DER, DAS, UND, VON meaningful_shared = shared_words - { \"DIE\" , \"DER\" , \"DAS\" , \"UND\" , \"VON\" , } if meaningful_shared : return True return False @classmethod def _calculate_confidence ( cls , fractions : List [ Dict ], party_counts : Counter ) -> str : \"\"\"Calculate confidence level based on extraction results. Confidence based on: - Number of fractions (typical Bundestag has 4-7 fractions) - Total mention count (more mentions = higher confidence) Args: fractions: List of extracted fractions party_counts: Counter of raw party mentions Returns: \"high\", \"medium\", or \"low\" \"\"\" num_fractions = len ( fractions ) total_mentions = sum ( party_counts . values ()) # Typical Bundestag has 4-7 fractions # High confidence: 4+ distinct fractions (typical composition) # OR: 2+ fractions with many mentions (confirms representative sample) if num_fractions >= 4 or ( num_fractions >= 2 and total_mentions >= 20 ): return \"high\" elif num_fractions >= 2 or total_mentions >= 10 : return \"medium\" else : return \"low\" @classmethod def _empty_result ( cls ) -> Dict : \"\"\"Return empty composition metadata for documents with no parties.\"\"\" return { \"fractions\" : [], \"extraction_source\" : \"none\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : \"low\" , } extract_from_protocol_text ( text ) classmethod Extract party composition from DIP protocol text dynamically. Uses pattern matching and heuristics to identify parties WITHOUT hardcoded party names. Parameters: text ( str ) \u2013 Full protocol text from DIP API Returns: Dict \u2013 Parliamentary composition metadata dictionary Source code in src/extraction/datasources/bundestag/party_extractor.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @classmethod def extract_from_protocol_text ( cls , text : str ) -> Dict : \"\"\"Extract party composition from DIP protocol text dynamically. Uses pattern matching and heuristics to identify parties WITHOUT hardcoded party names. Args: text: Full protocol text from DIP API Returns: Parliamentary composition metadata dictionary \"\"\" if not text : return cls . _empty_result () # Pattern: Matches \"Name (PARTY)\" speaker attributions # More flexible pattern that matches various name formats: # - \"Hans M\u00fcller (CDU/CSU)\" # - \"Dr. Maria Schmidt (SPD)\" # - \"Speaker1 (CDU)\" (for tests) # Captures name and content in parentheses pattern = r \"(\\b[A-Z\u00c4\u00d6\u00dca-z\u00e4\u00f6\u00fc\u00df][A-Z\u00c4\u00d6\u00dca-z\u00e4\u00f6\u00fc\u00df0-9\\.\\s]*?)\\s+\\(([^)]+)\\)\" matches = re . findall ( pattern , text ) # Extract candidates: text in parentheses after names candidates = [ match [ 1 ] . strip () for match in matches ] # Filter to likely parties using heuristics party_candidates = [] for candidate in candidates : if cls . _is_likely_party ( candidate ): party_candidates . append ( candidate ) if not party_candidates : cls . logger . debug ( \"No party candidates found in protocol text\" ) return cls . _empty_result () # Count occurrences for confidence scoring party_counts = Counter ( party_candidates ) cls . logger . debug ( f \"Found { len ( party_counts ) } unique party variations \" f \"with { sum ( party_counts . values ()) } total mentions\" ) # CRITICAL FILTER: Remove noise by requiring minimum mentions # Real parties appear throughout the protocol (many mentions) # Noise abbreviations (government agencies, technical terms) appear rarely (1 mention) # Threshold: At least 2 mentions to be considered a party # This filters out single-occurrence noise while catching real parties MIN_MENTIONS = 2 filtered_party_counts = Counter ( { name : count for name , count in party_counts . items () if count >= MIN_MENTIONS } ) if not filtered_party_counts : cls . logger . debug ( f \"After filtering for min { MIN_MENTIONS } mentions, no parties remain\" ) return cls . _empty_result () cls . logger . debug ( f \"After filtering for min { MIN_MENTIONS } mentions: \" f \" { len ( filtered_party_counts ) } candidates remain\" ) # Group related party names (e.g., CDU, CSU, CDU/CSU \u2192 CDU/CSU) party_groups = cls . _group_related_parties ( filtered_party_counts ) cls . logger . info ( f \"Grouped into { len ( party_groups ) } distinct fractions\" ) # Build fractions list fractions = [] for primary_name , related_names in party_groups . items (): total_mentions = sum ( filtered_party_counts [ name ] for name in related_names ) fractions . append ( { \"name\" : primary_name , \"variations\" : sorted ( list ( related_names )), \"type\" : \"fraction\" , \"mention_count\" : total_mentions , } ) # Sort by mention count (most mentioned first) fractions . sort ( key = lambda f : f [ \"mention_count\" ], reverse = True ) confidence = cls . _calculate_confidence ( fractions , filtered_party_counts ) cls . logger . info ( f \"Extracted { len ( fractions ) } fractions with { confidence } confidence: \" f \" { ', ' . join ( f [ 'name' ] for f in fractions ) } \" ) return { \"fractions\" : fractions , \"extraction_source\" : \"protocol_text\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : confidence , } extract_from_speaker_party ( party ) classmethod Extract party metadata from a single speech's speaker.party field. Stores raw party name without normalization. Parameters: party ( str ) \u2013 Party abbreviation from speaker metadata Returns: Dict \u2013 Single-party composition metadata Source code in src/extraction/datasources/bundestag/party_extractor.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 @classmethod def extract_from_speaker_party ( cls , party : str ) -> Dict : \"\"\"Extract party metadata from a single speech's speaker.party field. Stores raw party name without normalization. Args: party: Party abbreviation from speaker metadata Returns: Single-party composition metadata \"\"\" if not party : return cls . _empty_result () # Store raw party name - NO hardcoded normalization fractions = [ { \"name\" : party , \"variations\" : [ party ], \"type\" : \"fraction\" , \"mention_count\" : 1 , } ] return { \"fractions\" : fractions , \"extraction_source\" : \"speaker_metadata\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : \"low\" , # Single speech doesn't show full composition } Reader BundestagMineDatasourceReader Bases: BaseReader Reader for extracting data from multiple Bundestag sources. Supports multiple data sources: - BundestagMine API: Individual speeches from parliamentary sessions - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) Source code in src/extraction/datasources/bundestag/reader.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class BundestagMineDatasourceReader ( BaseReader ): \"\"\"Reader for extracting data from multiple Bundestag sources. Supports multiple data sources: - BundestagMine API: Individual speeches from parliamentary sessions - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) \"\"\" def __init__ ( self , configuration : BundestagMineDatasourceConfiguration , client : Optional [ BundestagMineClient ] = None , dip_client : Optional [ DIPClient ] = None , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Bundestag reader with multiple data sources. Args: configuration: Settings for Bundestag data access and export limits client: Client for BundestagMine API interactions (optional) dip_client: Client for DIP API interactions (optional) logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . configuration = configuration self . export_limit = configuration . export_limit self . client = client self . dip_client = dip_client self . logger = logger async def read_all_async ( self , ) -> AsyncIterator [ dict ]: \"\"\"Asynchronously fetch all documents from enabled Bundestag sources. Yields documents from multiple sources based on configuration: - BundestagMine: Individual speeches - DIP: Comprehensive parliamentary documents Each enabled source gets the full export_limit, so if both sources are enabled with export_limit=2, you get 2 documents from each source (4 total). Returns: AsyncIterator[dict]: An async iterator of document dictionaries containing content and metadata such as text, speaker data, and last update information \"\"\" self . logger . info ( f \"Reading Bundestag documents with limit { self . export_limit } per source\" ) # Source 1: BundestagMine speeches if self . configuration . include_bundestag_mine and self . client : self . logger . info ( f \"Fetching speeches from BundestagMine API (limit: { self . export_limit } )...\" ) speech_iterator = self . client . fetch_all_speeches () mine_counter = 0 for speech in speech_iterator : if self . _limit_reached ( mine_counter , self . export_limit ): break self . logger . info ( f \"Fetched BundestagMine speech { mine_counter + 1 } / { self . export_limit } .\" ) mine_counter += 1 yield speech # Source 2: DIP API documents if self . configuration . include_dip and self . dip_client : self . logger . info ( f \"Fetching documents from DIP API (limit: { self . export_limit } )...\" ) dip_iterator = self . dip_client . fetch_all () dip_counter = 0 for dip_document in dip_iterator : if self . _limit_reached ( dip_counter , self . export_limit ): break self . logger . info ( f \"Fetched DIP document { dip_counter + 1 } / { self . export_limit } .\" ) dip_counter += 1 yield dip_document __init__ ( configuration , client = None , dip_client = None , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the Bundestag reader with multiple data sources. Parameters: configuration ( BundestagMineDatasourceConfiguration ) \u2013 Settings for Bundestag data access and export limits client ( Optional [ BundestagMineClient ] , default: None ) \u2013 Client for BundestagMine API interactions (optional) dip_client ( Optional [ DIPClient ] , default: None ) \u2013 Client for DIP API interactions (optional) logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operation information Source code in src/extraction/datasources/bundestag/reader.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , configuration : BundestagMineDatasourceConfiguration , client : Optional [ BundestagMineClient ] = None , dip_client : Optional [ DIPClient ] = None , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Bundestag reader with multiple data sources. Args: configuration: Settings for Bundestag data access and export limits client: Client for BundestagMine API interactions (optional) dip_client: Client for DIP API interactions (optional) logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . configuration = configuration self . export_limit = configuration . export_limit self . client = client self . dip_client = dip_client self . logger = logger read_all_async () async Asynchronously fetch all documents from enabled Bundestag sources. Yields documents from multiple sources based on configuration: - BundestagMine: Individual speeches - DIP: Comprehensive parliamentary documents Each enabled source gets the full export_limit, so if both sources are enabled with export_limit=2, you get 2 documents from each source (4 total). Returns: AsyncIterator [ dict ] \u2013 AsyncIterator[dict]: An async iterator of document dictionaries containing AsyncIterator [ dict ] \u2013 content and metadata such as text, speaker data, and last update information Source code in src/extraction/datasources/bundestag/reader.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 async def read_all_async ( self , ) -> AsyncIterator [ dict ]: \"\"\"Asynchronously fetch all documents from enabled Bundestag sources. Yields documents from multiple sources based on configuration: - BundestagMine: Individual speeches - DIP: Comprehensive parliamentary documents Each enabled source gets the full export_limit, so if both sources are enabled with export_limit=2, you get 2 documents from each source (4 total). Returns: AsyncIterator[dict]: An async iterator of document dictionaries containing content and metadata such as text, speaker data, and last update information \"\"\" self . logger . info ( f \"Reading Bundestag documents with limit { self . export_limit } per source\" ) # Source 1: BundestagMine speeches if self . configuration . include_bundestag_mine and self . client : self . logger . info ( f \"Fetching speeches from BundestagMine API (limit: { self . export_limit } )...\" ) speech_iterator = self . client . fetch_all_speeches () mine_counter = 0 for speech in speech_iterator : if self . _limit_reached ( mine_counter , self . export_limit ): break self . logger . info ( f \"Fetched BundestagMine speech { mine_counter + 1 } / { self . export_limit } .\" ) mine_counter += 1 yield speech # Source 2: DIP API documents if self . configuration . include_dip and self . dip_client : self . logger . info ( f \"Fetching documents from DIP API (limit: { self . export_limit } )...\" ) dip_iterator = self . dip_client . fetch_all () dip_counter = 0 for dip_document in dip_iterator : if self . _limit_reached ( dip_counter , self . export_limit ): break self . logger . info ( f \"Fetched DIP document { dip_counter + 1 } / { self . export_limit } .\" ) dip_counter += 1 yield dip_document BundestagMineDatasourceReaderFactory Bases: Factory Factory for creating Bundestag reader instances. Creates and configures BundestagMineDatasourceReader objects with appropriate clients based on the provided configuration. Supports multiple data sources including BundestagMine and DIP APIs. Source code in src/extraction/datasources/bundestag/reader.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class BundestagMineDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating Bundestag reader instances. Creates and configures BundestagMineDatasourceReader objects with appropriate clients based on the provided configuration. Supports multiple data sources including BundestagMine and DIP APIs. \"\"\" _configuration_class = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineDatasourceReader : \"\"\"Creates a configured Bundestag reader instance. Initializes the appropriate clients (BundestagMine and/or DIP) based on configuration settings. Args: configuration: Bundestag connection and access settings Returns: BundestagMineDatasourceReader: Fully configured reader instance \"\"\" # Create BundestagMine client if enabled bundestag_mine_client = None if configuration . include_bundestag_mine : bundestag_mine_client = BundestagMineClientFactory . create ( configuration ) # Create DIP client if enabled dip_client = None if configuration . include_dip : dip_client = DIPClient ( api_key = configuration . dip_api_key , wahlperiode = configuration . dip_wahlperiode , fetch_sources = configuration . dip_sources , ) return BundestagMineDatasourceReader ( configuration = configuration , client = bundestag_mine_client , dip_client = dip_client , )","title":"Bundestag"},{"location":"src/extraction/datasources/bundestag/#bundestag-datasource","text":"This module contains functionality related to the Bundestag datasource.","title":"Bundestag Datasource"},{"location":"src/extraction/datasources/bundestag/#client","text":"","title":"Client"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient","text":"Bases: APIClient API Client for the bundestag-mine.de API. Source code in src/extraction/datasources/bundestag/client.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 class BundestagMineClient ( APIClient ): \"\"\" API Client for the bundestag-mine.de API. \"\"\" BASE_URL = \"https://bundestag-mine.de/api/DashboardController\" logger = LoggerConfiguration . get_logger ( __name__ ) def safe_get ( self , path : str ) -> Optional [ Any ]: \"\"\" Perform a GET request, raise for HTTP errors, parse JSON, check API status. Args: path: endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/<protocol_id>\" Returns: Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError: if HTTP status is not OK or unexpected JSON structure. \"\"\" url = f \" { self . BASE_URL } / { path . lstrip ( '/' ) } \" try : resp = self . get ( url ) except Exception as e : self . logger . warning ( f \"Failed to fetch speeches for { url } : { e } \" ) return None try : resp . raise_for_status () except Exception as e : self . logger . error ( f \"HTTP error for { url } : { e } \" ) return None data = resp . json () if not isinstance ( data , dict ) or data . get ( \"status\" ) != \"200\" : self . logger . error ( f \"Unexpected response for { url } : { data } \" ) return None result = data . get ( \"result\" ) if result is None : self . logger . debug ( f \"No result found for { url } \" ) return None return result def get_protocols ( self ) -> Iterator [ Protocol ]: \"\"\" Fetches the list of all protocols. Returns: Iterator[Protocol]: An iterator of valid protocols as Pydantic models. \"\"\" result = self . safe_get ( \"GetProtocols\" ) if not isinstance ( result , list ): raise ResponseParseError ( f \"Expected list of protocols, got: { result } \" ) for protocol_data in result : try : yield Protocol . model_validate ( protocol_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate protocol: { protocol_data } . Error: { e } \" ) def get_agenda_items ( self , protocol_id : str ) -> Iterator [ AgendaItem ]: \"\"\" Fetches agenda items for a specific protocol ID. Args: protocol_id (str): The ID of the protocol. Returns: Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. \"\"\" result = self . safe_get ( f \"GetAgendaItemsOfProtocol/ { protocol_id } \" ) if result is None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return items = result . get ( \"agendaItems\" ) if \"items\" == None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return if not isinstance ( items , list ): self . logger . error ( f \"Expected list of agendaItems for { protocol_id } , got: { items } \" ) return for item_data in items : try : yield AgendaItem . model_validate ( item_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate agenda item: { item_data } . Error: { e } \" ) def get_speaker_data ( self , speaker_id : str ) -> Optional [ Speaker ]: \"\"\" Fetches speaker data for a specific speaker ID. Args: speaker_id (str): The ID of the speaker. Returns: Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. \"\"\" result = self . safe_get ( f \"GetSpeakerById/ { speaker_id } \" ) if not isinstance ( result , dict ): self . logger . error ( f \"Expected speaker data for { speaker_id } , got: { result } \" ) return None try : return Speaker . model_validate ( result ) except ValidationError as e : self . logger . warning ( f \"Failed to validate speaker data for { speaker_id } : { result } . Error: { e } \" ) return None @retry_request def get_speeches ( self , protocol : Protocol , agenda_item : AgendaItem , ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches speeches for a specific agenda item within a protocol. Args: legislature_period (int): The legislature period. protocol_number (int): The protocol number. agenda_item_number (str): The agenda item number. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" raw = f \" { protocol . legislaturePeriod } , { protocol . number } , { agenda_item . agendaItemNumber } \" encoded = quote ( raw , safe = \"\" ) result = self . safe_get ( f \"GetSpeechesOfAgendaItem/ { encoded } \" ) if result is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return speeches = result . get ( \"speeches\" ) if speeches is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return if not isinstance ( speeches , list ): self . logger . warning ( f \"Expected list of speeches for { raw } , got: { speeches } \" ) return for speech in speeches : try : speech = BundestagSpeech . model_validate ( speech ) speech . protocol = protocol speech . agendaItem = agenda_item yield speech except ValidationError as e : self . logger . warning ( f \"Failed to validate speech: { speech } . Error: { e } \" ) def fetch_all_speeches ( self ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" for protocol in self . get_protocols (): self . logger . info ( f \"Processing protocol { protocol . id } \" ) for agenda_item in self . get_agenda_items ( protocol . id ): for speech in self . get_speeches ( protocol = protocol , agenda_item = agenda_item , ): speaker = self . get_speaker_data ( speech . speakerId ) if speaker : speech . speaker = speaker yield speech","title":"BundestagMineClient"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.fetch_all_speeches","text":"Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator [ BundestagSpeech ] \u2013 Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def fetch_all_speeches ( self ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" for protocol in self . get_protocols (): self . logger . info ( f \"Processing protocol { protocol . id } \" ) for agenda_item in self . get_agenda_items ( protocol . id ): for speech in self . get_speeches ( protocol = protocol , agenda_item = agenda_item , ): speaker = self . get_speaker_data ( speech . speakerId ) if speaker : speech . speaker = speaker yield speech","title":"fetch_all_speeches"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.get_agenda_items","text":"Fetches agenda items for a specific protocol ID. Parameters: protocol_id ( str ) \u2013 The ID of the protocol. Returns: Iterator [ AgendaItem ] \u2013 Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def get_agenda_items ( self , protocol_id : str ) -> Iterator [ AgendaItem ]: \"\"\" Fetches agenda items for a specific protocol ID. Args: protocol_id (str): The ID of the protocol. Returns: Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. \"\"\" result = self . safe_get ( f \"GetAgendaItemsOfProtocol/ { protocol_id } \" ) if result is None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return items = result . get ( \"agendaItems\" ) if \"items\" == None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return if not isinstance ( items , list ): self . logger . error ( f \"Expected list of agendaItems for { protocol_id } , got: { items } \" ) return for item_data in items : try : yield AgendaItem . model_validate ( item_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate agenda item: { item_data } . Error: { e } \" )","title":"get_agenda_items"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.get_protocols","text":"Fetches the list of all protocols. Returns: Iterator [ Protocol ] \u2013 Iterator[Protocol]: An iterator of valid protocols as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def get_protocols ( self ) -> Iterator [ Protocol ]: \"\"\" Fetches the list of all protocols. Returns: Iterator[Protocol]: An iterator of valid protocols as Pydantic models. \"\"\" result = self . safe_get ( \"GetProtocols\" ) if not isinstance ( result , list ): raise ResponseParseError ( f \"Expected list of protocols, got: { result } \" ) for protocol_data in result : try : yield Protocol . model_validate ( protocol_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate protocol: { protocol_data } . Error: { e } \" )","title":"get_protocols"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.get_speaker_data","text":"Fetches speaker data for a specific speaker ID. Parameters: speaker_id ( str ) \u2013 The ID of the speaker. Returns: Optional [ Speaker ] \u2013 Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. Source code in src/extraction/datasources/bundestag/client.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def get_speaker_data ( self , speaker_id : str ) -> Optional [ Speaker ]: \"\"\" Fetches speaker data for a specific speaker ID. Args: speaker_id (str): The ID of the speaker. Returns: Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. \"\"\" result = self . safe_get ( f \"GetSpeakerById/ { speaker_id } \" ) if not isinstance ( result , dict ): self . logger . error ( f \"Expected speaker data for { speaker_id } , got: { result } \" ) return None try : return Speaker . model_validate ( result ) except ValidationError as e : self . logger . warning ( f \"Failed to validate speaker data for { speaker_id } : { result } . Error: { e } \" ) return None","title":"get_speaker_data"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.get_speeches","text":"Fetches speeches for a specific agenda item within a protocol. Parameters: legislature_period ( int ) \u2013 The legislature period. protocol_number ( int ) \u2013 The protocol number. agenda_item_number ( str ) \u2013 The agenda item number. Returns: Iterator [ BundestagSpeech ] \u2013 Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @retry_request def get_speeches ( self , protocol : Protocol , agenda_item : AgendaItem , ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches speeches for a specific agenda item within a protocol. Args: legislature_period (int): The legislature period. protocol_number (int): The protocol number. agenda_item_number (str): The agenda item number. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" raw = f \" { protocol . legislaturePeriod } , { protocol . number } , { agenda_item . agendaItemNumber } \" encoded = quote ( raw , safe = \"\" ) result = self . safe_get ( f \"GetSpeechesOfAgendaItem/ { encoded } \" ) if result is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return speeches = result . get ( \"speeches\" ) if speeches is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return if not isinstance ( speeches , list ): self . logger . warning ( f \"Expected list of speeches for { raw } , got: { speeches } \" ) return for speech in speeches : try : speech = BundestagSpeech . model_validate ( speech ) speech . protocol = protocol speech . agendaItem = agenda_item yield speech except ValidationError as e : self . logger . warning ( f \"Failed to validate speech: { speech } . Error: { e } \" )","title":"get_speeches"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.safe_get","text":"Perform a GET request, raise for HTTP errors, parse JSON, check API status. Parameters: path ( str ) \u2013 endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/ \" Returns: Optional [ Any ] \u2013 Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError \u2013 if HTTP status is not OK or unexpected JSON structure. Source code in src/extraction/datasources/bundestag/client.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def safe_get ( self , path : str ) -> Optional [ Any ]: \"\"\" Perform a GET request, raise for HTTP errors, parse JSON, check API status. Args: path: endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/<protocol_id>\" Returns: Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError: if HTTP status is not OK or unexpected JSON structure. \"\"\" url = f \" { self . BASE_URL } / { path . lstrip ( '/' ) } \" try : resp = self . get ( url ) except Exception as e : self . logger . warning ( f \"Failed to fetch speeches for { url } : { e } \" ) return None try : resp . raise_for_status () except Exception as e : self . logger . error ( f \"HTTP error for { url } : { e } \" ) return None data = resp . json () if not isinstance ( data , dict ) or data . get ( \"status\" ) != \"200\" : self . logger . error ( f \"Unexpected response for { url } : { data } \" ) return None result = data . get ( \"result\" ) if result is None : self . logger . debug ( f \"No result found for { url } \" ) return None return result","title":"safe_get"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClientFactory","text":"Bases: SingletonFactory Factory for creating and managing Bundestag client instances. This factory ensures only one Bundestag client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. Source code in src/extraction/datasources/bundestag/client.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 class BundestagMineClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Bundestag client instances. This factory ensures only one Bundestag client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineClient : \"\"\" Creates a new BundestagMine client instance using the provided configuration. Args: configuration: Configuration object containing BundestagMine details Returns: A configured BundestagMine client instance ready for API interactions. \"\"\" return BundestagMineClient ()","title":"BundestagMineClientFactory"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagSpeech","text":"Bases: BaseModel Represents a speech from BundestagMine API. Source code in src/extraction/datasources/bundestag/client.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class BundestagSpeech ( BaseModel ): \"\"\"Represents a speech from BundestagMine API.\"\"\" id : str speakerId : str text : str speaker : Optional [ Speaker ] = None protocol : Optional [ Protocol ] = None agendaItem : Optional [ AgendaItem ] = None @model_validator ( mode = \"after\" ) def validate_text_not_empty ( self ) -> \"BundestagSpeech\" : if not self . text or self . text . strip () == \"\" : raise ValueError ( \"BundestagSpeech text cannot be empty\" ) return self","title":"BundestagSpeech"},{"location":"src/extraction/datasources/bundestag/#client_dip","text":"DIP API Client for Bundestag datasource using deutschland package. This client provides access to the DIP (Dokumentations- und Informationssystem f\u00fcr Parlamentsmaterialien) API, which is the official German Bundestag API for parliamentary materials.","title":"Client_dip"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client_dip.DIPClient","text":"Client for DIP (Dokumentations- und Informationssystem) API. Uses the deutschland Python package for API access, providing access to plenary protocols, printed materials, and proceedings. Source code in src/extraction/datasources/bundestag/client_dip.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 class DIPClient : \"\"\" Client for DIP (Dokumentations- und Informationssystem) API. Uses the deutschland Python package for API access, providing access to plenary protocols, printed materials, and proceedings. \"\"\" DEFAULT_API_KEY = \"OSOegLs.PR2lwJ1dwCeje9vTj7FPOt3hvpYKtwKkhw\" def __init__ ( self , api_key : Optional [ str ] = None , wahlperiode : int = 21 , fetch_sources : Optional [ List [ str ]] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\" Initialize DIP client. Args: api_key: DIP API key. If not provided, uses public test key. wahlperiode: Electoral period number (default: 20). fetch_sources: List of data sources to fetch. Options: \"protocols\", \"drucksachen\", \"proceedings\" logger: Logger instance for logging. \"\"\" self . api_key = api_key or self . DEFAULT_API_KEY self . wahlperiode = wahlperiode self . fetch_sources = fetch_sources or [ \"protocols\" , \"drucksachen\" ] self . logger = logger or logging . getLogger ( __name__ ) # Configure deutschland package self . configuration = dip_bundestag . Configuration ( host = \"https://search.dip.bundestag.de/api/v1\" ) # Use query parameter auth (more reliable) self . configuration . api_key [ \"ApiKeyQuery\" ] = self . api_key self . logger . info ( f \"Initialized DIP client for Wahlperiode { self . wahlperiode } , \" f \"sources: { self . fetch_sources } \" ) def fetch_all ( self ) -> Iterator [ DIPDocument ]: \"\"\" Fetch all enabled data sources. Yields: DIPDocument instances wrapping different content types. \"\"\" with dip_bundestag . ApiClient ( self . configuration ) as api_client : if \"protocols\" in self . fetch_sources : yield from self . _fetch_protocols ( api_client ) if \"drucksachen\" in self . fetch_sources : yield from self . _fetch_drucksachen ( api_client ) if \"proceedings\" in self . fetch_sources : yield from self . _fetch_proceedings ( api_client ) def _fetch_protocols ( self , api_client ) -> Iterator [ DIPDocument ]: \"\"\" Fetch plenary protocols with full text. Args: api_client: deutschland API client instance. Yields: DIPDocument instances for protocols. \"\"\" self . logger . info ( f \"Fetching protocols for Wahlperiode { self . wahlperiode } \" ) protokoll_api = plenarprotokolle_api . PlenarprotokolleApi ( api_client ) try : # Get list of protocols - need to paginate to find Bundestag protocols # The API returns Bundesrat protocols first, so we need to paginate all_bt_protocols = [] cursor = \"*\" max_pages = 10 # Limit pagination to avoid excessive API calls page_count = 0 while cursor and page_count < max_pages : response = protokoll_api . get_plenarprotokoll_list ( f_wahlperiode = self . wahlperiode , format = \"json\" , cursor = cursor ) # Filter for Bundestag protocols only (not Bundesrat) # herausgeber is a Zuordnung object, need to convert to string bt_protocols = [ p for p in response . documents if str ( getattr ( p , \"herausgeber\" , \"\" )) == \"BT\" ] all_bt_protocols . extend ( bt_protocols ) # Check if we have more results cursor = getattr ( response , \"cursor\" , None ) page_count += 1 self . logger . debug ( f \"Page { page_count } : Found { len ( bt_protocols ) } BT protocols, \" f \"total so far: { len ( all_bt_protocols ) } \" ) # Stop if we have enough if len ( all_bt_protocols ) >= 50 : # Reasonable limit for testing break protocol_ids = [ p . id for p in all_bt_protocols ] self . logger . info ( f \"Found { len ( protocol_ids ) } Bundestag protocols \" f \"for Wahlperiode { self . wahlperiode } (across { page_count } pages)\" ) # Fetch full text for each protocol for protocol_id in protocol_ids : try : # API expects integer ID fulltext = protokoll_api . get_plenarprotokoll_text ( id = int ( protocol_id ), format = \"json\" ) # Only yield if we have text if hasattr ( fulltext , \"text\" ) and fulltext . text : # Convert to dict for serialization content_dict = fulltext . to_dict () yield DIPDocument ( source_type = \"protocol\" , content = content_dict ) dokumentnummer = getattr ( fulltext , \"dokumentnummer\" , \"unknown\" ) text_length = len ( fulltext . text ) self . logger . debug ( f \"Fetched protocol { dokumentnummer } \" f \"( { text_length } chars)\" ) except Exception as e : self . logger . warning ( f \"Failed to fetch protocol { protocol_id } : { e } \" ) continue except Exception as e : self . logger . error ( f \"Failed to list protocols: { e } \" , exc_info = True ) def _fetch_drucksachen ( self , api_client ) -> Iterator [ DIPDocument ]: \"\"\" Fetch printed materials (drucksachen) with full text. Args: api_client: deutschland API client instance. Yields: DIPDocument instances for drucksachen. \"\"\" self . logger . info ( f \"Fetching drucksachen for Wahlperiode { self . wahlperiode } \" ) drucksache_api = drucksachen_api . DrucksachenApi ( api_client ) try : cursor = None fetched_count = 0 page = 1 while True : # Fetch page of documents cursor_param = cursor if cursor else \"\" response = drucksache_api . get_drucksache_list ( f_wahlperiode = self . wahlperiode , cursor = cursor_param , format = \"json\" , ) # Try to get full text for each document for doc_meta in response . documents : try : fulltext = drucksache_api . get_drucksache_text ( id = doc_meta . id , format = \"json\" ) # Only yield if we have text if hasattr ( fulltext , \"text\" ) and fulltext . text : # Convert to dict for serialization content_dict = fulltext . to_dict () yield DIPDocument ( source_type = \"drucksache\" , content = content_dict ) fetched_count += 1 except Exception : # Many drucksachen don't have full text, log at debug level self . logger . debug ( f \"No full text for drucksache { doc_meta . id } \" ) continue # Log progress self . logger . info ( f \"Drucksachen: page { page } complete, \" f \" { fetched_count } with full text so far\" ) # Check pagination new_cursor = getattr ( response , \"cursor\" , None ) if not new_cursor or new_cursor == cursor : break cursor = new_cursor page += 1 self . logger . info ( f \"Completed fetching drucksachen. \" f \"Total with full text: { fetched_count } \" ) except Exception as e : self . logger . error ( f \"Failed to fetch drucksachen: { e } \" , exc_info = True ) def _fetch_proceedings ( self , api_client ) -> Iterator [ DIPDocument ]: \"\"\" Fetch proceedings (vorg\u00e4nge) - legislative processes. Args: api_client: deutschland API client instance. Yields: DIPDocument instances for proceedings. \"\"\" self . logger . info ( f \"Fetching proceedings for Wahlperiode { self . wahlperiode } \" ) vorgang_api = vorgnge_api . VorgngeApi ( api_client ) try : cursor = None fetched_count = 0 page = 1 while True : # Fetch page of proceedings cursor_param = cursor if cursor else \"\" response = vorgang_api . get_vorgang_list ( f_wahlperiode = self . wahlperiode , cursor = cursor_param , format = \"json\" , ) for vorgang in response . documents : # Convert to dict for serialization content_dict = vorgang . to_dict () yield DIPDocument ( source_type = \"proceeding\" , content = content_dict ) fetched_count += 1 # Log progress self . logger . info ( f \"Proceedings: page { page } complete, \" f \" { fetched_count } total so far\" ) # Check pagination new_cursor = getattr ( response , \"cursor\" , None ) if not new_cursor or new_cursor == cursor : break cursor = new_cursor page += 1 self . logger . info ( f \"Completed fetching proceedings. \" f \"Total: { fetched_count } \" ) except Exception as e : self . logger . error ( f \"Failed to fetch proceedings: { e } \" , exc_info = True )","title":"DIPClient"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client_dip.DIPClient.__init__","text":"Initialize DIP client. Parameters: api_key ( Optional [ str ] , default: None ) \u2013 DIP API key. If not provided, uses public test key. wahlperiode ( int , default: 21 ) \u2013 Electoral period number (default: 20). fetch_sources ( Optional [ List [ str ]] , default: None ) \u2013 List of data sources to fetch. Options: \"protocols\", \"drucksachen\", \"proceedings\" logger ( Optional [ Logger ] , default: None ) \u2013 Logger instance for logging. Source code in src/extraction/datasources/bundestag/client_dip.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __init__ ( self , api_key : Optional [ str ] = None , wahlperiode : int = 21 , fetch_sources : Optional [ List [ str ]] = None , logger : Optional [ logging . Logger ] = None , ): \"\"\" Initialize DIP client. Args: api_key: DIP API key. If not provided, uses public test key. wahlperiode: Electoral period number (default: 20). fetch_sources: List of data sources to fetch. Options: \"protocols\", \"drucksachen\", \"proceedings\" logger: Logger instance for logging. \"\"\" self . api_key = api_key or self . DEFAULT_API_KEY self . wahlperiode = wahlperiode self . fetch_sources = fetch_sources or [ \"protocols\" , \"drucksachen\" ] self . logger = logger or logging . getLogger ( __name__ ) # Configure deutschland package self . configuration = dip_bundestag . Configuration ( host = \"https://search.dip.bundestag.de/api/v1\" ) # Use query parameter auth (more reliable) self . configuration . api_key [ \"ApiKeyQuery\" ] = self . api_key self . logger . info ( f \"Initialized DIP client for Wahlperiode { self . wahlperiode } , \" f \"sources: { self . fetch_sources } \" )","title":"__init__"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client_dip.DIPClient.fetch_all","text":"Fetch all enabled data sources. Yields: DIPDocument \u2013 DIPDocument instances wrapping different content types. Source code in src/extraction/datasources/bundestag/client_dip.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def fetch_all ( self ) -> Iterator [ DIPDocument ]: \"\"\" Fetch all enabled data sources. Yields: DIPDocument instances wrapping different content types. \"\"\" with dip_bundestag . ApiClient ( self . configuration ) as api_client : if \"protocols\" in self . fetch_sources : yield from self . _fetch_protocols ( api_client ) if \"drucksachen\" in self . fetch_sources : yield from self . _fetch_drucksachen ( api_client ) if \"proceedings\" in self . fetch_sources : yield from self . _fetch_proceedings ( api_client )","title":"fetch_all"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client_dip.DIPDocument","text":"Bases: BaseModel Unified model for all DIP data types. This wrapper provides a consistent interface for different document types from the DIP API (protocols, drucksachen, proceedings). Source code in src/extraction/datasources/bundestag/client_dip.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class DIPDocument ( BaseModel ): \"\"\" Unified model for all DIP data types. This wrapper provides a consistent interface for different document types from the DIP API (protocols, drucksachen, proceedings). \"\"\" model_config = ConfigDict ( arbitrary_types_allowed = True ) source_type : str # \"protocol\", \"proceeding\", \"drucksache\" content : Dict [ str , Any ] # Raw content from deutschland package @property def text ( self ) -> str : \"\"\"Extract text based on source type.\"\"\" if self . source_type in [ \"protocol\" , \"drucksache\" ]: return self . content . get ( \"text\" , \"\" ) elif self . source_type == \"proceeding\" : # For proceedings, use abstract as text return self . content . get ( \"abstract\" , \"\" ) return \"\"","title":"DIPDocument"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client_dip.DIPDocument.text","text":"Extract text based on source type.","title":"text"},{"location":"src/extraction/datasources/bundestag/#configuration","text":"","title":"Configuration"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.configuration.BundestagMineDatasourceConfiguration","text":"Bases: DatasourceConfiguration Configuration for the Bundestag datasource. Supports multiple data sources: - BundestagMine API: Individual speeches from parliamentary sessions - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) Source code in src/extraction/datasources/bundestag/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class BundestagMineDatasourceConfiguration ( DatasourceConfiguration ): \"\"\" Configuration for the Bundestag datasource. Supports multiple data sources: - BundestagMine API: Individual speeches from parliamentary sessions - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) \"\"\" name : Literal [ DatasourceName . BUNDESTAG ] = Field ( ... , description = \"Identifier specifying this configuration is for the Bundestag datasource\" , ) # BundestagMine settings include_bundestag_mine : bool = Field ( default = True , description = \"Fetch speeches from BundestagMine API (bundestag-mine.de)\" , ) # DIP API settings include_dip : bool = Field ( default = True , description = \"Fetch data from DIP (Dokumentations- und Informationssystem) API\" , ) dip_api_key : Optional [ str ] = Field ( default = None , description = \"API key for DIP API. If not provided, uses public test key.\" , ) dip_wahlperiode : int = Field ( default = 21 , description = \"Electoral period (Wahlperiode) for DIP data\" , ) dip_sources : List [ str ] = Field ( default_factory = lambda : [ \"protocols\" ], description = ( \"DIP data sources to fetch. \" \"Options: 'protocols' (plenary transcripts), \" \"'drucksachen' (printed materials), \" \"'proceedings' (legislative processes)\" ), )","title":"BundestagMineDatasourceConfiguration"},{"location":"src/extraction/datasources/bundestag/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.document.BundestagMineDocument","text":"Bases: BaseDocument Represents a document from the Bundestag datasource. Supports multiple data sources: - BundestagMine: Individual speeches with speaker information - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) Inherits from BaseDocument and includes additional metadata specific to Bundestag documents. Source code in src/extraction/datasources/bundestag/document.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BundestagMineDocument ( BaseDocument ): \"\"\" Represents a document from the Bundestag datasource. Supports multiple data sources: - BundestagMine: Individual speeches with speaker information - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) Inherits from BaseDocument and includes additional metadata specific to Bundestag documents. \"\"\" included_embed_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , \"speaker_party\" , \"speaker\" , \"protocol_number\" , \"legislature_period\" , \"document_type\" , \"document_number\" , \"document_subtype\" , \"agenda_item_number\" , \"source_client\" , \"publisher\" , \"document_art\" , \"document_id\" , \"parliamentary_composition\" , # NEW: Party/fraction composition metadata ] included_llm_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , \"speaker_party\" , \"speaker\" , \"protocol_number\" , \"legislature_period\" , \"document_type\" , \"document_number\" , \"document_subtype\" , \"agenda_item_number\" , \"source_client\" , \"publisher\" , \"document_art\" , \"document_id\" , \"distribution_date\" , \"xml_url\" , \"related_proceedings_count\" , \"parliamentary_composition\" , # NEW: Party/fraction composition metadata ]","title":"BundestagMineDocument"},{"location":"src/extraction/datasources/bundestag/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.manager.BundestagMineDatasourceManagerFactory","text":"Bases: Factory Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object Source code in src/extraction/datasources/bundestag/manager.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class BundestagMineDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class: Type of configuration object \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create an instance of the BundestagMine datasource manager. This method constructs a BasicDatasourceManager by creating the appropriate reader and parser based on the provided configuration. Args: configuration: Configuration specifying how to set up the BundestagMine datasource manager, reader, and parser. Returns: A configured BasicDatasourceManager instance for handling BundestagMine documents. \"\"\" reader = BundestagMineDatasourceReaderFactory . create ( configuration ) parser = BundestagMineDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration = configuration , reader = reader , parser = parser , )","title":"BundestagMineDatasourceManagerFactory"},{"location":"src/extraction/datasources/bundestag/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.parser.BundestagMineDatasourceParser","text":"Bases: BaseParser [ BundestagMineDocument ] Parser for Bundestag data from multiple sources. Handles parsing of: - BundestagMine speeches (individual speech data) - DIP documents (protocols, drucksachen, proceedings) Source code in src/extraction/datasources/bundestag/parser.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 class BundestagMineDatasourceParser ( BaseParser [ BundestagMineDocument ]): \"\"\"Parser for Bundestag data from multiple sources. Handles parsing of: - BundestagMine speeches (individual speech data) - DIP documents (protocols, drucksachen, proceedings) \"\"\" logger = LoggerConfiguration . get_logger ( __name__ ) def parse ( self , content : Union [ BundestagSpeech , DIPDocument ] ) -> BundestagMineDocument : \"\"\" Parse content into a BundestagMineDocument object. Args: content: Raw content to be parsed (BundestagSpeech or DIPDocument) Returns: Parsed document of type BundestagMineDocument \"\"\" if isinstance ( content , BundestagSpeech ): return self . _parse_bundestag_speech ( content ) elif isinstance ( content , DIPDocument ): return self . _parse_dip_document ( content ) else : raise ValueError ( f \"Unsupported content type: { type ( content ) } \" ) def _parse_bundestag_speech ( self , speech : BundestagSpeech ) -> BundestagMineDocument : \"\"\"Parse a BundestagMine speech into a document. Args: speech: BundestagSpeech object Returns: BundestagMineDocument with speech data \"\"\" metadata = self . _extract_metadata_from_speech ( speech ) return BundestagMineDocument ( text = speech . text , metadata = metadata ) def _parse_dip_document ( self , dip_doc : DIPDocument ) -> BundestagMineDocument : \"\"\"Parse a DIP document into a BundestagMineDocument. Args: dip_doc: DIPDocument object Returns: BundestagMineDocument with DIP data \"\"\" metadata = self . _extract_metadata_from_dip ( dip_doc ) # Filter protocol text to remove non-informative sections text = dip_doc . text if dip_doc . source_type == \"protocol\" : text = self . _filter_protocol_text ( text ) return BundestagMineDocument ( text = text , metadata = metadata ) def _filter_protocol_text ( self , text : str ) -> str : \"\"\"Filter protocol text to remove non-informative sections. Removes: - Anlage sections (attachments with attendance lists, voting records) - Content after \"Anlagen zum Stenografischen Bericht\" marker - Name list sections (lines with >80% proper nouns, no verbs) Args: text: Raw protocol text Returns: Filtered text with only substantive content \"\"\" lines = text . split ( \" \\n \" ) filtered_lines = [] in_anlage_section = False in_name_list = False removed_lines = 0 name_list_start = - 1 for i , line in enumerate ( lines ): stripped = line . strip () # Check for start of Anlagen section (usually near the end) if stripped . startswith ( \"Anlagen zum Stenografischen Bericht\" ): self . logger . debug ( f \"Found 'Anlagen zum Stenografischen Bericht' at line { i } , \" f \"removing remaining { len ( lines ) - i } lines\" ) removed_lines += len ( lines ) - i break # Check for individual Anlage markers followed by minimal content if stripped . startswith ( \"Anlage\" ): # Look ahead to see if this is a low-content section next_lines = lines [ i + 1 : min ( i + 20 , len ( lines ))] non_empty_next = [ l . strip () for l in next_lines if l . strip ()] # If Anlage is followed by very few words or just names/numbers, # it's likely an attachment section if len ( non_empty_next ) <= 3 : in_anlage_section = True self . logger . debug ( f \"Found standalone Anlage at line { i } : { stripped [: 50 ] } \" ) removed_lines += 1 continue # Check if followed by attendance/voting list markers elif any ( marker in \" \" . join ( non_empty_next [: 5 ]) for marker in [ \"Entschuldigte Abgeordnete\" , \"Namensverzeichnis\" , \"Ergebnis und Namensverzeichnis\" , ] ): in_anlage_section = True self . logger . debug ( f \"Found Anlage with attendance/voting list at line { i } \" ) removed_lines += 1 continue # Exit Anlage section when we hit substantive content if in_anlage_section : # Check for speaker pattern (name followed by colon) if ( \":\" in line and len ( stripped ) > 10 and stripped . endswith ( \":\" ) ): in_anlage_section = False self . logger . debug ( f \"Exiting Anlage section at line { i } \" ) # Check for long paragraph (likely substantive content) elif len ( stripped ) > 100 : in_anlage_section = False self . logger . debug ( f \"Exiting Anlage section at line { i } \" ) else : removed_lines += 1 continue # NEW: Detect name list sections (lines with mostly proper nouns, no verbs) if not in_name_list and self . _is_name_list_line ( stripped ): # Look ahead to see if this is start of a name list section next_lines = lines [ i + 1 : min ( i + 10 , len ( lines ))] name_count = sum ( 1 for l in next_lines if l . strip () and self . _is_name_list_line ( l . strip ()) ) # If 5+ consecutive name-like lines, it's a name list section if name_count >= 5 : in_name_list = True name_list_start = i removed_lines += 1 self . logger . debug ( f \"Entering name list section at line { i } \" ) continue # Exit name list when we hit substantive content if in_name_list : # Check for speaker pattern or long substantive text if ( ( \":\" in line and stripped . endswith ( \":\" )) or len ( stripped ) > 150 or self . _has_verbs ( stripped ) ): in_name_list = False list_length = i - name_list_start self . logger . debug ( f \"Exited name list section at line { i } (removed { list_length } lines)\" ) else : removed_lines += 1 continue filtered_lines . append ( line ) filtered_text = \" \\n \" . join ( filtered_lines ) if removed_lines > 0 : self . logger . info ( f \"Filtered protocol: removed { removed_lines } lines \" f \"( { len ( text ) - len ( filtered_text ) } chars) of non-substantive content\" ) return filtered_text def _is_name_list_line ( self , line : str ) -> bool : \"\"\"Check if a line looks like a name list entry. Name list characteristics: - Short line (< 80 chars) - 2-4 words - All words start with capital letter - No verbs or sentence structure Args: line: Line to check Returns: True if line looks like a name entry \"\"\" if not line or len ( line ) > 80 : return False words = line . split () if not ( 2 <= len ( words ) <= 5 ): return False # Check if all words are capitalized (typical for names) # Allow for common German particles: von, van, de, der particles = { \"von\" , \"van\" , \"de\" , \"der\" , \"den\" , \"zu\" } capitalized_count = sum ( 1 for w in words if w [ 0 ] . isupper () or w . lower () in particles ) return ( capitalized_count >= len ( words ) - 1 ) # Allow one non-capitalized word def _has_verbs ( self , text : str ) -> bool : \"\"\"Check if text contains common German verbs (indicates substantive content). Args: text: Text to check Returns: True if text contains verbs \"\"\" # Common German verbs and verb patterns verb_indicators = [ \" ist \" , \" sind \" , \" war \" , \" waren \" , \" hat \" , \" haben \" , \" hatte \" , \" wird \" , \" werden \" , \" wurde \" , \" wurden \" , \" kann \" , \" k\u00f6nnen \" , \" soll \" , \" muss \" , \" m\u00f6chte \" , \" sage \" , \" sagen \" , \" glaube \" , \" denke \" , \" meine \" , \" macht \" , \" machen \" , \" gibt \" , \" geht \" , ] text_lower = f \" { text . lower () } \" return any ( verb in text_lower for verb in verb_indicators ) def _extract_metadata_from_speech ( self , speech : BundestagSpeech ) -> dict : \"\"\" Extract metadata from a BundestagMine speech. Args: speech: BundestagSpeech object Returns: Dictionary containing extracted metadata \"\"\" legislature_period = speech . protocol . legislaturePeriod protocol_number = speech . protocol . number agenda_item_number = speech . agendaItem . agendaItemNumber url = f \"https://dserver.bundestag.de/btp/ { legislature_period } / { legislature_period }{ protocol_number } .pdf\" title = f \"Protocol/Legislature/AgendaItem { protocol_number } / { legislature_period } / { agenda_item_number } \" speaker_name = f \" { speech . speaker . firstName } { speech . speaker . lastName } \" metadata = { \"datasource\" : \"bundestag\" , \"source_client\" : \"bundestag_mine\" , \"language\" : \"de\" , \"url\" : url , \"title\" : title , \"format\" : \"md\" , \"created_time\" : speech . protocol . date , \"last_edited_time\" : speech . protocol . date , \"speaker_party\" : speech . speaker . party , \"speaker\" : speaker_name , \"agenda_item_number\" : agenda_item_number , \"protocol_number\" : protocol_number , \"document_number\" : f \" { legislature_period } / { protocol_number } \" , # Standardized identifier \"legislature_period\" : legislature_period , \"document_type\" : \"speech\" , } # Extract party composition from speaker metadata if speech . speaker and speech . speaker . party : parliamentary_composition = ( PartyExtractor . extract_from_speaker_party ( speech . speaker . party ) ) metadata [ \"parliamentary_composition\" ] = parliamentary_composition return metadata def _extract_metadata_from_dip ( self , dip_doc : DIPDocument ) -> dict : \"\"\" Extract metadata from a DIP document. Args: dip_doc: DIPDocument object Returns: Dictionary containing extracted metadata \"\"\" content = dip_doc . content source_type = dip_doc . source_type # Extract common fields present in all DIP documents metadata = { \"datasource\" : \"bundestag\" , \"source_client\" : \"dip\" , \"language\" : \"de\" , \"format\" : \"md\" , \"document_type\" : source_type , } # Extract common metadata across all document types fundstelle = content . get ( \"fundstelle\" , {}) # Add document ID if available if \"id\" in content : metadata [ \"document_id\" ] = str ( content [ \"id\" ]) # Add publisher (herausgeber) if available if \"herausgeber\" in content : metadata [ \"publisher\" ] = str ( content [ \"herausgeber\" ]) # Add document art (dokumentart) if available if \"dokumentart\" in content : metadata [ \"document_art\" ] = content [ \"dokumentart\" ] # Extract type-specific metadata if source_type == \"protocol\" : # Plenary protocol metadata dokumentnummer = content . get ( \"dokumentnummer\" , \"unknown\" ) wahlperiode = content . get ( \"wahlperiode\" , \"\" ) titel = content . get ( \"titel\" , f \"Plenary Protocol { dokumentnummer } \" ) metadata . update ( { \"title\" : titel , \"protocol_number\" : dokumentnummer , \"document_number\" : dokumentnummer , # Standardized identifier (same as protocol_number for protocols) \"legislature_period\" : wahlperiode , \"url\" : fundstelle . get ( \"pdf_url\" , \"\" ), \"created_time\" : content . get ( \"datum\" , \"\" ), \"last_edited_time\" : content . get ( \"aktualisiert\" , content . get ( \"datum\" , \"\" ) ), } ) # Add fundstelle reference metadata if \"verteildatum\" in fundstelle : metadata [ \"distribution_date\" ] = fundstelle [ \"verteildatum\" ] if \"xml_url\" in fundstelle : metadata [ \"xml_url\" ] = fundstelle [ \"xml_url\" ] # Add vorgangsbezug (proceedings references) count if \"vorgangsbezug_anzahl\" in content : metadata [ \"related_proceedings_count\" ] = content [ \"vorgangsbezug_anzahl\" ] # Extract parliamentary composition from protocol text text = content . get ( \"text\" , \"\" ) parliamentary_composition = ( PartyExtractor . extract_from_protocol_text ( text ) ) metadata [ \"parliamentary_composition\" ] = parliamentary_composition # Log extraction results num_fractions = len ( parliamentary_composition . get ( \"fractions\" , [])) confidence = parliamentary_composition . get ( \"confidence\" , \"unknown\" ) self . logger . info ( f \"Extracted { num_fractions } fractions from protocol { dokumentnummer } \" f \"(confidence: { confidence } )\" ) elif source_type == \"drucksache\" : # Printed material metadata dokumentnummer = content . get ( \"dokumentnummer\" , \"unknown\" ) wahlperiode = content . get ( \"wahlperiode\" , \"\" ) drucksachetyp = content . get ( \"drucksachetyp\" , \"\" ) metadata . update ( { \"title\" : f \"Drucksache { dokumentnummer } \" , \"document_number\" : dokumentnummer , \"document_subtype\" : drucksachetyp , \"legislature_period\" : wahlperiode , \"url\" : fundstelle . get ( \"pdf_url\" , \"\" ), \"created_time\" : content . get ( \"datum\" , \"\" ), \"last_edited_time\" : content . get ( \"aktualisiert\" , content . get ( \"datum\" , \"\" ) ), } ) # Add fundstelle reference metadata if \"verteildatum\" in fundstelle : metadata [ \"distribution_date\" ] = fundstelle [ \"verteildatum\" ] if \"xml_url\" in fundstelle : metadata [ \"xml_url\" ] = fundstelle [ \"xml_url\" ] elif source_type == \"proceeding\" : # Legislative proceeding metadata titel = content . get ( \"titel\" , \"\" ) vorgangsnummer = content . get ( \"vorgangsnummer\" , \"unknown\" ) wahlperiode = content . get ( \"wahlperiode\" , \"\" ) metadata . update ( { \"title\" : titel or f \"Proceeding { vorgangsnummer } \" , \"document_number\" : vorgangsnummer , \"legislature_period\" : wahlperiode , \"url\" : fundstelle . get ( \"url\" , \"\" ), \"created_time\" : content . get ( \"datum\" , \"\" ), \"last_edited_time\" : content . get ( \"aktualisiert\" , content . get ( \"datum\" , \"\" ) ), } ) return metadata","title":"BundestagMineDatasourceParser"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.parser.BundestagMineDatasourceParser.parse","text":"Parse content into a BundestagMineDocument object. Parameters: content ( Union [ BundestagSpeech , DIPDocument ] ) \u2013 Raw content to be parsed (BundestagSpeech or DIPDocument) Returns: BundestagMineDocument \u2013 Parsed document of type BundestagMineDocument Source code in src/extraction/datasources/bundestag/parser.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def parse ( self , content : Union [ BundestagSpeech , DIPDocument ] ) -> BundestagMineDocument : \"\"\" Parse content into a BundestagMineDocument object. Args: content: Raw content to be parsed (BundestagSpeech or DIPDocument) Returns: Parsed document of type BundestagMineDocument \"\"\" if isinstance ( content , BundestagSpeech ): return self . _parse_bundestag_speech ( content ) elif isinstance ( content , DIPDocument ): return self . _parse_dip_document ( content ) else : raise ValueError ( f \"Unsupported content type: { type ( content ) } \" )","title":"parse"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.parser.BundestagMineDatasourceParserFactory","text":"Bases: Factory Factory for creating instances of BundestagMineDatasourceParser. Creates and configures BundestagMineDatasourceParser objects according to the provided configuration. Source code in src/extraction/datasources/bundestag/parser.py 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 class BundestagMineDatasourceParserFactory ( Factory ): \"\"\" Factory for creating instances of BundestagMineDatasourceParser. Creates and configures BundestagMineDatasourceParser objects according to the provided configuration. \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineDatasourceParser : \"\"\" Create an instance of BundestagMineDatasourceParser. Args: configuration: Configuration for the parser (not used in this implementation) Returns: An instance of BundestagMineDatasourceParser \"\"\" return BundestagMineDatasourceParser ()","title":"BundestagMineDatasourceParserFactory"},{"location":"src/extraction/datasources/bundestag/#party_extractor","text":"Extract parliamentary composition metadata from Bundestag documents. This module uses DYNAMIC extraction without hardcoded party names, making it future-proof for new parties and changing compositions.","title":"Party_extractor"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.party_extractor.PartyExtractor","text":"Extracts party/fraction composition dynamically from protocol text. Design principle: Extract ALL party mentions using pattern matching and heuristics, without hardcoding specific party names. Source code in src/extraction/datasources/bundestag/party_extractor.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 class PartyExtractor : \"\"\"Extracts party/fraction composition dynamically from protocol text. Design principle: Extract ALL party mentions using pattern matching and heuristics, without hardcoding specific party names. \"\"\" logger = LoggerConfiguration . get_logger ( __name__ ) # Non-party keywords to filter out (roles, locations, organizations, etc.) NON_PARTY_KEYWORDS = { # Governmental roles \"Bundeskanzler\" , \"Bundeskanzlerin\" , \"Bundesminister\" , \"Bundesministerin\" , \"Bundespr\u00e4sident\" , \"Bundespr\u00e4sidentin\" , \"Pr\u00e4sident\" , \"Pr\u00e4sidentin\" , \"Staatsminister\" , \"Staatsministerin\" , \"Staatssekret\u00e4r\" , \"Staatssekret\u00e4rin\" , # Locations \"Berlin\" , \"Bonn\" , # Status \"parteilos\" , \"fraktionslos\" , \"Gast\" , # Guest speakers # Common organizational abbreviations (NOT parties) \"EU\" , \"UN\" , \"NATO\" , \"OSZE\" , \"WHO\" , \"IWF\" , \"EZB\" , \"BMWE\" , \"BMI\" , \"BMF\" , \"BMAS\" , \"BMZ\" , \"BMVI\" , \"BMVg\" , # German ministries \"BT\" , \"BR\" , # Bundestag/Bundesrat abbreviations \"USA\" , \"UK\" , \"FR\" , # Countries # Procedural terms \"TOP\" , \"ZP\" , # Tagesordnungspunkt, Zusatzpunkt } @classmethod def extract_from_protocol_text ( cls , text : str ) -> Dict : \"\"\"Extract party composition from DIP protocol text dynamically. Uses pattern matching and heuristics to identify parties WITHOUT hardcoded party names. Args: text: Full protocol text from DIP API Returns: Parliamentary composition metadata dictionary \"\"\" if not text : return cls . _empty_result () # Pattern: Matches \"Name (PARTY)\" speaker attributions # More flexible pattern that matches various name formats: # - \"Hans M\u00fcller (CDU/CSU)\" # - \"Dr. Maria Schmidt (SPD)\" # - \"Speaker1 (CDU)\" (for tests) # Captures name and content in parentheses pattern = r \"(\\b[A-Z\u00c4\u00d6\u00dca-z\u00e4\u00f6\u00fc\u00df][A-Z\u00c4\u00d6\u00dca-z\u00e4\u00f6\u00fc\u00df0-9\\.\\s]*?)\\s+\\(([^)]+)\\)\" matches = re . findall ( pattern , text ) # Extract candidates: text in parentheses after names candidates = [ match [ 1 ] . strip () for match in matches ] # Filter to likely parties using heuristics party_candidates = [] for candidate in candidates : if cls . _is_likely_party ( candidate ): party_candidates . append ( candidate ) if not party_candidates : cls . logger . debug ( \"No party candidates found in protocol text\" ) return cls . _empty_result () # Count occurrences for confidence scoring party_counts = Counter ( party_candidates ) cls . logger . debug ( f \"Found { len ( party_counts ) } unique party variations \" f \"with { sum ( party_counts . values ()) } total mentions\" ) # CRITICAL FILTER: Remove noise by requiring minimum mentions # Real parties appear throughout the protocol (many mentions) # Noise abbreviations (government agencies, technical terms) appear rarely (1 mention) # Threshold: At least 2 mentions to be considered a party # This filters out single-occurrence noise while catching real parties MIN_MENTIONS = 2 filtered_party_counts = Counter ( { name : count for name , count in party_counts . items () if count >= MIN_MENTIONS } ) if not filtered_party_counts : cls . logger . debug ( f \"After filtering for min { MIN_MENTIONS } mentions, no parties remain\" ) return cls . _empty_result () cls . logger . debug ( f \"After filtering for min { MIN_MENTIONS } mentions: \" f \" { len ( filtered_party_counts ) } candidates remain\" ) # Group related party names (e.g., CDU, CSU, CDU/CSU \u2192 CDU/CSU) party_groups = cls . _group_related_parties ( filtered_party_counts ) cls . logger . info ( f \"Grouped into { len ( party_groups ) } distinct fractions\" ) # Build fractions list fractions = [] for primary_name , related_names in party_groups . items (): total_mentions = sum ( filtered_party_counts [ name ] for name in related_names ) fractions . append ( { \"name\" : primary_name , \"variations\" : sorted ( list ( related_names )), \"type\" : \"fraction\" , \"mention_count\" : total_mentions , } ) # Sort by mention count (most mentioned first) fractions . sort ( key = lambda f : f [ \"mention_count\" ], reverse = True ) confidence = cls . _calculate_confidence ( fractions , filtered_party_counts ) cls . logger . info ( f \"Extracted { len ( fractions ) } fractions with { confidence } confidence: \" f \" { ', ' . join ( f [ 'name' ] for f in fractions ) } \" ) return { \"fractions\" : fractions , \"extraction_source\" : \"protocol_text\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : confidence , } @classmethod def extract_from_speaker_party ( cls , party : str ) -> Dict : \"\"\"Extract party metadata from a single speech's speaker.party field. Stores raw party name without normalization. Args: party: Party abbreviation from speaker metadata Returns: Single-party composition metadata \"\"\" if not party : return cls . _empty_result () # Store raw party name - NO hardcoded normalization fractions = [ { \"name\" : party , \"variations\" : [ party ], \"type\" : \"fraction\" , \"mention_count\" : 1 , } ] return { \"fractions\" : fractions , \"extraction_source\" : \"speaker_metadata\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : \"low\" , # Single speech doesn't show full composition } @classmethod def _is_likely_party ( cls , text : str ) -> bool : \"\"\"Determine if text is likely a party/fraction name using heuristics. CONSERVATIVE approach: Only match text that strongly resembles party names. Heuristics (NO hardcoded party names): 1. Length between 2-20 characters (parties are concise) 2. Not a common non-party phrase (roles, locations, organizations) 3. Matches specific party-name patterns: a) 2-4 char ALL-CAPS abbreviations (SPD, AfD, CDU, CSU, FDP) b) Compound party names with \"/\" (CDU/CSU, B\u00dcNDNIS 90/DIE GR\u00dcNEN) c) Party names starting with \"Die \" followed by capitalized word d) Party names starting with \"B\u00fcndnis\" or \"Bund\" Args: text: Candidate text from parentheses Returns: True if text is likely a party name \"\"\" text_clean = text . strip () # Must have content if not text_clean or len ( text_clean ) < 2 : return False # STRICTER length check (parties are typically 2-25 chars) # Allow up to 25 to accommodate \"B\u00dcNDNIS 90/DIE GR\u00dcNEN\" (23 chars) if len ( text_clean ) > 25 : return False # Exclude known non-party phrases FIRST if any ( keyword in text_clean for keyword in cls . NON_PARTY_KEYWORDS ): return False # Must contain at least one uppercase letter if not any ( c . isupper () for c in text_clean ): return False # Calculate character composition uppercase_count = sum ( 1 for c in text_clean if c . isupper ()) alpha_count = sum ( 1 for c in text_clean if c . isalpha ()) if alpha_count == 0 : return False uppercase_ratio = uppercase_count / alpha_count # PATTERN 1: Short abbreviations (2-6 characters) # Examples: SPD, AfD, CDU, CSU, FDP, BSW, GR\u00dcNE, LINKE if 2 <= len ( text_clean ) <= 6 : # Must be all letters (no numbers) and at least 2 uppercase letters # This allows \"AfD\" (2 uppercase out of 3) but rejects \"EU\", \"UK\" (too international) if alpha_count == len ( text_clean ) and uppercase_count >= 2 : return True return ( False # Reject short strings with numbers or too few uppercase ) # PATTERN 2: Compound names with slash \"/\" # Examples: \"CDU/CSU\", \"B\u00dcNDNIS 90/DIE GR\u00dcNEN\" if \"/\" in text_clean : # Must have at least 50% uppercase if uppercase_ratio >= 0.5 : # Additional check: both sides of \"/\" should have letters parts = text_clean . split ( \"/\" ) if len ( parts ) == 2 and all ( any ( c . isalpha () for c in p ) for p in parts ): return True return False # PATTERN 3: Names starting with \"Die \" or \"DIE \" # Examples: \"Die Linke\", \"DIE LINKE\" if text_clean . startswith ( \"Die \" ) or text_clean . startswith ( \"DIE \" ): # After \"Die \", should have at least one more capitalized word remaining = text_clean [ 4 :] . strip () if remaining and remaining [ 0 ] . isupper (): return True return False # PATTERN 4: Names starting with \"B\u00fcndnis\" or \"Bund\" # Examples: \"B\u00dcNDNIS 90\", \"B\u00fcndnis\" if ( text_clean . startswith ( \"B\u00fcndnis\" ) or text_clean . startswith ( \"B\u00dcNDNIS\" ) or text_clean . startswith ( \"Bund\" ) ): return True # All other patterns rejected return False @classmethod def _group_related_parties ( cls , party_counts : Counter ) -> Dict [ str , Set [ str ]]: \"\"\"Group related party name variations dynamically. Groups variations like: - \"CDU\", \"CSU\", \"CDU/CSU\" \u2192 \"CDU/CSU\" (longest/compound) - \"GR\u00dcNE\", \"DIE GR\u00dcNEN\", \"B\u00dcNDNIS 90/DIE GR\u00dcNEN\" \u2192 longest variant - \"Die Linke\", \"DIE LINKE\" \u2192 most common variant Strategy: Use Union-Find algorithm to merge related parties. Args: party_counts: Counter of party name occurrences Returns: Dict mapping canonical name to set of related names \"\"\" parties = list ( party_counts . keys ()) # Build Union-Find structure # parent[party] = canonical representative of its group parent = { p : p for p in parties } def find ( party ): \"\"\"Find root of party's group.\"\"\" if parent [ party ] != party : parent [ party ] = find ( parent [ party ]) # Path compression return parent [ party ] def union ( party1 , party2 ): \"\"\"Merge groups of party1 and party2.\"\"\" root1 = find ( party1 ) root2 = find ( party2 ) if root1 != root2 : # Merge: prefer party with slash, then longer, then more frequent if ( 1 if \"/\" in root1 else 0 , len ( root1 ), party_counts [ root1 ], ) >= ( 1 if \"/\" in root2 else 0 , len ( root2 ), party_counts [ root2 ], ): parent [ root2 ] = root1 else : parent [ root1 ] = root2 # Find all related pairs and union them for i , party1 in enumerate ( parties ): party1_upper = party1 . upper () for party2 in parties [ i + 1 :]: party2_upper = party2 . upper () # Check if related is_substring = ( party1_upper in party2_upper or party2_upper in party1_upper ) are_related = cls . _are_related_parties ( party1 , party2 ) if is_substring or are_related : union ( party1 , party2 ) # Build groups from Union-Find structure groups : Dict [ str , Set [ str ]] = {} for party in parties : root = find ( party ) if root not in groups : groups [ root ] = set () groups [ root ] . add ( party ) return groups @classmethod def _are_related_parties ( cls , party1 : str , party2 : str ) -> bool : \"\"\"Check if two party names are related variations. Uses word overlap heuristic WITHOUT hardcoded party knowledge. Args: party1: First party name party2: Second party name Returns: True if parties are related \"\"\" p1_upper = party1 . upper () p2_upper = party2 . upper () # Extract significant words (length >= 3, all caps) words1 = { w for w in re . findall ( r \"\\b[A-Z\u00c4\u00d6\u00dc]{3,}\\b\" , p1_upper )} words2 = { w for w in re . findall ( r \"\\b[A-Z\u00c4\u00d6\u00dc]{3,}\\b\" , p2_upper )} # Check for shared significant words # BUT: \"DIE\" and \"LINKE\" are both 3+ chars, so we need to be more careful # Only consider them related if they share a MEANINGFUL word shared_words = words1 & words2 if shared_words : # Exclude common articles/connectors: DIE, DER, DAS, UND, VON meaningful_shared = shared_words - { \"DIE\" , \"DER\" , \"DAS\" , \"UND\" , \"VON\" , } if meaningful_shared : return True return False @classmethod def _calculate_confidence ( cls , fractions : List [ Dict ], party_counts : Counter ) -> str : \"\"\"Calculate confidence level based on extraction results. Confidence based on: - Number of fractions (typical Bundestag has 4-7 fractions) - Total mention count (more mentions = higher confidence) Args: fractions: List of extracted fractions party_counts: Counter of raw party mentions Returns: \"high\", \"medium\", or \"low\" \"\"\" num_fractions = len ( fractions ) total_mentions = sum ( party_counts . values ()) # Typical Bundestag has 4-7 fractions # High confidence: 4+ distinct fractions (typical composition) # OR: 2+ fractions with many mentions (confirms representative sample) if num_fractions >= 4 or ( num_fractions >= 2 and total_mentions >= 20 ): return \"high\" elif num_fractions >= 2 or total_mentions >= 10 : return \"medium\" else : return \"low\" @classmethod def _empty_result ( cls ) -> Dict : \"\"\"Return empty composition metadata for documents with no parties.\"\"\" return { \"fractions\" : [], \"extraction_source\" : \"none\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : \"low\" , }","title":"PartyExtractor"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.party_extractor.PartyExtractor.extract_from_protocol_text","text":"Extract party composition from DIP protocol text dynamically. Uses pattern matching and heuristics to identify parties WITHOUT hardcoded party names. Parameters: text ( str ) \u2013 Full protocol text from DIP API Returns: Dict \u2013 Parliamentary composition metadata dictionary Source code in src/extraction/datasources/bundestag/party_extractor.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @classmethod def extract_from_protocol_text ( cls , text : str ) -> Dict : \"\"\"Extract party composition from DIP protocol text dynamically. Uses pattern matching and heuristics to identify parties WITHOUT hardcoded party names. Args: text: Full protocol text from DIP API Returns: Parliamentary composition metadata dictionary \"\"\" if not text : return cls . _empty_result () # Pattern: Matches \"Name (PARTY)\" speaker attributions # More flexible pattern that matches various name formats: # - \"Hans M\u00fcller (CDU/CSU)\" # - \"Dr. Maria Schmidt (SPD)\" # - \"Speaker1 (CDU)\" (for tests) # Captures name and content in parentheses pattern = r \"(\\b[A-Z\u00c4\u00d6\u00dca-z\u00e4\u00f6\u00fc\u00df][A-Z\u00c4\u00d6\u00dca-z\u00e4\u00f6\u00fc\u00df0-9\\.\\s]*?)\\s+\\(([^)]+)\\)\" matches = re . findall ( pattern , text ) # Extract candidates: text in parentheses after names candidates = [ match [ 1 ] . strip () for match in matches ] # Filter to likely parties using heuristics party_candidates = [] for candidate in candidates : if cls . _is_likely_party ( candidate ): party_candidates . append ( candidate ) if not party_candidates : cls . logger . debug ( \"No party candidates found in protocol text\" ) return cls . _empty_result () # Count occurrences for confidence scoring party_counts = Counter ( party_candidates ) cls . logger . debug ( f \"Found { len ( party_counts ) } unique party variations \" f \"with { sum ( party_counts . values ()) } total mentions\" ) # CRITICAL FILTER: Remove noise by requiring minimum mentions # Real parties appear throughout the protocol (many mentions) # Noise abbreviations (government agencies, technical terms) appear rarely (1 mention) # Threshold: At least 2 mentions to be considered a party # This filters out single-occurrence noise while catching real parties MIN_MENTIONS = 2 filtered_party_counts = Counter ( { name : count for name , count in party_counts . items () if count >= MIN_MENTIONS } ) if not filtered_party_counts : cls . logger . debug ( f \"After filtering for min { MIN_MENTIONS } mentions, no parties remain\" ) return cls . _empty_result () cls . logger . debug ( f \"After filtering for min { MIN_MENTIONS } mentions: \" f \" { len ( filtered_party_counts ) } candidates remain\" ) # Group related party names (e.g., CDU, CSU, CDU/CSU \u2192 CDU/CSU) party_groups = cls . _group_related_parties ( filtered_party_counts ) cls . logger . info ( f \"Grouped into { len ( party_groups ) } distinct fractions\" ) # Build fractions list fractions = [] for primary_name , related_names in party_groups . items (): total_mentions = sum ( filtered_party_counts [ name ] for name in related_names ) fractions . append ( { \"name\" : primary_name , \"variations\" : sorted ( list ( related_names )), \"type\" : \"fraction\" , \"mention_count\" : total_mentions , } ) # Sort by mention count (most mentioned first) fractions . sort ( key = lambda f : f [ \"mention_count\" ], reverse = True ) confidence = cls . _calculate_confidence ( fractions , filtered_party_counts ) cls . logger . info ( f \"Extracted { len ( fractions ) } fractions with { confidence } confidence: \" f \" { ', ' . join ( f [ 'name' ] for f in fractions ) } \" ) return { \"fractions\" : fractions , \"extraction_source\" : \"protocol_text\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : confidence , }","title":"extract_from_protocol_text"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.party_extractor.PartyExtractor.extract_from_speaker_party","text":"Extract party metadata from a single speech's speaker.party field. Stores raw party name without normalization. Parameters: party ( str ) \u2013 Party abbreviation from speaker metadata Returns: Dict \u2013 Single-party composition metadata Source code in src/extraction/datasources/bundestag/party_extractor.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 @classmethod def extract_from_speaker_party ( cls , party : str ) -> Dict : \"\"\"Extract party metadata from a single speech's speaker.party field. Stores raw party name without normalization. Args: party: Party abbreviation from speaker metadata Returns: Single-party composition metadata \"\"\" if not party : return cls . _empty_result () # Store raw party name - NO hardcoded normalization fractions = [ { \"name\" : party , \"variations\" : [ party ], \"type\" : \"fraction\" , \"mention_count\" : 1 , } ] return { \"fractions\" : fractions , \"extraction_source\" : \"speaker_metadata\" , \"extracted_at\" : datetime . utcnow () . isoformat (), \"confidence\" : \"low\" , # Single speech doesn't show full composition }","title":"extract_from_speaker_party"},{"location":"src/extraction/datasources/bundestag/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.reader.BundestagMineDatasourceReader","text":"Bases: BaseReader Reader for extracting data from multiple Bundestag sources. Supports multiple data sources: - BundestagMine API: Individual speeches from parliamentary sessions - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) Source code in src/extraction/datasources/bundestag/reader.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class BundestagMineDatasourceReader ( BaseReader ): \"\"\"Reader for extracting data from multiple Bundestag sources. Supports multiple data sources: - BundestagMine API: Individual speeches from parliamentary sessions - DIP API: Comprehensive parliamentary documents (protocols, drucksachen, proceedings) \"\"\" def __init__ ( self , configuration : BundestagMineDatasourceConfiguration , client : Optional [ BundestagMineClient ] = None , dip_client : Optional [ DIPClient ] = None , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Bundestag reader with multiple data sources. Args: configuration: Settings for Bundestag data access and export limits client: Client for BundestagMine API interactions (optional) dip_client: Client for DIP API interactions (optional) logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . configuration = configuration self . export_limit = configuration . export_limit self . client = client self . dip_client = dip_client self . logger = logger async def read_all_async ( self , ) -> AsyncIterator [ dict ]: \"\"\"Asynchronously fetch all documents from enabled Bundestag sources. Yields documents from multiple sources based on configuration: - BundestagMine: Individual speeches - DIP: Comprehensive parliamentary documents Each enabled source gets the full export_limit, so if both sources are enabled with export_limit=2, you get 2 documents from each source (4 total). Returns: AsyncIterator[dict]: An async iterator of document dictionaries containing content and metadata such as text, speaker data, and last update information \"\"\" self . logger . info ( f \"Reading Bundestag documents with limit { self . export_limit } per source\" ) # Source 1: BundestagMine speeches if self . configuration . include_bundestag_mine and self . client : self . logger . info ( f \"Fetching speeches from BundestagMine API (limit: { self . export_limit } )...\" ) speech_iterator = self . client . fetch_all_speeches () mine_counter = 0 for speech in speech_iterator : if self . _limit_reached ( mine_counter , self . export_limit ): break self . logger . info ( f \"Fetched BundestagMine speech { mine_counter + 1 } / { self . export_limit } .\" ) mine_counter += 1 yield speech # Source 2: DIP API documents if self . configuration . include_dip and self . dip_client : self . logger . info ( f \"Fetching documents from DIP API (limit: { self . export_limit } )...\" ) dip_iterator = self . dip_client . fetch_all () dip_counter = 0 for dip_document in dip_iterator : if self . _limit_reached ( dip_counter , self . export_limit ): break self . logger . info ( f \"Fetched DIP document { dip_counter + 1 } / { self . export_limit } .\" ) dip_counter += 1 yield dip_document","title":"BundestagMineDatasourceReader"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.reader.BundestagMineDatasourceReader.__init__","text":"Initialize the Bundestag reader with multiple data sources. Parameters: configuration ( BundestagMineDatasourceConfiguration ) \u2013 Settings for Bundestag data access and export limits client ( Optional [ BundestagMineClient ] , default: None ) \u2013 Client for BundestagMine API interactions (optional) dip_client ( Optional [ DIPClient ] , default: None ) \u2013 Client for DIP API interactions (optional) logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operation information Source code in src/extraction/datasources/bundestag/reader.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , configuration : BundestagMineDatasourceConfiguration , client : Optional [ BundestagMineClient ] = None , dip_client : Optional [ DIPClient ] = None , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Bundestag reader with multiple data sources. Args: configuration: Settings for Bundestag data access and export limits client: Client for BundestagMine API interactions (optional) dip_client: Client for DIP API interactions (optional) logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . configuration = configuration self . export_limit = configuration . export_limit self . client = client self . dip_client = dip_client self . logger = logger","title":"__init__"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.reader.BundestagMineDatasourceReader.read_all_async","text":"Asynchronously fetch all documents from enabled Bundestag sources. Yields documents from multiple sources based on configuration: - BundestagMine: Individual speeches - DIP: Comprehensive parliamentary documents Each enabled source gets the full export_limit, so if both sources are enabled with export_limit=2, you get 2 documents from each source (4 total). Returns: AsyncIterator [ dict ] \u2013 AsyncIterator[dict]: An async iterator of document dictionaries containing AsyncIterator [ dict ] \u2013 content and metadata such as text, speaker data, and last update information Source code in src/extraction/datasources/bundestag/reader.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 async def read_all_async ( self , ) -> AsyncIterator [ dict ]: \"\"\"Asynchronously fetch all documents from enabled Bundestag sources. Yields documents from multiple sources based on configuration: - BundestagMine: Individual speeches - DIP: Comprehensive parliamentary documents Each enabled source gets the full export_limit, so if both sources are enabled with export_limit=2, you get 2 documents from each source (4 total). Returns: AsyncIterator[dict]: An async iterator of document dictionaries containing content and metadata such as text, speaker data, and last update information \"\"\" self . logger . info ( f \"Reading Bundestag documents with limit { self . export_limit } per source\" ) # Source 1: BundestagMine speeches if self . configuration . include_bundestag_mine and self . client : self . logger . info ( f \"Fetching speeches from BundestagMine API (limit: { self . export_limit } )...\" ) speech_iterator = self . client . fetch_all_speeches () mine_counter = 0 for speech in speech_iterator : if self . _limit_reached ( mine_counter , self . export_limit ): break self . logger . info ( f \"Fetched BundestagMine speech { mine_counter + 1 } / { self . export_limit } .\" ) mine_counter += 1 yield speech # Source 2: DIP API documents if self . configuration . include_dip and self . dip_client : self . logger . info ( f \"Fetching documents from DIP API (limit: { self . export_limit } )...\" ) dip_iterator = self . dip_client . fetch_all () dip_counter = 0 for dip_document in dip_iterator : if self . _limit_reached ( dip_counter , self . export_limit ): break self . logger . info ( f \"Fetched DIP document { dip_counter + 1 } / { self . export_limit } .\" ) dip_counter += 1 yield dip_document","title":"read_all_async"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.reader.BundestagMineDatasourceReaderFactory","text":"Bases: Factory Factory for creating Bundestag reader instances. Creates and configures BundestagMineDatasourceReader objects with appropriate clients based on the provided configuration. Supports multiple data sources including BundestagMine and DIP APIs. Source code in src/extraction/datasources/bundestag/reader.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 class BundestagMineDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating Bundestag reader instances. Creates and configures BundestagMineDatasourceReader objects with appropriate clients based on the provided configuration. Supports multiple data sources including BundestagMine and DIP APIs. \"\"\" _configuration_class = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineDatasourceReader : \"\"\"Creates a configured Bundestag reader instance. Initializes the appropriate clients (BundestagMine and/or DIP) based on configuration settings. Args: configuration: Bundestag connection and access settings Returns: BundestagMineDatasourceReader: Fully configured reader instance \"\"\" # Create BundestagMine client if enabled bundestag_mine_client = None if configuration . include_bundestag_mine : bundestag_mine_client = BundestagMineClientFactory . create ( configuration ) # Create DIP client if enabled dip_client = None if configuration . include_dip : dip_client = DIPClient ( api_key = configuration . dip_api_key , wahlperiode = configuration . dip_wahlperiode , fetch_sources = configuration . dip_sources , ) return BundestagMineDatasourceReader ( configuration = configuration , client = bundestag_mine_client , dip_client = dip_client , )","title":"BundestagMineDatasourceReaderFactory"},{"location":"src/extraction/datasources/confluence/","text":"Confluence Datasource This module contains functionality related to the Confluence datasource. Client ConfluenceClientFactory Bases: SingletonFactory Factory for creating and managing Confluence client instances. This factory ensures only one Confluence client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. Source code in src/extraction/datasources/confluence/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class ConfluenceClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Confluence client instances. This factory ensures only one Confluence client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. \"\"\" _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> Confluence : \"\"\" Creates a new Confluence client instance using the provided configuration. Args: configuration: Configuration object containing Confluence connection details including base URL, username, and password. Returns: A configured Confluence client instance ready for API interactions. \"\"\" return Confluence ( url = configuration . base_url , username = configuration . secrets . username . get_secret_value (), password = configuration . secrets . password . get_secret_value (), ) Configuration ConfluenceDatasourceConfiguration Bases: DatasourceConfiguration Source code in src/extraction/datasources/confluence/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ConfluenceDatasourceConfiguration ( DatasourceConfiguration ): class Secrets ( BaseSecrets ): model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__DATASOURCES__CONFLUENCE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username credential used to authenticate with the Confluence instance\" , ) password : SecretStr = Field ( ... , description = \"Password credential used to authenticate with the Confluence instance\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Confluence server instance\" , ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"Communication protocol used to connect to the Confluence server\" , ) name : Literal [ DatasourceName . CONFLUENCE ] = Field ( ... , description = \"Identifier specifying this configuration is for a Confluence datasource\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials required to access the Confluence instance\" , ) @property def base_url ( self ) -> str : \"\"\" Constructs the complete base URL for the Confluence API from the protocol and host. Returns: str: The fully formed base URL to the Confluence instance \"\"\" return f \" { self . protocol } :// { self . host } \" base_url property Constructs the complete base URL for the Confluence API from the protocol and host. Returns: str ( str ) \u2013 The fully formed base URL to the Confluence instance Document ConfluenceDocument Bases: BaseDocument Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. Source code in src/extraction/datasources/confluence/document.py 4 5 6 7 8 9 10 11 class ConfluenceDocument ( BaseDocument ): \"\"\"Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. \"\"\" pass Manager ConfluenceDatasourceManagerFactory Bases: Factory Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class ( Type ) \u2013 Configuration class used for validating and processing Confluence-specific settings. Source code in src/extraction/datasources/confluence/manager.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class ConfluenceDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class: Configuration class used for validating and processing Confluence-specific settings. \"\"\" _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create a configured Confluence datasource manager. Sets up the necessary reader and parser components based on the provided configuration and assembles them into a functional manager. Args: configuration: Configuration object containing Confluence-specific parameters including authentication details, spaces to extract, and other extraction options. Returns: A fully initialized datasource manager that can extract and process data from Confluence. \"\"\" reader = ConfluenceDatasourceReaderFactory . create ( configuration ) parser = ConfluenceDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration , reader , parser ) Parser ConfluenceDatasourceParser Bases: BaseParser [ ConfluenceDocument ] Source code in src/extraction/datasources/confluence/parser.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class ConfluenceDatasourceParser ( BaseParser [ ConfluenceDocument ]): def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , parser : MarkItDown = MarkItDown (), ): \"\"\"Initialize the Confluence parser with the provided configuration. Args: configuration: Configuration object containing Confluence connection details parser: MarkItDown instance for converting HTML to markdown \"\"\" self . configuration = configuration self . parser = parser def parse ( self , page : ConfluencePage ) -> ConfluenceDocument : \"\"\"Parse a Confluence page into a document. Args: page: Confluence page details Returns: ConfluenceDocument: Parsed document with extracted text and metadata \"\"\" markdown = self . _get_page_markdown ( page ) metadata = self . _extract_metadata ( page , self . configuration . base_url ) return ConfluenceDocument ( text = markdown , metadata = metadata ) def _get_page_markdown ( self , page : ConfluencePage ) -> str : \"\"\"Extract markdown content from a Confluence page. Because of MarkItDown, we need to write the HTML content to a temporary file and then convert it to markdown. Args: page: Confluence page details Returns: str: Markdown content of the page \"\"\" html_content = page . body . view . value if not html_content : return \"\" with tempfile . NamedTemporaryFile ( mode = \"w\" , suffix = \".html\" ) as temp_file : temp_file . write ( html_content ) temp_file . flush () return self . parser . convert ( temp_file . name , file_extension = \".html\" ) . text_content @staticmethod def _extract_metadata ( page : ConfluencePage , base_url : str ) -> dict : \"\"\"Extract and format page metadata. Args: page: Confluence page details base_url: Base URL of the Confluence instance Returns: dict: Structured metadata including dates, IDs, and URLs \"\"\" return { \"created_time\" : page . history . createdDate , \"created_date\" : page . history . createdDate . split ( \"T\" )[ 0 ], \"datasource\" : \"confluence\" , \"format\" : \"md\" , \"last_edited_date\" : page . history . lastUpdated . when , \"last_edited_time\" : page . history . lastUpdated . when . split ( \"T\" )[ 0 ], \"page_id\" : page . id , \"space\" : page . expandable [ \"space\" ] . split ( \"/\" )[ - 1 ], \"title\" : page . title , \"type\" : \"page\" , \"url\" : base_url + page . links . webui , } __init__ ( configuration , parser = MarkItDown ()) Initialize the Confluence parser with the provided configuration. Parameters: configuration ( ConfluenceDatasourceConfiguration ) \u2013 Configuration object containing Confluence connection details parser ( MarkItDown , default: MarkItDown () ) \u2013 MarkItDown instance for converting HTML to markdown Source code in src/extraction/datasources/confluence/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , parser : MarkItDown = MarkItDown (), ): \"\"\"Initialize the Confluence parser with the provided configuration. Args: configuration: Configuration object containing Confluence connection details parser: MarkItDown instance for converting HTML to markdown \"\"\" self . configuration = configuration self . parser = parser parse ( page ) Parse a Confluence page into a document. Parameters: page ( ConfluencePage ) \u2013 Confluence page details Returns: ConfluenceDocument ( ConfluenceDocument ) \u2013 Parsed document with extracted text and metadata Source code in src/extraction/datasources/confluence/parser.py 31 32 33 34 35 36 37 38 39 40 41 42 def parse ( self , page : ConfluencePage ) -> ConfluenceDocument : \"\"\"Parse a Confluence page into a document. Args: page: Confluence page details Returns: ConfluenceDocument: Parsed document with extracted text and metadata \"\"\" markdown = self . _get_page_markdown ( page ) metadata = self . _extract_metadata ( page , self . configuration . base_url ) return ConfluenceDocument ( text = markdown , metadata = metadata ) ConfluenceDatasourceParserFactory Bases: Factory Source code in src/extraction/datasources/confluence/parser.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class ConfluenceDatasourceParserFactory ( Factory ): _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceParser : \"\"\"Creates a Confluence parser instance. Args: configuration: Configuration object containing Confluence connection details Returns: ConfluenceDatasourceParser: Configured Confluence parser instance \"\"\" return ConfluenceDatasourceParser ( configuration ) Reader ConfluenceDatasourceReader Bases: BaseReader Reader for extracting documents from Confluence spaces. Implements document extraction from Confluence spaces, handling pagination and export limits. Source code in src/extraction/datasources/confluence/reader.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class ConfluenceDatasourceReader ( BaseReader ): \"\"\"Reader for extracting documents from Confluence spaces. Implements document extraction from Confluence spaces, handling pagination and export limits. \"\"\" def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , client : Confluence , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Confluence reader. Args: configuration: Settings for Confluence access and export limits client: Client for Confluence API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger async def read_all_async ( self , ) -> AsyncIterator [ ConfluencePage ]: \"\"\"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. \"\"\" self . logger . info ( f \"Reading pages from Confluence with limit { self . export_limit } \" ) response = self . client . get_all_spaces ( space_type = \"global\" ) spaces = [ Space . model_validate ( space ) for space in response [ \"results\" ]] yield_counter = 0 for space in spaces : for page in self . _get_all_pages ( space . key ): if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Confluence page { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield page def _get_all_pages ( self , space : str ) -> Iterator [ ConfluencePage ]: \"\"\"Fetch all pages from a specific Confluence space. Handles pagination internally to retrieve all pages from the specified space, up to the optional limit. Pages include body content and update history. Args: space: Space key to fetch pages from limit: Maximum number of pages to fetch (None for unlimited) Returns: Iterator[ConfluencePage]: Iterator of Confluence pages with content and metadata \"\"\" start = 0 params = { \"space\" : space , \"start\" : start , \"status\" : None , \"expand\" : \"body.view,history.lastUpdated\" , } try : while True : pages_raw = self . client . get_all_pages_from_space ( ** params ) pages = [ ConfluencePage . model_validate ( page ) for page in pages_raw ] if not pages : return for page in pages : yield page start += len ( pages ) params [ \"start\" ] = start except HTTPError as e : self . logger . warning ( f \"Error while fetching Confluence pages from { space } : { e } \" ) __init__ ( configuration , client , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the Confluence reader. Parameters: configuration ( ConfluenceDatasourceConfiguration ) \u2013 Settings for Confluence access and export limits client ( Confluence ) \u2013 Client for Confluence API interactions logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operation information Source code in src/extraction/datasources/confluence/reader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , client : Confluence , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Confluence reader. Args: configuration: Settings for Confluence access and export limits client: Client for Confluence API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger read_all_async () async Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator [ ConfluencePage ] \u2013 AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. Source code in src/extraction/datasources/confluence/reader.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 async def read_all_async ( self , ) -> AsyncIterator [ ConfluencePage ]: \"\"\"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. \"\"\" self . logger . info ( f \"Reading pages from Confluence with limit { self . export_limit } \" ) response = self . client . get_all_spaces ( space_type = \"global\" ) spaces = [ Space . model_validate ( space ) for space in response [ \"results\" ]] yield_counter = 0 for space in spaces : for page in self . _get_all_pages ( space . key ): if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Confluence page { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield page ConfluenceDatasourceReaderFactory Bases: Factory Factory for creating Confluence reader instances. Creates and configures ConfluenceDatasourceReader objects with appropriate clients based on the provided configuration. Source code in src/extraction/datasources/confluence/reader.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 class ConfluenceDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating Confluence reader instances. Creates and configures ConfluenceDatasourceReader objects with appropriate clients based on the provided configuration. \"\"\" _configuration_class = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceReader : \"\"\"Creates a configured Confluence reader instance. Initializes the Confluence client and reader with the given configuration settings for credentials, URL, and export limits. Args: configuration: Confluence connection and access settings Returns: ConfluenceDatasourceReader: Fully configured reader instance \"\"\" client = ConfluenceClientFactory . create ( configuration ) return ConfluenceDatasourceReader ( configuration = configuration , client = client , ) ConfluencePage Bases: BaseModel Model representing a Confluence page from the API. Source code in src/extraction/datasources/confluence/reader.py 42 43 44 45 46 47 48 49 50 class ConfluencePage ( BaseModel ): \"\"\"Model representing a Confluence page from the API.\"\"\" id : str title : str body : Body history : History links : Links = Field ( alias = \"_links\" ) expandable : Dict [ str , str ] = Field ( alias = \"_expandable\" )","title":"Confluence"},{"location":"src/extraction/datasources/confluence/#confluence-datasource","text":"This module contains functionality related to the Confluence datasource.","title":"Confluence Datasource"},{"location":"src/extraction/datasources/confluence/#client","text":"","title":"Client"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.client.ConfluenceClientFactory","text":"Bases: SingletonFactory Factory for creating and managing Confluence client instances. This factory ensures only one Confluence client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. Source code in src/extraction/datasources/confluence/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class ConfluenceClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Confluence client instances. This factory ensures only one Confluence client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. \"\"\" _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> Confluence : \"\"\" Creates a new Confluence client instance using the provided configuration. Args: configuration: Configuration object containing Confluence connection details including base URL, username, and password. Returns: A configured Confluence client instance ready for API interactions. \"\"\" return Confluence ( url = configuration . base_url , username = configuration . secrets . username . get_secret_value (), password = configuration . secrets . password . get_secret_value (), )","title":"ConfluenceClientFactory"},{"location":"src/extraction/datasources/confluence/#configuration","text":"","title":"Configuration"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.configuration.ConfluenceDatasourceConfiguration","text":"Bases: DatasourceConfiguration Source code in src/extraction/datasources/confluence/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ConfluenceDatasourceConfiguration ( DatasourceConfiguration ): class Secrets ( BaseSecrets ): model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__DATASOURCES__CONFLUENCE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username credential used to authenticate with the Confluence instance\" , ) password : SecretStr = Field ( ... , description = \"Password credential used to authenticate with the Confluence instance\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Confluence server instance\" , ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"Communication protocol used to connect to the Confluence server\" , ) name : Literal [ DatasourceName . CONFLUENCE ] = Field ( ... , description = \"Identifier specifying this configuration is for a Confluence datasource\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials required to access the Confluence instance\" , ) @property def base_url ( self ) -> str : \"\"\" Constructs the complete base URL for the Confluence API from the protocol and host. Returns: str: The fully formed base URL to the Confluence instance \"\"\" return f \" { self . protocol } :// { self . host } \"","title":"ConfluenceDatasourceConfiguration"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.configuration.ConfluenceDatasourceConfiguration.base_url","text":"Constructs the complete base URL for the Confluence API from the protocol and host. Returns: str ( str ) \u2013 The fully formed base URL to the Confluence instance","title":"base_url"},{"location":"src/extraction/datasources/confluence/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.document.ConfluenceDocument","text":"Bases: BaseDocument Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. Source code in src/extraction/datasources/confluence/document.py 4 5 6 7 8 9 10 11 class ConfluenceDocument ( BaseDocument ): \"\"\"Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. \"\"\" pass","title":"ConfluenceDocument"},{"location":"src/extraction/datasources/confluence/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.manager.ConfluenceDatasourceManagerFactory","text":"Bases: Factory Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class ( Type ) \u2013 Configuration class used for validating and processing Confluence-specific settings. Source code in src/extraction/datasources/confluence/manager.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class ConfluenceDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class: Configuration class used for validating and processing Confluence-specific settings. \"\"\" _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create a configured Confluence datasource manager. Sets up the necessary reader and parser components based on the provided configuration and assembles them into a functional manager. Args: configuration: Configuration object containing Confluence-specific parameters including authentication details, spaces to extract, and other extraction options. Returns: A fully initialized datasource manager that can extract and process data from Confluence. \"\"\" reader = ConfluenceDatasourceReaderFactory . create ( configuration ) parser = ConfluenceDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration , reader , parser )","title":"ConfluenceDatasourceManagerFactory"},{"location":"src/extraction/datasources/confluence/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.parser.ConfluenceDatasourceParser","text":"Bases: BaseParser [ ConfluenceDocument ] Source code in src/extraction/datasources/confluence/parser.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class ConfluenceDatasourceParser ( BaseParser [ ConfluenceDocument ]): def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , parser : MarkItDown = MarkItDown (), ): \"\"\"Initialize the Confluence parser with the provided configuration. Args: configuration: Configuration object containing Confluence connection details parser: MarkItDown instance for converting HTML to markdown \"\"\" self . configuration = configuration self . parser = parser def parse ( self , page : ConfluencePage ) -> ConfluenceDocument : \"\"\"Parse a Confluence page into a document. Args: page: Confluence page details Returns: ConfluenceDocument: Parsed document with extracted text and metadata \"\"\" markdown = self . _get_page_markdown ( page ) metadata = self . _extract_metadata ( page , self . configuration . base_url ) return ConfluenceDocument ( text = markdown , metadata = metadata ) def _get_page_markdown ( self , page : ConfluencePage ) -> str : \"\"\"Extract markdown content from a Confluence page. Because of MarkItDown, we need to write the HTML content to a temporary file and then convert it to markdown. Args: page: Confluence page details Returns: str: Markdown content of the page \"\"\" html_content = page . body . view . value if not html_content : return \"\" with tempfile . NamedTemporaryFile ( mode = \"w\" , suffix = \".html\" ) as temp_file : temp_file . write ( html_content ) temp_file . flush () return self . parser . convert ( temp_file . name , file_extension = \".html\" ) . text_content @staticmethod def _extract_metadata ( page : ConfluencePage , base_url : str ) -> dict : \"\"\"Extract and format page metadata. Args: page: Confluence page details base_url: Base URL of the Confluence instance Returns: dict: Structured metadata including dates, IDs, and URLs \"\"\" return { \"created_time\" : page . history . createdDate , \"created_date\" : page . history . createdDate . split ( \"T\" )[ 0 ], \"datasource\" : \"confluence\" , \"format\" : \"md\" , \"last_edited_date\" : page . history . lastUpdated . when , \"last_edited_time\" : page . history . lastUpdated . when . split ( \"T\" )[ 0 ], \"page_id\" : page . id , \"space\" : page . expandable [ \"space\" ] . split ( \"/\" )[ - 1 ], \"title\" : page . title , \"type\" : \"page\" , \"url\" : base_url + page . links . webui , }","title":"ConfluenceDatasourceParser"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.parser.ConfluenceDatasourceParser.__init__","text":"Initialize the Confluence parser with the provided configuration. Parameters: configuration ( ConfluenceDatasourceConfiguration ) \u2013 Configuration object containing Confluence connection details parser ( MarkItDown , default: MarkItDown () ) \u2013 MarkItDown instance for converting HTML to markdown Source code in src/extraction/datasources/confluence/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , parser : MarkItDown = MarkItDown (), ): \"\"\"Initialize the Confluence parser with the provided configuration. Args: configuration: Configuration object containing Confluence connection details parser: MarkItDown instance for converting HTML to markdown \"\"\" self . configuration = configuration self . parser = parser","title":"__init__"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.parser.ConfluenceDatasourceParser.parse","text":"Parse a Confluence page into a document. Parameters: page ( ConfluencePage ) \u2013 Confluence page details Returns: ConfluenceDocument ( ConfluenceDocument ) \u2013 Parsed document with extracted text and metadata Source code in src/extraction/datasources/confluence/parser.py 31 32 33 34 35 36 37 38 39 40 41 42 def parse ( self , page : ConfluencePage ) -> ConfluenceDocument : \"\"\"Parse a Confluence page into a document. Args: page: Confluence page details Returns: ConfluenceDocument: Parsed document with extracted text and metadata \"\"\" markdown = self . _get_page_markdown ( page ) metadata = self . _extract_metadata ( page , self . configuration . base_url ) return ConfluenceDocument ( text = markdown , metadata = metadata )","title":"parse"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.parser.ConfluenceDatasourceParserFactory","text":"Bases: Factory Source code in src/extraction/datasources/confluence/parser.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class ConfluenceDatasourceParserFactory ( Factory ): _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceParser : \"\"\"Creates a Confluence parser instance. Args: configuration: Configuration object containing Confluence connection details Returns: ConfluenceDatasourceParser: Configured Confluence parser instance \"\"\" return ConfluenceDatasourceParser ( configuration )","title":"ConfluenceDatasourceParserFactory"},{"location":"src/extraction/datasources/confluence/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluenceDatasourceReader","text":"Bases: BaseReader Reader for extracting documents from Confluence spaces. Implements document extraction from Confluence spaces, handling pagination and export limits. Source code in src/extraction/datasources/confluence/reader.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class ConfluenceDatasourceReader ( BaseReader ): \"\"\"Reader for extracting documents from Confluence spaces. Implements document extraction from Confluence spaces, handling pagination and export limits. \"\"\" def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , client : Confluence , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Confluence reader. Args: configuration: Settings for Confluence access and export limits client: Client for Confluence API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger async def read_all_async ( self , ) -> AsyncIterator [ ConfluencePage ]: \"\"\"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. \"\"\" self . logger . info ( f \"Reading pages from Confluence with limit { self . export_limit } \" ) response = self . client . get_all_spaces ( space_type = \"global\" ) spaces = [ Space . model_validate ( space ) for space in response [ \"results\" ]] yield_counter = 0 for space in spaces : for page in self . _get_all_pages ( space . key ): if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Confluence page { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield page def _get_all_pages ( self , space : str ) -> Iterator [ ConfluencePage ]: \"\"\"Fetch all pages from a specific Confluence space. Handles pagination internally to retrieve all pages from the specified space, up to the optional limit. Pages include body content and update history. Args: space: Space key to fetch pages from limit: Maximum number of pages to fetch (None for unlimited) Returns: Iterator[ConfluencePage]: Iterator of Confluence pages with content and metadata \"\"\" start = 0 params = { \"space\" : space , \"start\" : start , \"status\" : None , \"expand\" : \"body.view,history.lastUpdated\" , } try : while True : pages_raw = self . client . get_all_pages_from_space ( ** params ) pages = [ ConfluencePage . model_validate ( page ) for page in pages_raw ] if not pages : return for page in pages : yield page start += len ( pages ) params [ \"start\" ] = start except HTTPError as e : self . logger . warning ( f \"Error while fetching Confluence pages from { space } : { e } \" )","title":"ConfluenceDatasourceReader"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluenceDatasourceReader.__init__","text":"Initialize the Confluence reader. Parameters: configuration ( ConfluenceDatasourceConfiguration ) \u2013 Settings for Confluence access and export limits client ( Confluence ) \u2013 Client for Confluence API interactions logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operation information Source code in src/extraction/datasources/confluence/reader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , client : Confluence , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Confluence reader. Args: configuration: Settings for Confluence access and export limits client: Client for Confluence API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger","title":"__init__"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluenceDatasourceReader.read_all_async","text":"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator [ ConfluencePage ] \u2013 AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. Source code in src/extraction/datasources/confluence/reader.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 async def read_all_async ( self , ) -> AsyncIterator [ ConfluencePage ]: \"\"\"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. \"\"\" self . logger . info ( f \"Reading pages from Confluence with limit { self . export_limit } \" ) response = self . client . get_all_spaces ( space_type = \"global\" ) spaces = [ Space . model_validate ( space ) for space in response [ \"results\" ]] yield_counter = 0 for space in spaces : for page in self . _get_all_pages ( space . key ): if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Confluence page { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield page","title":"read_all_async"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluenceDatasourceReaderFactory","text":"Bases: Factory Factory for creating Confluence reader instances. Creates and configures ConfluenceDatasourceReader objects with appropriate clients based on the provided configuration. Source code in src/extraction/datasources/confluence/reader.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 class ConfluenceDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating Confluence reader instances. Creates and configures ConfluenceDatasourceReader objects with appropriate clients based on the provided configuration. \"\"\" _configuration_class = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceReader : \"\"\"Creates a configured Confluence reader instance. Initializes the Confluence client and reader with the given configuration settings for credentials, URL, and export limits. Args: configuration: Confluence connection and access settings Returns: ConfluenceDatasourceReader: Fully configured reader instance \"\"\" client = ConfluenceClientFactory . create ( configuration ) return ConfluenceDatasourceReader ( configuration = configuration , client = client , )","title":"ConfluenceDatasourceReaderFactory"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluencePage","text":"Bases: BaseModel Model representing a Confluence page from the API. Source code in src/extraction/datasources/confluence/reader.py 42 43 44 45 46 47 48 49 50 class ConfluencePage ( BaseModel ): \"\"\"Model representing a Confluence page from the API.\"\"\" id : str title : str body : Body history : History links : Links = Field ( alias = \"_links\" ) expandable : Dict [ str , str ] = Field ( alias = \"_expandable\" )","title":"ConfluencePage"},{"location":"src/extraction/datasources/core/","text":"Core Datasource This module contains functionality related to the Core datasource. Cleaner BaseCleaner Bases: ABC , Generic [ DocType ] Abstract base class for document cleaning operations. Defines the interface for document cleaners with generic type support to ensure type safety across different document implementations. Source code in src/extraction/datasources/core/cleaner.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BaseCleaner ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document cleaning operations. Defines the interface for document cleaners with generic type support to ensure type safety across different document implementations. \"\"\" @abstractmethod def clean ( self , document : DocType ) -> DocType : \"\"\"Clean a single document. Args: document: The document to be cleaned Returns: The cleaned document or None if document should be filtered out \"\"\" pass clean ( document ) abstractmethod Clean a single document. Parameters: document ( DocType ) \u2013 The document to be cleaned Returns: DocType \u2013 The cleaned document or None if document should be filtered out Source code in src/extraction/datasources/core/cleaner.py 14 15 16 17 18 19 20 21 22 23 24 @abstractmethod def clean ( self , document : DocType ) -> DocType : \"\"\"Clean a single document. Args: document: The document to be cleaned Returns: The cleaned document or None if document should be filtered out \"\"\" pass BasicMarkdownCleaner Bases: BaseCleaner , Generic [ DocType ] Document cleaner for basic content validation. Checks for empty content in documents and filters them out. Works with any document type that has a text attribute. Source code in src/extraction/datasources/core/cleaner.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class BasicMarkdownCleaner ( BaseCleaner , Generic [ DocType ]): \"\"\"Document cleaner for basic content validation. Checks for empty content in documents and filters them out. Works with any document type that has a text attribute. \"\"\" def clean ( self , document : DocType ) -> DocType : \"\"\"Remove document if it contains empty content. Args: document: The document to validate Returns: The original document if content is not empty, None otherwise \"\"\" if not self . _has_empty_content ( document ): return document return None @staticmethod def _has_empty_content ( document : DocType ) -> bool : \"\"\"Check if document content is empty. Args: document: Document to check (must have a text attribute) Returns: True if document's text is empty or contains only whitespace \"\"\" return not document . text . strip () clean ( document ) Remove document if it contains empty content. Parameters: document ( DocType ) \u2013 The document to validate Returns: DocType \u2013 The original document if content is not empty, None otherwise Source code in src/extraction/datasources/core/cleaner.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def clean ( self , document : DocType ) -> DocType : \"\"\"Remove document if it contains empty content. Args: document: The document to validate Returns: The original document if content is not empty, None otherwise \"\"\" if not self . _has_empty_content ( document ): return document return None Document BaseDocument Bases: Document Base document class for structured content storage. Extends LlamaIndex Document to add support for attachments and metadata filtering for embedding and LLM contexts. Attributes: attachments ( Optional [ Dict [ str , str ]] ) \u2013 Dictionary mapping placeholder keys to attachment content included_embed_metadata_keys ( List [ str ] ) \u2013 Metadata fields to include in embeddings included_llm_metadata_keys ( List [ str ] ) \u2013 Metadata fields to include in LLM context Note DocType TypeVar ensures type safety when implementing document types. Default metadata includes title and timestamp information. Source code in src/extraction/datasources/core/document.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseDocument ( Document ): \"\"\"Base document class for structured content storage. Extends LlamaIndex Document to add support for attachments and metadata filtering for embedding and LLM contexts. Attributes: attachments: Dictionary mapping placeholder keys to attachment content included_embed_metadata_keys: Metadata fields to include in embeddings included_llm_metadata_keys: Metadata fields to include in LLM context Note: DocType TypeVar ensures type safety when implementing document types. Default metadata includes title and timestamp information. \"\"\" attachments : Optional [ Dict [ str , str ]] = Field ( description = \"Document attachments with placeholders as keys and content as values\" , default = None , ) included_embed_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , ] included_llm_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , ] def __init__ ( self , text : str , metadata : dict , attachments : dict = None ): \"\"\"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. \"\"\" super () . __init__ ( text = text , metadata = metadata ) self . attachments = attachments or {} self . excluded_embed_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_embed_metadata_keys ) self . excluded_llm_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_llm_metadata_keys ) @staticmethod def _set_excluded_metadata_keys ( metadata : dict , included_keys : List [ str ] ) -> List [ str ]: \"\"\"Identify metadata keys to exclude from processing. Returns all keys from metadata that aren't in the included_keys list. \"\"\" return [ key for key in metadata . keys () if key not in included_keys ] __init__ ( text , metadata , attachments = None ) Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. Source code in src/extraction/datasources/core/document.py 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , text : str , metadata : dict , attachments : dict = None ): \"\"\"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. \"\"\" super () . __init__ ( text = text , metadata = metadata ) self . attachments = attachments or {} self . excluded_embed_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_embed_metadata_keys ) self . excluded_llm_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_llm_metadata_keys ) Manager BaseDatasourceManager Bases: ABC , Generic [ DocType ] Abstract base class for datasource management. Defines the interface for managing the extraction, parsing, cleaning, and splitting of documents from a data source. This class serves as a template for implementing specific datasource managers, ensuring a consistent interface and behavior across different implementations. Source code in src/extraction/datasources/core/manager.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class BaseDatasourceManager ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for datasource management. Defines the interface for managing the extraction, parsing, cleaning, and splitting of documents from a data source. This class serves as a template for implementing specific datasource managers, ensuring a consistent interface and behavior across different implementations. \"\"\" def __init__ ( self , configuration : ExtractionConfiguration , reader : BaseReader , parser : BaseParser = BasicMarkdownParser (), cleaner : BaseCleaner = BasicMarkdownCleaner (), splitter : BaseSplitter = BasicMarkdownSplitter (), ): \"\"\"Initialize datasource manager. Args: configuration: Embedding and processing settings reader: Content extraction component cleaner: Content cleaning component splitter: Content splitting component \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner self . splitter = splitter @abstractmethod async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Extract and process all content from the datasource. Returns: An async iterator yielding processed document chunks of type DocType \"\"\" pass @abstractmethod def incremental_sync ( self ): \"\"\"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. \"\"\" pass __init__ ( configuration , reader , parser = BasicMarkdownParser (), cleaner = BasicMarkdownCleaner (), splitter = BasicMarkdownSplitter ()) Initialize datasource manager. Parameters: configuration ( ExtractionConfiguration ) \u2013 Embedding and processing settings reader ( BaseReader ) \u2013 Content extraction component cleaner ( BaseCleaner , default: BasicMarkdownCleaner () ) \u2013 Content cleaning component splitter ( BaseSplitter , default: BasicMarkdownSplitter () ) \u2013 Content splitting component Source code in src/extraction/datasources/core/manager.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , configuration : ExtractionConfiguration , reader : BaseReader , parser : BaseParser = BasicMarkdownParser (), cleaner : BaseCleaner = BasicMarkdownCleaner (), splitter : BaseSplitter = BasicMarkdownSplitter (), ): \"\"\"Initialize datasource manager. Args: configuration: Embedding and processing settings reader: Content extraction component cleaner: Content cleaning component splitter: Content splitting component \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner self . splitter = splitter full_refresh_sync () abstractmethod async Extract and process all content from the datasource. Returns: AsyncIterator [ DocType ] \u2013 An async iterator yielding processed document chunks of type DocType Source code in src/extraction/datasources/core/manager.py 52 53 54 55 56 57 58 59 60 61 @abstractmethod async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Extract and process all content from the datasource. Returns: An async iterator yielding processed document chunks of type DocType \"\"\" pass incremental_sync () abstractmethod Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. Source code in src/extraction/datasources/core/manager.py 63 64 65 66 67 68 69 70 71 @abstractmethod def incremental_sync ( self ): \"\"\"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. \"\"\" pass BasicDatasourceManager Bases: BaseDatasourceManager , Generic [ DocType ] Standard implementation of datasource content processing pipeline. Handles the extraction, parsing, cleaning, and splitting of documents from a data source. Processes documents using the provided components in a sequential pipeline to prepare them for embedding and storage. Source code in src/extraction/datasources/core/manager.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class BasicDatasourceManager ( BaseDatasourceManager , Generic [ DocType ]): \"\"\"Standard implementation of datasource content processing pipeline. Handles the extraction, parsing, cleaning, and splitting of documents from a data source. Processes documents using the provided components in a sequential pipeline to prepare them for embedding and storage. \"\"\" async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: An async iterator yielding processed document chunks of type DocType \"\"\" objects = self . reader . read_all_async () async for object in objects : md_document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( md_document ) if not cleaned_document : continue for split_document in self . splitter . split ( cleaned_document ): yield split_document def incremental_sync ( self ): \"\"\"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError: This feature is not yet implemented \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) full_refresh_sync () async Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: AsyncIterator [ DocType ] \u2013 An async iterator yielding processed document chunks of type DocType Source code in src/extraction/datasources/core/manager.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: An async iterator yielding processed document chunks of type DocType \"\"\" objects = self . reader . read_all_async () async for object in objects : md_document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( md_document ) if not cleaned_document : continue for split_document in self . splitter . split ( cleaned_document ): yield split_document incremental_sync () Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError \u2013 This feature is not yet implemented Source code in src/extraction/datasources/core/manager.py 106 107 108 109 110 111 112 113 114 115 def incremental_sync ( self ): \"\"\"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError: This feature is not yet implemented \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) Parser BaseParser Bases: ABC , Generic [ DocType ] Abstract base class for document parsers. Defines the interface for parsing content into documents of specified type (DocType). Source code in src/extraction/datasources/core/parser.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class BaseParser ( ABC , Generic [ DocType ]): \"\"\" Abstract base class for document parsers. Defines the interface for parsing content into documents of specified type (DocType). \"\"\" @abstractmethod def parse ( self , content : str ) -> DocType : \"\"\" Parse content into a document of type DocType. Args: content: Raw content string to be parsed Returns: Parsed document of type DocType \"\"\" pass parse ( content ) abstractmethod Parse content into a document of type DocType. Parameters: content ( str ) \u2013 Raw content string to be parsed Returns: DocType \u2013 Parsed document of type DocType Source code in src/extraction/datasources/core/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 @abstractmethod def parse ( self , content : str ) -> DocType : \"\"\" Parse content into a document of type DocType. Args: content: Raw content string to be parsed Returns: Parsed document of type DocType \"\"\" pass BasicMarkdownParser Bases: BaseParser [ Document ] Markdown parser that converts markdown text into Document objects. Implements the BaseParser interface for basic markdown content. Source code in src/extraction/datasources/core/parser.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class BasicMarkdownParser ( BaseParser [ Document ]): \"\"\" Markdown parser that converts markdown text into Document objects. Implements the BaseParser interface for basic markdown content. \"\"\" def parse ( self , markdown : str ) -> Document : \"\"\" Parse markdown content into a Document object. Args: markdown: Markdown content to be parsed Returns: Document object containing the markdown text \"\"\" return Document ( text = markdown , metadata = {}) parse ( markdown ) Parse markdown content into a Document object. Parameters: markdown ( str ) \u2013 Markdown content to be parsed Returns: Document \u2013 Document object containing the markdown text Source code in src/extraction/datasources/core/parser.py 38 39 40 41 42 43 44 45 46 47 48 def parse ( self , markdown : str ) -> Document : \"\"\" Parse markdown content into a Document object. Args: markdown: Markdown content to be parsed Returns: Document object containing the markdown text \"\"\" return Document ( text = markdown , metadata = {}) Reader BaseReader Bases: ABC Abstract base class for document source readers. This class defines a standard interface for extracting documents from various data sources. Concrete implementations should inherit from this class and implement the required methods to handle specific data source types. The generic typing allows for flexibility in the document types returned by different implementations. Source code in src/extraction/datasources/core/reader.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class BaseReader ( ABC ): \"\"\"Abstract base class for document source readers. This class defines a standard interface for extracting documents from various data sources. Concrete implementations should inherit from this class and implement the required methods to handle specific data source types. The generic typing allows for flexibility in the document types returned by different implementations. \"\"\" @abstractmethod async def read_all_async ( self ) -> AsyncIterator [ Any ]: \"\"\"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError: This abstract method must be implemented by subclasses. \"\"\" pass @staticmethod def _limit_reached ( yield_count : int , limit : Optional [ int ]) -> bool : \"\"\"Check if the object retrieval limit has been reached. Determines whether the number of fetched objects has reached or exceeded the specified limit. Args: yield_count: Current number of objects fetched limit: Maximum number of objects to retrieve (None for unlimited) Returns: bool: True if limit reached or exceeded, False otherwise \"\"\" return limit is not None and yield_count >= limit read_all_async () abstractmethod async Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator [ Any ] \u2013 AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError \u2013 This abstract method must be implemented by subclasses. Source code in src/extraction/datasources/core/reader.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @abstractmethod async def read_all_async ( self ) -> AsyncIterator [ Any ]: \"\"\"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError: This abstract method must be implemented by subclasses. \"\"\" pass Splitter BaseSplitter Bases: ABC , Generic [ DocType ] Abstract base class for document splitters. This class defines the interface for splitting documents into smaller chunks. All splitter implementations should inherit from this class. Source code in src/extraction/datasources/core/splitter.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BaseSplitter ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document splitters. This class defines the interface for splitting documents into smaller chunks. All splitter implementations should inherit from this class. \"\"\" @abstractmethod def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Split a document into multiple smaller documents. Args: document: The document to be split. Returns: A list of document chunks. \"\"\" pass split ( document ) abstractmethod Split a document into multiple smaller documents. Parameters: document ( DocType ) \u2013 The document to be split. Returns: List [ DocType ] \u2013 A list of document chunks. Source code in src/extraction/datasources/core/splitter.py 14 15 16 17 18 19 20 21 22 23 24 @abstractmethod def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Split a document into multiple smaller documents. Args: document: The document to be split. Returns: A list of document chunks. \"\"\" pass BasicMarkdownSplitter Bases: BaseSplitter , Generic [ DocType ] A simple splitter implementation that returns the document as-is. This splitter does not perform any actual splitting and is primarily used as a pass-through when splitting is not required. Source code in src/extraction/datasources/core/splitter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class BasicMarkdownSplitter ( BaseSplitter , Generic [ DocType ]): \"\"\"A simple splitter implementation that returns the document as-is. This splitter does not perform any actual splitting and is primarily used as a pass-through when splitting is not required. \"\"\" def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Return the document as a single-item list without splitting. Args: document: The document to be processed. Returns: A list containing the original document as the only element. \"\"\" return [ document ] split ( document ) Return the document as a single-item list without splitting. Parameters: document ( DocType ) \u2013 The document to be processed. Returns: List [ DocType ] \u2013 A list containing the original document as the only element. Source code in src/extraction/datasources/core/splitter.py 34 35 36 37 38 39 40 41 42 43 def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Return the document as a single-item list without splitting. Args: document: The document to be processed. Returns: A list containing the original document as the only element. \"\"\" return [ document ]","title":"Core"},{"location":"src/extraction/datasources/core/#core-datasource","text":"This module contains functionality related to the Core datasource.","title":"Core Datasource"},{"location":"src/extraction/datasources/core/#cleaner","text":"","title":"Cleaner"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.cleaner.BaseCleaner","text":"Bases: ABC , Generic [ DocType ] Abstract base class for document cleaning operations. Defines the interface for document cleaners with generic type support to ensure type safety across different document implementations. Source code in src/extraction/datasources/core/cleaner.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BaseCleaner ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document cleaning operations. Defines the interface for document cleaners with generic type support to ensure type safety across different document implementations. \"\"\" @abstractmethod def clean ( self , document : DocType ) -> DocType : \"\"\"Clean a single document. Args: document: The document to be cleaned Returns: The cleaned document or None if document should be filtered out \"\"\" pass","title":"BaseCleaner"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.cleaner.BaseCleaner.clean","text":"Clean a single document. Parameters: document ( DocType ) \u2013 The document to be cleaned Returns: DocType \u2013 The cleaned document or None if document should be filtered out Source code in src/extraction/datasources/core/cleaner.py 14 15 16 17 18 19 20 21 22 23 24 @abstractmethod def clean ( self , document : DocType ) -> DocType : \"\"\"Clean a single document. Args: document: The document to be cleaned Returns: The cleaned document or None if document should be filtered out \"\"\" pass","title":"clean"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.cleaner.BasicMarkdownCleaner","text":"Bases: BaseCleaner , Generic [ DocType ] Document cleaner for basic content validation. Checks for empty content in documents and filters them out. Works with any document type that has a text attribute. Source code in src/extraction/datasources/core/cleaner.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class BasicMarkdownCleaner ( BaseCleaner , Generic [ DocType ]): \"\"\"Document cleaner for basic content validation. Checks for empty content in documents and filters them out. Works with any document type that has a text attribute. \"\"\" def clean ( self , document : DocType ) -> DocType : \"\"\"Remove document if it contains empty content. Args: document: The document to validate Returns: The original document if content is not empty, None otherwise \"\"\" if not self . _has_empty_content ( document ): return document return None @staticmethod def _has_empty_content ( document : DocType ) -> bool : \"\"\"Check if document content is empty. Args: document: Document to check (must have a text attribute) Returns: True if document's text is empty or contains only whitespace \"\"\" return not document . text . strip ()","title":"BasicMarkdownCleaner"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.cleaner.BasicMarkdownCleaner.clean","text":"Remove document if it contains empty content. Parameters: document ( DocType ) \u2013 The document to validate Returns: DocType \u2013 The original document if content is not empty, None otherwise Source code in src/extraction/datasources/core/cleaner.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def clean ( self , document : DocType ) -> DocType : \"\"\"Remove document if it contains empty content. Args: document: The document to validate Returns: The original document if content is not empty, None otherwise \"\"\" if not self . _has_empty_content ( document ): return document return None","title":"clean"},{"location":"src/extraction/datasources/core/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.document.BaseDocument","text":"Bases: Document Base document class for structured content storage. Extends LlamaIndex Document to add support for attachments and metadata filtering for embedding and LLM contexts. Attributes: attachments ( Optional [ Dict [ str , str ]] ) \u2013 Dictionary mapping placeholder keys to attachment content included_embed_metadata_keys ( List [ str ] ) \u2013 Metadata fields to include in embeddings included_llm_metadata_keys ( List [ str ] ) \u2013 Metadata fields to include in LLM context Note DocType TypeVar ensures type safety when implementing document types. Default metadata includes title and timestamp information. Source code in src/extraction/datasources/core/document.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseDocument ( Document ): \"\"\"Base document class for structured content storage. Extends LlamaIndex Document to add support for attachments and metadata filtering for embedding and LLM contexts. Attributes: attachments: Dictionary mapping placeholder keys to attachment content included_embed_metadata_keys: Metadata fields to include in embeddings included_llm_metadata_keys: Metadata fields to include in LLM context Note: DocType TypeVar ensures type safety when implementing document types. Default metadata includes title and timestamp information. \"\"\" attachments : Optional [ Dict [ str , str ]] = Field ( description = \"Document attachments with placeholders as keys and content as values\" , default = None , ) included_embed_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , ] included_llm_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , ] def __init__ ( self , text : str , metadata : dict , attachments : dict = None ): \"\"\"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. \"\"\" super () . __init__ ( text = text , metadata = metadata ) self . attachments = attachments or {} self . excluded_embed_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_embed_metadata_keys ) self . excluded_llm_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_llm_metadata_keys ) @staticmethod def _set_excluded_metadata_keys ( metadata : dict , included_keys : List [ str ] ) -> List [ str ]: \"\"\"Identify metadata keys to exclude from processing. Returns all keys from metadata that aren't in the included_keys list. \"\"\" return [ key for key in metadata . keys () if key not in included_keys ]","title":"BaseDocument"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.document.BaseDocument.__init__","text":"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. Source code in src/extraction/datasources/core/document.py 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , text : str , metadata : dict , attachments : dict = None ): \"\"\"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. \"\"\" super () . __init__ ( text = text , metadata = metadata ) self . attachments = attachments or {} self . excluded_embed_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_embed_metadata_keys ) self . excluded_llm_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_llm_metadata_keys )","title":"__init__"},{"location":"src/extraction/datasources/core/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BaseDatasourceManager","text":"Bases: ABC , Generic [ DocType ] Abstract base class for datasource management. Defines the interface for managing the extraction, parsing, cleaning, and splitting of documents from a data source. This class serves as a template for implementing specific datasource managers, ensuring a consistent interface and behavior across different implementations. Source code in src/extraction/datasources/core/manager.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class BaseDatasourceManager ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for datasource management. Defines the interface for managing the extraction, parsing, cleaning, and splitting of documents from a data source. This class serves as a template for implementing specific datasource managers, ensuring a consistent interface and behavior across different implementations. \"\"\" def __init__ ( self , configuration : ExtractionConfiguration , reader : BaseReader , parser : BaseParser = BasicMarkdownParser (), cleaner : BaseCleaner = BasicMarkdownCleaner (), splitter : BaseSplitter = BasicMarkdownSplitter (), ): \"\"\"Initialize datasource manager. Args: configuration: Embedding and processing settings reader: Content extraction component cleaner: Content cleaning component splitter: Content splitting component \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner self . splitter = splitter @abstractmethod async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Extract and process all content from the datasource. Returns: An async iterator yielding processed document chunks of type DocType \"\"\" pass @abstractmethod def incremental_sync ( self ): \"\"\"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. \"\"\" pass","title":"BaseDatasourceManager"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BaseDatasourceManager.__init__","text":"Initialize datasource manager. Parameters: configuration ( ExtractionConfiguration ) \u2013 Embedding and processing settings reader ( BaseReader ) \u2013 Content extraction component cleaner ( BaseCleaner , default: BasicMarkdownCleaner () ) \u2013 Content cleaning component splitter ( BaseSplitter , default: BasicMarkdownSplitter () ) \u2013 Content splitting component Source code in src/extraction/datasources/core/manager.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , configuration : ExtractionConfiguration , reader : BaseReader , parser : BaseParser = BasicMarkdownParser (), cleaner : BaseCleaner = BasicMarkdownCleaner (), splitter : BaseSplitter = BasicMarkdownSplitter (), ): \"\"\"Initialize datasource manager. Args: configuration: Embedding and processing settings reader: Content extraction component cleaner: Content cleaning component splitter: Content splitting component \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner self . splitter = splitter","title":"__init__"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BaseDatasourceManager.full_refresh_sync","text":"Extract and process all content from the datasource. Returns: AsyncIterator [ DocType ] \u2013 An async iterator yielding processed document chunks of type DocType Source code in src/extraction/datasources/core/manager.py 52 53 54 55 56 57 58 59 60 61 @abstractmethod async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Extract and process all content from the datasource. Returns: An async iterator yielding processed document chunks of type DocType \"\"\" pass","title":"full_refresh_sync"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BaseDatasourceManager.incremental_sync","text":"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. Source code in src/extraction/datasources/core/manager.py 63 64 65 66 67 68 69 70 71 @abstractmethod def incremental_sync ( self ): \"\"\"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. \"\"\" pass","title":"incremental_sync"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BasicDatasourceManager","text":"Bases: BaseDatasourceManager , Generic [ DocType ] Standard implementation of datasource content processing pipeline. Handles the extraction, parsing, cleaning, and splitting of documents from a data source. Processes documents using the provided components in a sequential pipeline to prepare them for embedding and storage. Source code in src/extraction/datasources/core/manager.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class BasicDatasourceManager ( BaseDatasourceManager , Generic [ DocType ]): \"\"\"Standard implementation of datasource content processing pipeline. Handles the extraction, parsing, cleaning, and splitting of documents from a data source. Processes documents using the provided components in a sequential pipeline to prepare them for embedding and storage. \"\"\" async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: An async iterator yielding processed document chunks of type DocType \"\"\" objects = self . reader . read_all_async () async for object in objects : md_document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( md_document ) if not cleaned_document : continue for split_document in self . splitter . split ( cleaned_document ): yield split_document def incremental_sync ( self ): \"\"\"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError: This feature is not yet implemented \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" )","title":"BasicDatasourceManager"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BasicDatasourceManager.full_refresh_sync","text":"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: AsyncIterator [ DocType ] \u2013 An async iterator yielding processed document chunks of type DocType Source code in src/extraction/datasources/core/manager.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: An async iterator yielding processed document chunks of type DocType \"\"\" objects = self . reader . read_all_async () async for object in objects : md_document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( md_document ) if not cleaned_document : continue for split_document in self . splitter . split ( cleaned_document ): yield split_document","title":"full_refresh_sync"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BasicDatasourceManager.incremental_sync","text":"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError \u2013 This feature is not yet implemented Source code in src/extraction/datasources/core/manager.py 106 107 108 109 110 111 112 113 114 115 def incremental_sync ( self ): \"\"\"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError: This feature is not yet implemented \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" )","title":"incremental_sync"},{"location":"src/extraction/datasources/core/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.parser.BaseParser","text":"Bases: ABC , Generic [ DocType ] Abstract base class for document parsers. Defines the interface for parsing content into documents of specified type (DocType). Source code in src/extraction/datasources/core/parser.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class BaseParser ( ABC , Generic [ DocType ]): \"\"\" Abstract base class for document parsers. Defines the interface for parsing content into documents of specified type (DocType). \"\"\" @abstractmethod def parse ( self , content : str ) -> DocType : \"\"\" Parse content into a document of type DocType. Args: content: Raw content string to be parsed Returns: Parsed document of type DocType \"\"\" pass","title":"BaseParser"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.parser.BaseParser.parse","text":"Parse content into a document of type DocType. Parameters: content ( str ) \u2013 Raw content string to be parsed Returns: DocType \u2013 Parsed document of type DocType Source code in src/extraction/datasources/core/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 @abstractmethod def parse ( self , content : str ) -> DocType : \"\"\" Parse content into a document of type DocType. Args: content: Raw content string to be parsed Returns: Parsed document of type DocType \"\"\" pass","title":"parse"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.parser.BasicMarkdownParser","text":"Bases: BaseParser [ Document ] Markdown parser that converts markdown text into Document objects. Implements the BaseParser interface for basic markdown content. Source code in src/extraction/datasources/core/parser.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class BasicMarkdownParser ( BaseParser [ Document ]): \"\"\" Markdown parser that converts markdown text into Document objects. Implements the BaseParser interface for basic markdown content. \"\"\" def parse ( self , markdown : str ) -> Document : \"\"\" Parse markdown content into a Document object. Args: markdown: Markdown content to be parsed Returns: Document object containing the markdown text \"\"\" return Document ( text = markdown , metadata = {})","title":"BasicMarkdownParser"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.parser.BasicMarkdownParser.parse","text":"Parse markdown content into a Document object. Parameters: markdown ( str ) \u2013 Markdown content to be parsed Returns: Document \u2013 Document object containing the markdown text Source code in src/extraction/datasources/core/parser.py 38 39 40 41 42 43 44 45 46 47 48 def parse ( self , markdown : str ) -> Document : \"\"\" Parse markdown content into a Document object. Args: markdown: Markdown content to be parsed Returns: Document object containing the markdown text \"\"\" return Document ( text = markdown , metadata = {})","title":"parse"},{"location":"src/extraction/datasources/core/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.reader.BaseReader","text":"Bases: ABC Abstract base class for document source readers. This class defines a standard interface for extracting documents from various data sources. Concrete implementations should inherit from this class and implement the required methods to handle specific data source types. The generic typing allows for flexibility in the document types returned by different implementations. Source code in src/extraction/datasources/core/reader.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class BaseReader ( ABC ): \"\"\"Abstract base class for document source readers. This class defines a standard interface for extracting documents from various data sources. Concrete implementations should inherit from this class and implement the required methods to handle specific data source types. The generic typing allows for flexibility in the document types returned by different implementations. \"\"\" @abstractmethod async def read_all_async ( self ) -> AsyncIterator [ Any ]: \"\"\"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError: This abstract method must be implemented by subclasses. \"\"\" pass @staticmethod def _limit_reached ( yield_count : int , limit : Optional [ int ]) -> bool : \"\"\"Check if the object retrieval limit has been reached. Determines whether the number of fetched objects has reached or exceeded the specified limit. Args: yield_count: Current number of objects fetched limit: Maximum number of objects to retrieve (None for unlimited) Returns: bool: True if limit reached or exceeded, False otherwise \"\"\" return limit is not None and yield_count >= limit","title":"BaseReader"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.reader.BaseReader.read_all_async","text":"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator [ Any ] \u2013 AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError \u2013 This abstract method must be implemented by subclasses. Source code in src/extraction/datasources/core/reader.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @abstractmethod async def read_all_async ( self ) -> AsyncIterator [ Any ]: \"\"\"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError: This abstract method must be implemented by subclasses. \"\"\" pass","title":"read_all_async"},{"location":"src/extraction/datasources/core/#splitter","text":"","title":"Splitter"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.splitter.BaseSplitter","text":"Bases: ABC , Generic [ DocType ] Abstract base class for document splitters. This class defines the interface for splitting documents into smaller chunks. All splitter implementations should inherit from this class. Source code in src/extraction/datasources/core/splitter.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BaseSplitter ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document splitters. This class defines the interface for splitting documents into smaller chunks. All splitter implementations should inherit from this class. \"\"\" @abstractmethod def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Split a document into multiple smaller documents. Args: document: The document to be split. Returns: A list of document chunks. \"\"\" pass","title":"BaseSplitter"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.splitter.BaseSplitter.split","text":"Split a document into multiple smaller documents. Parameters: document ( DocType ) \u2013 The document to be split. Returns: List [ DocType ] \u2013 A list of document chunks. Source code in src/extraction/datasources/core/splitter.py 14 15 16 17 18 19 20 21 22 23 24 @abstractmethod def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Split a document into multiple smaller documents. Args: document: The document to be split. Returns: A list of document chunks. \"\"\" pass","title":"split"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.splitter.BasicMarkdownSplitter","text":"Bases: BaseSplitter , Generic [ DocType ] A simple splitter implementation that returns the document as-is. This splitter does not perform any actual splitting and is primarily used as a pass-through when splitting is not required. Source code in src/extraction/datasources/core/splitter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class BasicMarkdownSplitter ( BaseSplitter , Generic [ DocType ]): \"\"\"A simple splitter implementation that returns the document as-is. This splitter does not perform any actual splitting and is primarily used as a pass-through when splitting is not required. \"\"\" def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Return the document as a single-item list without splitting. Args: document: The document to be processed. Returns: A list containing the original document as the only element. \"\"\" return [ document ]","title":"BasicMarkdownSplitter"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.splitter.BasicMarkdownSplitter.split","text":"Return the document as a single-item list without splitting. Parameters: document ( DocType ) \u2013 The document to be processed. Returns: List [ DocType ] \u2013 A list containing the original document as the only element. Source code in src/extraction/datasources/core/splitter.py 34 35 36 37 38 39 40 41 42 43 def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Return the document as a single-item list without splitting. Args: document: The document to be processed. Returns: A list containing the original document as the only element. \"\"\" return [ document ]","title":"split"},{"location":"src/extraction/datasources/notion/","text":"Notion Datasource This module contains functionality related to the Notion datasource. Cleaner NotionDatasourceCleaner Bases: BasicMarkdownCleaner [ NotionDocument ] Cleaner for Notion document content. Implements cleaning logic for Notion databases and pages, removing HTML tags and comments while preserving meaningful content. Note Expects documents to be in markdown format. Source code in src/extraction/datasources/notion/cleaner.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class NotionDatasourceCleaner ( BasicMarkdownCleaner [ NotionDocument ]): \"\"\"Cleaner for Notion document content. Implements cleaning logic for Notion databases and pages, removing HTML tags and comments while preserving meaningful content. Note: Expects documents to be in markdown format. \"\"\" def clean ( self , document : NotionDocument ) -> NotionDocument : \"\"\"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Args: document: Notion document to clean Returns: NotionDocument: Cleaned document, or None if content is empty after cleaning \"\"\" if document . metadata [ \"type\" ] == \"database\" : cleaned_text = self . _clean_database ( document ) document . set_content ( cleaned_text ) if document . metadata [ \"type\" ] == \"page\" : cleaned_text = self . _clean_page ( document ) document . set_content ( cleaned_text ) if self . _has_empty_content ( document ): return None return document def _clean_database ( self , document : NotionDocument ) -> str : \"\"\"Clean Notion database content. Extracts and cleans the text content from a Notion database document, processing any embedded HTML elements. Args: document: Database document to clean Returns: str: Cleaned database content as markdown text \"\"\" return NotionDatasourceCleaner . _parse_html_in_markdown ( document . text ) def _clean_page ( self , document : NotionDocument ) -> str : \"\"\"Clean Notion page content. Extracts and cleans the text content from a Notion page document, processing any embedded HTML elements. Args: document: Page document to clean Returns: str: Cleaned page content as markdown text \"\"\" return NotionDatasourceCleaner . _parse_html_in_markdown ( document . text ) @staticmethod def _parse_html_in_markdown ( md_text : str ) -> str : \"\"\"Process HTML elements within markdown content. Performs two main cleaning operations: 1. Removes HTML comments completely 2. Converts HTML tags to markdown format 3. Removes elements that don't contain alphanumeric characters Args: md_text: Text containing markdown and HTML Returns: str: Cleaned markdown text with HTML properly converted or removed Note: Uses BeautifulSoup for HTML parsing and markdownify for HTML-to-markdown conversion \"\"\" def replace_html ( match ): html_content = match . group ( 0 ) soup = BeautifulSoup ( html_content , \"html.parser\" ) markdown = md ( str ( soup )) if not re . search ( r \"[a-zA-Z0-9]\" , markdown ): return \"\" return markdown md_text = re . sub ( r \"<!--.*?-->\" , \"\" , md_text , flags = re . DOTALL ) html_block_re = re . compile ( r \"<.*?>\" , re . DOTALL ) return re . sub ( html_block_re , replace_html , md_text ) clean ( document ) Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Parameters: document ( NotionDocument ) \u2013 Notion document to clean Returns: NotionDocument ( NotionDocument ) \u2013 Cleaned document, or None if content is empty after cleaning Source code in src/extraction/datasources/notion/cleaner.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def clean ( self , document : NotionDocument ) -> NotionDocument : \"\"\"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Args: document: Notion document to clean Returns: NotionDocument: Cleaned document, or None if content is empty after cleaning \"\"\" if document . metadata [ \"type\" ] == \"database\" : cleaned_text = self . _clean_database ( document ) document . set_content ( cleaned_text ) if document . metadata [ \"type\" ] == \"page\" : cleaned_text = self . _clean_page ( document ) document . set_content ( cleaned_text ) if self . _has_empty_content ( document ): return None return document NotionDatasourceCleanerFactory Bases: Factory Factory for creating NotionDatasourceCleaner instances. This factory is responsible for creating instances of NotionDatasourceCleaner with the appropriate configuration. Source code in src/extraction/datasources/notion/cleaner.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class NotionDatasourceCleanerFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceCleaner instances. This factory is responsible for creating instances of NotionDatasourceCleaner with the appropriate configuration. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , _ : NotionDatasourceConfiguration ) -> NotionDatasourceCleaner : \"\"\"Create a new instance of NotionDatasourceCleaner. Args: configuration: Configuration for the cleaner Returns: NotionDatasourceCleaner: Instance of NotionDatasourceCleaner \"\"\" return NotionDatasourceCleaner () Client NotionClientFactory Bases: SingletonFactory Factory for creating and managing Notion API client instances. This singleton factory ensures that only one Notion client instance is created for a specific configuration, promoting resource efficiency and consistency. Client instances are created using the Notion API authentication token from the provided configuration. The factory follows the singleton pattern to prevent multiple instantiations of clients with identical configurations. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object used to create the client Source code in src/extraction/datasources/notion/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class NotionClientFactory ( SingletonFactory ): \"\"\"Factory for creating and managing Notion API client instances. This singleton factory ensures that only one Notion client instance is created for a specific configuration, promoting resource efficiency and consistency. Client instances are created using the Notion API authentication token from the provided configuration. The factory follows the singleton pattern to prevent multiple instantiations of clients with identical configurations. Attributes: _configuration_class: Type of configuration object used to create the client \"\"\" _configuration_class : Type = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration ) -> Client : \"\"\"Create a new instance of the Notion API client. This method extracts the API token from the provided configuration's secrets and uses it to authenticate a new Notion client. Args: configuration: Configuration object containing Notion API credentials and other settings. Returns: A configured Notion API client instance ready for making API calls. \"\"\" return Client ( auth = configuration . secrets . api_token . get_secret_value ()) Configuration Document NotionDocument Bases: BaseDocument Document representation for Notion page content. Extends BaseDocument to handle Notion-specific document processing including metadata handling and filtering for embeddings and LLM contexts. Source code in src/extraction/datasources/notion/document.py 4 5 6 7 8 9 10 11 class NotionDocument ( BaseDocument ): \"\"\"Document representation for Notion page content. Extends BaseDocument to handle Notion-specific document processing including metadata handling and filtering for embeddings and LLM contexts. \"\"\" pass Exporter NotionExporter Exporter for converting Notion pages to markdown documents. Provides a high-level interface for extracting Notion content and converting it to structured NotionDocument instances. Source code in src/extraction/datasources/notion/exporter.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 class NotionExporter : \"\"\"Exporter for converting Notion pages to markdown documents. Provides a high-level interface for extracting Notion content and converting it to structured NotionDocument instances. \"\"\" def __init__ ( self , api_token : str , ): \"\"\"Initialize Notion exporter. Args: api_token: Authentication token for Notion API \"\"\" self . notion_exporter = _NotionExporterCore ( notion_token = api_token , export_child_pages = False , extract_page_metadata = True , ) async def run ( self , page_ids : List [ str ] = None , database_ids : List [ str ] = None ) -> List [ NotionDocument ]: \"\"\"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Args: page_ids: List of page IDs to export database_ids: List of database IDs to export Returns: List of NotionDocument objects containing content and metadata Raises: ValueError: If neither page_ids nor database_ids provided \"\"\" extracted_objects = await self . notion_exporter . async_export_pages ( page_ids = page_ids , database_ids = database_ids ) objects = [] for object_id , extracted_data in extracted_objects . items (): objects . append ( { \"metadata\" : extracted_data [ \"metadata\" ], \"markdown\" : extracted_data [ \"content\" ], } ) return objects __init__ ( api_token ) Initialize Notion exporter. Parameters: api_token ( str ) \u2013 Authentication token for Notion API Source code in src/extraction/datasources/notion/exporter.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def __init__ ( self , api_token : str , ): \"\"\"Initialize Notion exporter. Args: api_token: Authentication token for Notion API \"\"\" self . notion_exporter = _NotionExporterCore ( notion_token = api_token , export_child_pages = False , extract_page_metadata = True , ) run ( page_ids = None , database_ids = None ) async Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Parameters: page_ids ( List [ str ] , default: None ) \u2013 List of page IDs to export database_ids ( List [ str ] , default: None ) \u2013 List of database IDs to export Returns: List [ NotionDocument ] \u2013 List of NotionDocument objects containing content and metadata Raises: ValueError \u2013 If neither page_ids nor database_ids provided Source code in src/extraction/datasources/notion/exporter.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 async def run ( self , page_ids : List [ str ] = None , database_ids : List [ str ] = None ) -> List [ NotionDocument ]: \"\"\"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Args: page_ids: List of page IDs to export database_ids: List of database IDs to export Returns: List of NotionDocument objects containing content and metadata Raises: ValueError: If neither page_ids nor database_ids provided \"\"\" extracted_objects = await self . notion_exporter . async_export_pages ( page_ids = page_ids , database_ids = database_ids ) objects = [] for object_id , extracted_data in extracted_objects . items (): objects . append ( { \"metadata\" : extracted_data [ \"metadata\" ], \"markdown\" : extracted_data [ \"content\" ], } ) return objects NotionExporterFactory Bases: SingletonFactory Factory for creating NotionExporter instances. Ensures only one instance of NotionExporter is created and reused throughout the application, following the singleton pattern. Source code in src/extraction/datasources/notion/exporter.py 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 class NotionExporterFactory ( SingletonFactory ): \"\"\"Factory for creating NotionExporter instances. Ensures only one instance of NotionExporter is created and reused throughout the application, following the singleton pattern. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration ) -> NotionExporter : \"\"\"Create a NotionExporter instance with the given configuration. Args: configuration: Configuration containing Notion API token Returns: Configured NotionExporter instance \"\"\" return NotionExporter ( configuration . secrets . api_token . get_secret_value () ) Manager NotionDatasourceManager Bases: BaseDatasourceManager [ NotionDocument ] Manager for handling Notion datasource extraction and processing. This class coordinates the reading, parsing, and cleaning of Notion content to produce structured NotionDocument objects ready for further processing. Source code in src/extraction/datasources/notion/manager.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class NotionDatasourceManager ( BaseDatasourceManager [ NotionDocument ]): \"\"\"Manager for handling Notion datasource extraction and processing. This class coordinates the reading, parsing, and cleaning of Notion content to produce structured NotionDocument objects ready for further processing. \"\"\" def __init__ ( self , configuration : NotionDatasourceConfiguration , reader : NotionDatasourceReader , parser : NotionDatasourceParser , cleaner : NotionDatasourceCleaner , ): \"\"\"Initialize the Notion datasource manager. Args: configuration: Configuration for the Notion datasource reader: Component responsible for fetching data from Notion parser: Component responsible for parsing Notion data cleaner: Component responsible for cleaning parsed Notion documents \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner def incremental_sync ( self ): \"\"\" Not implemented. \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) async def full_refresh_sync ( self , ) -> AsyncIterator [ NotionDocument ]: \"\"\"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: An async iterator of cleaned NotionDocument objects \"\"\" objects = await self . reader . read_all_async () for object in objects : document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( document ) if cleaned_document : yield cleaned_document __init__ ( configuration , reader , parser , cleaner ) Initialize the Notion datasource manager. Parameters: configuration ( NotionDatasourceConfiguration ) \u2013 Configuration for the Notion datasource reader ( NotionDatasourceReader ) \u2013 Component responsible for fetching data from Notion parser ( NotionDatasourceParser ) \u2013 Component responsible for parsing Notion data cleaner ( NotionDatasourceCleaner ) \u2013 Component responsible for cleaning parsed Notion documents Source code in src/extraction/datasources/notion/manager.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , configuration : NotionDatasourceConfiguration , reader : NotionDatasourceReader , parser : NotionDatasourceParser , cleaner : NotionDatasourceCleaner , ): \"\"\"Initialize the Notion datasource manager. Args: configuration: Configuration for the Notion datasource reader: Component responsible for fetching data from Notion parser: Component responsible for parsing Notion data cleaner: Component responsible for cleaning parsed Notion documents \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner full_refresh_sync () async Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: AsyncIterator [ NotionDocument ] \u2013 An async iterator of cleaned NotionDocument objects Source code in src/extraction/datasources/notion/manager.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 async def full_refresh_sync ( self , ) -> AsyncIterator [ NotionDocument ]: \"\"\"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: An async iterator of cleaned NotionDocument objects \"\"\" objects = await self . reader . read_all_async () for object in objects : document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( document ) if cleaned_document : yield cleaned_document incremental_sync () Not implemented. Source code in src/extraction/datasources/notion/manager.py 50 51 52 53 54 def incremental_sync ( self ): \"\"\" Not implemented. \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) NotionDatasourceManagerFactory Bases: Factory Factory for creating NotionDatasourceManager instances. This factory is responsible for creating instances of the NotionDatasourceManager class, which manages the extraction and processing of content from Notion databases and pages. Attributes: _configuration_class \u2013 Type of configuration object for Notion datasource Source code in src/extraction/datasources/notion/manager.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class NotionDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceManager instances. This factory is responsible for creating instances of the NotionDatasourceManager class, which manages the extraction and processing of content from Notion databases and pages. Attributes: _configuration_class: Type of configuration object for Notion datasource \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration , ) -> NotionDatasourceManager : \"\"\"Create a new instance of NotionDatasourceManager. This method creates all necessary components (reader, parser, cleaner) and assembles them into a NotionDatasourceManager instance. Args: configuration: Configuration object for the Notion datasource Returns: A fully configured NotionDatasourceManager instance \"\"\" reader = NotionDatasourceReaderFactory . create ( configuration ) parser = NotionDatasourceParserFactory . create ( configuration ) cleaner = NotionDatasourceCleanerFactory . create ( configuration ) return NotionDatasourceManager ( configuration = configuration , reader = reader , parser = parser , cleaner = cleaner , ) Parser NotionDatasourceParser Bases: BaseParser [ NotionDocument ] Parser for Notion content. Transforms raw Notion page data into structured NotionDocument objects. Source code in src/extraction/datasources/notion/parser.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class NotionDatasourceParser ( BaseParser [ NotionDocument ]): \"\"\"Parser for Notion content. Transforms raw Notion page data into structured NotionDocument objects. \"\"\" def __init__ ( self ): \"\"\"Initialize the Notion parser.\"\"\" pass def parse ( self , object : str ) -> NotionDocument : \"\"\"Parse Notion page data into a structured document. Args: object: Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: A NotionDocument containing the parsed content and enhanced metadata. \"\"\" markdown = object [ \"markdown\" ] metadata = self . _extract_metadata ( object [ \"metadata\" ]) return NotionDocument ( text = markdown , metadata = metadata ) @staticmethod def _extract_metadata ( metadata : dict ) -> dict : \"\"\"Process and enhance page metadata. Args: metadata: Raw page metadata dictionary Returns: dict: Enhanced metadata including source and formatted dates \"\"\" metadata [ \"datasource\" ] = \"notion\" metadata [ \"created_date\" ] = metadata [ \"created_time\" ] . split ( \"T\" )[ 0 ] metadata [ \"last_edited_date\" ] = metadata [ \"last_edited_time\" ] . split ( \"T\" )[ 0 ] return metadata __init__ () Initialize the Notion parser. Source code in src/extraction/datasources/notion/parser.py 15 16 17 def __init__ ( self ): \"\"\"Initialize the Notion parser.\"\"\" pass parse ( object ) Parse Notion page data into a structured document. Parameters: object ( str ) \u2013 Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: NotionDocument \u2013 A NotionDocument containing the parsed content and enhanced metadata. Source code in src/extraction/datasources/notion/parser.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def parse ( self , object : str ) -> NotionDocument : \"\"\"Parse Notion page data into a structured document. Args: object: Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: A NotionDocument containing the parsed content and enhanced metadata. \"\"\" markdown = object [ \"markdown\" ] metadata = self . _extract_metadata ( object [ \"metadata\" ]) return NotionDocument ( text = markdown , metadata = metadata ) NotionDatasourceParserFactory Bases: Factory Factory for creating NotionDatasourceParser instances. Creates and configures parser instances for Notion content. Source code in src/extraction/datasources/notion/parser.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class NotionDatasourceParserFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceParser instances. Creates and configures parser instances for Notion content. \"\"\" _configuration_class : NotionDatasourceConfiguration = ( NotionDatasourceConfiguration ) @classmethod def _create_instance ( cls , _ : NotionDatasourceConfiguration ) -> NotionDatasourceParser : \"\"\" Create a NotionDatasourceParser instance. Returns: NotionDatasourceParser: Instance of NotionDatasourceParser. \"\"\" return NotionDatasourceParser () Reader NotionDatasourceReader Bases: BaseReader Reader for extracting documents from Notion workspace. Implements document extraction from Notion pages and databases with support for batched async operations and export limits. Source code in src/extraction/datasources/notion/reader.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 class NotionDatasourceReader ( BaseReader ): \"\"\"Reader for extracting documents from Notion workspace. Implements document extraction from Notion pages and databases with support for batched async operations and export limits. \"\"\" def __init__ ( self , configuration : NotionDatasourceConfiguration , client : Client , exporter : NotionExporter , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize Notion reader. Args: configuration: Settings for Notion access and limits client: Client for Notion API interaction exporter: Component for content export and conversion logger: Logger for logging messages and errors \"\"\" super () . __init__ () self . client = client self . export_batch_size = configuration . export_batch_size self . export_limit = configuration . export_limit self . exporter = exporter self . home_page_database_id = configuration . home_page_database_id self . logger = logger async def read_all_async ( self ) -> List [ NotionDocument ]: \"\"\"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List[NotionDocument]: Collection of processed documents \"\"\" if self . home_page_database_id is None : database_ids = [] page_ids = [] else : database_ids , page_ids = self . _get_ids_from_home_page () database_ids . extend ( self . _get_all_ids ( NotionObjectType . DATABASE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) page_ids . extend ( self . _get_all_ids ( NotionObjectType . PAGE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) # Process IDs database_ids = set ( database_ids ) database_ids . discard ( self . home_page_database_id ) page_ids = set ( page_ids ) # Batch and export chunked_database_ids = list ( chunked ( database_ids , self . export_batch_size ) ) chunked_page_ids = list ( chunked ( page_ids , self . export_batch_size )) databases , databases_failed = await self . _export_documents ( chunked_database_ids , NotionObjectType . DATABASE ) pages , pages_failed = await self . _export_documents ( chunked_page_ids , NotionObjectType . PAGE ) # Log failures if databases_failed : self . logger . warning ( f \"Failed to export { len ( databases_failed ) } databases: { databases_failed } \" ) if pages_failed : self . logger . warning ( f \"Failed to export { len ( pages_failed ) } pages: { pages_failed } \" ) # Apply limit if needed objects = databases + pages return ( objects if self . export_limit is None else objects [: self . export_limit ] ) async def _export_documents ( self , chunked_ids : List [ List [ str ]], objects_type : NotionObjectType ) -> Tuple [ List [ NotionDocument ], List [ str ]]: \"\"\"Export Notion documents in batches with progress tracking. Processes batches of Notion object IDs, exporting them through the exporter component. Handles errors gracefully by tracking failed exports and continuing with the next batch. Args: chunked_ids: List of ID batches, where each batch is a list of IDs to be processed together objects_type: Type of Notion objects to export (PAGE or DATABASE) Returns: Tuple containing: - List of successfully exported NotionDocument objects - List of IDs that failed during export Raises: ValueError: If objects_type is not a valid NotionObjectType \"\"\" all_objects = [] failed_exports = [] number_of_chunks = len ( chunked_ids ) for i , chunk_ids in enumerate ( chunked_ids ): self . logger . info ( f \"[ { i } / { number_of_chunks } ] Reading chunk of Notion { objects_type . name } s.\" ) try : objects = await self . exporter . run ( page_ids = ( chunk_ids if objects_type == NotionObjectType . PAGE else None ), database_ids = ( chunk_ids if objects_type == NotionObjectType . DATABASE else None ), ) all_objects . extend ( objects ) self . logger . debug ( f \"Added { len ( objects ) } { objects_type . name } s\" ) except Exception as e : self . logger . error ( f \"Export failed for { objects_type . name } : { chunk_ids } . { e } \" ) failed_exports . extend ( chunk_ids ) if failed_exports : self . logger . warning ( f \"Failed to export { len ( failed_exports ) } { objects_type . name } s\" ) return all_objects , failed_exports def _get_ids_from_home_page ( self ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Extract database and page IDs from home page database. Queries the configured home page database and extracts IDs for both databases and pages. Returns: Tuple containing: - List of database IDs - List of page IDs \"\"\" self . logger . info ( f \"Reading all object ids from Notion's home page with limit { self . export_limit } ...\" ) response = self . _collect_paginated_api ( function = self . client . databases . query , limit = self . export_limit , database_id = self . home_page_database_id , ) database_ids = [ entry [ \"id\" ] for entry in response if entry [ \"object\" ] == \"database\" ] page_ids = [ entry [ \"id\" ] for entry in response if entry [ \"object\" ] == \"page\" ] self . logger . info ( f \"Found { len ( database_ids ) } database ids and { len ( page_ids ) } page ids in Notion.\" ) return database_ids , page_ids def _get_all_ids ( self , objects_type : NotionObjectType , limit : int = None ) -> List [ str ]: \"\"\"Fetch all IDs for specified Notion object type. Args: objects_type: Type of Notion objects to fetch limit: Maximum number of IDs to fetch (None for unlimited) Returns: List[str]: Collection of object IDs Note: Returns empty list if limit is 0 or negative \"\"\" if limit is not None and limit <= 0 : return [] self . logger . info ( f \"Reading all ids of { objects_type . name } objects from Notion API with limit { limit } ...\" ) params = { \"filter\" : { \"value\" : objects_type . name . lower (), \"property\" : \"object\" , }, } results = NotionDatasourceReader . _collect_paginated_api ( self . client . search , limit , ** params ) object_ids = [ object [ \"id\" ] for object in results ] object_ids = object_ids [: limit ] if limit is not None else object_ids self . logger . info ( f \"Found { len ( object_ids ) } ids of { objects_type . name } objects in Notion.\" ) return object_ids def _get_current_limit ( self , database_ids : List [ str ], page_ids : List [ str ] ) -> int : \"\"\"Calculate remaining object limit based on existing IDs. Args: database_ids: Currently collected database IDs page_ids: Currently collected page IDs Returns: int: Remaining limit (None if no limit configured) Note: Subtracts total of existing IDs from configured export limit \"\"\" return ( self . export_limit - len ( database_ids ) - len ( page_ids ) if self . export_limit else None ) @staticmethod def _collect_paginated_api ( function : Callable [ ... , Any ], limit : int , ** kwargs : Any ) -> List [ Any ]: \"\"\"Collect all results from paginated Notion API endpoint. Args: function: API function to call limit: Maximum number of results to collect **kwargs: Additional arguments for API function Returns: List[Any]: Collected API results \"\"\" next_cursor = kwargs . pop ( \"start_cursor\" , None ) result = [] while True : response = function ( ** kwargs , start_cursor = next_cursor ) result . extend ( response . get ( \"results\" )) if NotionDatasourceReader . _limit_reached ( result , limit ): return result [: limit ] if not NotionDatasourceReader . _has_more_pages ( response ): return result [: limit ] if limit else result next_cursor = response . get ( \"next_cursor\" ) @staticmethod def _limit_reached ( result : List [ dict ], limit : int ) -> bool : \"\"\"Check if result count has reached limit. Args: result: Current results limit: Maximum allowed results Returns: bool: True if limit reached \"\"\" return limit is not None and len ( result ) >= limit @staticmethod def _has_more_pages ( response : dict ) -> bool : \"\"\"Check if more pages are available. Args: response: API response dictionary Returns: bool: True if more pages available \"\"\" return response . get ( \"has_more\" ) and response . get ( \"next_cursor\" ) __init__ ( configuration , client , exporter , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize Notion reader. Parameters: configuration ( NotionDatasourceConfiguration ) \u2013 Settings for Notion access and limits client ( Client ) \u2013 Client for Notion API interaction exporter ( NotionExporter ) \u2013 Component for content export and conversion logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger for logging messages and errors Source code in src/extraction/datasources/notion/reader.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , configuration : NotionDatasourceConfiguration , client : Client , exporter : NotionExporter , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize Notion reader. Args: configuration: Settings for Notion access and limits client: Client for Notion API interaction exporter: Component for content export and conversion logger: Logger for logging messages and errors \"\"\" super () . __init__ () self . client = client self . export_batch_size = configuration . export_batch_size self . export_limit = configuration . export_limit self . exporter = exporter self . home_page_database_id = configuration . home_page_database_id self . logger = logger read_all_async () async Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List [ NotionDocument ] \u2013 List[NotionDocument]: Collection of processed documents Source code in src/extraction/datasources/notion/reader.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 async def read_all_async ( self ) -> List [ NotionDocument ]: \"\"\"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List[NotionDocument]: Collection of processed documents \"\"\" if self . home_page_database_id is None : database_ids = [] page_ids = [] else : database_ids , page_ids = self . _get_ids_from_home_page () database_ids . extend ( self . _get_all_ids ( NotionObjectType . DATABASE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) page_ids . extend ( self . _get_all_ids ( NotionObjectType . PAGE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) # Process IDs database_ids = set ( database_ids ) database_ids . discard ( self . home_page_database_id ) page_ids = set ( page_ids ) # Batch and export chunked_database_ids = list ( chunked ( database_ids , self . export_batch_size ) ) chunked_page_ids = list ( chunked ( page_ids , self . export_batch_size )) databases , databases_failed = await self . _export_documents ( chunked_database_ids , NotionObjectType . DATABASE ) pages , pages_failed = await self . _export_documents ( chunked_page_ids , NotionObjectType . PAGE ) # Log failures if databases_failed : self . logger . warning ( f \"Failed to export { len ( databases_failed ) } databases: { databases_failed } \" ) if pages_failed : self . logger . warning ( f \"Failed to export { len ( pages_failed ) } pages: { pages_failed } \" ) # Apply limit if needed objects = databases + pages return ( objects if self . export_limit is None else objects [: self . export_limit ] ) NotionDatasourceReaderFactory Bases: Factory Factory for creating NotionDatasourceReader instances. This class is responsible for creating instances of NotionDatasourceReader with the provided configuration and Notion client. Attributes: _configuration_class \u2013 The configuration class used to create the NotionDatasourceReader instance. Source code in src/extraction/datasources/notion/reader.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 class NotionDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceReader instances. This class is responsible for creating instances of NotionDatasourceReader with the provided configuration and Notion client. Attributes: _configuration_class: The configuration class used to create the NotionDatasourceReader instance. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration , ) -> NotionDatasourceReader : client = NotionClientFactory . create ( configuration ) exporter = NotionExporterFactory . create ( configuration ) return NotionDatasourceReader ( configuration = configuration , client = client , exporter = exporter , ) NotionObjectType Bases: Enum Enum representing Notion object types. This enum is used to specify the type of Notion object being processed, such as a page or a database. Source code in src/extraction/datasources/notion/reader.py 22 23 24 25 26 27 28 29 30 class NotionObjectType ( Enum ): \"\"\" Enum representing Notion object types. This enum is used to specify the type of Notion object being processed, such as a page or a database. \"\"\" PAGE = \"page\" DATABASE = \"database\"","title":"Notion"},{"location":"src/extraction/datasources/notion/#notion-datasource","text":"This module contains functionality related to the Notion datasource.","title":"Notion Datasource"},{"location":"src/extraction/datasources/notion/#cleaner","text":"","title":"Cleaner"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.cleaner.NotionDatasourceCleaner","text":"Bases: BasicMarkdownCleaner [ NotionDocument ] Cleaner for Notion document content. Implements cleaning logic for Notion databases and pages, removing HTML tags and comments while preserving meaningful content. Note Expects documents to be in markdown format. Source code in src/extraction/datasources/notion/cleaner.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class NotionDatasourceCleaner ( BasicMarkdownCleaner [ NotionDocument ]): \"\"\"Cleaner for Notion document content. Implements cleaning logic for Notion databases and pages, removing HTML tags and comments while preserving meaningful content. Note: Expects documents to be in markdown format. \"\"\" def clean ( self , document : NotionDocument ) -> NotionDocument : \"\"\"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Args: document: Notion document to clean Returns: NotionDocument: Cleaned document, or None if content is empty after cleaning \"\"\" if document . metadata [ \"type\" ] == \"database\" : cleaned_text = self . _clean_database ( document ) document . set_content ( cleaned_text ) if document . metadata [ \"type\" ] == \"page\" : cleaned_text = self . _clean_page ( document ) document . set_content ( cleaned_text ) if self . _has_empty_content ( document ): return None return document def _clean_database ( self , document : NotionDocument ) -> str : \"\"\"Clean Notion database content. Extracts and cleans the text content from a Notion database document, processing any embedded HTML elements. Args: document: Database document to clean Returns: str: Cleaned database content as markdown text \"\"\" return NotionDatasourceCleaner . _parse_html_in_markdown ( document . text ) def _clean_page ( self , document : NotionDocument ) -> str : \"\"\"Clean Notion page content. Extracts and cleans the text content from a Notion page document, processing any embedded HTML elements. Args: document: Page document to clean Returns: str: Cleaned page content as markdown text \"\"\" return NotionDatasourceCleaner . _parse_html_in_markdown ( document . text ) @staticmethod def _parse_html_in_markdown ( md_text : str ) -> str : \"\"\"Process HTML elements within markdown content. Performs two main cleaning operations: 1. Removes HTML comments completely 2. Converts HTML tags to markdown format 3. Removes elements that don't contain alphanumeric characters Args: md_text: Text containing markdown and HTML Returns: str: Cleaned markdown text with HTML properly converted or removed Note: Uses BeautifulSoup for HTML parsing and markdownify for HTML-to-markdown conversion \"\"\" def replace_html ( match ): html_content = match . group ( 0 ) soup = BeautifulSoup ( html_content , \"html.parser\" ) markdown = md ( str ( soup )) if not re . search ( r \"[a-zA-Z0-9]\" , markdown ): return \"\" return markdown md_text = re . sub ( r \"<!--.*?-->\" , \"\" , md_text , flags = re . DOTALL ) html_block_re = re . compile ( r \"<.*?>\" , re . DOTALL ) return re . sub ( html_block_re , replace_html , md_text )","title":"NotionDatasourceCleaner"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.cleaner.NotionDatasourceCleaner.clean","text":"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Parameters: document ( NotionDocument ) \u2013 Notion document to clean Returns: NotionDocument ( NotionDocument ) \u2013 Cleaned document, or None if content is empty after cleaning Source code in src/extraction/datasources/notion/cleaner.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def clean ( self , document : NotionDocument ) -> NotionDocument : \"\"\"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Args: document: Notion document to clean Returns: NotionDocument: Cleaned document, or None if content is empty after cleaning \"\"\" if document . metadata [ \"type\" ] == \"database\" : cleaned_text = self . _clean_database ( document ) document . set_content ( cleaned_text ) if document . metadata [ \"type\" ] == \"page\" : cleaned_text = self . _clean_page ( document ) document . set_content ( cleaned_text ) if self . _has_empty_content ( document ): return None return document","title":"clean"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.cleaner.NotionDatasourceCleanerFactory","text":"Bases: Factory Factory for creating NotionDatasourceCleaner instances. This factory is responsible for creating instances of NotionDatasourceCleaner with the appropriate configuration. Source code in src/extraction/datasources/notion/cleaner.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class NotionDatasourceCleanerFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceCleaner instances. This factory is responsible for creating instances of NotionDatasourceCleaner with the appropriate configuration. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , _ : NotionDatasourceConfiguration ) -> NotionDatasourceCleaner : \"\"\"Create a new instance of NotionDatasourceCleaner. Args: configuration: Configuration for the cleaner Returns: NotionDatasourceCleaner: Instance of NotionDatasourceCleaner \"\"\" return NotionDatasourceCleaner ()","title":"NotionDatasourceCleanerFactory"},{"location":"src/extraction/datasources/notion/#client","text":"","title":"Client"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.client.NotionClientFactory","text":"Bases: SingletonFactory Factory for creating and managing Notion API client instances. This singleton factory ensures that only one Notion client instance is created for a specific configuration, promoting resource efficiency and consistency. Client instances are created using the Notion API authentication token from the provided configuration. The factory follows the singleton pattern to prevent multiple instantiations of clients with identical configurations. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object used to create the client Source code in src/extraction/datasources/notion/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class NotionClientFactory ( SingletonFactory ): \"\"\"Factory for creating and managing Notion API client instances. This singleton factory ensures that only one Notion client instance is created for a specific configuration, promoting resource efficiency and consistency. Client instances are created using the Notion API authentication token from the provided configuration. The factory follows the singleton pattern to prevent multiple instantiations of clients with identical configurations. Attributes: _configuration_class: Type of configuration object used to create the client \"\"\" _configuration_class : Type = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration ) -> Client : \"\"\"Create a new instance of the Notion API client. This method extracts the API token from the provided configuration's secrets and uses it to authenticate a new Notion client. Args: configuration: Configuration object containing Notion API credentials and other settings. Returns: A configured Notion API client instance ready for making API calls. \"\"\" return Client ( auth = configuration . secrets . api_token . get_secret_value ())","title":"NotionClientFactory"},{"location":"src/extraction/datasources/notion/#configuration","text":"","title":"Configuration"},{"location":"src/extraction/datasources/notion/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.document.NotionDocument","text":"Bases: BaseDocument Document representation for Notion page content. Extends BaseDocument to handle Notion-specific document processing including metadata handling and filtering for embeddings and LLM contexts. Source code in src/extraction/datasources/notion/document.py 4 5 6 7 8 9 10 11 class NotionDocument ( BaseDocument ): \"\"\"Document representation for Notion page content. Extends BaseDocument to handle Notion-specific document processing including metadata handling and filtering for embeddings and LLM contexts. \"\"\" pass","title":"NotionDocument"},{"location":"src/extraction/datasources/notion/#exporter","text":"","title":"Exporter"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.exporter.NotionExporter","text":"Exporter for converting Notion pages to markdown documents. Provides a high-level interface for extracting Notion content and converting it to structured NotionDocument instances. Source code in src/extraction/datasources/notion/exporter.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 class NotionExporter : \"\"\"Exporter for converting Notion pages to markdown documents. Provides a high-level interface for extracting Notion content and converting it to structured NotionDocument instances. \"\"\" def __init__ ( self , api_token : str , ): \"\"\"Initialize Notion exporter. Args: api_token: Authentication token for Notion API \"\"\" self . notion_exporter = _NotionExporterCore ( notion_token = api_token , export_child_pages = False , extract_page_metadata = True , ) async def run ( self , page_ids : List [ str ] = None , database_ids : List [ str ] = None ) -> List [ NotionDocument ]: \"\"\"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Args: page_ids: List of page IDs to export database_ids: List of database IDs to export Returns: List of NotionDocument objects containing content and metadata Raises: ValueError: If neither page_ids nor database_ids provided \"\"\" extracted_objects = await self . notion_exporter . async_export_pages ( page_ids = page_ids , database_ids = database_ids ) objects = [] for object_id , extracted_data in extracted_objects . items (): objects . append ( { \"metadata\" : extracted_data [ \"metadata\" ], \"markdown\" : extracted_data [ \"content\" ], } ) return objects","title":"NotionExporter"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.exporter.NotionExporter.__init__","text":"Initialize Notion exporter. Parameters: api_token ( str ) \u2013 Authentication token for Notion API Source code in src/extraction/datasources/notion/exporter.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def __init__ ( self , api_token : str , ): \"\"\"Initialize Notion exporter. Args: api_token: Authentication token for Notion API \"\"\" self . notion_exporter = _NotionExporterCore ( notion_token = api_token , export_child_pages = False , extract_page_metadata = True , )","title":"__init__"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.exporter.NotionExporter.run","text":"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Parameters: page_ids ( List [ str ] , default: None ) \u2013 List of page IDs to export database_ids ( List [ str ] , default: None ) \u2013 List of database IDs to export Returns: List [ NotionDocument ] \u2013 List of NotionDocument objects containing content and metadata Raises: ValueError \u2013 If neither page_ids nor database_ids provided Source code in src/extraction/datasources/notion/exporter.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 async def run ( self , page_ids : List [ str ] = None , database_ids : List [ str ] = None ) -> List [ NotionDocument ]: \"\"\"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Args: page_ids: List of page IDs to export database_ids: List of database IDs to export Returns: List of NotionDocument objects containing content and metadata Raises: ValueError: If neither page_ids nor database_ids provided \"\"\" extracted_objects = await self . notion_exporter . async_export_pages ( page_ids = page_ids , database_ids = database_ids ) objects = [] for object_id , extracted_data in extracted_objects . items (): objects . append ( { \"metadata\" : extracted_data [ \"metadata\" ], \"markdown\" : extracted_data [ \"content\" ], } ) return objects","title":"run"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.exporter.NotionExporterFactory","text":"Bases: SingletonFactory Factory for creating NotionExporter instances. Ensures only one instance of NotionExporter is created and reused throughout the application, following the singleton pattern. Source code in src/extraction/datasources/notion/exporter.py 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 class NotionExporterFactory ( SingletonFactory ): \"\"\"Factory for creating NotionExporter instances. Ensures only one instance of NotionExporter is created and reused throughout the application, following the singleton pattern. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration ) -> NotionExporter : \"\"\"Create a NotionExporter instance with the given configuration. Args: configuration: Configuration containing Notion API token Returns: Configured NotionExporter instance \"\"\" return NotionExporter ( configuration . secrets . api_token . get_secret_value () )","title":"NotionExporterFactory"},{"location":"src/extraction/datasources/notion/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManager","text":"Bases: BaseDatasourceManager [ NotionDocument ] Manager for handling Notion datasource extraction and processing. This class coordinates the reading, parsing, and cleaning of Notion content to produce structured NotionDocument objects ready for further processing. Source code in src/extraction/datasources/notion/manager.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class NotionDatasourceManager ( BaseDatasourceManager [ NotionDocument ]): \"\"\"Manager for handling Notion datasource extraction and processing. This class coordinates the reading, parsing, and cleaning of Notion content to produce structured NotionDocument objects ready for further processing. \"\"\" def __init__ ( self , configuration : NotionDatasourceConfiguration , reader : NotionDatasourceReader , parser : NotionDatasourceParser , cleaner : NotionDatasourceCleaner , ): \"\"\"Initialize the Notion datasource manager. Args: configuration: Configuration for the Notion datasource reader: Component responsible for fetching data from Notion parser: Component responsible for parsing Notion data cleaner: Component responsible for cleaning parsed Notion documents \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner def incremental_sync ( self ): \"\"\" Not implemented. \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) async def full_refresh_sync ( self , ) -> AsyncIterator [ NotionDocument ]: \"\"\"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: An async iterator of cleaned NotionDocument objects \"\"\" objects = await self . reader . read_all_async () for object in objects : document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( document ) if cleaned_document : yield cleaned_document","title":"NotionDatasourceManager"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManager.__init__","text":"Initialize the Notion datasource manager. Parameters: configuration ( NotionDatasourceConfiguration ) \u2013 Configuration for the Notion datasource reader ( NotionDatasourceReader ) \u2013 Component responsible for fetching data from Notion parser ( NotionDatasourceParser ) \u2013 Component responsible for parsing Notion data cleaner ( NotionDatasourceCleaner ) \u2013 Component responsible for cleaning parsed Notion documents Source code in src/extraction/datasources/notion/manager.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , configuration : NotionDatasourceConfiguration , reader : NotionDatasourceReader , parser : NotionDatasourceParser , cleaner : NotionDatasourceCleaner , ): \"\"\"Initialize the Notion datasource manager. Args: configuration: Configuration for the Notion datasource reader: Component responsible for fetching data from Notion parser: Component responsible for parsing Notion data cleaner: Component responsible for cleaning parsed Notion documents \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner","title":"__init__"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManager.full_refresh_sync","text":"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: AsyncIterator [ NotionDocument ] \u2013 An async iterator of cleaned NotionDocument objects Source code in src/extraction/datasources/notion/manager.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 async def full_refresh_sync ( self , ) -> AsyncIterator [ NotionDocument ]: \"\"\"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: An async iterator of cleaned NotionDocument objects \"\"\" objects = await self . reader . read_all_async () for object in objects : document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( document ) if cleaned_document : yield cleaned_document","title":"full_refresh_sync"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManager.incremental_sync","text":"Not implemented. Source code in src/extraction/datasources/notion/manager.py 50 51 52 53 54 def incremental_sync ( self ): \"\"\" Not implemented. \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" )","title":"incremental_sync"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManagerFactory","text":"Bases: Factory Factory for creating NotionDatasourceManager instances. This factory is responsible for creating instances of the NotionDatasourceManager class, which manages the extraction and processing of content from Notion databases and pages. Attributes: _configuration_class \u2013 Type of configuration object for Notion datasource Source code in src/extraction/datasources/notion/manager.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class NotionDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceManager instances. This factory is responsible for creating instances of the NotionDatasourceManager class, which manages the extraction and processing of content from Notion databases and pages. Attributes: _configuration_class: Type of configuration object for Notion datasource \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration , ) -> NotionDatasourceManager : \"\"\"Create a new instance of NotionDatasourceManager. This method creates all necessary components (reader, parser, cleaner) and assembles them into a NotionDatasourceManager instance. Args: configuration: Configuration object for the Notion datasource Returns: A fully configured NotionDatasourceManager instance \"\"\" reader = NotionDatasourceReaderFactory . create ( configuration ) parser = NotionDatasourceParserFactory . create ( configuration ) cleaner = NotionDatasourceCleanerFactory . create ( configuration ) return NotionDatasourceManager ( configuration = configuration , reader = reader , parser = parser , cleaner = cleaner , )","title":"NotionDatasourceManagerFactory"},{"location":"src/extraction/datasources/notion/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.parser.NotionDatasourceParser","text":"Bases: BaseParser [ NotionDocument ] Parser for Notion content. Transforms raw Notion page data into structured NotionDocument objects. Source code in src/extraction/datasources/notion/parser.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class NotionDatasourceParser ( BaseParser [ NotionDocument ]): \"\"\"Parser for Notion content. Transforms raw Notion page data into structured NotionDocument objects. \"\"\" def __init__ ( self ): \"\"\"Initialize the Notion parser.\"\"\" pass def parse ( self , object : str ) -> NotionDocument : \"\"\"Parse Notion page data into a structured document. Args: object: Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: A NotionDocument containing the parsed content and enhanced metadata. \"\"\" markdown = object [ \"markdown\" ] metadata = self . _extract_metadata ( object [ \"metadata\" ]) return NotionDocument ( text = markdown , metadata = metadata ) @staticmethod def _extract_metadata ( metadata : dict ) -> dict : \"\"\"Process and enhance page metadata. Args: metadata: Raw page metadata dictionary Returns: dict: Enhanced metadata including source and formatted dates \"\"\" metadata [ \"datasource\" ] = \"notion\" metadata [ \"created_date\" ] = metadata [ \"created_time\" ] . split ( \"T\" )[ 0 ] metadata [ \"last_edited_date\" ] = metadata [ \"last_edited_time\" ] . split ( \"T\" )[ 0 ] return metadata","title":"NotionDatasourceParser"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.parser.NotionDatasourceParser.__init__","text":"Initialize the Notion parser. Source code in src/extraction/datasources/notion/parser.py 15 16 17 def __init__ ( self ): \"\"\"Initialize the Notion parser.\"\"\" pass","title":"__init__"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.parser.NotionDatasourceParser.parse","text":"Parse Notion page data into a structured document. Parameters: object ( str ) \u2013 Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: NotionDocument \u2013 A NotionDocument containing the parsed content and enhanced metadata. Source code in src/extraction/datasources/notion/parser.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def parse ( self , object : str ) -> NotionDocument : \"\"\"Parse Notion page data into a structured document. Args: object: Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: A NotionDocument containing the parsed content and enhanced metadata. \"\"\" markdown = object [ \"markdown\" ] metadata = self . _extract_metadata ( object [ \"metadata\" ]) return NotionDocument ( text = markdown , metadata = metadata )","title":"parse"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.parser.NotionDatasourceParserFactory","text":"Bases: Factory Factory for creating NotionDatasourceParser instances. Creates and configures parser instances for Notion content. Source code in src/extraction/datasources/notion/parser.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class NotionDatasourceParserFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceParser instances. Creates and configures parser instances for Notion content. \"\"\" _configuration_class : NotionDatasourceConfiguration = ( NotionDatasourceConfiguration ) @classmethod def _create_instance ( cls , _ : NotionDatasourceConfiguration ) -> NotionDatasourceParser : \"\"\" Create a NotionDatasourceParser instance. Returns: NotionDatasourceParser: Instance of NotionDatasourceParser. \"\"\" return NotionDatasourceParser ()","title":"NotionDatasourceParserFactory"},{"location":"src/extraction/datasources/notion/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionDatasourceReader","text":"Bases: BaseReader Reader for extracting documents from Notion workspace. Implements document extraction from Notion pages and databases with support for batched async operations and export limits. Source code in src/extraction/datasources/notion/reader.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 class NotionDatasourceReader ( BaseReader ): \"\"\"Reader for extracting documents from Notion workspace. Implements document extraction from Notion pages and databases with support for batched async operations and export limits. \"\"\" def __init__ ( self , configuration : NotionDatasourceConfiguration , client : Client , exporter : NotionExporter , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize Notion reader. Args: configuration: Settings for Notion access and limits client: Client for Notion API interaction exporter: Component for content export and conversion logger: Logger for logging messages and errors \"\"\" super () . __init__ () self . client = client self . export_batch_size = configuration . export_batch_size self . export_limit = configuration . export_limit self . exporter = exporter self . home_page_database_id = configuration . home_page_database_id self . logger = logger async def read_all_async ( self ) -> List [ NotionDocument ]: \"\"\"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List[NotionDocument]: Collection of processed documents \"\"\" if self . home_page_database_id is None : database_ids = [] page_ids = [] else : database_ids , page_ids = self . _get_ids_from_home_page () database_ids . extend ( self . _get_all_ids ( NotionObjectType . DATABASE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) page_ids . extend ( self . _get_all_ids ( NotionObjectType . PAGE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) # Process IDs database_ids = set ( database_ids ) database_ids . discard ( self . home_page_database_id ) page_ids = set ( page_ids ) # Batch and export chunked_database_ids = list ( chunked ( database_ids , self . export_batch_size ) ) chunked_page_ids = list ( chunked ( page_ids , self . export_batch_size )) databases , databases_failed = await self . _export_documents ( chunked_database_ids , NotionObjectType . DATABASE ) pages , pages_failed = await self . _export_documents ( chunked_page_ids , NotionObjectType . PAGE ) # Log failures if databases_failed : self . logger . warning ( f \"Failed to export { len ( databases_failed ) } databases: { databases_failed } \" ) if pages_failed : self . logger . warning ( f \"Failed to export { len ( pages_failed ) } pages: { pages_failed } \" ) # Apply limit if needed objects = databases + pages return ( objects if self . export_limit is None else objects [: self . export_limit ] ) async def _export_documents ( self , chunked_ids : List [ List [ str ]], objects_type : NotionObjectType ) -> Tuple [ List [ NotionDocument ], List [ str ]]: \"\"\"Export Notion documents in batches with progress tracking. Processes batches of Notion object IDs, exporting them through the exporter component. Handles errors gracefully by tracking failed exports and continuing with the next batch. Args: chunked_ids: List of ID batches, where each batch is a list of IDs to be processed together objects_type: Type of Notion objects to export (PAGE or DATABASE) Returns: Tuple containing: - List of successfully exported NotionDocument objects - List of IDs that failed during export Raises: ValueError: If objects_type is not a valid NotionObjectType \"\"\" all_objects = [] failed_exports = [] number_of_chunks = len ( chunked_ids ) for i , chunk_ids in enumerate ( chunked_ids ): self . logger . info ( f \"[ { i } / { number_of_chunks } ] Reading chunk of Notion { objects_type . name } s.\" ) try : objects = await self . exporter . run ( page_ids = ( chunk_ids if objects_type == NotionObjectType . PAGE else None ), database_ids = ( chunk_ids if objects_type == NotionObjectType . DATABASE else None ), ) all_objects . extend ( objects ) self . logger . debug ( f \"Added { len ( objects ) } { objects_type . name } s\" ) except Exception as e : self . logger . error ( f \"Export failed for { objects_type . name } : { chunk_ids } . { e } \" ) failed_exports . extend ( chunk_ids ) if failed_exports : self . logger . warning ( f \"Failed to export { len ( failed_exports ) } { objects_type . name } s\" ) return all_objects , failed_exports def _get_ids_from_home_page ( self ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Extract database and page IDs from home page database. Queries the configured home page database and extracts IDs for both databases and pages. Returns: Tuple containing: - List of database IDs - List of page IDs \"\"\" self . logger . info ( f \"Reading all object ids from Notion's home page with limit { self . export_limit } ...\" ) response = self . _collect_paginated_api ( function = self . client . databases . query , limit = self . export_limit , database_id = self . home_page_database_id , ) database_ids = [ entry [ \"id\" ] for entry in response if entry [ \"object\" ] == \"database\" ] page_ids = [ entry [ \"id\" ] for entry in response if entry [ \"object\" ] == \"page\" ] self . logger . info ( f \"Found { len ( database_ids ) } database ids and { len ( page_ids ) } page ids in Notion.\" ) return database_ids , page_ids def _get_all_ids ( self , objects_type : NotionObjectType , limit : int = None ) -> List [ str ]: \"\"\"Fetch all IDs for specified Notion object type. Args: objects_type: Type of Notion objects to fetch limit: Maximum number of IDs to fetch (None for unlimited) Returns: List[str]: Collection of object IDs Note: Returns empty list if limit is 0 or negative \"\"\" if limit is not None and limit <= 0 : return [] self . logger . info ( f \"Reading all ids of { objects_type . name } objects from Notion API with limit { limit } ...\" ) params = { \"filter\" : { \"value\" : objects_type . name . lower (), \"property\" : \"object\" , }, } results = NotionDatasourceReader . _collect_paginated_api ( self . client . search , limit , ** params ) object_ids = [ object [ \"id\" ] for object in results ] object_ids = object_ids [: limit ] if limit is not None else object_ids self . logger . info ( f \"Found { len ( object_ids ) } ids of { objects_type . name } objects in Notion.\" ) return object_ids def _get_current_limit ( self , database_ids : List [ str ], page_ids : List [ str ] ) -> int : \"\"\"Calculate remaining object limit based on existing IDs. Args: database_ids: Currently collected database IDs page_ids: Currently collected page IDs Returns: int: Remaining limit (None if no limit configured) Note: Subtracts total of existing IDs from configured export limit \"\"\" return ( self . export_limit - len ( database_ids ) - len ( page_ids ) if self . export_limit else None ) @staticmethod def _collect_paginated_api ( function : Callable [ ... , Any ], limit : int , ** kwargs : Any ) -> List [ Any ]: \"\"\"Collect all results from paginated Notion API endpoint. Args: function: API function to call limit: Maximum number of results to collect **kwargs: Additional arguments for API function Returns: List[Any]: Collected API results \"\"\" next_cursor = kwargs . pop ( \"start_cursor\" , None ) result = [] while True : response = function ( ** kwargs , start_cursor = next_cursor ) result . extend ( response . get ( \"results\" )) if NotionDatasourceReader . _limit_reached ( result , limit ): return result [: limit ] if not NotionDatasourceReader . _has_more_pages ( response ): return result [: limit ] if limit else result next_cursor = response . get ( \"next_cursor\" ) @staticmethod def _limit_reached ( result : List [ dict ], limit : int ) -> bool : \"\"\"Check if result count has reached limit. Args: result: Current results limit: Maximum allowed results Returns: bool: True if limit reached \"\"\" return limit is not None and len ( result ) >= limit @staticmethod def _has_more_pages ( response : dict ) -> bool : \"\"\"Check if more pages are available. Args: response: API response dictionary Returns: bool: True if more pages available \"\"\" return response . get ( \"has_more\" ) and response . get ( \"next_cursor\" )","title":"NotionDatasourceReader"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionDatasourceReader.__init__","text":"Initialize Notion reader. Parameters: configuration ( NotionDatasourceConfiguration ) \u2013 Settings for Notion access and limits client ( Client ) \u2013 Client for Notion API interaction exporter ( NotionExporter ) \u2013 Component for content export and conversion logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger for logging messages and errors Source code in src/extraction/datasources/notion/reader.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , configuration : NotionDatasourceConfiguration , client : Client , exporter : NotionExporter , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize Notion reader. Args: configuration: Settings for Notion access and limits client: Client for Notion API interaction exporter: Component for content export and conversion logger: Logger for logging messages and errors \"\"\" super () . __init__ () self . client = client self . export_batch_size = configuration . export_batch_size self . export_limit = configuration . export_limit self . exporter = exporter self . home_page_database_id = configuration . home_page_database_id self . logger = logger","title":"__init__"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionDatasourceReader.read_all_async","text":"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List [ NotionDocument ] \u2013 List[NotionDocument]: Collection of processed documents Source code in src/extraction/datasources/notion/reader.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 async def read_all_async ( self ) -> List [ NotionDocument ]: \"\"\"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List[NotionDocument]: Collection of processed documents \"\"\" if self . home_page_database_id is None : database_ids = [] page_ids = [] else : database_ids , page_ids = self . _get_ids_from_home_page () database_ids . extend ( self . _get_all_ids ( NotionObjectType . DATABASE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) page_ids . extend ( self . _get_all_ids ( NotionObjectType . PAGE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) # Process IDs database_ids = set ( database_ids ) database_ids . discard ( self . home_page_database_id ) page_ids = set ( page_ids ) # Batch and export chunked_database_ids = list ( chunked ( database_ids , self . export_batch_size ) ) chunked_page_ids = list ( chunked ( page_ids , self . export_batch_size )) databases , databases_failed = await self . _export_documents ( chunked_database_ids , NotionObjectType . DATABASE ) pages , pages_failed = await self . _export_documents ( chunked_page_ids , NotionObjectType . PAGE ) # Log failures if databases_failed : self . logger . warning ( f \"Failed to export { len ( databases_failed ) } databases: { databases_failed } \" ) if pages_failed : self . logger . warning ( f \"Failed to export { len ( pages_failed ) } pages: { pages_failed } \" ) # Apply limit if needed objects = databases + pages return ( objects if self . export_limit is None else objects [: self . export_limit ] )","title":"read_all_async"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionDatasourceReaderFactory","text":"Bases: Factory Factory for creating NotionDatasourceReader instances. This class is responsible for creating instances of NotionDatasourceReader with the provided configuration and Notion client. Attributes: _configuration_class \u2013 The configuration class used to create the NotionDatasourceReader instance. Source code in src/extraction/datasources/notion/reader.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 class NotionDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceReader instances. This class is responsible for creating instances of NotionDatasourceReader with the provided configuration and Notion client. Attributes: _configuration_class: The configuration class used to create the NotionDatasourceReader instance. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration , ) -> NotionDatasourceReader : client = NotionClientFactory . create ( configuration ) exporter = NotionExporterFactory . create ( configuration ) return NotionDatasourceReader ( configuration = configuration , client = client , exporter = exporter , )","title":"NotionDatasourceReaderFactory"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionObjectType","text":"Bases: Enum Enum representing Notion object types. This enum is used to specify the type of Notion object being processed, such as a page or a database. Source code in src/extraction/datasources/notion/reader.py 22 23 24 25 26 27 28 29 30 class NotionObjectType ( Enum ): \"\"\" Enum representing Notion object types. This enum is used to specify the type of Notion object being processed, such as a page or a database. \"\"\" PAGE = \"page\" DATABASE = \"database\"","title":"NotionObjectType"},{"location":"src/extraction/datasources/pdf/","text":"Pdf Datasource This module contains functionality related to the Pdf datasource. Configuration PDFDatasourceConfiguration Bases: DatasourceConfiguration Configuration for PDF data source. This class defines the configuration parameters required for extracting data from PDF files. It inherits from the base DatasourceConfiguration class. Source code in src/extraction/datasources/pdf/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 class PDFDatasourceConfiguration ( DatasourceConfiguration ): \"\"\"Configuration for PDF data source. This class defines the configuration parameters required for extracting data from PDF files. It inherits from the base DatasourceConfiguration class. \"\"\" name : Literal [ DatasourceName . PDF ] = Field ( ... , description = \"The name of the data source.\" ) base_path : str = Field ( ... , description = \"Base path to the directory containing PDF files\" ) Document PDFDocument Bases: BaseDocument Document representation for PDF file content. Extends BaseDocument to handle PDF-specific document processing including metadata filtering for embeddings and LLM contexts. Source code in src/extraction/datasources/pdf/document.py 4 5 6 7 8 9 10 11 class PDFDocument ( BaseDocument ): \"\"\"Document representation for PDF file content. Extends BaseDocument to handle PDF-specific document processing including metadata filtering for embeddings and LLM contexts. \"\"\" pass Manager PDFDatasourceManagerFactory Bases: Factory Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object Source code in src/extraction/datasources/pdf/manager.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class PDFDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class: Type of configuration object \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : PDFDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create an instance of the PDF datasource manager. This method constructs a BasicDatasourceManager by creating the appropriate reader and parser based on the provided configuration. Args: configuration: Configuration specifying how to set up the PDF datasource manager, reader, and parser. Returns: A configured BasicDatasourceManager instance for handling PDF documents. \"\"\" reader = PDFDatasourceReaderFactory . create ( configuration ) parser = PDFDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration = configuration , reader = reader , parser = parser , ) Parser PDFDatasourceParser Bases: BaseParser [ PDFDocument ] Parser for PDF documents that converts them to structured PDFDocument objects. Uses MarkItDown to convert PDF files to markdown format for easier processing. Source code in src/extraction/datasources/pdf/parser.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class PDFDatasourceParser ( BaseParser [ PDFDocument ]): \"\"\" Parser for PDF documents that converts them to structured PDFDocument objects. Uses MarkItDown to convert PDF files to markdown format for easier processing. \"\"\" def __init__ ( self , parser : MarkItDown = MarkItDown ()): \"\"\" Initialize the PDF parser. Attributes: parser: MarkItDown parser instance for PDF to markdown conversion \"\"\" self . parser = parser def parse ( self , file_path : str ) -> PDFDocument : \"\"\" Parses the given PDF file into a structured document. Args: file_path: Path to the PDF file Returns: PDFDocument object containing the parsed content and metadata \"\"\" markdown = self . parser . convert ( file_path , file_extension = \".pdf\" ) . text_content metadata = self . _extract_metadata ( file_path ) return PDFDocument ( text = markdown , metadata = metadata ) def _extract_metadata ( self , file_path : str ) -> dict : \"\"\" Extract and process PDF metadata from the file. Args: file_path: Path to the PDF file Returns: Processed metadata dictionary with standardized fields \"\"\" metadata = default_file_metadata_func ( file_path ) metadata . update ( { \"datasource\" : \"pdf\" , \"format\" : \"pdf\" , \"url\" : None , \"title\" : os . path . basename ( file_path ), \"last_edited_date\" : metadata [ \"last_modified_date\" ], \"created_date\" : metadata [ \"creation_date\" ], } ) del metadata [ \"last_modified_date\" ] del metadata [ \"creation_date\" ] return metadata __init__ ( parser = MarkItDown ()) Initialize the PDF parser. Attributes: parser \u2013 MarkItDown parser instance for PDF to markdown conversion Source code in src/extraction/datasources/pdf/parser.py 20 21 22 23 24 25 26 27 def __init__ ( self , parser : MarkItDown = MarkItDown ()): \"\"\" Initialize the PDF parser. Attributes: parser: MarkItDown parser instance for PDF to markdown conversion \"\"\" self . parser = parser parse ( file_path ) Parses the given PDF file into a structured document. Parameters: file_path ( str ) \u2013 Path to the PDF file Returns: PDFDocument \u2013 PDFDocument object containing the parsed content and metadata Source code in src/extraction/datasources/pdf/parser.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def parse ( self , file_path : str ) -> PDFDocument : \"\"\" Parses the given PDF file into a structured document. Args: file_path: Path to the PDF file Returns: PDFDocument object containing the parsed content and metadata \"\"\" markdown = self . parser . convert ( file_path , file_extension = \".pdf\" ) . text_content metadata = self . _extract_metadata ( file_path ) return PDFDocument ( text = markdown , metadata = metadata ) PDFDatasourceParserFactory Bases: Factory Factory for creating PDF parser instances. Creates and configures PDFDatasourceParser objects according to the provided configuration. Source code in src/extraction/datasources/pdf/parser.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class PDFDatasourceParserFactory ( Factory ): \"\"\" Factory for creating PDF parser instances. Creates and configures PDFDatasourceParser objects according to the provided configuration. \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , _ : PDFDatasourceConfiguration ) -> PDFDatasourceParser : \"\"\" Creates a new instance of the PDF parser. Args: _: Configuration object for the parser (not used in this implementation) Returns: PDFDatasourceParser: Configured parser instance \"\"\" return PDFDatasourceParser () Reader PDFDatasourceReader Bases: BaseReader Source code in src/extraction/datasources/pdf/reader.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class PDFDatasourceReader ( BaseReader ): def __init__ ( self , configuration : PDFDatasourceConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize PDF reader. Args: configuration: Settings for PDF processing logger: Logger instance for logging messages \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . base_path = configuration . base_path self . logger = logger async def read_all_async ( self ) -> AsyncIterator [ str ]: \"\"\"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator[str]: An asynchronous iterator of PDF file paths \"\"\" self . logger . info ( f \"Reading PDF files from ' { self . base_path } ' with limit { self . export_limit } \" ) pdf_files = [ f for f in os . listdir ( self . base_path ) if f . endswith ( \".pdf\" ) ] files_to_load = ( pdf_files if self . export_limit is None else pdf_files [: self . export_limit ] ) for i , file_name in enumerate ( files_to_load ): self . logger . info ( f \"[ { i } / { self . export_limit } ] Reading PDF file ' { file_name } '\" ) file_path = os . path . join ( self . base_path , file_name ) if os . path . isfile ( file_path ): yield file_path __init__ ( configuration , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize PDF reader. Parameters: configuration ( PDFDatasourceConfiguration ) \u2013 Settings for PDF processing logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/extraction/datasources/pdf/reader.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , configuration : PDFDatasourceConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize PDF reader. Args: configuration: Settings for PDF processing logger: Logger instance for logging messages \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . base_path = configuration . base_path self . logger = logger read_all_async () async Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator [ str ] \u2013 AsyncIterator[str]: An asynchronous iterator of PDF file paths Source code in src/extraction/datasources/pdf/reader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 async def read_all_async ( self ) -> AsyncIterator [ str ]: \"\"\"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator[str]: An asynchronous iterator of PDF file paths \"\"\" self . logger . info ( f \"Reading PDF files from ' { self . base_path } ' with limit { self . export_limit } \" ) pdf_files = [ f for f in os . listdir ( self . base_path ) if f . endswith ( \".pdf\" ) ] files_to_load = ( pdf_files if self . export_limit is None else pdf_files [: self . export_limit ] ) for i , file_name in enumerate ( files_to_load ): self . logger . info ( f \"[ { i } / { self . export_limit } ] Reading PDF file ' { file_name } '\" ) file_path = os . path . join ( self . base_path , file_name ) if os . path . isfile ( file_path ): yield file_path PDFDatasourceReaderFactory Bases: Factory Factory for creating PDF reader instances. Implements the factory pattern to produce configured PDFDatasourceReader objects based on the provided configuration settings. Source code in src/extraction/datasources/pdf/reader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class PDFDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating PDF reader instances. Implements the factory pattern to produce configured PDFDatasourceReader objects based on the provided configuration settings. \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : PDFDatasourceConfiguration ) -> PDFDatasourceReader : \"\"\"Create a new PDFDatasourceReader with the specified configuration. Args: configuration: Settings that control PDF processing behavior including base path and export limits Returns: PDFDatasourceReader: A fully configured reader instance ready for use \"\"\" return PDFDatasourceReader ( configuration = configuration )","title":"Pdf"},{"location":"src/extraction/datasources/pdf/#pdf-datasource","text":"This module contains functionality related to the Pdf datasource.","title":"Pdf Datasource"},{"location":"src/extraction/datasources/pdf/#configuration","text":"","title":"Configuration"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.configuration.PDFDatasourceConfiguration","text":"Bases: DatasourceConfiguration Configuration for PDF data source. This class defines the configuration parameters required for extracting data from PDF files. It inherits from the base DatasourceConfiguration class. Source code in src/extraction/datasources/pdf/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 class PDFDatasourceConfiguration ( DatasourceConfiguration ): \"\"\"Configuration for PDF data source. This class defines the configuration parameters required for extracting data from PDF files. It inherits from the base DatasourceConfiguration class. \"\"\" name : Literal [ DatasourceName . PDF ] = Field ( ... , description = \"The name of the data source.\" ) base_path : str = Field ( ... , description = \"Base path to the directory containing PDF files\" )","title":"PDFDatasourceConfiguration"},{"location":"src/extraction/datasources/pdf/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.document.PDFDocument","text":"Bases: BaseDocument Document representation for PDF file content. Extends BaseDocument to handle PDF-specific document processing including metadata filtering for embeddings and LLM contexts. Source code in src/extraction/datasources/pdf/document.py 4 5 6 7 8 9 10 11 class PDFDocument ( BaseDocument ): \"\"\"Document representation for PDF file content. Extends BaseDocument to handle PDF-specific document processing including metadata filtering for embeddings and LLM contexts. \"\"\" pass","title":"PDFDocument"},{"location":"src/extraction/datasources/pdf/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.manager.PDFDatasourceManagerFactory","text":"Bases: Factory Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object Source code in src/extraction/datasources/pdf/manager.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class PDFDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class: Type of configuration object \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : PDFDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create an instance of the PDF datasource manager. This method constructs a BasicDatasourceManager by creating the appropriate reader and parser based on the provided configuration. Args: configuration: Configuration specifying how to set up the PDF datasource manager, reader, and parser. Returns: A configured BasicDatasourceManager instance for handling PDF documents. \"\"\" reader = PDFDatasourceReaderFactory . create ( configuration ) parser = PDFDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration = configuration , reader = reader , parser = parser , )","title":"PDFDatasourceManagerFactory"},{"location":"src/extraction/datasources/pdf/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.parser.PDFDatasourceParser","text":"Bases: BaseParser [ PDFDocument ] Parser for PDF documents that converts them to structured PDFDocument objects. Uses MarkItDown to convert PDF files to markdown format for easier processing. Source code in src/extraction/datasources/pdf/parser.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class PDFDatasourceParser ( BaseParser [ PDFDocument ]): \"\"\" Parser for PDF documents that converts them to structured PDFDocument objects. Uses MarkItDown to convert PDF files to markdown format for easier processing. \"\"\" def __init__ ( self , parser : MarkItDown = MarkItDown ()): \"\"\" Initialize the PDF parser. Attributes: parser: MarkItDown parser instance for PDF to markdown conversion \"\"\" self . parser = parser def parse ( self , file_path : str ) -> PDFDocument : \"\"\" Parses the given PDF file into a structured document. Args: file_path: Path to the PDF file Returns: PDFDocument object containing the parsed content and metadata \"\"\" markdown = self . parser . convert ( file_path , file_extension = \".pdf\" ) . text_content metadata = self . _extract_metadata ( file_path ) return PDFDocument ( text = markdown , metadata = metadata ) def _extract_metadata ( self , file_path : str ) -> dict : \"\"\" Extract and process PDF metadata from the file. Args: file_path: Path to the PDF file Returns: Processed metadata dictionary with standardized fields \"\"\" metadata = default_file_metadata_func ( file_path ) metadata . update ( { \"datasource\" : \"pdf\" , \"format\" : \"pdf\" , \"url\" : None , \"title\" : os . path . basename ( file_path ), \"last_edited_date\" : metadata [ \"last_modified_date\" ], \"created_date\" : metadata [ \"creation_date\" ], } ) del metadata [ \"last_modified_date\" ] del metadata [ \"creation_date\" ] return metadata","title":"PDFDatasourceParser"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.parser.PDFDatasourceParser.__init__","text":"Initialize the PDF parser. Attributes: parser \u2013 MarkItDown parser instance for PDF to markdown conversion Source code in src/extraction/datasources/pdf/parser.py 20 21 22 23 24 25 26 27 def __init__ ( self , parser : MarkItDown = MarkItDown ()): \"\"\" Initialize the PDF parser. Attributes: parser: MarkItDown parser instance for PDF to markdown conversion \"\"\" self . parser = parser","title":"__init__"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.parser.PDFDatasourceParser.parse","text":"Parses the given PDF file into a structured document. Parameters: file_path ( str ) \u2013 Path to the PDF file Returns: PDFDocument \u2013 PDFDocument object containing the parsed content and metadata Source code in src/extraction/datasources/pdf/parser.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def parse ( self , file_path : str ) -> PDFDocument : \"\"\" Parses the given PDF file into a structured document. Args: file_path: Path to the PDF file Returns: PDFDocument object containing the parsed content and metadata \"\"\" markdown = self . parser . convert ( file_path , file_extension = \".pdf\" ) . text_content metadata = self . _extract_metadata ( file_path ) return PDFDocument ( text = markdown , metadata = metadata )","title":"parse"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.parser.PDFDatasourceParserFactory","text":"Bases: Factory Factory for creating PDF parser instances. Creates and configures PDFDatasourceParser objects according to the provided configuration. Source code in src/extraction/datasources/pdf/parser.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class PDFDatasourceParserFactory ( Factory ): \"\"\" Factory for creating PDF parser instances. Creates and configures PDFDatasourceParser objects according to the provided configuration. \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , _ : PDFDatasourceConfiguration ) -> PDFDatasourceParser : \"\"\" Creates a new instance of the PDF parser. Args: _: Configuration object for the parser (not used in this implementation) Returns: PDFDatasourceParser: Configured parser instance \"\"\" return PDFDatasourceParser ()","title":"PDFDatasourceParserFactory"},{"location":"src/extraction/datasources/pdf/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.reader.PDFDatasourceReader","text":"Bases: BaseReader Source code in src/extraction/datasources/pdf/reader.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class PDFDatasourceReader ( BaseReader ): def __init__ ( self , configuration : PDFDatasourceConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize PDF reader. Args: configuration: Settings for PDF processing logger: Logger instance for logging messages \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . base_path = configuration . base_path self . logger = logger async def read_all_async ( self ) -> AsyncIterator [ str ]: \"\"\"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator[str]: An asynchronous iterator of PDF file paths \"\"\" self . logger . info ( f \"Reading PDF files from ' { self . base_path } ' with limit { self . export_limit } \" ) pdf_files = [ f for f in os . listdir ( self . base_path ) if f . endswith ( \".pdf\" ) ] files_to_load = ( pdf_files if self . export_limit is None else pdf_files [: self . export_limit ] ) for i , file_name in enumerate ( files_to_load ): self . logger . info ( f \"[ { i } / { self . export_limit } ] Reading PDF file ' { file_name } '\" ) file_path = os . path . join ( self . base_path , file_name ) if os . path . isfile ( file_path ): yield file_path","title":"PDFDatasourceReader"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.reader.PDFDatasourceReader.__init__","text":"Initialize PDF reader. Parameters: configuration ( PDFDatasourceConfiguration ) \u2013 Settings for PDF processing logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/extraction/datasources/pdf/reader.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , configuration : PDFDatasourceConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize PDF reader. Args: configuration: Settings for PDF processing logger: Logger instance for logging messages \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . base_path = configuration . base_path self . logger = logger","title":"__init__"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.reader.PDFDatasourceReader.read_all_async","text":"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator [ str ] \u2013 AsyncIterator[str]: An asynchronous iterator of PDF file paths Source code in src/extraction/datasources/pdf/reader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 async def read_all_async ( self ) -> AsyncIterator [ str ]: \"\"\"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator[str]: An asynchronous iterator of PDF file paths \"\"\" self . logger . info ( f \"Reading PDF files from ' { self . base_path } ' with limit { self . export_limit } \" ) pdf_files = [ f for f in os . listdir ( self . base_path ) if f . endswith ( \".pdf\" ) ] files_to_load = ( pdf_files if self . export_limit is None else pdf_files [: self . export_limit ] ) for i , file_name in enumerate ( files_to_load ): self . logger . info ( f \"[ { i } / { self . export_limit } ] Reading PDF file ' { file_name } '\" ) file_path = os . path . join ( self . base_path , file_name ) if os . path . isfile ( file_path ): yield file_path","title":"read_all_async"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.reader.PDFDatasourceReaderFactory","text":"Bases: Factory Factory for creating PDF reader instances. Implements the factory pattern to produce configured PDFDatasourceReader objects based on the provided configuration settings. Source code in src/extraction/datasources/pdf/reader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class PDFDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating PDF reader instances. Implements the factory pattern to produce configured PDFDatasourceReader objects based on the provided configuration settings. \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : PDFDatasourceConfiguration ) -> PDFDatasourceReader : \"\"\"Create a new PDFDatasourceReader with the specified configuration. Args: configuration: Settings that control PDF processing behavior including base path and export limits Returns: PDFDatasourceReader: A fully configured reader instance ready for use \"\"\" return PDFDatasourceReader ( configuration = configuration )","title":"PDFDatasourceReaderFactory"},{"location":"src/extraction/datasources/registry/","text":"Registry This module contains functionality related to the the registry module for extraction.datasources . Registry DatasourceManagerRegistry Bases: Registry Registry for datasource managers. This registry maps DatasourceName enums to their corresponding manager implementations. It provides a centralized way to register and retrieve datasource managers. Attributes: _key_class ( Type ) \u2013 Type of the key used in the registry (DatasourceName). Source code in src/extraction/datasources/registry.py 7 8 9 10 11 12 13 14 15 16 17 class DatasourceManagerRegistry ( Registry ): \"\"\"Registry for datasource managers. This registry maps DatasourceName enums to their corresponding manager implementations. It provides a centralized way to register and retrieve datasource managers. Attributes: _key_class: Type of the key used in the registry (DatasourceName). \"\"\" _key_class : Type = DatasourceName","title":"Registry"},{"location":"src/extraction/datasources/registry/#registry","text":"This module contains functionality related to the the registry module for extraction.datasources .","title":"Registry"},{"location":"src/extraction/datasources/registry/#registry_1","text":"","title":"Registry"},{"location":"src/extraction/datasources/registry/#src.extraction.datasources.registry.DatasourceManagerRegistry","text":"Bases: Registry Registry for datasource managers. This registry maps DatasourceName enums to their corresponding manager implementations. It provides a centralized way to register and retrieve datasource managers. Attributes: _key_class ( Type ) \u2013 Type of the key used in the registry (DatasourceName). Source code in src/extraction/datasources/registry.py 7 8 9 10 11 12 13 14 15 16 17 class DatasourceManagerRegistry ( Registry ): \"\"\"Registry for datasource managers. This registry maps DatasourceName enums to their corresponding manager implementations. It provides a centralized way to register and retrieve datasource managers. Attributes: _key_class: Type of the key used in the registry (DatasourceName). \"\"\" _key_class : Type = DatasourceName","title":"DatasourceManagerRegistry"},{"location":"src/extraction/orchestrators/base_orchestator/","text":"Base_orchestator This module contains functionality related to the the base_orchestator module for extraction.orchestrators . Base_orchestator BaseDatasourceOrchestrator Bases: ABC Abstract base class for datasource orchestration. Defines interface for managing content extraction, embedding generation, and vector storage operations across datasources. This class serves as a coordinator for multiple datasource managers, providing unified methods for extracting and processing documents from various data sources in both full and incremental sync modes. Note All implementing classes must provide concrete implementations of extract, embed, save and update methods. Source code in src/extraction/orchestrators/base_orchestator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseDatasourceOrchestrator ( ABC ): \"\"\"Abstract base class for datasource orchestration. Defines interface for managing content extraction, embedding generation, and vector storage operations across datasources. This class serves as a coordinator for multiple datasource managers, providing unified methods for extracting and processing documents from various data sources in both full and incremental sync modes. Note: All implementing classes must provide concrete implementations of extract, embed, save and update methods. \"\"\" def __init__ ( self , datasource_managers : List [ BaseDatasourceManager ], ): \"\"\"Initialize the orchestrator with datasource managers. Args: datasource_managers: A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. \"\"\" self . datasource_managers = datasource_managers @abstractmethod async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: An asynchronous iterator yielding BaseDocument objects representing the extracted content from all datasources. \"\"\" pass @abstractmethod async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: An asynchronous iterator yielding BaseDocument objects representing newly added or modified content from all datasources. \"\"\" pass __init__ ( datasource_managers ) Initialize the orchestrator with datasource managers. Parameters: datasource_managers ( List [ BaseDatasourceManager ] ) \u2013 A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. Source code in src/extraction/orchestrators/base_orchestator.py 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , datasource_managers : List [ BaseDatasourceManager ], ): \"\"\"Initialize the orchestrator with datasource managers. Args: datasource_managers: A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. \"\"\" self . datasource_managers = datasource_managers full_refresh_sync () abstractmethod async Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: AsyncIterator [ BaseDocument ] \u2013 An asynchronous iterator yielding BaseDocument objects representing AsyncIterator [ BaseDocument ] \u2013 the extracted content from all datasources. Source code in src/extraction/orchestrators/base_orchestator.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @abstractmethod async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: An asynchronous iterator yielding BaseDocument objects representing the extracted content from all datasources. \"\"\" pass incremental_sync () abstractmethod async Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: AsyncIterator [ BaseDocument ] \u2013 An asynchronous iterator yielding BaseDocument objects representing AsyncIterator [ BaseDocument ] \u2013 newly added or modified content from all datasources. Source code in src/extraction/orchestrators/base_orchestator.py 52 53 54 55 56 57 58 59 60 61 62 63 64 @abstractmethod async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: An asynchronous iterator yielding BaseDocument objects representing newly added or modified content from all datasources. \"\"\" pass","title":"Base_orchestator"},{"location":"src/extraction/orchestrators/base_orchestator/#base_orchestator","text":"This module contains functionality related to the the base_orchestator module for extraction.orchestrators .","title":"Base_orchestator"},{"location":"src/extraction/orchestrators/base_orchestator/#base_orchestator_1","text":"","title":"Base_orchestator"},{"location":"src/extraction/orchestrators/base_orchestator/#src.extraction.orchestrators.base_orchestator.BaseDatasourceOrchestrator","text":"Bases: ABC Abstract base class for datasource orchestration. Defines interface for managing content extraction, embedding generation, and vector storage operations across datasources. This class serves as a coordinator for multiple datasource managers, providing unified methods for extracting and processing documents from various data sources in both full and incremental sync modes. Note All implementing classes must provide concrete implementations of extract, embed, save and update methods. Source code in src/extraction/orchestrators/base_orchestator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseDatasourceOrchestrator ( ABC ): \"\"\"Abstract base class for datasource orchestration. Defines interface for managing content extraction, embedding generation, and vector storage operations across datasources. This class serves as a coordinator for multiple datasource managers, providing unified methods for extracting and processing documents from various data sources in both full and incremental sync modes. Note: All implementing classes must provide concrete implementations of extract, embed, save and update methods. \"\"\" def __init__ ( self , datasource_managers : List [ BaseDatasourceManager ], ): \"\"\"Initialize the orchestrator with datasource managers. Args: datasource_managers: A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. \"\"\" self . datasource_managers = datasource_managers @abstractmethod async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: An asynchronous iterator yielding BaseDocument objects representing the extracted content from all datasources. \"\"\" pass @abstractmethod async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: An asynchronous iterator yielding BaseDocument objects representing newly added or modified content from all datasources. \"\"\" pass","title":"BaseDatasourceOrchestrator"},{"location":"src/extraction/orchestrators/base_orchestator/#src.extraction.orchestrators.base_orchestator.BaseDatasourceOrchestrator.__init__","text":"Initialize the orchestrator with datasource managers. Parameters: datasource_managers ( List [ BaseDatasourceManager ] ) \u2013 A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. Source code in src/extraction/orchestrators/base_orchestator.py 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , datasource_managers : List [ BaseDatasourceManager ], ): \"\"\"Initialize the orchestrator with datasource managers. Args: datasource_managers: A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. \"\"\" self . datasource_managers = datasource_managers","title":"__init__"},{"location":"src/extraction/orchestrators/base_orchestator/#src.extraction.orchestrators.base_orchestator.BaseDatasourceOrchestrator.full_refresh_sync","text":"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: AsyncIterator [ BaseDocument ] \u2013 An asynchronous iterator yielding BaseDocument objects representing AsyncIterator [ BaseDocument ] \u2013 the extracted content from all datasources. Source code in src/extraction/orchestrators/base_orchestator.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @abstractmethod async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: An asynchronous iterator yielding BaseDocument objects representing the extracted content from all datasources. \"\"\" pass","title":"full_refresh_sync"},{"location":"src/extraction/orchestrators/base_orchestator/#src.extraction.orchestrators.base_orchestator.BaseDatasourceOrchestrator.incremental_sync","text":"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: AsyncIterator [ BaseDocument ] \u2013 An asynchronous iterator yielding BaseDocument objects representing AsyncIterator [ BaseDocument ] \u2013 newly added or modified content from all datasources. Source code in src/extraction/orchestrators/base_orchestator.py 52 53 54 55 56 57 58 59 60 61 62 63 64 @abstractmethod async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: An asynchronous iterator yielding BaseDocument objects representing newly added or modified content from all datasources. \"\"\" pass","title":"incremental_sync"},{"location":"src/extraction/orchestrators/registry/","text":"Registry This module contains functionality related to the the registry module for extraction.orchestrators . Registry DatasourceOrchestratorRegistry Bases: Registry Registry for datasource orchestrators. A specialized registry implementation that maps OrchestratorName enum values to their corresponding orchestrator factories. Attributes: _key_class \u2013 The class type used as registry keys, specifically OrchestratorName. Source code in src/extraction/orchestrators/registry.py 5 6 7 8 9 10 11 12 13 14 15 16 class DatasourceOrchestratorRegistry ( Registry ): \"\"\" Registry for datasource orchestrators. A specialized registry implementation that maps OrchestratorName enum values to their corresponding orchestrator factories. Attributes: _key_class: The class type used as registry keys, specifically OrchestratorName. \"\"\" _key_class = OrchestratorName","title":"Registry"},{"location":"src/extraction/orchestrators/registry/#registry","text":"This module contains functionality related to the the registry module for extraction.orchestrators .","title":"Registry"},{"location":"src/extraction/orchestrators/registry/#registry_1","text":"","title":"Registry"},{"location":"src/extraction/orchestrators/registry/#src.extraction.orchestrators.registry.DatasourceOrchestratorRegistry","text":"Bases: Registry Registry for datasource orchestrators. A specialized registry implementation that maps OrchestratorName enum values to their corresponding orchestrator factories. Attributes: _key_class \u2013 The class type used as registry keys, specifically OrchestratorName. Source code in src/extraction/orchestrators/registry.py 5 6 7 8 9 10 11 12 13 14 15 16 class DatasourceOrchestratorRegistry ( Registry ): \"\"\" Registry for datasource orchestrators. A specialized registry implementation that maps OrchestratorName enum values to their corresponding orchestrator factories. Attributes: _key_class: The class type used as registry keys, specifically OrchestratorName. \"\"\" _key_class = OrchestratorName","title":"DatasourceOrchestratorRegistry"},{"location":"src/extraction/orchestrators/basic/orchestrator/","text":"Orchestrator This module contains functionality related to the the orchestrator module for extraction.orchestrators.basic . Orchestrator BasicDatasourceOrchestrator Bases: BaseDatasourceOrchestrator Orchestrator for multi-datasource content processing. Source code in src/extraction/orchestrators/basic/orchestrator.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class BasicDatasourceOrchestrator ( BaseDatasourceOrchestrator ): \"\"\" Orchestrator for multi-datasource content processing. \"\"\" async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources \"\"\" for datasource_manager in self . datasource_managers : async for document in datasource_manager . full_refresh_sync (): yield document async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\" Not implemented yet. \"\"\" raise NotImplementedError ( \"Incremental sync is not supported yet.\" ) full_refresh_sync () async Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator [ BaseDocument ] \u2013 AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources Source code in src/extraction/orchestrators/basic/orchestrator.py 17 18 19 20 21 22 23 24 25 26 27 async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources \"\"\" for datasource_manager in self . datasource_managers : async for document in datasource_manager . full_refresh_sync (): yield document incremental_sync () async Not implemented yet. Source code in src/extraction/orchestrators/basic/orchestrator.py 29 30 31 32 33 async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\" Not implemented yet. \"\"\" raise NotImplementedError ( \"Incremental sync is not supported yet.\" ) BasicDatasourceOrchestratorFactory Bases: Factory Factory for creating BasicDatasourceOrchestrator instances. Creates orchestrator instances configured with appropriate datasource managers based on the provided extraction configuration. Source code in src/extraction/orchestrators/basic/orchestrator.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class BasicDatasourceOrchestratorFactory ( Factory ): \"\"\"Factory for creating BasicDatasourceOrchestrator instances. Creates orchestrator instances configured with appropriate datasource managers based on the provided extraction configuration. \"\"\" _configuration_class : Type = ExtractionConfiguration @classmethod def _create_instance ( cls , configuration : ExtractionConfiguration ) -> BasicDatasourceOrchestrator : \"\"\"Creates a configured BasicDatasourceOrchestrator. Initializes datasource managers for each configured datasource and creates an orchestrator instance with those managers. Args: configuration: Settings for extraction process configuration Returns: BasicDatasourceOrchestrator: Configured orchestrator instance \"\"\" datasource_managers = [ DatasourceManagerRegistry . get ( datasource_configuration . name ) . create ( datasource_configuration ) for datasource_configuration in configuration . extraction . datasources ] return BasicDatasourceOrchestrator ( datasource_managers = datasource_managers )","title":"Orchestrator"},{"location":"src/extraction/orchestrators/basic/orchestrator/#orchestrator","text":"This module contains functionality related to the the orchestrator module for extraction.orchestrators.basic .","title":"Orchestrator"},{"location":"src/extraction/orchestrators/basic/orchestrator/#orchestrator_1","text":"","title":"Orchestrator"},{"location":"src/extraction/orchestrators/basic/orchestrator/#src.extraction.orchestrators.basic.orchestrator.BasicDatasourceOrchestrator","text":"Bases: BaseDatasourceOrchestrator Orchestrator for multi-datasource content processing. Source code in src/extraction/orchestrators/basic/orchestrator.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class BasicDatasourceOrchestrator ( BaseDatasourceOrchestrator ): \"\"\" Orchestrator for multi-datasource content processing. \"\"\" async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources \"\"\" for datasource_manager in self . datasource_managers : async for document in datasource_manager . full_refresh_sync (): yield document async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\" Not implemented yet. \"\"\" raise NotImplementedError ( \"Incremental sync is not supported yet.\" )","title":"BasicDatasourceOrchestrator"},{"location":"src/extraction/orchestrators/basic/orchestrator/#src.extraction.orchestrators.basic.orchestrator.BasicDatasourceOrchestrator.full_refresh_sync","text":"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator [ BaseDocument ] \u2013 AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources Source code in src/extraction/orchestrators/basic/orchestrator.py 17 18 19 20 21 22 23 24 25 26 27 async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources \"\"\" for datasource_manager in self . datasource_managers : async for document in datasource_manager . full_refresh_sync (): yield document","title":"full_refresh_sync"},{"location":"src/extraction/orchestrators/basic/orchestrator/#src.extraction.orchestrators.basic.orchestrator.BasicDatasourceOrchestrator.incremental_sync","text":"Not implemented yet. Source code in src/extraction/orchestrators/basic/orchestrator.py 29 30 31 32 33 async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\" Not implemented yet. \"\"\" raise NotImplementedError ( \"Incremental sync is not supported yet.\" )","title":"incremental_sync"},{"location":"src/extraction/orchestrators/basic/orchestrator/#src.extraction.orchestrators.basic.orchestrator.BasicDatasourceOrchestratorFactory","text":"Bases: Factory Factory for creating BasicDatasourceOrchestrator instances. Creates orchestrator instances configured with appropriate datasource managers based on the provided extraction configuration. Source code in src/extraction/orchestrators/basic/orchestrator.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class BasicDatasourceOrchestratorFactory ( Factory ): \"\"\"Factory for creating BasicDatasourceOrchestrator instances. Creates orchestrator instances configured with appropriate datasource managers based on the provided extraction configuration. \"\"\" _configuration_class : Type = ExtractionConfiguration @classmethod def _create_instance ( cls , configuration : ExtractionConfiguration ) -> BasicDatasourceOrchestrator : \"\"\"Creates a configured BasicDatasourceOrchestrator. Initializes datasource managers for each configured datasource and creates an orchestrator instance with those managers. Args: configuration: Settings for extraction process configuration Returns: BasicDatasourceOrchestrator: Configured orchestrator instance \"\"\" datasource_managers = [ DatasourceManagerRegistry . get ( datasource_configuration . name ) . create ( datasource_configuration ) for datasource_configuration in configuration . extraction . datasources ] return BasicDatasourceOrchestrator ( datasource_managers = datasource_managers )","title":"BasicDatasourceOrchestratorFactory"},{"location":"src/jobs/queries_retention/","text":"Queries_retention This module contains functionality related to the the queries_retention module for jobs . Queries_retention LangfuseRenetionJob Langfuse V2 API does not support trace deletion, therefore we need to delete traces manually. Source code in src/jobs/queries_retention.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class LangfuseRenetionJob : \"\"\"Langfuse V2 API does not support trace deletion, therefore we need to delete traces manually.\"\"\" def __init__ ( self , configuration : LangfuseConfiguration , client : Langfuse , logger : Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: langfuse_client (Langfuse): The Langfuse client instance to interact with the Langfuse API. logger (Logger): Logger instance for logging messages. \"\"\" self . configuration = configuration self . client = client self . logger = logger def run ( self ): retention_days = self . configuration . retention_job . retention_days cutoff_date = datetime . now () - timedelta ( days = retention_days ) ids = list ( self . _get_traces ( cutoff_date )) self . logger . info ( f \"Found { len ( ids ) } traces to delete older than { cutoff_date } .\" ) self . delete_traces ( ids ) def _get_traces ( self , cutoff_date : datetime ) -> Iterator [ int ]: \"\"\" Fetches traces from Langfuse that are older than the specified cutoff date. Args: cuttoff_date (datetime): The date before which traces will be fetched. Returns: Iterator: An iterator over the fetched traces. \"\"\" limit = 100 response = self . client . fetch_traces ( to_timestamp = cutoff_date , limit = limit ) total_pages = response . meta . total_pages for trace in response . data : yield trace . id for page in range ( 2 , total_pages + 1 ): response = self . client . fetch_traces ( to_timestamp = cutoff_date , page = page , limit = limit ) for trace in response . data : yield trace . id def delete_traces ( self , ids : List [ int ]) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" if not ids : self . logger . info ( \"No traces to delete.\" ) return connnection = self . _get_pg_connection () try : self . _delete_related_dataset_run_items ( trace_ids = ids , connection = connnection ) self . _delete_related_dataset_items ( trace_ids = ids , connection = connnection ) self . _delete_related_scores ( trace_ids = ids , connection = connnection ) self . _delete_related_observations ( trace_ids = ids , connection = connnection ) self . _delete_traces ( trace_ids = ids , connection = connnection ) self . logger . info ( f \"Deleted { len ( ids ) } traces.\" ) except psycopg2 . Error as e : self . logger . error ( f \"Failed to delete traces. Database error: { e } \" ) connnection . rollback () finally : connnection . close () def _delete_related_dataset_items ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes datasets related to the specified trace IDs. Args: ids (List[int]): A list of trace IDs for which related datasets will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM dataset_items WHERE source_trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } datasets related to traces.\" ) def _delete_related_dataset_run_items ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes dataset runs related to the specified trace IDs. Args: ids (List[int]): A list of trace IDs for which related dataset runs will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM dataset_run_items WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } dataset runs related to traces.\" ) def _delete_related_scores ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes scores related to the specified trace IDs. Args: trace_ids (List[int]): A list of trace IDs for which related scores will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM scores WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } scores related to traces.\" ) def _delete_related_observations ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes observations (spans, generations, events) related to the specified trace IDs. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM observations WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } observations related to traces.\" ) def _delete_traces ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM traces WHERE id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { len ( trace_ids ) } traces.\" ) def _get_pg_connection ( self ): \"\"\" Retrieves a PostgreSQL connection using the configuration. Returns: psycopg2.extensions.connection: A PostgreSQL connection object. \"\"\" return psycopg2 . connect ( host = self . configuration . database . host , port = self . configuration . database . port , database = self . configuration . database . db , user = self . configuration . database . secrets . user . get_secret_value (), password = self . configuration . database . secrets . password . get_secret_value (), ) __init__ ( configuration , client , logger = LoggerConfiguration . get_logger ( __name__ )) Parameters: langfuse_client ( Langfuse ) \u2013 The Langfuse client instance to interact with the Langfuse API. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages. Source code in src/jobs/queries_retention.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , configuration : LangfuseConfiguration , client : Langfuse , logger : Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: langfuse_client (Langfuse): The Langfuse client instance to interact with the Langfuse API. logger (Logger): Logger instance for logging messages. \"\"\" self . configuration = configuration self . client = client self . logger = logger delete_traces ( ids ) Deletes traces with the specified IDs from Langfuse. Parameters: trace_ids ( List [ int ] ) \u2013 A list of trace IDs to delete. Source code in src/jobs/queries_retention.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def delete_traces ( self , ids : List [ int ]) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" if not ids : self . logger . info ( \"No traces to delete.\" ) return connnection = self . _get_pg_connection () try : self . _delete_related_dataset_run_items ( trace_ids = ids , connection = connnection ) self . _delete_related_dataset_items ( trace_ids = ids , connection = connnection ) self . _delete_related_scores ( trace_ids = ids , connection = connnection ) self . _delete_related_observations ( trace_ids = ids , connection = connnection ) self . _delete_traces ( trace_ids = ids , connection = connnection ) self . logger . info ( f \"Deleted { len ( ids ) } traces.\" ) except psycopg2 . Error as e : self . logger . error ( f \"Failed to delete traces. Database error: { e } \" ) connnection . rollback () finally : connnection . close () LangfuseRenetionJobFactory Bases: Factory Source code in src/jobs/queries_retention.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class LangfuseRenetionJobFactory ( Factory ): _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> LangfuseRenetionJob : \"\"\" Create an instance of QueriesRetentionService using the provided configuration. Args: configuration (LangfuseConfiguration): The configuration object used to create the instance. Returns: QueriesRetentionService: A new instance of QueriesRetentionService. \"\"\" langfuse_client = LangfuseClientFactory . create ( configuration = configuration ) return LangfuseRenetionJob ( configuration = configuration , client = langfuse_client )","title":"Queries_retention"},{"location":"src/jobs/queries_retention/#queries_retention","text":"This module contains functionality related to the the queries_retention module for jobs .","title":"Queries_retention"},{"location":"src/jobs/queries_retention/#queries_retention_1","text":"","title":"Queries_retention"},{"location":"src/jobs/queries_retention/#src.jobs.queries_retention.LangfuseRenetionJob","text":"Langfuse V2 API does not support trace deletion, therefore we need to delete traces manually. Source code in src/jobs/queries_retention.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class LangfuseRenetionJob : \"\"\"Langfuse V2 API does not support trace deletion, therefore we need to delete traces manually.\"\"\" def __init__ ( self , configuration : LangfuseConfiguration , client : Langfuse , logger : Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: langfuse_client (Langfuse): The Langfuse client instance to interact with the Langfuse API. logger (Logger): Logger instance for logging messages. \"\"\" self . configuration = configuration self . client = client self . logger = logger def run ( self ): retention_days = self . configuration . retention_job . retention_days cutoff_date = datetime . now () - timedelta ( days = retention_days ) ids = list ( self . _get_traces ( cutoff_date )) self . logger . info ( f \"Found { len ( ids ) } traces to delete older than { cutoff_date } .\" ) self . delete_traces ( ids ) def _get_traces ( self , cutoff_date : datetime ) -> Iterator [ int ]: \"\"\" Fetches traces from Langfuse that are older than the specified cutoff date. Args: cuttoff_date (datetime): The date before which traces will be fetched. Returns: Iterator: An iterator over the fetched traces. \"\"\" limit = 100 response = self . client . fetch_traces ( to_timestamp = cutoff_date , limit = limit ) total_pages = response . meta . total_pages for trace in response . data : yield trace . id for page in range ( 2 , total_pages + 1 ): response = self . client . fetch_traces ( to_timestamp = cutoff_date , page = page , limit = limit ) for trace in response . data : yield trace . id def delete_traces ( self , ids : List [ int ]) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" if not ids : self . logger . info ( \"No traces to delete.\" ) return connnection = self . _get_pg_connection () try : self . _delete_related_dataset_run_items ( trace_ids = ids , connection = connnection ) self . _delete_related_dataset_items ( trace_ids = ids , connection = connnection ) self . _delete_related_scores ( trace_ids = ids , connection = connnection ) self . _delete_related_observations ( trace_ids = ids , connection = connnection ) self . _delete_traces ( trace_ids = ids , connection = connnection ) self . logger . info ( f \"Deleted { len ( ids ) } traces.\" ) except psycopg2 . Error as e : self . logger . error ( f \"Failed to delete traces. Database error: { e } \" ) connnection . rollback () finally : connnection . close () def _delete_related_dataset_items ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes datasets related to the specified trace IDs. Args: ids (List[int]): A list of trace IDs for which related datasets will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM dataset_items WHERE source_trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } datasets related to traces.\" ) def _delete_related_dataset_run_items ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes dataset runs related to the specified trace IDs. Args: ids (List[int]): A list of trace IDs for which related dataset runs will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM dataset_run_items WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } dataset runs related to traces.\" ) def _delete_related_scores ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes scores related to the specified trace IDs. Args: trace_ids (List[int]): A list of trace IDs for which related scores will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM scores WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } scores related to traces.\" ) def _delete_related_observations ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes observations (spans, generations, events) related to the specified trace IDs. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM observations WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } observations related to traces.\" ) def _delete_traces ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM traces WHERE id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { len ( trace_ids ) } traces.\" ) def _get_pg_connection ( self ): \"\"\" Retrieves a PostgreSQL connection using the configuration. Returns: psycopg2.extensions.connection: A PostgreSQL connection object. \"\"\" return psycopg2 . connect ( host = self . configuration . database . host , port = self . configuration . database . port , database = self . configuration . database . db , user = self . configuration . database . secrets . user . get_secret_value (), password = self . configuration . database . secrets . password . get_secret_value (), )","title":"LangfuseRenetionJob"},{"location":"src/jobs/queries_retention/#src.jobs.queries_retention.LangfuseRenetionJob.__init__","text":"Parameters: langfuse_client ( Langfuse ) \u2013 The Langfuse client instance to interact with the Langfuse API. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages. Source code in src/jobs/queries_retention.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , configuration : LangfuseConfiguration , client : Langfuse , logger : Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: langfuse_client (Langfuse): The Langfuse client instance to interact with the Langfuse API. logger (Logger): Logger instance for logging messages. \"\"\" self . configuration = configuration self . client = client self . logger = logger","title":"__init__"},{"location":"src/jobs/queries_retention/#src.jobs.queries_retention.LangfuseRenetionJob.delete_traces","text":"Deletes traces with the specified IDs from Langfuse. Parameters: trace_ids ( List [ int ] ) \u2013 A list of trace IDs to delete. Source code in src/jobs/queries_retention.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def delete_traces ( self , ids : List [ int ]) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" if not ids : self . logger . info ( \"No traces to delete.\" ) return connnection = self . _get_pg_connection () try : self . _delete_related_dataset_run_items ( trace_ids = ids , connection = connnection ) self . _delete_related_dataset_items ( trace_ids = ids , connection = connnection ) self . _delete_related_scores ( trace_ids = ids , connection = connnection ) self . _delete_related_observations ( trace_ids = ids , connection = connnection ) self . _delete_traces ( trace_ids = ids , connection = connnection ) self . logger . info ( f \"Deleted { len ( ids ) } traces.\" ) except psycopg2 . Error as e : self . logger . error ( f \"Failed to delete traces. Database error: { e } \" ) connnection . rollback () finally : connnection . close ()","title":"delete_traces"},{"location":"src/jobs/queries_retention/#src.jobs.queries_retention.LangfuseRenetionJobFactory","text":"Bases: Factory Source code in src/jobs/queries_retention.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class LangfuseRenetionJobFactory ( Factory ): _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> LangfuseRenetionJob : \"\"\" Create an instance of QueriesRetentionService using the provided configuration. Args: configuration (LangfuseConfiguration): The configuration object used to create the instance. Returns: QueriesRetentionService: A new instance of QueriesRetentionService. \"\"\" langfuse_client = LangfuseClientFactory . create ( configuration = configuration ) return LangfuseRenetionJob ( configuration = configuration , client = langfuse_client )","title":"LangfuseRenetionJobFactory"}]}