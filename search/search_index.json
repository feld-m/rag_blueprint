{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RAG Blueprint Documentation Overview The RAG blueprint project is a Retrieval-Augmented Generation system that integrates with several datasources to provide intelligent document search and analysis. The system combines the power of different large language models with knowledge bases to deliver accurate, context-aware responses through a chat interface. Codebase provides out-of-the-box integration with UI and observability services. Data Sources Data Source Description Confluence Enterprise wiki and knowledge base integration Notion Workspace and document management integration PDF PDF document processing and text extraction Bundestag Data source fetching speeches from BundestagMine Check how to configure datasources here . Embeddding Models Models Provider Description * HuggingFace Open-sourced, run locally embedding models provided by HuggingFace * OpenAI Embedding models provided by OpenAI * VoyageAI Embedding models provided by VoyageAI Check how to configure embedding model here . Language Models Model Provider Description * LiteLLM Availability of many LLMs via providers like OpenAI , Google or Anthropic as well as local LLMs Check how to configure LLM here . Vector Databases Vector Store Description Qdrant High-performance vector similarity search engine Chroma Lightweight embedded vector database PGVector Postgres extension for embedding data support Check how to configure vector store here . Key Features Multiple Knowledge Base Integration : Seamless extraction from several Data Sources (Confluence, Notion, PDF) Wide Models Support : Availability of numerous embedding and language models Vector Search : Efficient similarity search using vector stores Interactive Chat : User-friendly interface for querying knowledge on Chainlit Performance Monitoring : Query and response tracking with Langfuse Evaluation : Comprehensive evaluation metrics using RAGAS Setup flexibility : Easy and flexible setup process of the pipeline Quick Start QuickStart Setup Dveloper Setup","title":"Home"},{"location":"#rag-blueprint-documentation","text":"","title":"RAG Blueprint Documentation"},{"location":"#overview","text":"The RAG blueprint project is a Retrieval-Augmented Generation system that integrates with several datasources to provide intelligent document search and analysis. The system combines the power of different large language models with knowledge bases to deliver accurate, context-aware responses through a chat interface. Codebase provides out-of-the-box integration with UI and observability services.","title":"Overview"},{"location":"#data-sources","text":"Data Source Description Confluence Enterprise wiki and knowledge base integration Notion Workspace and document management integration PDF PDF document processing and text extraction Bundestag Data source fetching speeches from BundestagMine Check how to configure datasources here .","title":"Data Sources"},{"location":"#embeddding-models","text":"Models Provider Description * HuggingFace Open-sourced, run locally embedding models provided by HuggingFace * OpenAI Embedding models provided by OpenAI * VoyageAI Embedding models provided by VoyageAI Check how to configure embedding model here .","title":"Embeddding Models"},{"location":"#language-models","text":"Model Provider Description * LiteLLM Availability of many LLMs via providers like OpenAI , Google or Anthropic as well as local LLMs Check how to configure LLM here .","title":"Language Models"},{"location":"#vector-databases","text":"Vector Store Description Qdrant High-performance vector similarity search engine Chroma Lightweight embedded vector database PGVector Postgres extension for embedding data support Check how to configure vector store here .","title":"Vector Databases"},{"location":"#key-features","text":"Multiple Knowledge Base Integration : Seamless extraction from several Data Sources (Confluence, Notion, PDF) Wide Models Support : Availability of numerous embedding and language models Vector Search : Efficient similarity search using vector stores Interactive Chat : User-friendly interface for querying knowledge on Chainlit Performance Monitoring : Query and response tracking with Langfuse Evaluation : Comprehensive evaluation metrics using RAGAS Setup flexibility : Easy and flexible setup process of the pipeline","title":"Key Features"},{"location":"#quick-start","text":"QuickStart Setup Dveloper Setup","title":"Quick Start"},{"location":"evaluation/in_progress/","text":"In progress...","title":"In progress"},{"location":"how_to/how_to_add_new_datasource/","text":"How to Add a New Datasource Implementation This guide demonstrates how to add support for a new datasource implementation, using Confluence as an example. Architecture Datasources are managed by the DatasourceManager , which aggregates required components and orchestrates them to retrieve documents, clean them, and parse them to markdown format - which is strictly required by the embedding process. The general datasource manager flow is: Reader -> Parser (Optional) -> Cleaner (Optional) -> Splitter (Optional). Therefore, adding support for a new datasource requires implementing these components and their respective manager. Implementation Step 1: Dependencies Add the required packages to pyproject.toml under the following section: [project.optional-dependencies] extraction = [ \"atlassian-python-api>=3.41.19\", ... ] Step 2: Datasource Enum In datasources.py , add the new datasource to the DatasourceName enumeration: class DatasourceName(str, Enum): ... CONFLUENCE = \"confluence\" Step 3: Datasource Secrets Create a new directory src/extraction/datasources/confluence and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal, Union from pydantic import ConfigDict, Field, SecretStr from core.base_configuration import BaseSecrets from extraction.bootstrap.configuration.datasources import ( DatasourceConfiguration, DatasourceName, ) class ConfluenceDatasourceConfiguration(DatasourceConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAG__DATASOURCES__CONFLUENCE__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) username: SecretStr = Field( ..., description=\"Username credential used to authenticate with the Confluence instance\", ) password: SecretStr = Field( ..., description=\"Password credential used to authenticate with the Confluence instance\", ) secrets: Secrets = Field( None, description=\"Authentication credentials required to access the Confluence instance\", ) The first part is to create a configuration that extends DatasourceConfiguration . The Secrets inner class defines secret fields that will be present in the environment secret file under the RAG__DATASOURCES__CONFLUENCE__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAG__DATASOURCES__CONFLUENCE__USERNAME=<confluence_username> RAG__DATASOURCES__CONFLUENCE__PASSWORD=<confluence_password> Note : If your datasource doesn't require secrets, you can skip this step. Step 4: Datasource Configuration Finish up ConfluenceDatasourceConfiguration implementation and add the rest of the configuration required for the datasource: ... class ConfluenceDatasourceConfiguration(DatasourceConfiguration): ... host: str = Field( \"127.0.0.1\", description=\"Hostname or IP address of the Confluence server instance\", ) protocol: Union[Literal[\"http\"], Literal[\"https\"]] = Field( \"http\", description=\"Communication protocol used to connect to the Confluence server\", ) name: Literal[DatasourceName.CONFLUENCE] = Field( ..., description=\"Identifier specifying this configuration is for a Confluence datasource\", ) @property def base_url(self) -> str: return f\"{self.protocol}://{self.host}\" provider field constraints the value to DatasourceName.CONFLUENCE , which serves as an indicator for pydantic validator. Step 5: Confluence Document The next step is to create a Confluence document data class in document.py : from extraction.datasources.core.document import BaseDocument class ConfluenceDocument(BaseDocument): \"\"\"Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. \"\"\" pass In our case, we don't need anything beyond the BaseDocument implementation. Step 6: Confluence Client To create a Confluence client, we implement ConfluenceClientFactory in client.py . It extends SingletonFactory , which provides an interface for initializing a single instance for the duration of the application runtime. from typing import Type from atlassian import Confluence from core import SingletonFactory from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) class ConfluenceClientFactory(SingletonFactory): _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> Confluence: return Confluence( url=configuration.base_url, username=configuration.secrets.username.get_secret_value(), password=configuration.secrets.password.get_secret_value(), ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding client initialization. Step 7: Datasource Reader Create a Confluence reader in reader.py that implements the BaseReader interface: from extraction.datasources.core.reader import BaseReader ... class ConfluenceDatasourceReader(BaseReader): async def read_all_async( self, ) -> AsyncIterator[dict]: # read Confluence pages implementation This method returns an iterator, which improves runtime memory management. Next, implement a factory that defines how the ConfluenceDatasourceReader is initialized: from core import Factory ... class ConfluenceDatasourceReaderFactory(Factory): _configuration_class = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceReader: client = ConfluenceClientFactory.create(configuration) return ConfluenceDatasourceReader( configuration=configuration, client=client, ) Note that instead of initializing the Confluence client directly, the factory uses ConfluenceClientFactory to handle this task. Step 8: Datasource Parser In parser.py implement a parser responsible for converting the raw Confluence page to markdown format: from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) from extraction.datasources.confluence.document import ConfluenceDocument from extraction.datasources.core.parser import BaseParser class ConfluenceDatasourceParser(BaseParser[ConfluenceDocument]): def parse(self, page: str) -> ConfluenceDocument: # parse Confluence page implementation As before, define a factory for the parser: class ConfluenceDatasourceParserFactory(Factory): _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceParser: return ConfluenceDatasourceParser(configuration) Step 9: Datasource Manager To orchestrate all the previous components, we will reuse BasicDatasourceManager and implement a factory for it in manager.py : class ConfluenceDatasourceManagerFactory(Factory): \"\"\"Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class: Configuration class used for validating and processing Confluence-specific settings. \"\"\" _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> BasicDatasourceManager: \"\"\"Create a configured Confluence datasource manager. Sets up the necessary reader and parser components based on the provided configuration and assembles them into a functional manager. Args: configuration: Configuration object containing Confluence-specific parameters including authentication details, spaces to extract, and other extraction options. Returns: A fully initialized datasource manager that can extract and process data from Confluence. \"\"\" reader = ConfluenceDatasourceReaderFactory.create(configuration) parser = ConfluenceDatasourceParserFactory.create(configuration) return BasicDatasourceManager(configuration, reader, parser) Following the design pattern, ConfluenceDatasourceManagerFactory uses reader and parser factories to obtain the instances needed for the manager. Step 10: Datasource Integration Create an __init__.py file as follows: from extraction.bootstrap.configuration.datasources import ( DatasourceConfigurationRegistry, DatasourceName, ) from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) from extraction.datasources.confluence.manager import ( ConfluenceDatasourceManagerFactory, ) from extraction.datasources.registry import DatasourceManagerRegistry def register() -> None: DatasourceManagerRegistry.register( DatasourceName.CONFLUENCE, ConfluenceDatasourceManagerFactory ) DatasourceConfigurationRegistry.register( DatasourceName.CONFLUENCE, ConfluenceDatasourceConfiguration ) The initialization file includes a register() method responsible for registering our configuration and manager factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Confluence configuration in configurations/configuration.{environment}.json file: \"extraction\": { \"datasources\": [ { \"name\": \"confluence\", \"host\": \"wissen.feld-m.de\", \"protocol\": \"https\" } ] ... } ... We can dynamically retrieve the corresponding manager implementation by using the name specified in the configuration: datasource_config = read_datasource_from_config() datasource_manager = DatasourceManagerRegistry.get(datasource_config.name).create(datasource_config) This mechanism is later used by DatasourceOrchestrator to initialize datasources defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 extraction/ \u2514\u2500\u2500 datasources/ \u2514\u2500\u2500 confluence/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 client.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 document.py \u251c\u2500\u2500 manager.py \u251c\u2500\u2500 parser.py \u2514\u2500\u2500 reader.py Notes Below is the __init__ method of BasicDatasourceManager used in our tutorial: class BasicDatasourceManager(BaseDatasourceManager, Generic[DocType]): def __init__( self, configuration: ExtractionConfiguration, reader: BaseReader, parser: BaseParser = BasicMarkdownParser(), cleaner: BaseCleaner = BasicMarkdownCleaner(), splitter: BaseSplitter = BasicMarkdownSplitter(), ): Note that in this guide we skipped the implementation of custom cleaner and splitter components, instead using the default ones. When building a new datasource integration, you might need to implement custom versions of these components based on your specific requirements.","title":"Add a New Datasource"},{"location":"how_to/how_to_add_new_datasource/#how-to-add-a-new-datasource-implementation","text":"This guide demonstrates how to add support for a new datasource implementation, using Confluence as an example.","title":"How to Add a New Datasource Implementation"},{"location":"how_to/how_to_add_new_datasource/#architecture","text":"Datasources are managed by the DatasourceManager , which aggregates required components and orchestrates them to retrieve documents, clean them, and parse them to markdown format - which is strictly required by the embedding process. The general datasource manager flow is: Reader -> Parser (Optional) -> Cleaner (Optional) -> Splitter (Optional). Therefore, adding support for a new datasource requires implementing these components and their respective manager.","title":"Architecture"},{"location":"how_to/how_to_add_new_datasource/#implementation","text":"","title":"Implementation"},{"location":"how_to/how_to_add_new_datasource/#step-1-dependencies","text":"Add the required packages to pyproject.toml under the following section: [project.optional-dependencies] extraction = [ \"atlassian-python-api>=3.41.19\", ... ]","title":"Step 1: Dependencies"},{"location":"how_to/how_to_add_new_datasource/#step-2-datasource-enum","text":"In datasources.py , add the new datasource to the DatasourceName enumeration: class DatasourceName(str, Enum): ... CONFLUENCE = \"confluence\"","title":"Step 2: Datasource Enum"},{"location":"how_to/how_to_add_new_datasource/#step-3-datasource-secrets","text":"Create a new directory src/extraction/datasources/confluence and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal, Union from pydantic import ConfigDict, Field, SecretStr from core.base_configuration import BaseSecrets from extraction.bootstrap.configuration.datasources import ( DatasourceConfiguration, DatasourceName, ) class ConfluenceDatasourceConfiguration(DatasourceConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAG__DATASOURCES__CONFLUENCE__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) username: SecretStr = Field( ..., description=\"Username credential used to authenticate with the Confluence instance\", ) password: SecretStr = Field( ..., description=\"Password credential used to authenticate with the Confluence instance\", ) secrets: Secrets = Field( None, description=\"Authentication credentials required to access the Confluence instance\", ) The first part is to create a configuration that extends DatasourceConfiguration . The Secrets inner class defines secret fields that will be present in the environment secret file under the RAG__DATASOURCES__CONFLUENCE__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAG__DATASOURCES__CONFLUENCE__USERNAME=<confluence_username> RAG__DATASOURCES__CONFLUENCE__PASSWORD=<confluence_password> Note : If your datasource doesn't require secrets, you can skip this step.","title":"Step 3: Datasource Secrets"},{"location":"how_to/how_to_add_new_datasource/#step-4-datasource-configuration","text":"Finish up ConfluenceDatasourceConfiguration implementation and add the rest of the configuration required for the datasource: ... class ConfluenceDatasourceConfiguration(DatasourceConfiguration): ... host: str = Field( \"127.0.0.1\", description=\"Hostname or IP address of the Confluence server instance\", ) protocol: Union[Literal[\"http\"], Literal[\"https\"]] = Field( \"http\", description=\"Communication protocol used to connect to the Confluence server\", ) name: Literal[DatasourceName.CONFLUENCE] = Field( ..., description=\"Identifier specifying this configuration is for a Confluence datasource\", ) @property def base_url(self) -> str: return f\"{self.protocol}://{self.host}\" provider field constraints the value to DatasourceName.CONFLUENCE , which serves as an indicator for pydantic validator.","title":"Step 4: Datasource Configuration"},{"location":"how_to/how_to_add_new_datasource/#step-5-confluence-document","text":"The next step is to create a Confluence document data class in document.py : from extraction.datasources.core.document import BaseDocument class ConfluenceDocument(BaseDocument): \"\"\"Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. \"\"\" pass In our case, we don't need anything beyond the BaseDocument implementation.","title":"Step 5: Confluence Document"},{"location":"how_to/how_to_add_new_datasource/#step-6-confluence-client","text":"To create a Confluence client, we implement ConfluenceClientFactory in client.py . It extends SingletonFactory , which provides an interface for initializing a single instance for the duration of the application runtime. from typing import Type from atlassian import Confluence from core import SingletonFactory from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) class ConfluenceClientFactory(SingletonFactory): _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> Confluence: return Confluence( url=configuration.base_url, username=configuration.secrets.username.get_secret_value(), password=configuration.secrets.password.get_secret_value(), ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding client initialization.","title":"Step 6: Confluence Client"},{"location":"how_to/how_to_add_new_datasource/#step-7-datasource-reader","text":"Create a Confluence reader in reader.py that implements the BaseReader interface: from extraction.datasources.core.reader import BaseReader ... class ConfluenceDatasourceReader(BaseReader): async def read_all_async( self, ) -> AsyncIterator[dict]: # read Confluence pages implementation This method returns an iterator, which improves runtime memory management. Next, implement a factory that defines how the ConfluenceDatasourceReader is initialized: from core import Factory ... class ConfluenceDatasourceReaderFactory(Factory): _configuration_class = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceReader: client = ConfluenceClientFactory.create(configuration) return ConfluenceDatasourceReader( configuration=configuration, client=client, ) Note that instead of initializing the Confluence client directly, the factory uses ConfluenceClientFactory to handle this task.","title":"Step 7: Datasource Reader"},{"location":"how_to/how_to_add_new_datasource/#step-8-datasource-parser","text":"In parser.py implement a parser responsible for converting the raw Confluence page to markdown format: from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) from extraction.datasources.confluence.document import ConfluenceDocument from extraction.datasources.core.parser import BaseParser class ConfluenceDatasourceParser(BaseParser[ConfluenceDocument]): def parse(self, page: str) -> ConfluenceDocument: # parse Confluence page implementation As before, define a factory for the parser: class ConfluenceDatasourceParserFactory(Factory): _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceParser: return ConfluenceDatasourceParser(configuration)","title":"Step 8: Datasource Parser"},{"location":"how_to/how_to_add_new_datasource/#step-9-datasource-manager","text":"To orchestrate all the previous components, we will reuse BasicDatasourceManager and implement a factory for it in manager.py : class ConfluenceDatasourceManagerFactory(Factory): \"\"\"Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class: Configuration class used for validating and processing Confluence-specific settings. \"\"\" _configuration_class: Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance( cls, configuration: ConfluenceDatasourceConfiguration ) -> BasicDatasourceManager: \"\"\"Create a configured Confluence datasource manager. Sets up the necessary reader and parser components based on the provided configuration and assembles them into a functional manager. Args: configuration: Configuration object containing Confluence-specific parameters including authentication details, spaces to extract, and other extraction options. Returns: A fully initialized datasource manager that can extract and process data from Confluence. \"\"\" reader = ConfluenceDatasourceReaderFactory.create(configuration) parser = ConfluenceDatasourceParserFactory.create(configuration) return BasicDatasourceManager(configuration, reader, parser) Following the design pattern, ConfluenceDatasourceManagerFactory uses reader and parser factories to obtain the instances needed for the manager.","title":"Step 9: Datasource Manager"},{"location":"how_to/how_to_add_new_datasource/#step-10-datasource-integration","text":"Create an __init__.py file as follows: from extraction.bootstrap.configuration.datasources import ( DatasourceConfigurationRegistry, DatasourceName, ) from extraction.datasources.confluence.configuration import ( ConfluenceDatasourceConfiguration, ) from extraction.datasources.confluence.manager import ( ConfluenceDatasourceManagerFactory, ) from extraction.datasources.registry import DatasourceManagerRegistry def register() -> None: DatasourceManagerRegistry.register( DatasourceName.CONFLUENCE, ConfluenceDatasourceManagerFactory ) DatasourceConfigurationRegistry.register( DatasourceName.CONFLUENCE, ConfluenceDatasourceConfiguration ) The initialization file includes a register() method responsible for registering our configuration and manager factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Confluence configuration in configurations/configuration.{environment}.json file: \"extraction\": { \"datasources\": [ { \"name\": \"confluence\", \"host\": \"wissen.feld-m.de\", \"protocol\": \"https\" } ] ... } ... We can dynamically retrieve the corresponding manager implementation by using the name specified in the configuration: datasource_config = read_datasource_from_config() datasource_manager = DatasourceManagerRegistry.get(datasource_config.name).create(datasource_config) This mechanism is later used by DatasourceOrchestrator to initialize datasources defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 extraction/ \u2514\u2500\u2500 datasources/ \u2514\u2500\u2500 confluence/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 client.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 document.py \u251c\u2500\u2500 manager.py \u251c\u2500\u2500 parser.py \u2514\u2500\u2500 reader.py","title":"Step 10: Datasource Integration"},{"location":"how_to/how_to_add_new_datasource/#notes","text":"Below is the __init__ method of BasicDatasourceManager used in our tutorial: class BasicDatasourceManager(BaseDatasourceManager, Generic[DocType]): def __init__( self, configuration: ExtractionConfiguration, reader: BaseReader, parser: BaseParser = BasicMarkdownParser(), cleaner: BaseCleaner = BasicMarkdownCleaner(), splitter: BaseSplitter = BasicMarkdownSplitter(), ): Note that in this guide we skipped the implementation of custom cleaner and splitter components, instead using the default ones. When building a new datasource integration, you might need to implement custom versions of these components based on your specific requirements.","title":"Notes"},{"location":"how_to/how_to_add_new_embedding_model/","text":"How to Add a New Embedding Model Implementation This guide demonstrates how to add support for a new embedding model implementation, using VoyageAI provider as an example. Architecture Embedding models are used to generate the user query and datasource embeddings. These embeddings are used for semantic search and retrieval in the RAG pipeline. Therefore, adding support for a new embedding model requires implementing the configuration and Llamaindex integration. Implementation Step 1: Dependencies Add the required packages to pyproject.toml under the following section: [project.optional-dependencies] embedding = [ \"llama-index-embeddings-voyageai>=0.3.5\", ... ] Step 2: Embedding Model Enum Embedding model configuration is scoped by provider. Each provider, such as Voyage , requires its own Pydantic configuration class. Begin by assigning a meaningful name to the new provider in the LLMProviderName enumeration in embedding_model_configuration.py : class EmbeddingModelProviderName(str, Enum): ... VOYAGE = \"voyage\" Step 3: Embedding Model Secrets And Configuration Create a new directory src/embedding/embedding_models/voyage and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal from pydantic import ConfigDict, Field, SecretStr from core.base_configuration import BaseSecrets from embedding.bootstrap.configuration.embedding_model_configuration import ( EmbeddingModelConfiguration, EmbeddingModelProviderName, ) class VoyageEmbeddingModelConfiguration(EmbeddingModelConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAGKB__EMBEDDING_MODELS__VOYAGE__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) api_key: SecretStr = Field(..., description=\"API key for the model\") provider: Literal[EmbeddingModelProviderName.VOYAGE] = Field( ..., description=\"The provider of the embedding model.\" ) secrets: Secrets = Field( None, description=\"The secrets for the language model.\" ) The first part is to create a configuration that extends EmbeddingModelConfiguration . provider field constraints the value to EmbeddingModelProviderName.VOYAGE , which serves as an indicator for pydantic validator. The Secrets inner class defines secret fields that will be present in the environment secret file under the RAGKB__EMBEDDING_MODELS__VOYAGE__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAGKB__EMBEDDING_MODELS__VOYAGE__API_KEY=<voyage_api_key> Note : If your embedding model doesn't require secrets, you can skip this step. Step 4: Embedding Model Implementation In the embedding_model.py file, create singleton embedding model factory. It provides a framework, where embedding model can be retrieved through VoyageEmbeddingModelFactory and is initialized only once per runtime, saving up the memory (e.g. in cases of small in-memory embedding models). To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Callable, Type from llama_index.embeddings.voyageai import VoyageEmbedding from transformers import AutoTokenizer from core import SingletonFactory from embedding.embedding_models.voyage.configuration import ( VoyageEmbeddingModelConfiguration, ) class VoyageEmbeddingModelFactory(SingletonFactory): _configuration_class: Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance( cls, configuration: VoyageEmbeddingModelConfiguration ) -> VoyageEmbedding: return VoyageEmbedding( voyage_api_key=configuration.secrets.api_key.get_secret_value(), model_name=configuration.name, embed_batch_size=configuration.batch_size, ) In the same file implement a factory that defines the tokenizer used along with this embedding model. For the same reasons as previously we use singleton factory. class VoyageEmbeddingModelTokenizerFactory(SingletonFactory): _configuration_class: Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance( cls, configuration: VoyageEmbeddingModelConfiguration ) -> Callable: return AutoTokenizer.from_pretrained( configuration.tokenizer_name ).tokenize Step 7: Embedding Model Integration Create an __init__.py file as follows: from embedding.bootstrap.configuration.embedding_model_configuration import ( EmbeddingModelConfigurationRegistry, EmbeddingModelProviderName, ) from embedding.embedding_models.registry import ( EmbeddingModelRegistry, EmbeddingModelTokenizerRegistry, ) from embedding.embedding_models.voyage.configuration import ( VoyageEmbeddingModelConfiguration, ) from embedding.embedding_models.voyage.embedding_model import ( VoyageEmbeddingModelFactory, VoyageEmbeddingModelTokenizerFactory, ) def register(): EmbeddingModelConfigurationRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelConfiguration ) EmbeddingModelRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelFactory ) EmbeddingModelTokenizerRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelTokenizerFactory ) The initialization file includes a register() method responsible for registering our configuration, embedding model and its tokenizer factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Voyage configuration in configurations/configuration.{environment}.json file: \"embedding\": { \"embedding_model\": { \"provider\": \"voyage\", \"name\": \"voyage-3\", // any model name compatible with VoyageAI API \"tokenizer_name\": \"voyageai/voyage-3\", // any tokenizer name compatible with VoyageAI and AutoTokenizer \"splitter\": { \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } ... } ... Note : You can use any model_name and tokenizer_name exposed by VoyageAI We can dynamically retrieve the corresponding embedding model implementation by using the name specified in the configuration: embedding_model_config = read_embedding_model_from_config() embedding_model = EmbeddingModelRegistry.get(embedding_model_config.name).create(embedding_model_config) This mechanism is later used by the embedding manager to initialize the embedding model defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 embedding/ \u2514\u2500\u2500 embedding_models/ \u2514\u2500\u2500 voyage/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 configuration.py \u2514\u2500\u2500 embedding_model.py","title":"Add a New Embedding Model"},{"location":"how_to/how_to_add_new_embedding_model/#how-to-add-a-new-embedding-model-implementation","text":"This guide demonstrates how to add support for a new embedding model implementation, using VoyageAI provider as an example.","title":"How to Add a New Embedding Model Implementation"},{"location":"how_to/how_to_add_new_embedding_model/#architecture","text":"Embedding models are used to generate the user query and datasource embeddings. These embeddings are used for semantic search and retrieval in the RAG pipeline. Therefore, adding support for a new embedding model requires implementing the configuration and Llamaindex integration.","title":"Architecture"},{"location":"how_to/how_to_add_new_embedding_model/#implementation","text":"","title":"Implementation"},{"location":"how_to/how_to_add_new_embedding_model/#step-1-dependencies","text":"Add the required packages to pyproject.toml under the following section: [project.optional-dependencies] embedding = [ \"llama-index-embeddings-voyageai>=0.3.5\", ... ]","title":"Step 1: Dependencies"},{"location":"how_to/how_to_add_new_embedding_model/#step-2-embedding-model-enum","text":"Embedding model configuration is scoped by provider. Each provider, such as Voyage , requires its own Pydantic configuration class. Begin by assigning a meaningful name to the new provider in the LLMProviderName enumeration in embedding_model_configuration.py : class EmbeddingModelProviderName(str, Enum): ... VOYAGE = \"voyage\"","title":"Step 2: Embedding Model Enum"},{"location":"how_to/how_to_add_new_embedding_model/#step-3-embedding-model-secrets-and-configuration","text":"Create a new directory src/embedding/embedding_models/voyage and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal from pydantic import ConfigDict, Field, SecretStr from core.base_configuration import BaseSecrets from embedding.bootstrap.configuration.embedding_model_configuration import ( EmbeddingModelConfiguration, EmbeddingModelProviderName, ) class VoyageEmbeddingModelConfiguration(EmbeddingModelConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAGKB__EMBEDDING_MODELS__VOYAGE__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) api_key: SecretStr = Field(..., description=\"API key for the model\") provider: Literal[EmbeddingModelProviderName.VOYAGE] = Field( ..., description=\"The provider of the embedding model.\" ) secrets: Secrets = Field( None, description=\"The secrets for the language model.\" ) The first part is to create a configuration that extends EmbeddingModelConfiguration . provider field constraints the value to EmbeddingModelProviderName.VOYAGE , which serves as an indicator for pydantic validator. The Secrets inner class defines secret fields that will be present in the environment secret file under the RAGKB__EMBEDDING_MODELS__VOYAGE__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAGKB__EMBEDDING_MODELS__VOYAGE__API_KEY=<voyage_api_key> Note : If your embedding model doesn't require secrets, you can skip this step.","title":"Step 3: Embedding Model Secrets And Configuration"},{"location":"how_to/how_to_add_new_embedding_model/#step-4-embedding-model-implementation","text":"In the embedding_model.py file, create singleton embedding model factory. It provides a framework, where embedding model can be retrieved through VoyageEmbeddingModelFactory and is initialized only once per runtime, saving up the memory (e.g. in cases of small in-memory embedding models). To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Callable, Type from llama_index.embeddings.voyageai import VoyageEmbedding from transformers import AutoTokenizer from core import SingletonFactory from embedding.embedding_models.voyage.configuration import ( VoyageEmbeddingModelConfiguration, ) class VoyageEmbeddingModelFactory(SingletonFactory): _configuration_class: Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance( cls, configuration: VoyageEmbeddingModelConfiguration ) -> VoyageEmbedding: return VoyageEmbedding( voyage_api_key=configuration.secrets.api_key.get_secret_value(), model_name=configuration.name, embed_batch_size=configuration.batch_size, ) In the same file implement a factory that defines the tokenizer used along with this embedding model. For the same reasons as previously we use singleton factory. class VoyageEmbeddingModelTokenizerFactory(SingletonFactory): _configuration_class: Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance( cls, configuration: VoyageEmbeddingModelConfiguration ) -> Callable: return AutoTokenizer.from_pretrained( configuration.tokenizer_name ).tokenize","title":"Step 4: Embedding Model Implementation"},{"location":"how_to/how_to_add_new_embedding_model/#step-7-embedding-model-integration","text":"Create an __init__.py file as follows: from embedding.bootstrap.configuration.embedding_model_configuration import ( EmbeddingModelConfigurationRegistry, EmbeddingModelProviderName, ) from embedding.embedding_models.registry import ( EmbeddingModelRegistry, EmbeddingModelTokenizerRegistry, ) from embedding.embedding_models.voyage.configuration import ( VoyageEmbeddingModelConfiguration, ) from embedding.embedding_models.voyage.embedding_model import ( VoyageEmbeddingModelFactory, VoyageEmbeddingModelTokenizerFactory, ) def register(): EmbeddingModelConfigurationRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelConfiguration ) EmbeddingModelRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelFactory ) EmbeddingModelTokenizerRegistry.register( EmbeddingModelProviderName.VOYAGE, VoyageEmbeddingModelTokenizerFactory ) The initialization file includes a register() method responsible for registering our configuration, embedding model and its tokenizer factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Voyage configuration in configurations/configuration.{environment}.json file: \"embedding\": { \"embedding_model\": { \"provider\": \"voyage\", \"name\": \"voyage-3\", // any model name compatible with VoyageAI API \"tokenizer_name\": \"voyageai/voyage-3\", // any tokenizer name compatible with VoyageAI and AutoTokenizer \"splitter\": { \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } ... } ... Note : You can use any model_name and tokenizer_name exposed by VoyageAI We can dynamically retrieve the corresponding embedding model implementation by using the name specified in the configuration: embedding_model_config = read_embedding_model_from_config() embedding_model = EmbeddingModelRegistry.get(embedding_model_config.name).create(embedding_model_config) This mechanism is later used by the embedding manager to initialize the embedding model defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 embedding/ \u2514\u2500\u2500 embedding_models/ \u2514\u2500\u2500 voyage/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 configuration.py \u2514\u2500\u2500 embedding_model.py","title":"Step 7: Embedding Model Integration"},{"location":"how_to/how_to_add_new_llm/","text":"How to Add a New LLM Implementation This guide demonstrates how to add support for a new Language Model (LLM) implementation, using OpenAI as an example. Architecture Large Language Models are mainly responsible for generating the answers based on the user query and retrieved nodes, injected as a context. They are also used in the evaluation process. Additionally, they can be used in various components e.g. AutoRetriever . Implementation Step 1: Dependencies Add the required packages to pyproject.toml : [project.optional-dependencies] augmentation = [ \"llama-index-llms-openai>=0.3.25\", ... ] Step 2: LLM Enum LLM configuration is scoped by provider. Each provider, such as OpenAI , requires its own Pydantic configuration class. Begin by assigning a meaningful name to the new provider in the LLMProviderName enumeration in llm_configuration.py : class LLMProviderName(str, Enum): ... OPENAI = \"openai\" Step 3: LLM Configuration And Secrets Create a new directory src/augmentation/components/llms/openai and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal from pydantic import ConfigDict, Field, SecretStr from augmentation.bootstrap.configuration.components.llm_configuration import ( LLMConfiguration, LLMProviderName, ) from core.base_configuration import BaseSecrets class OpenAILLMConfiguration(LLMConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAG__LLMS__OPENAI__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) api_key: SecretStr = Field( ..., description=\"API key for the model provider.\" ) provider: Literal[LLMProviderName.OPENAI] = Field( ..., description=\"The name of the language model provider.\" ) secrets: Secrets = Field( None, description=\"The secrets for the language model.\" ) The first part is to create a configuration that extends LLMConfiguration . provider field constraints the value to LLMProviderName.OPENAI , which serves as an indicator for pydantic validator. The Secrets inner class defines secret fields that will be present in the environment secret file under the RAG__LLMS__OPENAI__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAG__LLMS__OPENAI__API_KEY=<openai_api_key> Step 4: LLM Implementation In the llm.py file, create singleton LLM factory. It provides a framework, where LLM can be retrieved through OpenaAILLMFactory and is initialized only once per runtime, saving up the memory (e.g. in cases of small in-memory LLMs). To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Type from llama_index.llms.openai import OpenAI from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from core import SingletonFactory class OpenaAILLMFactory(SingletonFactory): _configuration_class: Type = OpenAILLMConfiguration @classmethod def _create_instance(cls, configuration: OpenAILLMConfiguration) -> OpenAI: return OpenAI( api_key=configuration.secrets.api_key.get_secret_value(), model=configuration.name, max_tokens=configuration.max_tokens, max_retries=configuration.max_retries, ) Step 5: LLM Output Extractor Human feedback feature between Chainlit and Langfuse require extraction of information about LLM response. Each provider returns the differently structured output dictionary. Therefore, we need to implement an extractor of required fields. Create output_extractor.py : from typing import Type from langfuse.api.resources.commons.types.trace_with_details import ( TraceWithDetails, ) from augmentation.components.llms.core.base_output_extractor import ( BaseLlamaindexLLMOutputExtractor, ) from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from core.base_factory import Factory class OpenAILlamaindexLLMOutputExtractor(BaseLlamaindexLLMOutputExtractor): def get_text(self, trace: TraceWithDetails) -> str: return trace.output[\"blocks\"][0][\"text\"] def get_generated_by_model(self, trace: TraceWithDetails) -> str: return self.configuration.name Implemented interface BaseLlamaindexLLMOutputExtractor , provide sufficient extractor for ChainlitFeedbackService purposes. Now just add correspodning factory: class OpenAILlamaindexLLMOutputExtractorFactory(Factory): _configuration_class: Type = OpenAILLMConfiguration @classmethod def _create_instance( cls, configuration: OpenAILLMConfiguration ) -> OpenAILlamaindexLLMOutputExtractor: return OpenAILlamaindexLLMOutputExtractor(configuration) Step 6: LLM Integration Create an __init__.py file as follows: from augmentation.bootstrap.configuration.components.llm_configuration import ( LLMConfigurationRegistry, LLMProviderName, ) from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from augmentation.components.llms.openai.llm import OpenaAILLMFactory from augmentation.components.llms.openai.output_extractor import ( OpenAILlamaindexLLMOutputExtractorFactory, ) from augmentation.components.llms.registry import ( LlamaindexLLMOutputExtractorRegistry, LLMRegistry, ) def register() -> None: LLMRegistry.register(LLMProviderName.OPENAI, OpenaAILLMFactory) LLMConfigurationRegistry.register( LLMProviderName.OPENAI, OpenAILLMConfiguration ) LlamaindexLLMOutputExtractorRegistry.register( LLMProviderName.OPENAI, OpenAILlamaindexLLMOutputExtractorFactory ) The initialization file includes a register() method responsible for registering our configuration, output extractor and LLM factories. Registries are used to dynamically inform the system about available implementations. This way, with the following OpenAI configuration in configurations/configuration.{environment}.json file: \"augmentation\": { \"chat_engine\": { \"llm\": { \"provider\": \"openai\", \"name\": \"gpt-4o\", // any model name compatible with OpenAI API } } ... } Note : You can use any name exposed by OpenAI We can dynamically retrieve the corresponding LLM implementation by using the name specified in the configuration: llm_config = read_llm_from_config() llm_model = LLMRegistry.get(llm_config.name).create(llm_config) This mechanism is later used by the chat engine to initialize the llm defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 augmentation/ \u2514\u2500\u2500 components/ \u2514\u2500\u2500 llms/ \u2514\u2500\u2500 openai/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 llm.py \u2514\u2500\u2500 output_extractor.py","title":"Add a New LLM"},{"location":"how_to/how_to_add_new_llm/#how-to-add-a-new-llm-implementation","text":"This guide demonstrates how to add support for a new Language Model (LLM) implementation, using OpenAI as an example.","title":"How to Add a New LLM Implementation"},{"location":"how_to/how_to_add_new_llm/#architecture","text":"Large Language Models are mainly responsible for generating the answers based on the user query and retrieved nodes, injected as a context. They are also used in the evaluation process. Additionally, they can be used in various components e.g. AutoRetriever .","title":"Architecture"},{"location":"how_to/how_to_add_new_llm/#implementation","text":"","title":"Implementation"},{"location":"how_to/how_to_add_new_llm/#step-1-dependencies","text":"Add the required packages to pyproject.toml : [project.optional-dependencies] augmentation = [ \"llama-index-llms-openai>=0.3.25\", ... ]","title":"Step 1: Dependencies"},{"location":"how_to/how_to_add_new_llm/#step-2-llm-enum","text":"LLM configuration is scoped by provider. Each provider, such as OpenAI , requires its own Pydantic configuration class. Begin by assigning a meaningful name to the new provider in the LLMProviderName enumeration in llm_configuration.py : class LLMProviderName(str, Enum): ... OPENAI = \"openai\"","title":"Step 2: LLM Enum"},{"location":"how_to/how_to_add_new_llm/#step-3-llm-configuration-and-secrets","text":"Create a new directory src/augmentation/components/llms/openai and create a configuration.py file in it. This configuration file will contain necessary fields and secrets for setup. from typing import Literal from pydantic import ConfigDict, Field, SecretStr from augmentation.bootstrap.configuration.components.llm_configuration import ( LLMConfiguration, LLMProviderName, ) from core.base_configuration import BaseSecrets class OpenAILLMConfiguration(LLMConfiguration): class Secrets(BaseSecrets): model_config = ConfigDict( env_file_encoding=\"utf-8\", env_prefix=\"RAG__LLMS__OPENAI__\", env_nested_delimiter=\"__\", extra=\"ignore\", ) api_key: SecretStr = Field( ..., description=\"API key for the model provider.\" ) provider: Literal[LLMProviderName.OPENAI] = Field( ..., description=\"The name of the language model provider.\" ) secrets: Secrets = Field( None, description=\"The secrets for the language model.\" ) The first part is to create a configuration that extends LLMConfiguration . provider field constraints the value to LLMProviderName.OPENAI , which serves as an indicator for pydantic validator. The Secrets inner class defines secret fields that will be present in the environment secret file under the RAG__LLMS__OPENAI__ prefix. Add the corresponding environment variables to configurations/secrets.{environment}.env : RAG__LLMS__OPENAI__API_KEY=<openai_api_key>","title":"Step 3: LLM Configuration And Secrets"},{"location":"how_to/how_to_add_new_llm/#step-4-llm-implementation","text":"In the llm.py file, create singleton LLM factory. It provides a framework, where LLM can be retrieved through OpenaAILLMFactory and is initialized only once per runtime, saving up the memory (e.g. in cases of small in-memory LLMs). To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Type from llama_index.llms.openai import OpenAI from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from core import SingletonFactory class OpenaAILLMFactory(SingletonFactory): _configuration_class: Type = OpenAILLMConfiguration @classmethod def _create_instance(cls, configuration: OpenAILLMConfiguration) -> OpenAI: return OpenAI( api_key=configuration.secrets.api_key.get_secret_value(), model=configuration.name, max_tokens=configuration.max_tokens, max_retries=configuration.max_retries, )","title":"Step 4: LLM Implementation"},{"location":"how_to/how_to_add_new_llm/#step-5-llm-output-extractor","text":"Human feedback feature between Chainlit and Langfuse require extraction of information about LLM response. Each provider returns the differently structured output dictionary. Therefore, we need to implement an extractor of required fields. Create output_extractor.py : from typing import Type from langfuse.api.resources.commons.types.trace_with_details import ( TraceWithDetails, ) from augmentation.components.llms.core.base_output_extractor import ( BaseLlamaindexLLMOutputExtractor, ) from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from core.base_factory import Factory class OpenAILlamaindexLLMOutputExtractor(BaseLlamaindexLLMOutputExtractor): def get_text(self, trace: TraceWithDetails) -> str: return trace.output[\"blocks\"][0][\"text\"] def get_generated_by_model(self, trace: TraceWithDetails) -> str: return self.configuration.name Implemented interface BaseLlamaindexLLMOutputExtractor , provide sufficient extractor for ChainlitFeedbackService purposes. Now just add correspodning factory: class OpenAILlamaindexLLMOutputExtractorFactory(Factory): _configuration_class: Type = OpenAILLMConfiguration @classmethod def _create_instance( cls, configuration: OpenAILLMConfiguration ) -> OpenAILlamaindexLLMOutputExtractor: return OpenAILlamaindexLLMOutputExtractor(configuration)","title":"Step 5: LLM Output Extractor"},{"location":"how_to/how_to_add_new_llm/#step-6-llm-integration","text":"Create an __init__.py file as follows: from augmentation.bootstrap.configuration.components.llm_configuration import ( LLMConfigurationRegistry, LLMProviderName, ) from augmentation.components.llms.openai.configuration import ( OpenAILLMConfiguration, ) from augmentation.components.llms.openai.llm import OpenaAILLMFactory from augmentation.components.llms.openai.output_extractor import ( OpenAILlamaindexLLMOutputExtractorFactory, ) from augmentation.components.llms.registry import ( LlamaindexLLMOutputExtractorRegistry, LLMRegistry, ) def register() -> None: LLMRegistry.register(LLMProviderName.OPENAI, OpenaAILLMFactory) LLMConfigurationRegistry.register( LLMProviderName.OPENAI, OpenAILLMConfiguration ) LlamaindexLLMOutputExtractorRegistry.register( LLMProviderName.OPENAI, OpenAILlamaindexLLMOutputExtractorFactory ) The initialization file includes a register() method responsible for registering our configuration, output extractor and LLM factories. Registries are used to dynamically inform the system about available implementations. This way, with the following OpenAI configuration in configurations/configuration.{environment}.json file: \"augmentation\": { \"chat_engine\": { \"llm\": { \"provider\": \"openai\", \"name\": \"gpt-4o\", // any model name compatible with OpenAI API } } ... } Note : You can use any name exposed by OpenAI We can dynamically retrieve the corresponding LLM implementation by using the name specified in the configuration: llm_config = read_llm_from_config() llm_model = LLMRegistry.get(llm_config.name).create(llm_config) This mechanism is later used by the chat engine to initialize the llm defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 augmentation/ \u2514\u2500\u2500 components/ \u2514\u2500\u2500 llms/ \u2514\u2500\u2500 openai/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 llm.py \u2514\u2500\u2500 output_extractor.py","title":"Step 6: LLM Integration"},{"location":"how_to/how_to_add_new_vector_store/","text":"How to Add a New Vector Store Implementation This guide demonstrates how to add support for a new vector store implementation, using Chroma as an example. Architecture Vector store is used for storing and retrieving embeddings of datasource nodes. Implementation Step 1: Add Dependencies Add the required packages to pyproject.toml : [project.optional-dependencies] embedding = [ \"chromadb>=0.6.3\", \"llama-index-vector-stores-chroma>=0.4.1\", ... ] Step 2: Docker Service Add the vector store service to docker-compose.yml : name: rag services: ... chroma: image: chromadb/chroma:0.6.4.dev19 environment: CHROMA_HOST_PORT: ${RAG__VECTOR_STORE__PORT_REST} ports: - \"${RAG__VECTOR_STORE__PORT_REST}:${RAG__VECTOR_STORE__PORT_REST}\" restart: unless-stopped volumes: - ./.docker-data/chroma:/chroma/chroma/ ... It enables easy vector store initialization using init.sh script. Step 3: Vector Store Enum Add the vector store to the VectorStoreName enum in vector_store_configuration.py : class VectorStoreName(str, Enum): ... CHROMA = \"chroma\" The enum value must match the service name in the Docker configuration. Step 4: Vector Store Configuration Create a new directory src/embedding/vector_stores/chroma and create a configuration.py file in it. This configuration file will contain necessary fields for setup. from typing import Literal from pydantic import Field from embedding.bootstrap.configuration.vector_store_configuration import ( VectorStoreConfiguration, VectorStoreName, ) class ChromaVectorStoreConfiguration(VectorStoreConfiguration): \"\"\"Configuration for the ChromaDB vector store.\"\"\" name: Literal[VectorStoreName.CHROMA] = Field( ..., description=\"The name of the vector store.\" ) The first part is to create a configuration that extends VectorStoreConfiguration . name field constraints the value to VectorStoreName.CHROMA , which serves as an indicator for pydantic validator. Note : For adding potentially needed secrets support follow the same approach as explained in How to Add a New LLM Implementation guide. Step 5: Vector Store Implementation In the vector_store.py file, create singleton vector store factory. It provides a framework, where vector store can be retrieved through ChromaVectorStoreFactory and is initialized only once per runtime, saving up the memory. To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Type from llama_index.vector_stores.chroma import ChromaVectorStore from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) class ChromaVectorStoreFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaVectorStore: return ChromaVectorStore( host=configuration.host, port=str(configuration.port), collection_name=configuration.collection_name, ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding vector store initialization. Step 6: Vector Store Client We will want to validate our vector store before the run, for that we need and HTTP client. To create a Chroma client, we implement ChromaVectorStoreClientFactory in client.py . It extends SingletonFactory , which provides an interface for initializing a single instance for the duration of the application runtime. from typing import Type from chromadb import HttpClient as ChromaHttpClient from chromadb.api import ClientAPI as ChromaClient from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) class ChromaVectorStoreClientFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaClient: return ChromaHttpClient( host=configuration.host, port=configuration.port, ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding client initialization. Step 7: Vector Store Validator Now we can implement the validator that will check if defined vector store collection already exists. Nevertheless, it can be extended to validate other apsects as well. Create validator.py file and create ChromaVectorStoreValidator that implements BaseVectorStoreValidator interface: from typing import Type from chromadb.api import ClientAPI as ChromaClient from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.client import ChromaVectorStoreClientFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) from embedding.vector_stores.core.exceptions import CollectionExistsException from embedding.vector_stores.core.validator import BaseVectorStoreValidator class ChromaVectorStoreValidator(BaseVectorStoreValidator): def __init__( self, configuration: ChromaVectorStoreConfiguration, client: ChromaClient, ): self.configuration = configuration self.client = client def validate(self) -> None: self.validate_collection() def validate_collection(self) -> None: collection_name = self.configuration.collection_name if collection_name in self.client.list_collections(): raise CollectionExistsException(collection_name) Now add the factory that defines validator initialization. class ChromaVectorStoreValidatorFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaVectorStoreValidator: client = ChromaVectorStoreClientFactory.create(configuration) return ChromaVectorStoreValidator( configuration=configuration, client=client ) You can notice that we use previously implemented ChromaVectorStoreClientFactory to get required client instance. Step 8: Vector Store Integration Create an __init__.py file as follows: from embedding.bootstrap.configuration.vector_store_configuration import ( VectorStoreConfigurationRegistry, VectorStoreName, ) from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) from embedding.vector_stores.chroma.validator import ( ChromaVectorStoreValidatorFactory, ) from embedding.vector_stores.chroma.vector_store import ChromaVectorStoreFactory from embedding.vector_stores.registry import ( VectorStoreRegistry, VectorStoreValidatorRegistry, ) def register() -> None: VectorStoreConfigurationRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreConfiguration, ) VectorStoreRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreFactory ) VectorStoreValidatorRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreValidatorFactory ) The initialization file includes a register() method responsible for registering our configuration, and validator and vector store factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Chroma configuration in configurations/configuration.{environment}.json file: \"embedding\": { \"vector_store\": { \"name\": \"chroma\", \"collection_name\": \"new-collection\", \"host\": \"chroma\", \"protocol\": \"http\", \"port\": 6000 } ... }, We can dynamically retrieve the corresponding vector store implementation by using the name specified in the configuration: vector_store_config = read_vector_store_from_config() vector_store = VectorStoreRegistry.get(vector_store_config.name).create(vector_store_config) vector_store_validator = VectorStoreValidatorRegistry.get(vector_store_config.name).create(vector_store_config) This mechanism is later used by the embedding orchestrator to initialize the vector store defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 embedding/ \u2514\u2500\u2500 vector_stores/ \u2514\u2500\u2500 chroma/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 client.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 validator.py \u2514\u2500\u2500 vector_store.py","title":"Add a New Vector Store"},{"location":"how_to/how_to_add_new_vector_store/#how-to-add-a-new-vector-store-implementation","text":"This guide demonstrates how to add support for a new vector store implementation, using Chroma as an example.","title":"How to Add a New Vector Store Implementation"},{"location":"how_to/how_to_add_new_vector_store/#architecture","text":"Vector store is used for storing and retrieving embeddings of datasource nodes.","title":"Architecture"},{"location":"how_to/how_to_add_new_vector_store/#implementation","text":"","title":"Implementation"},{"location":"how_to/how_to_add_new_vector_store/#step-1-add-dependencies","text":"Add the required packages to pyproject.toml : [project.optional-dependencies] embedding = [ \"chromadb>=0.6.3\", \"llama-index-vector-stores-chroma>=0.4.1\", ... ]","title":"Step 1: Add Dependencies"},{"location":"how_to/how_to_add_new_vector_store/#step-2-docker-service","text":"Add the vector store service to docker-compose.yml : name: rag services: ... chroma: image: chromadb/chroma:0.6.4.dev19 environment: CHROMA_HOST_PORT: ${RAG__VECTOR_STORE__PORT_REST} ports: - \"${RAG__VECTOR_STORE__PORT_REST}:${RAG__VECTOR_STORE__PORT_REST}\" restart: unless-stopped volumes: - ./.docker-data/chroma:/chroma/chroma/ ... It enables easy vector store initialization using init.sh script.","title":"Step 2: Docker Service"},{"location":"how_to/how_to_add_new_vector_store/#step-3-vector-store-enum","text":"Add the vector store to the VectorStoreName enum in vector_store_configuration.py : class VectorStoreName(str, Enum): ... CHROMA = \"chroma\" The enum value must match the service name in the Docker configuration.","title":"Step 3: Vector Store Enum"},{"location":"how_to/how_to_add_new_vector_store/#step-4-vector-store-configuration","text":"Create a new directory src/embedding/vector_stores/chroma and create a configuration.py file in it. This configuration file will contain necessary fields for setup. from typing import Literal from pydantic import Field from embedding.bootstrap.configuration.vector_store_configuration import ( VectorStoreConfiguration, VectorStoreName, ) class ChromaVectorStoreConfiguration(VectorStoreConfiguration): \"\"\"Configuration for the ChromaDB vector store.\"\"\" name: Literal[VectorStoreName.CHROMA] = Field( ..., description=\"The name of the vector store.\" ) The first part is to create a configuration that extends VectorStoreConfiguration . name field constraints the value to VectorStoreName.CHROMA , which serves as an indicator for pydantic validator. Note : For adding potentially needed secrets support follow the same approach as explained in How to Add a New LLM Implementation guide.","title":"Step 4: Vector Store Configuration"},{"location":"how_to/how_to_add_new_vector_store/#step-5-vector-store-implementation","text":"In the vector_store.py file, create singleton vector store factory. It provides a framework, where vector store can be retrieved through ChromaVectorStoreFactory and is initialized only once per runtime, saving up the memory. To do so, define expected _configuration_class type and provide _create_instance implementation using llamaindex . from typing import Type from llama_index.vector_stores.chroma import ChromaVectorStore from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) class ChromaVectorStoreFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaVectorStore: return ChromaVectorStore( host=configuration.host, port=str(configuration.port), collection_name=configuration.collection_name, ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding vector store initialization.","title":"Step 5: Vector Store Implementation"},{"location":"how_to/how_to_add_new_vector_store/#step-6-vector-store-client","text":"We will want to validate our vector store before the run, for that we need and HTTP client. To create a Chroma client, we implement ChromaVectorStoreClientFactory in client.py . It extends SingletonFactory , which provides an interface for initializing a single instance for the duration of the application runtime. from typing import Type from chromadb import HttpClient as ChromaHttpClient from chromadb.api import ClientAPI as ChromaClient from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) class ChromaVectorStoreClientFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaClient: return ChromaHttpClient( host=configuration.host, port=configuration.port, ) The field _configuration_class defines the required configuration type. The rest involves implementing the required _create_instance method with the corresponding client initialization.","title":"Step 6: Vector Store Client"},{"location":"how_to/how_to_add_new_vector_store/#step-7-vector-store-validator","text":"Now we can implement the validator that will check if defined vector store collection already exists. Nevertheless, it can be extended to validate other apsects as well. Create validator.py file and create ChromaVectorStoreValidator that implements BaseVectorStoreValidator interface: from typing import Type from chromadb.api import ClientAPI as ChromaClient from core.base_factory import SingletonFactory from embedding.vector_stores.chroma.client import ChromaVectorStoreClientFactory from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) from embedding.vector_stores.core.exceptions import CollectionExistsException from embedding.vector_stores.core.validator import BaseVectorStoreValidator class ChromaVectorStoreValidator(BaseVectorStoreValidator): def __init__( self, configuration: ChromaVectorStoreConfiguration, client: ChromaClient, ): self.configuration = configuration self.client = client def validate(self) -> None: self.validate_collection() def validate_collection(self) -> None: collection_name = self.configuration.collection_name if collection_name in self.client.list_collections(): raise CollectionExistsException(collection_name) Now add the factory that defines validator initialization. class ChromaVectorStoreValidatorFactory(SingletonFactory): _configuration_class: Type = ChromaVectorStoreConfiguration @classmethod def _create_instance( cls, configuration: ChromaVectorStoreConfiguration ) -> ChromaVectorStoreValidator: client = ChromaVectorStoreClientFactory.create(configuration) return ChromaVectorStoreValidator( configuration=configuration, client=client ) You can notice that we use previously implemented ChromaVectorStoreClientFactory to get required client instance.","title":"Step 7: Vector Store Validator"},{"location":"how_to/how_to_add_new_vector_store/#step-8-vector-store-integration","text":"Create an __init__.py file as follows: from embedding.bootstrap.configuration.vector_store_configuration import ( VectorStoreConfigurationRegistry, VectorStoreName, ) from embedding.vector_stores.chroma.configuration import ( ChromaVectorStoreConfiguration, ) from embedding.vector_stores.chroma.validator import ( ChromaVectorStoreValidatorFactory, ) from embedding.vector_stores.chroma.vector_store import ChromaVectorStoreFactory from embedding.vector_stores.registry import ( VectorStoreRegistry, VectorStoreValidatorRegistry, ) def register() -> None: VectorStoreConfigurationRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreConfiguration, ) VectorStoreRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreFactory ) VectorStoreValidatorRegistry.register( VectorStoreName.CHROMA, ChromaVectorStoreValidatorFactory ) The initialization file includes a register() method responsible for registering our configuration, and validator and vector store factories. Registries are used to dynamically inform the system about available implementations. This way, with the following Chroma configuration in configurations/configuration.{environment}.json file: \"embedding\": { \"vector_store\": { \"name\": \"chroma\", \"collection_name\": \"new-collection\", \"host\": \"chroma\", \"protocol\": \"http\", \"port\": 6000 } ... }, We can dynamically retrieve the corresponding vector store implementation by using the name specified in the configuration: vector_store_config = read_vector_store_from_config() vector_store = VectorStoreRegistry.get(vector_store_config.name).create(vector_store_config) vector_store_validator = VectorStoreValidatorRegistry.get(vector_store_config.name).create(vector_store_config) This mechanism is later used by the embedding orchestrator to initialize the vector store defined in the configuration. These steps conclude the implementation, resulting in the following file structure: src/ \u2514\u2500\u2500 embedding/ \u2514\u2500\u2500 vector_stores/ \u2514\u2500\u2500 chroma/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 client.py \u251c\u2500\u2500 configuration.py \u251c\u2500\u2500 validator.py \u2514\u2500\u2500 vector_store.py","title":"Step 8: Vector Store Integration"},{"location":"how_to/how_to_configure/","text":"How to Configure the RAG System This guide explains how to customize the RAG system pipeline through configuration files. Environments Definition The following environments are supported: class EnvironmentName(str, Enum): DEFAULT = \"default\" LOCAL = \"local\" DEV = \"dev\" TEST = \"test\" PROD = \"prod\" Each environment requires corresponding configuration and secrets files in the configurations directory: Configuration files: configuration.{environment}.json Secrets files: secrets.{environment}.env The configuration files define the pipeline setup, while secrets files store credentials and tokens. For security, all files in the configurations directory are git-ignored except for configuration.default.json and configuration.local.json . Usage Run the pipeline with a specific configuration using the --env flag: build/workstation/init.sh --env default python src/embed.py --env default Datasource Configuration Currently, the following datasources are available: class DatasourceName(str, Enum): NOTION = \"notion\" CONFLUENCE = \"confluence\" PDF = \"pdf\" Blueprint allows the usage of single or multiple datasources. Adjust the corresponding configuration accordingly: { \"extraction\": { \"datasources\": [ { \"name\": \"notion\", \"export_limit\": 100 }, { \"name\": \"pdf\", \"export_limit\": 100, \"base_path\": \"data/\" } ] } } Each entry in datasources corresponds to a single source that will be sequentially used for the extraction of documents to be further processed. The name of each entry must correspond to one of the implemented enums. Datasources' secrets must be added to the environment's secret file. To check configurable options for specific datasources, visit configuration.py of a datasource. LLM Configuration The system supports the following LLM providers: class LLMProviderName(str, Enum): LITE_LLM = \"lite_llm\" LITE_LLM leverages the LiteLLM service, providing a unified interface for cloud-hosted models (e.g., OpenAI, Google, Anthropic) and self-hosted LLMs. Minimal setup requires the use of LLMs in augmentation and evaluation processes. To configure this, adjust the following JSON entries: { \"pipeline\": { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", }, }, \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } }, \"evaluation\": { \"judge_llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } } } The provider field must be one of the values from LLMProviderName , and the name field indicates the specific model exposed by the provider. To check configurable options for specific providers, visit configuration.py of an LLM. In the above case, augmentation and evaluation processes use the same LLM, which might be suboptimal. To change it, simply adjust the entry of one of these: { \"pipeline\": { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", }, }, \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } }, \"evaluation\": { \"judge_llm\": { \"provider\": \"lite_llm\", \"name\": \"gpt-4o-mini\", // another llm \"max_tokens\": 512 // different parameters } } } } Note : You can also use different LLM for any other component like guardrails . Secrets Each model requires api key. For the above LLMs add following secrets: RAG__LLMS__GEMINI_2_0_FLASH_EXP_API_KEY={your-gemini-api-key} RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} The variable name includes the uppercased name of the model you are using, whereas all non-alphanumeric characters are replaced by underscores e.g. gpt-3.5-turbo -> GPT_3_5_TURBO . If you want to use your local model called for instance my-llm , which is exposed via openai-like API you can use the following configuration: \"provider\": \"lite_llm\", \"name\": \"openai/my-llm\", \"api_base\": \"http://127.0.0.1/v1 And the secrets will look as follows: RAG__LLMS__OPENAI_MY_LLM__API_KEY={your-api-key} Note You can use different API structure for your local LLMs, then just replace openai/ prefix with corresponding provider. Embedding Model Configuration Currently, embedding models from these providers are supported: class EmbeddingModelProviderName(str, Enum): HUGGING_FACE = \"hugging_face\" OPENAI = \"openai\" VOYAGE = \"voyage\" Any model exposed by these providers can be used in the setup. Minimal setup requires the use of embedding models in different processes. To configure this, adjust the following JSON entries: { \"embedding\": { \"embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\", \"splitter\": { \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } }, \"evaluation\": { \"judge_embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\" } } } Providers' secrets must be added to the environment's secret file. The provider field must be one of the values from EmbeddingModelProviderName , and the name field indicates the specific model exposed by the provider. The tokenizer_name field indicates the tokenizer used in pair with the embedding model, and it should be compatible with the specified embedding model. The splitter defines how the documents should be chunked in the embedding process and is required for embedding configuration. To check configurable options for specific providers, visit configuration.py of a embedding model. Note : The same embedding model is used for embedding and retrieval processes, therefore it is defined in the embedding configuration only. In the above case, embedding/retrieval and evaluation processes use the same embedding model, which might be suboptimal. To change it, simply adjust the entry of one of these: { \"embedding\": { \"embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\", \"splitting\": { \"name\": \"basic\", \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } }, \"evaluation\": { \"judge_embedding_model\": { \"provider\": \"openai\", // different provider \"name\": \"text-embedding-3-small\", // different embedding model \"tokenizer_name\": \"text-embedding-3-small\", // different tokenizer \"batch_size\": 64 // different parameters } } } Vector Store Configuration Currently, the following vector stores are supported: class VectorStoreName(str, Enum): QDRANT = \"qdrant\" CHROMA = \"chroma\" PGVECTOR = \"pgvector\" To configure the vector store, update the following entry: { \"embedding\": { \"vector_store\": { \"name\": \"qdrant\", \"collection_name\": \"collection-default\", \"host\": \"qdrant\", \"protocol\": \"http\", \"port\": 6333 } } } The name field indicates one of the vector stores from VectorStoreName , and the collection_name defines the vector store collection for embedded documents. The next fields define the connection to the vector store. Corresponding secrets must be added to the environment's secrets file. To check configurable options for specific datasources, visit configuration.py of a vector store. Note : If collection_name already exists in the vector store, the embedding process will be skipped. To run it, delete the collection or use a different name. Langfuse and Chainlit Configuration Configuration contains the entries related to Langfuse and Chainlit: { \"augmentation\": { \"langfuse\": { \"host\": \"langfuse\", \"protocol\": \"http\", \"port\": 3000, \"database\": { \"host\": \"langfuse-db\", \"port\": 5432, \"db\": \"langfuse\" } }, \"chainlit\": { \"port\": 8000 } } } Field chailit.port defines on which port chat UI should be run. Fields in langfuse define connection details to Langfuse server and langfuse.database details of its database. Corresponding secrets for Langfuse have to be added to environment's secrets file. Prompt Templates Configuration For prompts management system uses Langfuse Prompt service. By default four prompt templates are created in Langfuse Prompts service - default_system_prompt , default_context_refine_prompt , default_context_prompt , default_condense_prompt , default_input_guardrail_prompt and default_output_guardrail_prompt . To find out more about these templates visit Llamaindex guide . Prompts are used during the augmentation process, which affects the final answers of the system. They can be adjusted via Langfuse Prompts UI. If you want to provide and use templates under different names, you need to add them to Langfuse Prompts and change the configuration as follows: { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"input_prompt_name\": \"default_input_guardrail_prompt\", \"output_prompt_name\": \"default_output_guardrail_prompt\", }, \"prompt_templates\": { \"condense_prompt_name\": \"new_condense_prompt\", \"context_prompt_name\": \"new_context_prompt\", \"context_refine_prompt_name\": \"new_context_refine_prompt\", \"system_prompt_name\": \"new_system_prompt\" } } } } Upcoming Docs Docs about configurable retrievers, postprocessors and others are in progress..","title":"Configure RAG System"},{"location":"how_to/how_to_configure/#how-to-configure-the-rag-system","text":"This guide explains how to customize the RAG system pipeline through configuration files.","title":"How to Configure the RAG System"},{"location":"how_to/how_to_configure/#environments","text":"","title":"Environments"},{"location":"how_to/how_to_configure/#definition","text":"The following environments are supported: class EnvironmentName(str, Enum): DEFAULT = \"default\" LOCAL = \"local\" DEV = \"dev\" TEST = \"test\" PROD = \"prod\" Each environment requires corresponding configuration and secrets files in the configurations directory: Configuration files: configuration.{environment}.json Secrets files: secrets.{environment}.env The configuration files define the pipeline setup, while secrets files store credentials and tokens. For security, all files in the configurations directory are git-ignored except for configuration.default.json and configuration.local.json .","title":"Definition"},{"location":"how_to/how_to_configure/#usage","text":"Run the pipeline with a specific configuration using the --env flag: build/workstation/init.sh --env default python src/embed.py --env default","title":"Usage"},{"location":"how_to/how_to_configure/#datasource-configuration","text":"Currently, the following datasources are available: class DatasourceName(str, Enum): NOTION = \"notion\" CONFLUENCE = \"confluence\" PDF = \"pdf\" Blueprint allows the usage of single or multiple datasources. Adjust the corresponding configuration accordingly: { \"extraction\": { \"datasources\": [ { \"name\": \"notion\", \"export_limit\": 100 }, { \"name\": \"pdf\", \"export_limit\": 100, \"base_path\": \"data/\" } ] } } Each entry in datasources corresponds to a single source that will be sequentially used for the extraction of documents to be further processed. The name of each entry must correspond to one of the implemented enums. Datasources' secrets must be added to the environment's secret file. To check configurable options for specific datasources, visit configuration.py of a datasource.","title":"Datasource Configuration"},{"location":"how_to/how_to_configure/#llm-configuration","text":"The system supports the following LLM providers: class LLMProviderName(str, Enum): LITE_LLM = \"lite_llm\" LITE_LLM leverages the LiteLLM service, providing a unified interface for cloud-hosted models (e.g., OpenAI, Google, Anthropic) and self-hosted LLMs. Minimal setup requires the use of LLMs in augmentation and evaluation processes. To configure this, adjust the following JSON entries: { \"pipeline\": { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", }, }, \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } }, \"evaluation\": { \"judge_llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } } } The provider field must be one of the values from LLMProviderName , and the name field indicates the specific model exposed by the provider. To check configurable options for specific providers, visit configuration.py of an LLM. In the above case, augmentation and evaluation processes use the same LLM, which might be suboptimal. To change it, simply adjust the entry of one of these: { \"pipeline\": { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", }, }, \"llm\": { \"provider\": \"lite_llm\", \"name\": \"gemini-2.0-flash-exp\", \"max_tokens\": 1024, \"max_retries\": 3, \"context_window\": 16384 } } }, \"evaluation\": { \"judge_llm\": { \"provider\": \"lite_llm\", \"name\": \"gpt-4o-mini\", // another llm \"max_tokens\": 512 // different parameters } } } } Note : You can also use different LLM for any other component like guardrails .","title":"LLM Configuration"},{"location":"how_to/how_to_configure/#secrets","text":"Each model requires api key. For the above LLMs add following secrets: RAG__LLMS__GEMINI_2_0_FLASH_EXP_API_KEY={your-gemini-api-key} RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} The variable name includes the uppercased name of the model you are using, whereas all non-alphanumeric characters are replaced by underscores e.g. gpt-3.5-turbo -> GPT_3_5_TURBO . If you want to use your local model called for instance my-llm , which is exposed via openai-like API you can use the following configuration: \"provider\": \"lite_llm\", \"name\": \"openai/my-llm\", \"api_base\": \"http://127.0.0.1/v1 And the secrets will look as follows: RAG__LLMS__OPENAI_MY_LLM__API_KEY={your-api-key} Note You can use different API structure for your local LLMs, then just replace openai/ prefix with corresponding provider.","title":"Secrets"},{"location":"how_to/how_to_configure/#embedding-model-configuration","text":"Currently, embedding models from these providers are supported: class EmbeddingModelProviderName(str, Enum): HUGGING_FACE = \"hugging_face\" OPENAI = \"openai\" VOYAGE = \"voyage\" Any model exposed by these providers can be used in the setup. Minimal setup requires the use of embedding models in different processes. To configure this, adjust the following JSON entries: { \"embedding\": { \"embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\", \"splitter\": { \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } }, \"evaluation\": { \"judge_embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\" } } } Providers' secrets must be added to the environment's secret file. The provider field must be one of the values from EmbeddingModelProviderName , and the name field indicates the specific model exposed by the provider. The tokenizer_name field indicates the tokenizer used in pair with the embedding model, and it should be compatible with the specified embedding model. The splitter defines how the documents should be chunked in the embedding process and is required for embedding configuration. To check configurable options for specific providers, visit configuration.py of a embedding model. Note : The same embedding model is used for embedding and retrieval processes, therefore it is defined in the embedding configuration only. In the above case, embedding/retrieval and evaluation processes use the same embedding model, which might be suboptimal. To change it, simply adjust the entry of one of these: { \"embedding\": { \"embedding_model\": { \"provider\": \"hugging_face\", \"name\": \"BAAI/bge-small-en-v1.5\", \"tokenizer_name\": \"BAAI/bge-small-en-v1.5\", \"splitting\": { \"name\": \"basic\", \"chunk_overlap_in_tokens\": 50, \"chunk_size_in_tokens\": 384 } } }, \"evaluation\": { \"judge_embedding_model\": { \"provider\": \"openai\", // different provider \"name\": \"text-embedding-3-small\", // different embedding model \"tokenizer_name\": \"text-embedding-3-small\", // different tokenizer \"batch_size\": 64 // different parameters } } }","title":"Embedding Model Configuration"},{"location":"how_to/how_to_configure/#vector-store-configuration","text":"Currently, the following vector stores are supported: class VectorStoreName(str, Enum): QDRANT = \"qdrant\" CHROMA = \"chroma\" PGVECTOR = \"pgvector\" To configure the vector store, update the following entry: { \"embedding\": { \"vector_store\": { \"name\": \"qdrant\", \"collection_name\": \"collection-default\", \"host\": \"qdrant\", \"protocol\": \"http\", \"port\": 6333 } } } The name field indicates one of the vector stores from VectorStoreName , and the collection_name defines the vector store collection for embedded documents. The next fields define the connection to the vector store. Corresponding secrets must be added to the environment's secrets file. To check configurable options for specific datasources, visit configuration.py of a vector store. Note : If collection_name already exists in the vector store, the embedding process will be skipped. To run it, delete the collection or use a different name.","title":"Vector Store Configuration"},{"location":"how_to/how_to_configure/#langfuse-and-chainlit-configuration","text":"Configuration contains the entries related to Langfuse and Chainlit: { \"augmentation\": { \"langfuse\": { \"host\": \"langfuse\", \"protocol\": \"http\", \"port\": 3000, \"database\": { \"host\": \"langfuse-db\", \"port\": 5432, \"db\": \"langfuse\" } }, \"chainlit\": { \"port\": 8000 } } } Field chailit.port defines on which port chat UI should be run. Fields in langfuse define connection details to Langfuse server and langfuse.database details of its database. Corresponding secrets for Langfuse have to be added to environment's secrets file.","title":"Langfuse and Chainlit Configuration"},{"location":"how_to/how_to_configure/#prompt-templates-configuration","text":"For prompts management system uses Langfuse Prompt service. By default four prompt templates are created in Langfuse Prompts service - default_system_prompt , default_context_refine_prompt , default_context_prompt , default_condense_prompt , default_input_guardrail_prompt and default_output_guardrail_prompt . To find out more about these templates visit Llamaindex guide . Prompts are used during the augmentation process, which affects the final answers of the system. They can be adjusted via Langfuse Prompts UI. If you want to provide and use templates under different names, you need to add them to Langfuse Prompts and change the configuration as follows: { \"augmentation\": { \"chat_engine\": { \"guardrails\": { \"input_prompt_name\": \"default_input_guardrail_prompt\", \"output_prompt_name\": \"default_output_guardrail_prompt\", }, \"prompt_templates\": { \"condense_prompt_name\": \"new_condense_prompt\", \"context_prompt_name\": \"new_context_prompt\", \"context_refine_prompt_name\": \"new_context_refine_prompt\", \"system_prompt_name\": \"new_system_prompt\" } } } }","title":"Prompt Templates Configuration"},{"location":"how_to/how_to_configure/#upcoming-docs","text":"Docs about configurable retrievers, postprocessors and others are in progress..","title":"Upcoming Docs"},{"location":"monitoring/in_progress/","text":"In progres..","title":"In progress"},{"location":"quickstart/developer_setup/","text":"Local Development Setup This guide outlines the steps required to set up the RAG system on your local machine for development purposes. Requirements: Python >=3.10,<3.13 Docker Configuration & Secrets First, clone the repository and navigate to rag_blueprint : git clone https://github.com/feld-m/rag_blueprint.git cd rag_blueprint The local configuration is located in configuration.local.json . This file configures toy PDF dataset as the document datasource and defines local settings for embedding, augmentation, and evaluation stages. To customize the setup, refer to the How to Configure the RAG System guide. Secrets Configuration Create a secrets file at configurations/secrets.local.env . Below is a template: # LLMs RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} # Langfuse RAG__LANGFUSE__DATABASE__USER=user RAG__LANGFUSE__DATABASE__PASSWORD=password RAG__LANGFUSE__SECRET_KEY=required_placeholder RAG__LANGFUSE__PUBLIC_KEY=required_placeholder RAG__LLMS__GPT_4O_MINI__API_KEY : Required for connecting to OpenAI GPT-4o-mini LLM. Langfuse Keys : RAG__LANGFUSE__SECRET_KEY and RAG__LANGFUSE__PUBLIC_KEY are generated after initialization and will need to be updated later. Initialization Python Environment Install uv on your OS following this installation guide. In the root of the project, create a virtual environment and activate it: uv venv source .venv/bin/activate Install the required dependencies: uv sync --all-extras Services Initialization To initialize the Langfuse and vector store services, run the initialization script: build/workstation/init.sh --env local NOTE: Depending on your OS and the setup you might need to give execute permission to the initialization script e.g. chmod u+x build/workstation/init.sh . Once initialized, access the Langfuse web server on your localhost (port defined in configuration.local.json under augmentation.langfuse.port ). Use the Langfuse UI to: Create a user. Set up a project for the application. Generate secret and public keys for the project. Add the generated keys to the configurations/secrets.local.env file as follows: RAG__LANGFUSE__SECRET_KEY=<generated_secret_key> RAG__LANGFUSE__PUBLIC_KEY=<generated_public_key> Development Running RAG For the first run, it is recommended to execute the scripts in the specified order to ensure proper initialization of resources like vector store collections. Embedding Stage Run the embedding stage script: python src/embed.py --env local --on-prem-config Note : The embedding process may take significant time, depending on the size of your datasource. Use export_limit fields in configuration to speed up the process. Moreover, for configuration.local.json setup you can keep required_placeholders in configurations/secrets.local.env . Augmentation Stage Run the augmentation stage script: python src/augment.py --env local --on-prem-config This initializes the RAG system's chat engine and the Chainlit application, leveraging the embeddings generated in the previous step. Evaluation Stage Run the evaluation stage script: python src/evaluation.py --env local --on-prem-config Important : For evaluation to proceed, Langfuse datasets must be populated either manually or via Chainlit's human feedback feature. For additional details, refer to the Evaluation Docs . Git setup The .pre-commit-config.yaml file configures code formatters to enforce consistency before committing changes. After cloning the repository and installing dependencies, enable pre-commit hooks: pre-commit install","title":"Local Developement Setup"},{"location":"quickstart/developer_setup/#local-development-setup","text":"This guide outlines the steps required to set up the RAG system on your local machine for development purposes. Requirements: Python >=3.10,<3.13 Docker","title":"Local Development Setup"},{"location":"quickstart/developer_setup/#configuration-secrets","text":"First, clone the repository and navigate to rag_blueprint : git clone https://github.com/feld-m/rag_blueprint.git cd rag_blueprint The local configuration is located in configuration.local.json . This file configures toy PDF dataset as the document datasource and defines local settings for embedding, augmentation, and evaluation stages. To customize the setup, refer to the How to Configure the RAG System guide.","title":"Configuration &amp; Secrets"},{"location":"quickstart/developer_setup/#secrets-configuration","text":"Create a secrets file at configurations/secrets.local.env . Below is a template: # LLMs RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} # Langfuse RAG__LANGFUSE__DATABASE__USER=user RAG__LANGFUSE__DATABASE__PASSWORD=password RAG__LANGFUSE__SECRET_KEY=required_placeholder RAG__LANGFUSE__PUBLIC_KEY=required_placeholder RAG__LLMS__GPT_4O_MINI__API_KEY : Required for connecting to OpenAI GPT-4o-mini LLM. Langfuse Keys : RAG__LANGFUSE__SECRET_KEY and RAG__LANGFUSE__PUBLIC_KEY are generated after initialization and will need to be updated later.","title":"Secrets Configuration"},{"location":"quickstart/developer_setup/#initialization","text":"","title":"Initialization"},{"location":"quickstart/developer_setup/#python-environment","text":"Install uv on your OS following this installation guide. In the root of the project, create a virtual environment and activate it: uv venv source .venv/bin/activate Install the required dependencies: uv sync --all-extras","title":"Python Environment"},{"location":"quickstart/developer_setup/#services-initialization","text":"To initialize the Langfuse and vector store services, run the initialization script: build/workstation/init.sh --env local NOTE: Depending on your OS and the setup you might need to give execute permission to the initialization script e.g. chmod u+x build/workstation/init.sh . Once initialized, access the Langfuse web server on your localhost (port defined in configuration.local.json under augmentation.langfuse.port ). Use the Langfuse UI to: Create a user. Set up a project for the application. Generate secret and public keys for the project. Add the generated keys to the configurations/secrets.local.env file as follows: RAG__LANGFUSE__SECRET_KEY=<generated_secret_key> RAG__LANGFUSE__PUBLIC_KEY=<generated_public_key>","title":"Services Initialization"},{"location":"quickstart/developer_setup/#development","text":"","title":"Development"},{"location":"quickstart/developer_setup/#running-rag","text":"For the first run, it is recommended to execute the scripts in the specified order to ensure proper initialization of resources like vector store collections.","title":"Running RAG"},{"location":"quickstart/developer_setup/#embedding-stage","text":"Run the embedding stage script: python src/embed.py --env local --on-prem-config Note : The embedding process may take significant time, depending on the size of your datasource. Use export_limit fields in configuration to speed up the process. Moreover, for configuration.local.json setup you can keep required_placeholders in configurations/secrets.local.env .","title":"Embedding Stage"},{"location":"quickstart/developer_setup/#augmentation-stage","text":"Run the augmentation stage script: python src/augment.py --env local --on-prem-config This initializes the RAG system's chat engine and the Chainlit application, leveraging the embeddings generated in the previous step.","title":"Augmentation Stage"},{"location":"quickstart/developer_setup/#evaluation-stage","text":"Run the evaluation stage script: python src/evaluation.py --env local --on-prem-config Important : For evaluation to proceed, Langfuse datasets must be populated either manually or via Chainlit's human feedback feature. For additional details, refer to the Evaluation Docs .","title":"Evaluation Stage"},{"location":"quickstart/developer_setup/#git-setup","text":"The .pre-commit-config.yaml file configures code formatters to enforce consistency before committing changes. After cloning the repository and installing dependencies, enable pre-commit hooks: pre-commit install","title":"Git setup"},{"location":"quickstart/quickstart_setup/","text":"Quickstart Setup This guide outlines the steps to set up and deploy the RAG system on your server or local machine. Requirements: Python >=3.10,<3.13 Docker Configuration & Secrets First, clone the repository and navigate to rag_blueprint : git clone https://github.com/feld-m/rag_blueprint.git cd rag_blueprint The default configuration is located in configuration.default.json . This file configures toy PDF dataset as the document datasource and defines default settings for embedding, augmentation, and evaluation stages. To customize the setup, refer to the How to Configure the RAG System guide. Secrets Configuration Create a secrets file at configurations/secrets.default.env . Below is a template: # LLMs RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} # Langfuse RAG__LANGFUSE__DATABASE__USER=user RAG__LANGFUSE__DATABASE__PASSWORD=password RAG__LANGFUSE__SECRET_KEY=required_placeholder RAG__LANGFUSE__PUBLIC_KEY=required_placeholder RAG__LLMS__GPT_4O_MINI__API_KEY : Required for connecting to OpenAI GPT-4o-mini LLM. Langfuse Keys : RAG__LANGFUSE__SECRET_KEY and RAG__LANGFUSE__PUBLIC_KEY are generated after initialization and will need to be updated later. Initialization Python Environment Install uv on your OS following this installation guide. In the root of the project, create a virtual environment and activate it: uv venv source .venv/bin/activate Install the required dependencies: uv sync --all-extras Services Initialization To initialize the Langfuse and vector store services, run the initialization script: build/workstation/init.sh --env default NOTE: Depending on your OS and the setup you might need to give execute permission to the initialization script e.g. chmod u+x build/workstation/init.sh Once initialized, access the Langfuse web server on your localhost (port defined in configuration.default.json under augmentation.langfuse.port ). Use the Langfuse UI to: Create a user. Set up a project for the application. Generate secret and public keys for the project. Add the generated keys to the configurations/secrets.default.env file as follows: RAG__LANGFUSE__SECRET_KEY=<generated_secret_key> RAG__LANGFUSE__PUBLIC_KEY=<generated_public_key> Deployment After completing the initialization, deploy the RAG system using the following command: build/workstation/deploy.sh --env default This command sets up and runs the RAG system on your workstation, enabling it for use.","title":"Quickstart Setup"},{"location":"quickstart/quickstart_setup/#quickstart-setup","text":"This guide outlines the steps to set up and deploy the RAG system on your server or local machine. Requirements: Python >=3.10,<3.13 Docker","title":"Quickstart Setup"},{"location":"quickstart/quickstart_setup/#configuration-secrets","text":"First, clone the repository and navigate to rag_blueprint : git clone https://github.com/feld-m/rag_blueprint.git cd rag_blueprint The default configuration is located in configuration.default.json . This file configures toy PDF dataset as the document datasource and defines default settings for embedding, augmentation, and evaluation stages. To customize the setup, refer to the How to Configure the RAG System guide.","title":"Configuration &amp; Secrets"},{"location":"quickstart/quickstart_setup/#secrets-configuration","text":"Create a secrets file at configurations/secrets.default.env . Below is a template: # LLMs RAG__LLMS__GPT_4O_MINI__API_KEY={your-openai-api-key} # Langfuse RAG__LANGFUSE__DATABASE__USER=user RAG__LANGFUSE__DATABASE__PASSWORD=password RAG__LANGFUSE__SECRET_KEY=required_placeholder RAG__LANGFUSE__PUBLIC_KEY=required_placeholder RAG__LLMS__GPT_4O_MINI__API_KEY : Required for connecting to OpenAI GPT-4o-mini LLM. Langfuse Keys : RAG__LANGFUSE__SECRET_KEY and RAG__LANGFUSE__PUBLIC_KEY are generated after initialization and will need to be updated later.","title":"Secrets Configuration"},{"location":"quickstart/quickstart_setup/#initialization","text":"","title":"Initialization"},{"location":"quickstart/quickstart_setup/#python-environment","text":"Install uv on your OS following this installation guide. In the root of the project, create a virtual environment and activate it: uv venv source .venv/bin/activate Install the required dependencies: uv sync --all-extras","title":"Python Environment"},{"location":"quickstart/quickstart_setup/#services-initialization","text":"To initialize the Langfuse and vector store services, run the initialization script: build/workstation/init.sh --env default NOTE: Depending on your OS and the setup you might need to give execute permission to the initialization script e.g. chmod u+x build/workstation/init.sh Once initialized, access the Langfuse web server on your localhost (port defined in configuration.default.json under augmentation.langfuse.port ). Use the Langfuse UI to: Create a user. Set up a project for the application. Generate secret and public keys for the project. Add the generated keys to the configurations/secrets.default.env file as follows: RAG__LANGFUSE__SECRET_KEY=<generated_secret_key> RAG__LANGFUSE__PUBLIC_KEY=<generated_public_key>","title":"Services Initialization"},{"location":"quickstart/quickstart_setup/#deployment","text":"After completing the initialization, deploy the RAG system using the following command: build/workstation/deploy.sh --env default This command sets up and runs the RAG system on your workstation, enabling it for use.","title":"Deployment"},{"location":"src/augment/","text":"Augment This module contains functionality related to the the augment script. Augment This script is used to handle chat interactions using the ChainLit library and a chat engine. Actions are observed by Langfuse. To make it work vector storage should be filled with the embeddings of the documents. To run the script execute the following command from the root directory of the project: python src/chat.py app_shutdown () async Clean up resources on application shutdown. Stops the scheduler if it is running. Source code in src/augment.py 94 95 96 97 98 99 100 101 102 103 104 @cl . on_app_shutdown async def app_shutdown () -> None : \"\"\" Clean up resources on application shutdown. Stops the scheduler if it is running. \"\"\" try : initializer . get_scheduler () . stop () logger . info ( \"Scheduler stopped successfully\" ) except Exception as e : logger . warning ( f \"Failed to stop scheduler: { e } \" ) app_startup () async Initialize the application on startup. Sets up the augmentation initializer and configuration, and starts the scheduler. Source code in src/augment.py 25 26 27 28 29 30 31 32 33 34 @cl . on_app_startup async def app_startup () -> None : \"\"\" Initialize the application on startup. Sets up the augmentation initializer and configuration, and starts the scheduler. \"\"\" global initializer , configuration initializer = AugmentationInitializer () configuration = initializer . get_configuration () initializer . get_scheduler () . start () get_data_layer () Initialize Chainlit's data layer with the custom service. Returns: ChainlitService ( ChainlitService ) \u2013 The custom service for data layer. Source code in src/augment.py 37 38 39 40 41 42 43 44 45 @cl . data_layer def get_data_layer () -> ChainlitService : \"\"\" Initialize Chainlit's data layer with the custom service. Returns: ChainlitService: The custom service for data layer. \"\"\" return ChainlitServiceFactory . create ( configuration . augmentation ) main ( user_message ) async Process user messages and generate responses. Parameters: user_message ( Message ) \u2013 Message received from user Source code in src/augment.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @cl . on_message async def main ( user_message : cl . Message ) -> None : \"\"\" Process user messages and generate responses. Args: user_message: Message received from user \"\"\" try : chat_engine = cl . user_session . get ( \"chat_engine\" ) assistant_message = cl . Message ( content = \"\" , author = \"Assistant\" ) response = await cl . make_async ( chat_engine . stream_chat )( message = user_message . content , chainlit_message_id = assistant_message . parent_id , ) for token in response . response_gen : await assistant_message . stream_token ( token ) utils = ChainlitUtilsFactory . create ( configuration . augmentation . chainlit ) utils . add_references ( assistant_message , response ) await assistant_message . send () except Exception as e : # It is imprecise to catch all exceptions, but llamaindex doesn't provide unified RateLimitError logger . error ( f \"Error in main: { e } \" , exc_info = True ) await cl . ErrorMessage ( content = \"You have reached the request rate limit. Please try again later.\" , ) . send () start () async Initialize chat session with chat engine. Sets up session-specific chat engine and displays welcome message. Source code in src/augment.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @cl . on_chat_start async def start () -> None : \"\"\" Initialize chat session with chat engine. Sets up session-specific chat engine and displays welcome message. \"\"\" chat_engine = ChatEngineRegistry . get ( configuration . augmentation . chat_engine . name ) . create ( configuration ) chat_engine . set_session_id ( cl . user_session . get ( \"id\" )) cl . user_session . set ( \"chat_engine\" , chat_engine ) utils = ChainlitUtilsFactory . create ( configuration . augmentation . chainlit ) await utils . get_disclaimer_message () . send () await utils . get_welcome_message () . send ()","title":"Augment"},{"location":"src/augment/#augment","text":"This module contains functionality related to the the augment script.","title":"Augment"},{"location":"src/augment/#augment_1","text":"This script is used to handle chat interactions using the ChainLit library and a chat engine. Actions are observed by Langfuse. To make it work vector storage should be filled with the embeddings of the documents. To run the script execute the following command from the root directory of the project: python src/chat.py","title":"Augment"},{"location":"src/augment/#src.augment.app_shutdown","text":"Clean up resources on application shutdown. Stops the scheduler if it is running. Source code in src/augment.py 94 95 96 97 98 99 100 101 102 103 104 @cl . on_app_shutdown async def app_shutdown () -> None : \"\"\" Clean up resources on application shutdown. Stops the scheduler if it is running. \"\"\" try : initializer . get_scheduler () . stop () logger . info ( \"Scheduler stopped successfully\" ) except Exception as e : logger . warning ( f \"Failed to stop scheduler: { e } \" )","title":"app_shutdown"},{"location":"src/augment/#src.augment.app_startup","text":"Initialize the application on startup. Sets up the augmentation initializer and configuration, and starts the scheduler. Source code in src/augment.py 25 26 27 28 29 30 31 32 33 34 @cl . on_app_startup async def app_startup () -> None : \"\"\" Initialize the application on startup. Sets up the augmentation initializer and configuration, and starts the scheduler. \"\"\" global initializer , configuration initializer = AugmentationInitializer () configuration = initializer . get_configuration () initializer . get_scheduler () . start ()","title":"app_startup"},{"location":"src/augment/#src.augment.get_data_layer","text":"Initialize Chainlit's data layer with the custom service. Returns: ChainlitService ( ChainlitService ) \u2013 The custom service for data layer. Source code in src/augment.py 37 38 39 40 41 42 43 44 45 @cl . data_layer def get_data_layer () -> ChainlitService : \"\"\" Initialize Chainlit's data layer with the custom service. Returns: ChainlitService: The custom service for data layer. \"\"\" return ChainlitServiceFactory . create ( configuration . augmentation )","title":"get_data_layer"},{"location":"src/augment/#src.augment.main","text":"Process user messages and generate responses. Parameters: user_message ( Message ) \u2013 Message received from user Source code in src/augment.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @cl . on_message async def main ( user_message : cl . Message ) -> None : \"\"\" Process user messages and generate responses. Args: user_message: Message received from user \"\"\" try : chat_engine = cl . user_session . get ( \"chat_engine\" ) assistant_message = cl . Message ( content = \"\" , author = \"Assistant\" ) response = await cl . make_async ( chat_engine . stream_chat )( message = user_message . content , chainlit_message_id = assistant_message . parent_id , ) for token in response . response_gen : await assistant_message . stream_token ( token ) utils = ChainlitUtilsFactory . create ( configuration . augmentation . chainlit ) utils . add_references ( assistant_message , response ) await assistant_message . send () except Exception as e : # It is imprecise to catch all exceptions, but llamaindex doesn't provide unified RateLimitError logger . error ( f \"Error in main: { e } \" , exc_info = True ) await cl . ErrorMessage ( content = \"You have reached the request rate limit. Please try again later.\" , ) . send ()","title":"main"},{"location":"src/augment/#src.augment.start","text":"Initialize chat session with chat engine. Sets up session-specific chat engine and displays welcome message. Source code in src/augment.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 @cl . on_chat_start async def start () -> None : \"\"\" Initialize chat session with chat engine. Sets up session-specific chat engine and displays welcome message. \"\"\" chat_engine = ChatEngineRegistry . get ( configuration . augmentation . chat_engine . name ) . create ( configuration ) chat_engine . set_session_id ( cl . user_session . get ( \"id\" )) cl . user_session . set ( \"chat_engine\" , chat_engine ) utils = ChainlitUtilsFactory . create ( configuration . augmentation . chainlit ) await utils . get_disclaimer_message () . send () await utils . get_welcome_message () . send ()","title":"start"},{"location":"src/embed/","text":"Embed This module contains functionality related to the the embed script. Embed This script is the entry point for the embedding process. It initializes the embedding orchestrator and starts the embedding workflow. To run the script, execute the following command from the root directory of the project: python src/embed.py run ( logger = LoggerConfiguration . get_logger ( __name__ )) async Execute the embedding process. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/embed.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 async def run ( logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Execute the embedding process. Args: logger: Logger instance for logging messages \"\"\" initializer = EmbeddingInitializer () configuration = initializer . get_configuration () vector_store = configuration . embedding . vector_store validator = VectorStoreValidatorRegistry . get ( vector_store . name ) . create ( vector_store ) try : validator . validate () except CollectionExistsException as e : logger . info ( f \"Collection ' { e . collection_name } ' already exists. \" \"Skipping embedding process.\" ) return logger . info ( \"Starting embedding process.\" ) orchestrator = EmbeddingOrchestratorRegistry . get ( configuration . embedding . orchestrator_name ) . create ( configuration ) await orchestrator . embed () logger . info ( \"Embedding process finished.\" )","title":"Embed"},{"location":"src/embed/#embed","text":"This module contains functionality related to the the embed script.","title":"Embed"},{"location":"src/embed/#embed_1","text":"This script is the entry point for the embedding process. It initializes the embedding orchestrator and starts the embedding workflow. To run the script, execute the following command from the root directory of the project: python src/embed.py","title":"Embed"},{"location":"src/embed/#src.embed.run","text":"Execute the embedding process. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/embed.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 async def run ( logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Execute the embedding process. Args: logger: Logger instance for logging messages \"\"\" initializer = EmbeddingInitializer () configuration = initializer . get_configuration () vector_store = configuration . embedding . vector_store validator = VectorStoreValidatorRegistry . get ( vector_store . name ) . create ( vector_store ) try : validator . validate () except CollectionExistsException as e : logger . info ( f \"Collection ' { e . collection_name } ' already exists. \" \"Skipping embedding process.\" ) return logger . info ( \"Starting embedding process.\" ) orchestrator = EmbeddingOrchestratorRegistry . get ( configuration . embedding . orchestrator_name ) . create ( configuration ) await orchestrator . embed () logger . info ( \"Embedding process finished.\" )","title":"run"},{"location":"src/evaluate/","text":"Evaluate This module contains functionality related to the the evaluate script. Evaluate This script is used to evaluate RAG system using langfuse datasets. To add a new item to manual dataset, visit Langfuse UI. Vector storage should be running with ready collection of embeddings. To run the script execute the following command from the root directory of the project: python src/evaluate.py run ( logger = LoggerConfiguration . get_logger ( __name__ )) Execute RAG system evaluation workflow. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/evaluate.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def run ( logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ) -> None : \"\"\" Execute RAG system evaluation workflow. Args: logger: Logger instance for logging messages \"\"\" initializer = EvaluationInitializer () configuration = initializer . get_configuration () langfuse_evaluator = LangfuseEvaluatorFactory . create ( configuration ) logger . info ( f \"Evaluating { langfuse_evaluator . run_name } ...\" ) langfuse_evaluator . evaluate ( dataset_name = configuration . augmentation . langfuse . datasets . feedback_dataset . name ) langfuse_evaluator . evaluate ( dataset_name = configuration . augmentation . langfuse . datasets . manual_dataset . name ) logger . info ( f \"Evaluation complete for { configuration . metadata . build_name } .\" )","title":"Evaluate"},{"location":"src/evaluate/#evaluate","text":"This module contains functionality related to the the evaluate script.","title":"Evaluate"},{"location":"src/evaluate/#evaluate_1","text":"This script is used to evaluate RAG system using langfuse datasets. To add a new item to manual dataset, visit Langfuse UI. Vector storage should be running with ready collection of embeddings. To run the script execute the following command from the root directory of the project: python src/evaluate.py","title":"Evaluate"},{"location":"src/evaluate/#src.evaluate.run","text":"Execute RAG system evaluation workflow. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/evaluate.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def run ( logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ) -> None : \"\"\" Execute RAG system evaluation workflow. Args: logger: Logger instance for logging messages \"\"\" initializer = EvaluationInitializer () configuration = initializer . get_configuration () langfuse_evaluator = LangfuseEvaluatorFactory . create ( configuration ) logger . info ( f \"Evaluating { langfuse_evaluator . run_name } ...\" ) langfuse_evaluator . evaluate ( dataset_name = configuration . augmentation . langfuse . datasets . feedback_dataset . name ) langfuse_evaluator . evaluate ( dataset_name = configuration . augmentation . langfuse . datasets . manual_dataset . name ) logger . info ( f \"Evaluation complete for { configuration . metadata . build_name } .\" )","title":"run"},{"location":"src/augmentation/bootstrap/initializer/","text":"Initializer This module contains functionality related to the the initializer module for augmentation.bootstrap . Initializer AugmentationInitializer Bases: EmbeddingInitializer Initializer for the augmentation process. Extends the EmbeddingInitializer to set up the environment for augmentation tasks. This initializer is responsible for loading the required configuration and registering all necessary components with the dependency injection container. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. Source code in src/augmentation/bootstrap/initializer.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 class AugmentationInitializer ( EmbeddingInitializer ): \"\"\"Initializer for the augmentation process. Extends the EmbeddingInitializer to set up the environment for augmentation tasks. This initializer is responsible for loading the required configuration and registering all necessary components with the dependency injection container. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = AugmentationConfiguration , package_loader : BasePackageLoader = AugmentationPackageLoader (), ): \"\"\"Initialize the AugmentationInitializer. Args: configuration_class: The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader: Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) self . scheduler = AugmentationScheduler ( configuration = self . get_configuration (), logger = LoggerConfiguration . get_logger ( __name__ ), ) self . _initialize_default_prompt () def get_scheduler ( self ) -> AugmentationScheduler : \"\"\"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler: The scheduler instance used for scheduling jobs. \"\"\" return self . scheduler def _initialize_default_prompt ( self ) -> None : \"\"\" Initialize the default prompt templates for the augmentation process managed by Langfuse. \"\"\" configuration = self . get_configuration () langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . augmentation . langfuse ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_condense_prompt\" , prompt_template = DEFAULT_CONDENSE_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_context_prompt\" , prompt_template = DEFAULT_CONTEXT_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_context_refine_prompt\" , prompt_template = DEFAULT_CONTEXT_REFINE_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_system_prompt\" , prompt_template = \"\" , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_input_guardrail_prompt\" , prompt_template = DEFAULT_INPUT_GUARDRAIL_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_output_guardrail_prompt\" , prompt_template = DEFAULT_OUTPUT_GUARDRAIL_PROMPT_TEMPLATE , ) __init__ ( configuration_class = AugmentationConfiguration , package_loader = AugmentationPackageLoader ()) Initialize the AugmentationInitializer. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: AugmentationConfiguration ) \u2013 The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader ( BasePackageLoader , default: AugmentationPackageLoader () ) \u2013 Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. Source code in src/augmentation/bootstrap/initializer.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = AugmentationConfiguration , package_loader : BasePackageLoader = AugmentationPackageLoader (), ): \"\"\"Initialize the AugmentationInitializer. Args: configuration_class: The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader: Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) self . scheduler = AugmentationScheduler ( configuration = self . get_configuration (), logger = LoggerConfiguration . get_logger ( __name__ ), ) self . _initialize_default_prompt () get_scheduler () Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler ( AugmentationScheduler ) \u2013 The scheduler instance used for scheduling jobs. Source code in src/augmentation/bootstrap/initializer.py 190 191 192 193 194 195 196 def get_scheduler ( self ) -> AugmentationScheduler : \"\"\"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler: The scheduler instance used for scheduling jobs. \"\"\" return self . scheduler AugmentationPackageLoader Bases: EmbeddingPackageLoader Package loader for augmentation components. Extends the EmbeddingPackageLoader to load additional packages required for the augmentation process, including LLMs, retrievers, postprocessors, and chat engines. Source code in src/augmentation/bootstrap/initializer.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class AugmentationPackageLoader ( EmbeddingPackageLoader ): \"\"\"Package loader for augmentation components. Extends the EmbeddingPackageLoader to load additional packages required for the augmentation process, including LLMs, retrievers, postprocessors, and chat engines. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the AugmentationPackageLoader. Args: logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.augmentation.components.guardrails\" , \"src.augmentation.components.llms\" , \"src.augmentation.components.retrievers\" , \"src.augmentation.components.postprocessors\" , \"src.augmentation.components.chat_engines\" , ] ) __init__ ( logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the AugmentationPackageLoader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information. Defaults to a logger configured with the current module name. Source code in src/augmentation/bootstrap/initializer.py 124 125 126 127 128 129 130 131 132 133 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the AugmentationPackageLoader. Args: logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" super () . __init__ ( logger ) load_packages () Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. Source code in src/augmentation/bootstrap/initializer.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def load_packages ( self ) -> None : \"\"\"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.augmentation.components.guardrails\" , \"src.augmentation.components.llms\" , \"src.augmentation.components.retrievers\" , \"src.augmentation.components.postprocessors\" , \"src.augmentation.components.chat_engines\" , ] ) AugmentationScheduler Source code in src/augmentation/bootstrap/initializer.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class AugmentationScheduler : def __init__ ( self , configuration : AugmentationConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: configuration (AugmentationConfiguration): The configuration object for the augmentation process. logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" self . logger = logger self . configuration = configuration self . scheduler = AsyncIOScheduler () def start ( self ) -> None : \"\"\"Start the scheduler and schedule the daily queries retention job.\"\"\" langfuse_configuration = self . configuration . augmentation . langfuse queries_retention_job = LangfuseRenetionJobFactory . create ( langfuse_configuration ) try : self . scheduler . add_job ( queries_retention_job . run , CronTrigger . from_crontab ( langfuse_configuration . retention_job . crontab ), id = langfuse_configuration . retention_job . name , replace_existing = True , ) self . logger . info ( \"Daily queries retention job scheduled successfully\" ) self . scheduler . start () self . logger . info ( \"Scheduler started successfully\" ) except Exception as e : self . logger . error ( f \"Failed to initialize scheduler: { e } \" ) def stop ( self ) -> None : \"\"\"Stop the scheduler if it is running.\"\"\" if self . scheduler . running : self . scheduler . shutdown () self . logger . info ( \"Scheduler stopped successfully\" ) else : self . logger . warning ( \"Scheduler was not running\" ) __init__ ( configuration , logger = LoggerConfiguration . get_logger ( __name__ )) Parameters: configuration ( AugmentationConfiguration ) \u2013 The configuration object for the augmentation process. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information. Defaults to a logger configured with the current module name. Source code in src/augmentation/bootstrap/initializer.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , configuration : AugmentationConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: configuration (AugmentationConfiguration): The configuration object for the augmentation process. logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" self . logger = logger self . configuration = configuration self . scheduler = AsyncIOScheduler () start () Start the scheduler and schedule the daily queries retention job. Source code in src/augmentation/bootstrap/initializer.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def start ( self ) -> None : \"\"\"Start the scheduler and schedule the daily queries retention job.\"\"\" langfuse_configuration = self . configuration . augmentation . langfuse queries_retention_job = LangfuseRenetionJobFactory . create ( langfuse_configuration ) try : self . scheduler . add_job ( queries_retention_job . run , CronTrigger . from_crontab ( langfuse_configuration . retention_job . crontab ), id = langfuse_configuration . retention_job . name , replace_existing = True , ) self . logger . info ( \"Daily queries retention job scheduled successfully\" ) self . scheduler . start () self . logger . info ( \"Scheduler started successfully\" ) except Exception as e : self . logger . error ( f \"Failed to initialize scheduler: { e } \" ) stop () Stop the scheduler if it is running. Source code in src/augmentation/bootstrap/initializer.py 107 108 109 110 111 112 113 def stop ( self ) -> None : \"\"\"Stop the scheduler if it is running.\"\"\" if self . scheduler . running : self . scheduler . shutdown () self . logger . info ( \"Scheduler stopped successfully\" ) else : self . logger . warning ( \"Scheduler was not running\" )","title":"Initializer"},{"location":"src/augmentation/bootstrap/initializer/#initializer","text":"This module contains functionality related to the the initializer module for augmentation.bootstrap .","title":"Initializer"},{"location":"src/augmentation/bootstrap/initializer/#initializer_1","text":"","title":"Initializer"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationInitializer","text":"Bases: EmbeddingInitializer Initializer for the augmentation process. Extends the EmbeddingInitializer to set up the environment for augmentation tasks. This initializer is responsible for loading the required configuration and registering all necessary components with the dependency injection container. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. Source code in src/augmentation/bootstrap/initializer.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 class AugmentationInitializer ( EmbeddingInitializer ): \"\"\"Initializer for the augmentation process. Extends the EmbeddingInitializer to set up the environment for augmentation tasks. This initializer is responsible for loading the required configuration and registering all necessary components with the dependency injection container. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = AugmentationConfiguration , package_loader : BasePackageLoader = AugmentationPackageLoader (), ): \"\"\"Initialize the AugmentationInitializer. Args: configuration_class: The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader: Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) self . scheduler = AugmentationScheduler ( configuration = self . get_configuration (), logger = LoggerConfiguration . get_logger ( __name__ ), ) self . _initialize_default_prompt () def get_scheduler ( self ) -> AugmentationScheduler : \"\"\"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler: The scheduler instance used for scheduling jobs. \"\"\" return self . scheduler def _initialize_default_prompt ( self ) -> None : \"\"\" Initialize the default prompt templates for the augmentation process managed by Langfuse. \"\"\" configuration = self . get_configuration () langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . augmentation . langfuse ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_condense_prompt\" , prompt_template = DEFAULT_CONDENSE_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_context_prompt\" , prompt_template = DEFAULT_CONTEXT_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_context_refine_prompt\" , prompt_template = DEFAULT_CONTEXT_REFINE_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_system_prompt\" , prompt_template = \"\" , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_input_guardrail_prompt\" , prompt_template = DEFAULT_INPUT_GUARDRAIL_PROMPT_TEMPLATE , ) langfuse_prompt_service . create_prompt_if_not_exists ( prompt_name = \"default_output_guardrail_prompt\" , prompt_template = DEFAULT_OUTPUT_GUARDRAIL_PROMPT_TEMPLATE , )","title":"AugmentationInitializer"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationInitializer.__init__","text":"Initialize the AugmentationInitializer. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: AugmentationConfiguration ) \u2013 The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader ( BasePackageLoader , default: AugmentationPackageLoader () ) \u2013 Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. Source code in src/augmentation/bootstrap/initializer.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = AugmentationConfiguration , package_loader : BasePackageLoader = AugmentationPackageLoader (), ): \"\"\"Initialize the AugmentationInitializer. Args: configuration_class: The configuration class to use for loading settings. Defaults to AugmentationConfiguration. package_loader: Package loader instance responsible for loading required packages. Defaults to a new AugmentationPackageLoader instance. \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) self . scheduler = AugmentationScheduler ( configuration = self . get_configuration (), logger = LoggerConfiguration . get_logger ( __name__ ), ) self . _initialize_default_prompt ()","title":"__init__"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationInitializer.get_scheduler","text":"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler ( AugmentationScheduler ) \u2013 The scheduler instance used for scheduling jobs. Source code in src/augmentation/bootstrap/initializer.py 190 191 192 193 194 195 196 def get_scheduler ( self ) -> AugmentationScheduler : \"\"\"Get the scheduler instance for managing scheduled tasks. Returns: AsyncIOScheduler: The scheduler instance used for scheduling jobs. \"\"\" return self . scheduler","title":"get_scheduler"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationPackageLoader","text":"Bases: EmbeddingPackageLoader Package loader for augmentation components. Extends the EmbeddingPackageLoader to load additional packages required for the augmentation process, including LLMs, retrievers, postprocessors, and chat engines. Source code in src/augmentation/bootstrap/initializer.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class AugmentationPackageLoader ( EmbeddingPackageLoader ): \"\"\"Package loader for augmentation components. Extends the EmbeddingPackageLoader to load additional packages required for the augmentation process, including LLMs, retrievers, postprocessors, and chat engines. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the AugmentationPackageLoader. Args: logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.augmentation.components.guardrails\" , \"src.augmentation.components.llms\" , \"src.augmentation.components.retrievers\" , \"src.augmentation.components.postprocessors\" , \"src.augmentation.components.chat_engines\" , ] )","title":"AugmentationPackageLoader"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationPackageLoader.__init__","text":"Initialize the AugmentationPackageLoader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information. Defaults to a logger configured with the current module name. Source code in src/augmentation/bootstrap/initializer.py 124 125 126 127 128 129 130 131 132 133 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the AugmentationPackageLoader. Args: logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" super () . __init__ ( logger )","title":"__init__"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationPackageLoader.load_packages","text":"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. Source code in src/augmentation/bootstrap/initializer.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def load_packages ( self ) -> None : \"\"\"Load all required packages for augmentation. Calls the parent class's load_packages method first to load embedding packages, then loads additional packages specific to augmentation. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.augmentation.components.guardrails\" , \"src.augmentation.components.llms\" , \"src.augmentation.components.retrievers\" , \"src.augmentation.components.postprocessors\" , \"src.augmentation.components.chat_engines\" , ] )","title":"load_packages"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationScheduler","text":"Source code in src/augmentation/bootstrap/initializer.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class AugmentationScheduler : def __init__ ( self , configuration : AugmentationConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: configuration (AugmentationConfiguration): The configuration object for the augmentation process. logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" self . logger = logger self . configuration = configuration self . scheduler = AsyncIOScheduler () def start ( self ) -> None : \"\"\"Start the scheduler and schedule the daily queries retention job.\"\"\" langfuse_configuration = self . configuration . augmentation . langfuse queries_retention_job = LangfuseRenetionJobFactory . create ( langfuse_configuration ) try : self . scheduler . add_job ( queries_retention_job . run , CronTrigger . from_crontab ( langfuse_configuration . retention_job . crontab ), id = langfuse_configuration . retention_job . name , replace_existing = True , ) self . logger . info ( \"Daily queries retention job scheduled successfully\" ) self . scheduler . start () self . logger . info ( \"Scheduler started successfully\" ) except Exception as e : self . logger . error ( f \"Failed to initialize scheduler: { e } \" ) def stop ( self ) -> None : \"\"\"Stop the scheduler if it is running.\"\"\" if self . scheduler . running : self . scheduler . shutdown () self . logger . info ( \"Scheduler stopped successfully\" ) else : self . logger . warning ( \"Scheduler was not running\" )","title":"AugmentationScheduler"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationScheduler.__init__","text":"Parameters: configuration ( AugmentationConfiguration ) \u2013 The configuration object for the augmentation process. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information. Defaults to a logger configured with the current module name. Source code in src/augmentation/bootstrap/initializer.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , configuration : AugmentationConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: configuration (AugmentationConfiguration): The configuration object for the augmentation process. logger: Logger instance for logging information. Defaults to a logger configured with the current module name. \"\"\" self . logger = logger self . configuration = configuration self . scheduler = AsyncIOScheduler ()","title":"__init__"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationScheduler.start","text":"Start the scheduler and schedule the daily queries retention job. Source code in src/augmentation/bootstrap/initializer.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def start ( self ) -> None : \"\"\"Start the scheduler and schedule the daily queries retention job.\"\"\" langfuse_configuration = self . configuration . augmentation . langfuse queries_retention_job = LangfuseRenetionJobFactory . create ( langfuse_configuration ) try : self . scheduler . add_job ( queries_retention_job . run , CronTrigger . from_crontab ( langfuse_configuration . retention_job . crontab ), id = langfuse_configuration . retention_job . name , replace_existing = True , ) self . logger . info ( \"Daily queries retention job scheduled successfully\" ) self . scheduler . start () self . logger . info ( \"Scheduler started successfully\" ) except Exception as e : self . logger . error ( f \"Failed to initialize scheduler: { e } \" )","title":"start"},{"location":"src/augmentation/bootstrap/initializer/#src.augmentation.bootstrap.initializer.AugmentationScheduler.stop","text":"Stop the scheduler if it is running. Source code in src/augmentation/bootstrap/initializer.py 107 108 109 110 111 112 113 def stop ( self ) -> None : \"\"\"Stop the scheduler if it is running.\"\"\" if self . scheduler . running : self . scheduler . shutdown () self . logger . info ( \"Scheduler stopped successfully\" ) else : self . logger . warning ( \"Scheduler was not running\" )","title":"stop"},{"location":"src/augmentation/bootstrap/configuration/chainlit_configuration/","text":"Chainlit_configuration This module contains functionality related to the the chainlit_configuration module for augmentation.bootstrap.configuration . Chainlit_configuration ChainlitConfiguration Bases: BaseConfiguration Configuration for Chainlit service. This class handles the configuration parameters needed to run a Chainlit service for the RAG application interface. Source code in src/augmentation/bootstrap/configuration/chainlit_configuration.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class ChainlitConfiguration ( BaseConfiguration ): \"\"\"Configuration for Chainlit service. This class handles the configuration parameters needed to run a Chainlit service for the RAG application interface. \"\"\" port : int = Field ( 8000 , description = \"Port to run the chainlit service on.\" ) disclaimer_title : str = Field ( \"Bavarian Beer Chat\" , description = \"Title of the disclaimer message to be displayed.\" , ) disclaimer_text : str = Field ( \"This content is AI-generated and may contain inaccuracies. Please verify any critical information independently. Additional details, including imprint and privacy policy information, can be found in the Readme file located a the top-right corner of the page.\" , description = \"Disclaimer text to be displayed to users.\" , ) welcome_message : str = Field ( \"Welcome to our Bavarian Beer Chat! \ud83c\udf7b We're here to guide you through the rich tapestry of Bavarian beer culture. Whether you're curious about traditional brews, local beer festivals, or the history behind Bavaria's renowned beer purity law, you've come to the right place. Type your question below, and let's embark on this flavorful journey together. Prost!\" , description = \"Welcome message to display to users when they start a conversation.\" , )","title":"Chainlit_configuration"},{"location":"src/augmentation/bootstrap/configuration/chainlit_configuration/#chainlit_configuration","text":"This module contains functionality related to the the chainlit_configuration module for augmentation.bootstrap.configuration .","title":"Chainlit_configuration"},{"location":"src/augmentation/bootstrap/configuration/chainlit_configuration/#chainlit_configuration_1","text":"","title":"Chainlit_configuration"},{"location":"src/augmentation/bootstrap/configuration/chainlit_configuration/#src.augmentation.bootstrap.configuration.chainlit_configuration.ChainlitConfiguration","text":"Bases: BaseConfiguration Configuration for Chainlit service. This class handles the configuration parameters needed to run a Chainlit service for the RAG application interface. Source code in src/augmentation/bootstrap/configuration/chainlit_configuration.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class ChainlitConfiguration ( BaseConfiguration ): \"\"\"Configuration for Chainlit service. This class handles the configuration parameters needed to run a Chainlit service for the RAG application interface. \"\"\" port : int = Field ( 8000 , description = \"Port to run the chainlit service on.\" ) disclaimer_title : str = Field ( \"Bavarian Beer Chat\" , description = \"Title of the disclaimer message to be displayed.\" , ) disclaimer_text : str = Field ( \"This content is AI-generated and may contain inaccuracies. Please verify any critical information independently. Additional details, including imprint and privacy policy information, can be found in the Readme file located a the top-right corner of the page.\" , description = \"Disclaimer text to be displayed to users.\" , ) welcome_message : str = Field ( \"Welcome to our Bavarian Beer Chat! \ud83c\udf7b We're here to guide you through the rich tapestry of Bavarian beer culture. Whether you're curious about traditional brews, local beer festivals, or the history behind Bavaria's renowned beer purity law, you've come to the right place. Type your question below, and let's embark on this flavorful journey together. Prost!\" , description = \"Welcome message to display to users when they start a conversation.\" , )","title":"ChainlitConfiguration"},{"location":"src/augmentation/bootstrap/configuration/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.bootstrap.configuration . Configuration AugmentationConfiguration Bases: EmbeddingConfiguration Main configuration class for the augmentation module. Extends the base embedding configuration with additional augmentation-specific settings to provide a complete configuration for text augmentation processes. Source code in src/augmentation/bootstrap/configuration/configuration.py 58 59 60 61 62 63 64 65 66 67 68 class AugmentationConfiguration ( EmbeddingConfiguration ): \"\"\" Main configuration class for the augmentation module. Extends the base embedding configuration with additional augmentation-specific settings to provide a complete configuration for text augmentation processes. \"\"\" augmentation : _AugmentationConfiguration = Field ( ... , description = \"Configuration of the augmentation process.\" )","title":"Configuration"},{"location":"src/augmentation/bootstrap/configuration/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.bootstrap.configuration .","title":"Configuration"},{"location":"src/augmentation/bootstrap/configuration/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/bootstrap/configuration/configuration/#src.augmentation.bootstrap.configuration.configuration.AugmentationConfiguration","text":"Bases: EmbeddingConfiguration Main configuration class for the augmentation module. Extends the base embedding configuration with additional augmentation-specific settings to provide a complete configuration for text augmentation processes. Source code in src/augmentation/bootstrap/configuration/configuration.py 58 59 60 61 62 63 64 65 66 67 68 class AugmentationConfiguration ( EmbeddingConfiguration ): \"\"\" Main configuration class for the augmentation module. Extends the base embedding configuration with additional augmentation-specific settings to provide a complete configuration for text augmentation processes. \"\"\" augmentation : _AugmentationConfiguration = Field ( ... , description = \"Configuration of the augmentation process.\" )","title":"AugmentationConfiguration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/","text":"Langfuse_configuration This module contains functionality related to the the langfuse_configuration module for augmentation.bootstrap.configuration . Langfuse_configuration LangfuseConfiguration Bases: BaseConfigurationWithSecrets Main configuration for the Langfuse integration. Contains all settings required to connect to and use the Langfuse platform, including server connection details, authentication, and dataset configurations. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class LangfuseConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Main configuration for the Langfuse integration. Contains all settings required to connect to and use the Langfuse platform, including server connection details, authentication, and dataset configurations. \"\"\" class Secrets ( BaseSecrets ): \"\"\"API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) public_key : SecretStr = Field ( ... , description = \"Public API key for authenticating with the Langfuse service\" , ) secret_key : SecretStr = Field ( ... , description = \"Secret API key for authenticating with the Langfuse service\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Langfuse server\" ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"Network protocol to use when connecting to the Langfuse server\" , ) port : int = Field ( 3000 , description = \"TCP port number on which the Langfuse server is listening\" , ) database : LangfuseDatabaseConfiguration = Field ( description = \"Database connection configuration for the Langfuse server\" , default_factory = LangfuseDatabaseConfiguration , ) datasets : LangfuseDatasetsConfiguration = Field ( description = \"Configuration for all datasets managed in the Langfuse platform\" , default_factory = LangfuseDatasetsConfiguration , ) chainlit_tag_format : str = Field ( \"chainlit_message_id: {message_id} \" , description = \"Template string for generating tags that link Chainlit messages to Langfuse traces\" , ) retention_job : LangfuseRetentionJobConfiguration = Field ( description = \"Configuration for the Langfuse retention job that manages trace data lifecycle\" , default_factory = LangfuseRetentionJobConfiguration , ) secrets : Secrets = Field ( None , description = \"API authentication credentials for the Langfuse service\" , ) @property def url ( self ) -> str : \"\"\"Generate the complete URL for connecting to the Langfuse server. Returns: str: Fully formatted URL with protocol, host, and port \"\"\" return f \" { self . protocol } :// { self . host } : { self . port } \" url property Generate the complete URL for connecting to the Langfuse server. Returns: str ( str ) \u2013 Fully formatted URL with protocol, host, and port Secrets Bases: BaseSecrets API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class Secrets ( BaseSecrets ): \"\"\"API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) public_key : SecretStr = Field ( ... , description = \"Public API key for authenticating with the Langfuse service\" , ) secret_key : SecretStr = Field ( ... , description = \"Secret API key for authenticating with the Langfuse service\" , ) LangfuseDatabaseConfiguration Bases: BaseConfigurationWithSecrets Configuration for connecting to the Langfuse PostgreSQL database. Contains all database connection parameters and credentials required for the Langfuse observation and analytics platform. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class LangfuseDatabaseConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Configuration for connecting to the Langfuse PostgreSQL database. Contains all database connection parameters and credentials required for the Langfuse observation and analytics platform. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__DATABASE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) user : SecretStr = Field ( ... , description = \"Username for authenticating with the Langfuse database server\" , ) password : SecretStr = Field ( ... , description = \"Password for authenticating with the Langfuse database server\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Langfuse database server\" , ) port : int = Field ( 5432 , description = \"TCP port number on which the Langfuse database server is listening\" , ) db : str = Field ( \"langfuse\" , description = \"Name of the specific database to connect to on the Langfuse server\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials for the Langfuse database\" ) Secrets Bases: BaseSecrets Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Secrets ( BaseSecrets ): \"\"\"Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__DATABASE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) user : SecretStr = Field ( ... , description = \"Username for authenticating with the Langfuse database server\" , ) password : SecretStr = Field ( ... , description = \"Password for authenticating with the Langfuse database server\" , ) LangfuseDatasetConfiguration Bases: BaseConfiguration Configuration for a single dataset in Langfuse. Defines properties of a dataset that will be created or used in the Langfuse platform. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class LangfuseDatasetConfiguration ( BaseConfiguration ): \"\"\"Configuration for a single dataset in Langfuse. Defines properties of a dataset that will be created or used in the Langfuse platform. \"\"\" name : str = Field ( ... , description = \"Unique identifier for the dataset in the Langfuse server\" , ) description : str = Field ( ... , description = \"Human-readable explanation of the dataset's purpose and contents\" , ) metadata : dict = Field ( {}, description = \"Additional structured information about the dataset as key-value pairs\" , ) LangfuseDatasetsConfiguration Bases: BaseConfiguration Configuration for all datasets used in the Langfuse platform. Contains definitions for specialized datasets used for different purposes. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class LangfuseDatasetsConfiguration ( BaseConfiguration ): \"\"\"Configuration for all datasets used in the Langfuse platform. Contains definitions for specialized datasets used for different purposes. \"\"\" feedback_dataset : LangfuseDatasetConfiguration = Field ( LangfuseDatasetConfiguration ( name = \"feedback-dataset\" , description = \"Dataset created out of positive feedbacks from the chatbot\" , ), description = \"Dataset for storing and analyzing user feedback collected from the chatbot interactions\" , ) manual_dataset : LangfuseDatasetConfiguration = Field ( LangfuseDatasetConfiguration ( name = \"manual-dataset\" , description = \"Dataset created directly by the user indicating the query and the correct answer\" , ), description = \"Dataset for storing manually curated query-answer pairs for evaluation and training\" , )","title":"Langfuse_configuration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#langfuse_configuration","text":"This module contains functionality related to the the langfuse_configuration module for augmentation.bootstrap.configuration .","title":"Langfuse_configuration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#langfuse_configuration_1","text":"","title":"Langfuse_configuration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseConfiguration","text":"Bases: BaseConfigurationWithSecrets Main configuration for the Langfuse integration. Contains all settings required to connect to and use the Langfuse platform, including server connection details, authentication, and dataset configurations. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class LangfuseConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Main configuration for the Langfuse integration. Contains all settings required to connect to and use the Langfuse platform, including server connection details, authentication, and dataset configurations. \"\"\" class Secrets ( BaseSecrets ): \"\"\"API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) public_key : SecretStr = Field ( ... , description = \"Public API key for authenticating with the Langfuse service\" , ) secret_key : SecretStr = Field ( ... , description = \"Secret API key for authenticating with the Langfuse service\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Langfuse server\" ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"Network protocol to use when connecting to the Langfuse server\" , ) port : int = Field ( 3000 , description = \"TCP port number on which the Langfuse server is listening\" , ) database : LangfuseDatabaseConfiguration = Field ( description = \"Database connection configuration for the Langfuse server\" , default_factory = LangfuseDatabaseConfiguration , ) datasets : LangfuseDatasetsConfiguration = Field ( description = \"Configuration for all datasets managed in the Langfuse platform\" , default_factory = LangfuseDatasetsConfiguration , ) chainlit_tag_format : str = Field ( \"chainlit_message_id: {message_id} \" , description = \"Template string for generating tags that link Chainlit messages to Langfuse traces\" , ) retention_job : LangfuseRetentionJobConfiguration = Field ( description = \"Configuration for the Langfuse retention job that manages trace data lifecycle\" , default_factory = LangfuseRetentionJobConfiguration , ) secrets : Secrets = Field ( None , description = \"API authentication credentials for the Langfuse service\" , ) @property def url ( self ) -> str : \"\"\"Generate the complete URL for connecting to the Langfuse server. Returns: str: Fully formatted URL with protocol, host, and port \"\"\" return f \" { self . protocol } :// { self . host } : { self . port } \"","title":"LangfuseConfiguration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseConfiguration.url","text":"Generate the complete URL for connecting to the Langfuse server. Returns: str ( str ) \u2013 Fully formatted URL with protocol, host, and port","title":"url"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseConfiguration.Secrets","text":"Bases: BaseSecrets API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class Secrets ( BaseSecrets ): \"\"\"API authentication credentials for Langfuse. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) public_key : SecretStr = Field ( ... , description = \"Public API key for authenticating with the Langfuse service\" , ) secret_key : SecretStr = Field ( ... , description = \"Secret API key for authenticating with the Langfuse service\" , )","title":"Secrets"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseDatabaseConfiguration","text":"Bases: BaseConfigurationWithSecrets Configuration for connecting to the Langfuse PostgreSQL database. Contains all database connection parameters and credentials required for the Langfuse observation and analytics platform. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class LangfuseDatabaseConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Configuration for connecting to the Langfuse PostgreSQL database. Contains all database connection parameters and credentials required for the Langfuse observation and analytics platform. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__DATABASE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) user : SecretStr = Field ( ... , description = \"Username for authenticating with the Langfuse database server\" , ) password : SecretStr = Field ( ... , description = \"Password for authenticating with the Langfuse database server\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Langfuse database server\" , ) port : int = Field ( 5432 , description = \"TCP port number on which the Langfuse database server is listening\" , ) db : str = Field ( \"langfuse\" , description = \"Name of the specific database to connect to on the Langfuse server\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials for the Langfuse database\" )","title":"LangfuseDatabaseConfiguration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseDatabaseConfiguration.Secrets","text":"Bases: BaseSecrets Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Secrets ( BaseSecrets ): \"\"\"Secret credentials for Langfuse database authentication. All fields are loaded from environment variables with the prefix RAG__LANGFUSE__DATABASE__. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__LANGFUSE__DATABASE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) user : SecretStr = Field ( ... , description = \"Username for authenticating with the Langfuse database server\" , ) password : SecretStr = Field ( ... , description = \"Password for authenticating with the Langfuse database server\" , )","title":"Secrets"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseDatasetConfiguration","text":"Bases: BaseConfiguration Configuration for a single dataset in Langfuse. Defines properties of a dataset that will be created or used in the Langfuse platform. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class LangfuseDatasetConfiguration ( BaseConfiguration ): \"\"\"Configuration for a single dataset in Langfuse. Defines properties of a dataset that will be created or used in the Langfuse platform. \"\"\" name : str = Field ( ... , description = \"Unique identifier for the dataset in the Langfuse server\" , ) description : str = Field ( ... , description = \"Human-readable explanation of the dataset's purpose and contents\" , ) metadata : dict = Field ( {}, description = \"Additional structured information about the dataset as key-value pairs\" , )","title":"LangfuseDatasetConfiguration"},{"location":"src/augmentation/bootstrap/configuration/langfuse_configuration/#src.augmentation.bootstrap.configuration.langfuse_configuration.LangfuseDatasetsConfiguration","text":"Bases: BaseConfiguration Configuration for all datasets used in the Langfuse platform. Contains definitions for specialized datasets used for different purposes. Source code in src/augmentation/bootstrap/configuration/langfuse_configuration.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class LangfuseDatasetsConfiguration ( BaseConfiguration ): \"\"\"Configuration for all datasets used in the Langfuse platform. Contains definitions for specialized datasets used for different purposes. \"\"\" feedback_dataset : LangfuseDatasetConfiguration = Field ( LangfuseDatasetConfiguration ( name = \"feedback-dataset\" , description = \"Dataset created out of positive feedbacks from the chatbot\" , ), description = \"Dataset for storing and analyzing user feedback collected from the chatbot interactions\" , ) manual_dataset : LangfuseDatasetConfiguration = Field ( LangfuseDatasetConfiguration ( name = \"manual-dataset\" , description = \"Dataset created directly by the user indicating the query and the correct answer\" , ), description = \"Dataset for storing manually curated query-answer pairs for evaluation and training\" , )","title":"LangfuseDatasetsConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/","text":"Chat_engine_configuration This module contains functionality related to the the chat_engine_configuration module for augmentation.bootstrap.configuration.components . Chat_engine_configuration BaseChatEngineConfiguration Bases: BaseConfiguration Base configuration class for chat engines. This class defines the standard configuration structure for all chat engines, including retriever, llm, and postprocessor components. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class BaseChatEngineConfiguration ( BaseConfiguration ): \"\"\"Base configuration class for chat engines. This class defines the standard configuration structure for all chat engines, including retriever, llm, and postprocessor components. \"\"\" guardrails : Any = Field ( ... , description = \"Optional guardrails configuration for the chat engine.\" , ) retriever : Any = Field ( ... , description = \"The retriever configuration for the augmentation pipeline.\" , ) llm : Any = Field ( ... , description = \"The llm configuration for the chat engine.\" ) postprocessors : List [ Any ] = Field ( ... , description = \"The list of postprocessors for the chat engine.\" ) prompt_templates : ChatEnginePromptTemplates = Field ( ... , description = \"The prompt templates configuration for the chat engine.\" , default_factory = ChatEnginePromptTemplates , ) @field_validator ( \"guardrails\" ) @classmethod def _validate_guardrails ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate retriever configuration against registered retriever types. Args: value: The retriever configuration to validate info: Validation context information Returns: Validated retriever configuration \"\"\" return super () . _validate ( value , info = info , registry = GurdrailsConfigurationRegistry , ) @field_validator ( \"retriever\" ) @classmethod def _validate_retriever ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate retriever configuration against registered retriever types. Args: value: The retriever configuration to validate info: Validation context information Returns: Validated retriever configuration \"\"\" return super () . _validate ( value , info = info , registry = RetrieverConfigurationRegistry , ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate llm configuration against registered llm types. Args: value: The llm configuration to validate info: Validation context information Returns: Validated llm configuration \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , ) @field_validator ( \"postprocessors\" ) @classmethod def _validate_postprocessors ( cls , value : Any , info : ValidationInfo ) -> List [ Any ]: \"\"\"Validate postprocessors configuration against registered postprocessor types. Args: value: The postprocessor configurations to validate info: Validation context information Returns: List of validated postprocessor configurations \"\"\" return super () . _validate ( value , info = info , registry = PostProcessorConfigurationRegistry , ) ChatEngineConfigurationRegistry Bases: ConfigurationRegistry Registry for chat engine configurations. Maps chat engine names to their corresponding configuration classes to facilitate configuration validation and factory creation. Attributes: _key_class ( Type ) \u2013 The enumeration class for chat engine names. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 159 160 161 162 163 164 165 166 167 168 169 class ChatEngineConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for chat engine configurations. Maps chat engine names to their corresponding configuration classes to facilitate configuration validation and factory creation. Attributes: _key_class: The enumeration class for chat engine names. \"\"\" _key_class : Type = ChatEngineName ChatEngineName Bases: str , Enum Enum defining available chat engine types. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 22 23 24 25 class ChatEngineName ( str , Enum ): \"\"\"Enum defining available chat engine types.\"\"\" LANGFUSE = \"langfuse\"","title":"Chat_engine_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#chat_engine_configuration","text":"This module contains functionality related to the the chat_engine_configuration module for augmentation.bootstrap.configuration.components .","title":"Chat_engine_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#chat_engine_configuration_1","text":"","title":"Chat_engine_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#src.augmentation.bootstrap.configuration.components.chat_engine_configuration.BaseChatEngineConfiguration","text":"Bases: BaseConfiguration Base configuration class for chat engines. This class defines the standard configuration structure for all chat engines, including retriever, llm, and postprocessor components. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class BaseChatEngineConfiguration ( BaseConfiguration ): \"\"\"Base configuration class for chat engines. This class defines the standard configuration structure for all chat engines, including retriever, llm, and postprocessor components. \"\"\" guardrails : Any = Field ( ... , description = \"Optional guardrails configuration for the chat engine.\" , ) retriever : Any = Field ( ... , description = \"The retriever configuration for the augmentation pipeline.\" , ) llm : Any = Field ( ... , description = \"The llm configuration for the chat engine.\" ) postprocessors : List [ Any ] = Field ( ... , description = \"The list of postprocessors for the chat engine.\" ) prompt_templates : ChatEnginePromptTemplates = Field ( ... , description = \"The prompt templates configuration for the chat engine.\" , default_factory = ChatEnginePromptTemplates , ) @field_validator ( \"guardrails\" ) @classmethod def _validate_guardrails ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate retriever configuration against registered retriever types. Args: value: The retriever configuration to validate info: Validation context information Returns: Validated retriever configuration \"\"\" return super () . _validate ( value , info = info , registry = GurdrailsConfigurationRegistry , ) @field_validator ( \"retriever\" ) @classmethod def _validate_retriever ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate retriever configuration against registered retriever types. Args: value: The retriever configuration to validate info: Validation context information Returns: Validated retriever configuration \"\"\" return super () . _validate ( value , info = info , registry = RetrieverConfigurationRegistry , ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate llm configuration against registered llm types. Args: value: The llm configuration to validate info: Validation context information Returns: Validated llm configuration \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , ) @field_validator ( \"postprocessors\" ) @classmethod def _validate_postprocessors ( cls , value : Any , info : ValidationInfo ) -> List [ Any ]: \"\"\"Validate postprocessors configuration against registered postprocessor types. Args: value: The postprocessor configurations to validate info: Validation context information Returns: List of validated postprocessor configurations \"\"\" return super () . _validate ( value , info = info , registry = PostProcessorConfigurationRegistry , )","title":"BaseChatEngineConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#src.augmentation.bootstrap.configuration.components.chat_engine_configuration.ChatEngineConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for chat engine configurations. Maps chat engine names to their corresponding configuration classes to facilitate configuration validation and factory creation. Attributes: _key_class ( Type ) \u2013 The enumeration class for chat engine names. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 159 160 161 162 163 164 165 166 167 168 169 class ChatEngineConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for chat engine configurations. Maps chat engine names to their corresponding configuration classes to facilitate configuration validation and factory creation. Attributes: _key_class: The enumeration class for chat engine names. \"\"\" _key_class : Type = ChatEngineName","title":"ChatEngineConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/chat_engine_configuration/#src.augmentation.bootstrap.configuration.components.chat_engine_configuration.ChatEngineName","text":"Bases: str , Enum Enum defining available chat engine types. Source code in src/augmentation/bootstrap/configuration/components/chat_engine_configuration.py 22 23 24 25 class ChatEngineName ( str , Enum ): \"\"\"Enum defining available chat engine types.\"\"\" LANGFUSE = \"langfuse\"","title":"ChatEngineName"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/","text":"Guardrails_configuration This module contains functionality related to the the guardrails_configuration module for augmentation.bootstrap.configuration.components . Guardrails_configuration GuardrailsConfiguration Bases: BaseConfiguration Configuration settings for guardrails. This class defines the necessary parameters for configuring and interacting with guardrails used in the system. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class GuardrailsConfiguration ( BaseConfiguration ): \"\"\" Configuration settings for guardrails. This class defines the necessary parameters for configuring and interacting with guardrails used in the system. \"\"\" llm : Any = Field ( ... , description = \"The llm configuration for the guardrails.\" ) input_prompt_name : str = Field ( \"default_input_guardrail_prompt\" , description = ( \"The name of the input guardrail prompt to use available in Langfuse prompts.\" , \"The prompt is used to determine if the input is valid to be passed to the chat engine.\" , \"The LLM should respond with 'yes' or 'true' if the input should be blocked.\" , ), ) output_prompt_name : str = Field ( \"default_output_guardrail_prompt\" , description = ( \"The name of the output guardrail prompt to use available in Langfuse prompts.\" , \"The prompt is used to determine if the output of the chat engine is valid to be returned to the user.\" , \"The LLM should respond with 'yes' or 'true' if the output should be blocked.\" , ), ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate llm configuration against registered llm types. Args: value: The llm configuration to validate info: Validation context information Returns: Validated llm configuration \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , ) GuardrailsName Bases: str , Enum Enumeration of supported guardrails. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 13 14 15 16 17 18 class GuardrailsName ( str , Enum ): \"\"\" Enumeration of supported guardrails. \"\"\" BASIC = \"basic\" GurdrailsConfigurationRegistry Bases: ConfigurationRegistry Registry for guardrails configurations. This registry maintains a mapping between guardrails names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class ( Type ) \u2013 The enumeration class for guardrails names. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class GurdrailsConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for guardrails configurations. This registry maintains a mapping between guardrails names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class: The enumeration class for guardrails names. \"\"\" _key_class : Type = GuardrailsName","title":"Guardrails_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#guardrails_configuration","text":"This module contains functionality related to the the guardrails_configuration module for augmentation.bootstrap.configuration.components .","title":"Guardrails_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#guardrails_configuration_1","text":"","title":"Guardrails_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#src.augmentation.bootstrap.configuration.components.guardrails_configuration.GuardrailsConfiguration","text":"Bases: BaseConfiguration Configuration settings for guardrails. This class defines the necessary parameters for configuring and interacting with guardrails used in the system. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class GuardrailsConfiguration ( BaseConfiguration ): \"\"\" Configuration settings for guardrails. This class defines the necessary parameters for configuring and interacting with guardrails used in the system. \"\"\" llm : Any = Field ( ... , description = \"The llm configuration for the guardrails.\" ) input_prompt_name : str = Field ( \"default_input_guardrail_prompt\" , description = ( \"The name of the input guardrail prompt to use available in Langfuse prompts.\" , \"The prompt is used to determine if the input is valid to be passed to the chat engine.\" , \"The LLM should respond with 'yes' or 'true' if the input should be blocked.\" , ), ) output_prompt_name : str = Field ( \"default_output_guardrail_prompt\" , description = ( \"The name of the output guardrail prompt to use available in Langfuse prompts.\" , \"The prompt is used to determine if the output of the chat engine is valid to be returned to the user.\" , \"The LLM should respond with 'yes' or 'true' if the output should be blocked.\" , ), ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate llm configuration against registered llm types. Args: value: The llm configuration to validate info: Validation context information Returns: Validated llm configuration \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , )","title":"GuardrailsConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#src.augmentation.bootstrap.configuration.components.guardrails_configuration.GuardrailsName","text":"Bases: str , Enum Enumeration of supported guardrails. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 13 14 15 16 17 18 class GuardrailsName ( str , Enum ): \"\"\" Enumeration of supported guardrails. \"\"\" BASIC = \"basic\"","title":"GuardrailsName"},{"location":"src/augmentation/bootstrap/configuration/components/guardrails_configuration/#src.augmentation.bootstrap.configuration.components.guardrails_configuration.GurdrailsConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for guardrails configurations. This registry maintains a mapping between guardrails names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class ( Type ) \u2013 The enumeration class for guardrails names. Source code in src/augmentation/bootstrap/configuration/components/guardrails_configuration.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class GurdrailsConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for guardrails configurations. This registry maintains a mapping between guardrails names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class: The enumeration class for guardrails names. \"\"\" _key_class : Type = GuardrailsName","title":"GurdrailsConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/","text":"Llm_configuration This module contains functionality related to the the llm_configuration module for augmentation.bootstrap.configuration.components . Llm_configuration LLMConfiguration Bases: BaseConfigurationWithSecrets Configuration settings for language models. This class defines the necessary parameters for configuring and interacting with language models (LLMs) used in the system. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class LLMConfiguration ( BaseConfigurationWithSecrets ): \"\"\" Configuration settings for language models. This class defines the necessary parameters for configuring and interacting with language models (LLMs) used in the system. \"\"\" name : str = Field ( ... , description = \"The name of the language model.\" ) max_tokens : int = Field ( ... , description = \"The maximum number of tokens for the language model.\" ) max_retries : int = Field ( ... , description = \"The maximum number of retries for the language model.\" ) LLMConfigurationRegistry Bases: ConfigurationRegistry Registry for language model configurations. This registry maintains a mapping between LLM provider names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class ( Type ) \u2013 The enumeration class for LLM provider names. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class LLMConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for language model configurations. This registry maintains a mapping between LLM provider names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class: The enumeration class for LLM provider names. \"\"\" _key_class : Type = LLMProviderName LLMProviderName Bases: str , Enum Enumeration of supported language model providers. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 10 11 12 13 14 15 class LLMProviderName ( str , Enum ): \"\"\" Enumeration of supported language model providers. \"\"\" LITE_LLM = \"lite_llm\"","title":"Llm_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#llm_configuration","text":"This module contains functionality related to the the llm_configuration module for augmentation.bootstrap.configuration.components .","title":"Llm_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#llm_configuration_1","text":"","title":"Llm_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#src.augmentation.bootstrap.configuration.components.llm_configuration.LLMConfiguration","text":"Bases: BaseConfigurationWithSecrets Configuration settings for language models. This class defines the necessary parameters for configuring and interacting with language models (LLMs) used in the system. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class LLMConfiguration ( BaseConfigurationWithSecrets ): \"\"\" Configuration settings for language models. This class defines the necessary parameters for configuring and interacting with language models (LLMs) used in the system. \"\"\" name : str = Field ( ... , description = \"The name of the language model.\" ) max_tokens : int = Field ( ... , description = \"The maximum number of tokens for the language model.\" ) max_retries : int = Field ( ... , description = \"The maximum number of retries for the language model.\" )","title":"LLMConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#src.augmentation.bootstrap.configuration.components.llm_configuration.LLMConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for language model configurations. This registry maintains a mapping between LLM provider names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class ( Type ) \u2013 The enumeration class for LLM provider names. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class LLMConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for language model configurations. This registry maintains a mapping between LLM provider names and their corresponding configuration classes, allowing the system to dynamically select and instantiate the appropriate configuration based on the provider name. Attributes: _key_class: The enumeration class for LLM provider names. \"\"\" _key_class : Type = LLMProviderName","title":"LLMConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/llm_configuration/#src.augmentation.bootstrap.configuration.components.llm_configuration.LLMProviderName","text":"Bases: str , Enum Enumeration of supported language model providers. Source code in src/augmentation/bootstrap/configuration/components/llm_configuration.py 10 11 12 13 14 15 class LLMProviderName ( str , Enum ): \"\"\" Enumeration of supported language model providers. \"\"\" LITE_LLM = \"lite_llm\"","title":"LLMProviderName"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/","text":"Postprocessors_configuration This module contains functionality related to the the postprocessors_configuration module for augmentation.bootstrap.configuration.components . Postprocessors_configuration PostProcessorConfiguration Bases: BaseConfiguration Base configuration for post-processors. Post-processors refine search results after initial retrieval to improve relevance and quality. Each post-processor implementation should inherit from this class and add its specific configuration parameters. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 19 20 21 22 23 24 25 26 27 28 29 class PostProcessorConfiguration ( BaseConfiguration ): \"\"\"Base configuration for post-processors. Post-processors refine search results after initial retrieval to improve relevance and quality. Each post-processor implementation should inherit from this class and add its specific configuration parameters. \"\"\" name : PostProcessorName = Field ( ... , description = \"The name of the postprocessor.\" ) PostProcessorConfigurationRegistry Bases: ConfigurationRegistry Registry for post-processor configurations. This registry maintains a collection of available post-processor configurations and provides methods for retrieving them based on the PostProcessorName. Attributes: _key_class ( Type ) \u2013 The enumeration class for post-processor names. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class PostProcessorConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for post-processor configurations. This registry maintains a collection of available post-processor configurations and provides methods for retrieving them based on the PostProcessorName. Attributes: _key_class: The enumeration class for post-processor names. \"\"\" _key_class : Type = PostProcessorName @classmethod def get_union_type ( self ) -> List [ PostProcessorConfiguration ]: \"\"\"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List[PostProcessorConfiguration]: A type representing all available post-processor configurations. \"\"\" return List [ super () . get_union_type ()] get_union_type () classmethod Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List [ PostProcessorConfiguration ] \u2013 List[PostProcessorConfiguration]: A type representing all available post-processor configurations. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 44 45 46 47 48 49 50 51 52 53 54 @classmethod def get_union_type ( self ) -> List [ PostProcessorConfiguration ]: \"\"\"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List[PostProcessorConfiguration]: A type representing all available post-processor configurations. \"\"\" return List [ super () . get_union_type ()] PostProcessorName Bases: str , Enum Enumeration of supported post-processor names. These identify the different post-processing options available for refining search results. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 10 11 12 13 14 15 16 class PostProcessorName ( str , Enum ): \"\"\"Enumeration of supported post-processor names. These identify the different post-processing options available for refining search results. \"\"\" COLBERT_RERANK = \"colbert_reranker\"","title":"Postprocessors_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#postprocessors_configuration","text":"This module contains functionality related to the the postprocessors_configuration module for augmentation.bootstrap.configuration.components .","title":"Postprocessors_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#postprocessors_configuration_1","text":"","title":"Postprocessors_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#src.augmentation.bootstrap.configuration.components.postprocessors_configuration.PostProcessorConfiguration","text":"Bases: BaseConfiguration Base configuration for post-processors. Post-processors refine search results after initial retrieval to improve relevance and quality. Each post-processor implementation should inherit from this class and add its specific configuration parameters. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 19 20 21 22 23 24 25 26 27 28 29 class PostProcessorConfiguration ( BaseConfiguration ): \"\"\"Base configuration for post-processors. Post-processors refine search results after initial retrieval to improve relevance and quality. Each post-processor implementation should inherit from this class and add its specific configuration parameters. \"\"\" name : PostProcessorName = Field ( ... , description = \"The name of the postprocessor.\" )","title":"PostProcessorConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#src.augmentation.bootstrap.configuration.components.postprocessors_configuration.PostProcessorConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for post-processor configurations. This registry maintains a collection of available post-processor configurations and provides methods for retrieving them based on the PostProcessorName. Attributes: _key_class ( Type ) \u2013 The enumeration class for post-processor names. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class PostProcessorConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for post-processor configurations. This registry maintains a collection of available post-processor configurations and provides methods for retrieving them based on the PostProcessorName. Attributes: _key_class: The enumeration class for post-processor names. \"\"\" _key_class : Type = PostProcessorName @classmethod def get_union_type ( self ) -> List [ PostProcessorConfiguration ]: \"\"\"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List[PostProcessorConfiguration]: A type representing all available post-processor configurations. \"\"\" return List [ super () . get_union_type ()]","title":"PostProcessorConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#src.augmentation.bootstrap.configuration.components.postprocessors_configuration.PostProcessorConfigurationRegistry.get_union_type","text":"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List [ PostProcessorConfiguration ] \u2013 List[PostProcessorConfiguration]: A type representing all available post-processor configurations. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 44 45 46 47 48 49 50 51 52 53 54 @classmethod def get_union_type ( self ) -> List [ PostProcessorConfiguration ]: \"\"\"Returns the union type of all registered post-processor configurations. This method is used for type validation and dynamic configuration loading, allowing the system to work with any of the registered post-processor types. Returns: List[PostProcessorConfiguration]: A type representing all available post-processor configurations. \"\"\" return List [ super () . get_union_type ()]","title":"get_union_type"},{"location":"src/augmentation/bootstrap/configuration/components/postprocessors_configuration/#src.augmentation.bootstrap.configuration.components.postprocessors_configuration.PostProcessorName","text":"Bases: str , Enum Enumeration of supported post-processor names. These identify the different post-processing options available for refining search results. Source code in src/augmentation/bootstrap/configuration/components/postprocessors_configuration.py 10 11 12 13 14 15 16 class PostProcessorName ( str , Enum ): \"\"\"Enumeration of supported post-processor names. These identify the different post-processing options available for refining search results. \"\"\" COLBERT_RERANK = \"colbert_reranker\"","title":"PostProcessorName"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/","text":"Retriever_configuration This module contains functionality related to the the retriever_configuration module for augmentation.bootstrap.configuration.components . Retriever_configuration RetrieverConfiguration Bases: BaseConfiguration Configuration class for retrievers. This class defines the parameters needed to configure a retriever component in the RAG pipeline. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 17 18 19 20 21 22 23 24 25 26 27 28 class RetrieverConfiguration ( BaseConfiguration ): \"\"\" Configuration class for retrievers. This class defines the parameters needed to configure a retriever component in the RAG pipeline. \"\"\" name : RetrieverName = Field ( ... , description = \"The name of the retriever.\" ) similarity_top_k : int = Field ( ... , description = \"The number of top similar items to retrieve.\" ) RetrieverConfigurationRegistry Bases: ConfigurationRegistry Registry for retriever configurations. Maps RetrieverName enum values to their corresponding configuration classes. Used for dynamic instantiation of retriever components based on configuration. Attributes: _key_class ( Type ) \u2013 The enumeration class for retriever names. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 31 32 33 34 35 36 37 38 39 40 41 42 class RetrieverConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for retriever configurations. Maps RetrieverName enum values to their corresponding configuration classes. Used for dynamic instantiation of retriever components based on configuration. Attributes: _key_class: The enumeration class for retriever names. \"\"\" _key_class : Type = RetrieverName RetrieverName Bases: str , Enum Enumeration of supported retriever types. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 10 11 12 13 14 class RetrieverName ( str , Enum ): \"\"\"Enumeration of supported retriever types.\"\"\" BASIC = \"basic\" AUTO = \"auto\"","title":"Retriever_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#retriever_configuration","text":"This module contains functionality related to the the retriever_configuration module for augmentation.bootstrap.configuration.components .","title":"Retriever_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#retriever_configuration_1","text":"","title":"Retriever_configuration"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#src.augmentation.bootstrap.configuration.components.retriever_configuration.RetrieverConfiguration","text":"Bases: BaseConfiguration Configuration class for retrievers. This class defines the parameters needed to configure a retriever component in the RAG pipeline. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 17 18 19 20 21 22 23 24 25 26 27 28 class RetrieverConfiguration ( BaseConfiguration ): \"\"\" Configuration class for retrievers. This class defines the parameters needed to configure a retriever component in the RAG pipeline. \"\"\" name : RetrieverName = Field ( ... , description = \"The name of the retriever.\" ) similarity_top_k : int = Field ( ... , description = \"The number of top similar items to retrieve.\" )","title":"RetrieverConfiguration"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#src.augmentation.bootstrap.configuration.components.retriever_configuration.RetrieverConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for retriever configurations. Maps RetrieverName enum values to their corresponding configuration classes. Used for dynamic instantiation of retriever components based on configuration. Attributes: _key_class ( Type ) \u2013 The enumeration class for retriever names. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 31 32 33 34 35 36 37 38 39 40 41 42 class RetrieverConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for retriever configurations. Maps RetrieverName enum values to their corresponding configuration classes. Used for dynamic instantiation of retriever components based on configuration. Attributes: _key_class: The enumeration class for retriever names. \"\"\" _key_class : Type = RetrieverName","title":"RetrieverConfigurationRegistry"},{"location":"src/augmentation/bootstrap/configuration/components/retriever_configuration/#src.augmentation.bootstrap.configuration.components.retriever_configuration.RetrieverName","text":"Bases: str , Enum Enumeration of supported retriever types. Source code in src/augmentation/bootstrap/configuration/components/retriever_configuration.py 10 11 12 13 14 class RetrieverName ( str , Enum ): \"\"\"Enumeration of supported retriever types.\"\"\" BASIC = \"basic\" AUTO = \"auto\"","title":"RetrieverName"},{"location":"src/augmentation/chainlit/exceptions/","text":"Exceptions This module contains functionality related to the the exceptions module for augmentation.chainlit . Exceptions TraceNotFoundException Bases: Exception Exception raised when a trace for a given message ID cannot be found. This exception is typically thrown when attempting to access or manipulate a trace using a message ID that doesn't exist in the system. It helps in handling missing trace scenarios gracefully. Source code in src/augmentation/chainlit/exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class TraceNotFoundException ( Exception ): \"\"\"Exception raised when a trace for a given message ID cannot be found. This exception is typically thrown when attempting to access or manipulate a trace using a message ID that doesn't exist in the system. It helps in handling missing trace scenarios gracefully. \"\"\" def __init__ ( self , message_id : str ) -> None : \"\"\"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Args: message_id: ID of the message whose trace was not found. This ID is stored and used in the error message. \"\"\" self . message_id = message_id self . message = f \"Trace for message with id { message_id } not found\" super () . __init__ ( self . message ) __init__ ( message_id ) Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Parameters: message_id ( str ) \u2013 ID of the message whose trace was not found. This ID is stored and used in the error message. Source code in src/augmentation/chainlit/exceptions.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , message_id : str ) -> None : \"\"\"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Args: message_id: ID of the message whose trace was not found. This ID is stored and used in the error message. \"\"\" self . message_id = message_id self . message = f \"Trace for message with id { message_id } not found\" super () . __init__ ( self . message )","title":"Exceptions"},{"location":"src/augmentation/chainlit/exceptions/#exceptions","text":"This module contains functionality related to the the exceptions module for augmentation.chainlit .","title":"Exceptions"},{"location":"src/augmentation/chainlit/exceptions/#exceptions_1","text":"","title":"Exceptions"},{"location":"src/augmentation/chainlit/exceptions/#src.augmentation.chainlit.exceptions.TraceNotFoundException","text":"Bases: Exception Exception raised when a trace for a given message ID cannot be found. This exception is typically thrown when attempting to access or manipulate a trace using a message ID that doesn't exist in the system. It helps in handling missing trace scenarios gracefully. Source code in src/augmentation/chainlit/exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class TraceNotFoundException ( Exception ): \"\"\"Exception raised when a trace for a given message ID cannot be found. This exception is typically thrown when attempting to access or manipulate a trace using a message ID that doesn't exist in the system. It helps in handling missing trace scenarios gracefully. \"\"\" def __init__ ( self , message_id : str ) -> None : \"\"\"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Args: message_id: ID of the message whose trace was not found. This ID is stored and used in the error message. \"\"\" self . message_id = message_id self . message = f \"Trace for message with id { message_id } not found\" super () . __init__ ( self . message )","title":"TraceNotFoundException"},{"location":"src/augmentation/chainlit/exceptions/#src.augmentation.chainlit.exceptions.TraceNotFoundException.__init__","text":"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Parameters: message_id ( str ) \u2013 ID of the message whose trace was not found. This ID is stored and used in the error message. Source code in src/augmentation/chainlit/exceptions.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , message_id : str ) -> None : \"\"\"Initialize the TraceNotFoundException with the specified message ID. This constructor creates a formatted error message that includes the missing message ID for better error reporting and debugging. Args: message_id: ID of the message whose trace was not found. This ID is stored and used in the error message. \"\"\" self . message_id = message_id self . message = f \"Trace for message with id { message_id } not found\" super () . __init__ ( self . message )","title":"__init__"},{"location":"src/augmentation/chainlit/feedback_service/","text":"Feedback_service This module contains functionality related to the the feedback_service module for augmentation.chainlit . Feedback_service ChainlitFeedbackService Service for handling Chainlit feedback and Langfuse integration. This service processes user feedback from the Chainlit UI and integrates it with Langfuse for tracking and analysis. It associates feedback with traces in Langfuse, saves positive feedback examples to datasets, and provides mechanisms for retrieving relevant trace information. The service tracks feedback scores, comments, and message content, allowing for detailed analytics in the Langfuse UI and quality improvement of responses over time. Attributes: SCORE_NAME \u2013 The standardized name used for feedback scores in Langfuse. Source code in src/augmentation/chainlit/feedback_service.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class ChainlitFeedbackService : \"\"\"Service for handling Chainlit feedback and Langfuse integration. This service processes user feedback from the Chainlit UI and integrates it with Langfuse for tracking and analysis. It associates feedback with traces in Langfuse, saves positive feedback examples to datasets, and provides mechanisms for retrieving relevant trace information. The service tracks feedback scores, comments, and message content, allowing for detailed analytics in the Langfuse UI and quality improvement of responses over time. Attributes: SCORE_NAME: The standardized name used for feedback scores in Langfuse. \"\"\" SCORE_NAME = \"User Feedback\" def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , langfuse_client : Langfuse , feedback_dataset : LangfuseDatasetConfiguration , chainlit_tag_format : str , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Args: langfuse_dataset_service: Service for creating and managing Langfuse datasets. langfuse_client: Client for interacting with the Langfuse API. feedback_dataset: Configuration for the dataset where positive feedback is stored. chainlit_tag_format: Format string for generating tags to retrieve traces by message ID. logger: Logger instance for recording service activities. \"\"\" self . langfuse_dataset_service = langfuse_dataset_service self . langfuse_client = langfuse_client self . feedback_dataset = feedback_dataset self . chainlit_tag_format = chainlit_tag_format self . logger = logger self . langfuse_dataset_service . create_if_does_not_exist ( feedback_dataset ) async def upsert ( self , feedback : Feedback ) -> bool : \"\"\"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Args: feedback: Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool: True if feedback was successfully processed and stored, False if an error occurred. \"\"\" trace = None try : trace = self . _fetch_trace ( feedback . forId ) if self . _is_positive ( feedback ): self . logger . info ( f \"Uploading trace { trace . id } to dataset { self . feedback_dataset . name } .\" ) self . _upload_trace_to_dataset ( trace ) self . langfuse_client . score ( trace_id = trace . id , name = ChainlitFeedbackService . SCORE_NAME , value = feedback . value , comment = feedback . comment , ) self . logger . info ( f \"Upserted feedback for { trace . id } trace with value { feedback . value } .\" ) return True except Exception as e : trace_id = trace . id if trace else None self . logger . warning ( f \"Failed to upsert feedback for { trace_id } trace: { e } \" ) return False def _fetch_trace ( self , message_id : str ) -> TraceWithDetails : \"\"\"Retrieve Langfuse trace associated with a Chainlit message ID. Uses the configured tag format to locate the trace related to a specific Chainlit message. Args: message_id: The unique identifier of the Chainlit message. Returns: TraceWithDetails: The complete trace data for the message. Raises: TraceNotFoundException: If no trace exists with the tag for this message ID. \"\"\" response = self . langfuse_client . fetch_traces ( tags = [ self . chainlit_tag_format . format ( message_id = message_id )] ) trace = response . data [ 0 ] if response . data else None if trace is None : raise TraceNotFoundException ( message_id ) return trace def _upload_trace_to_dataset ( self , trace : TraceWithDetails ) -> None : \"\"\"Save a trace to the feedback dataset for model improvement. Extracts relevant data from the trace including input query, retrieved nodes, templating information, and the final response, then creates a dataset item that can be used for model evaluation or fine-tuning. Args: trace: The trace containing the complete interaction details. \"\"\" retrieve_observation = self . _fetch_last_retrieve_observation ( trace ) last_templating_observation = self . _fetch_last_templating_observation ( trace ) output_generation_observation = ( self . _fetch_pre_last_generation_observation ( trace ) ) self . langfuse_client . create_dataset_item ( dataset_name = self . feedback_dataset . name , input = { \"query_str\" : trace . input , \"nodes\" : retrieve_observation . output . get ( \"nodes\" ), \"templating\" : last_templating_observation . input , }, expected_output = { \"result\" : output_generation_observation . output [ \"blocks\" ][ 0 ][ \"text\" ], }, source_trace_id = trace . id , metadata = { \"generated_by\" : output_generation_observation . model , }, ) def _fetch_last_retrieve_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the most recent retrieval observation from a trace. Retrieves information about the knowledge retrieval step in the RAG pipeline, including which nodes/documents were retrieved. Args: trace: The trace containing all observations. Returns: ObservationsView: The most recent retrieval observation, containing retrieved nodes. \"\"\" retrieve_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , name = \"retrieve\" , ) return max ( retrieve_observations . data , key = lambda x : x . createdAt ) def _fetch_last_templating_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the most recent templating observation from a trace. Retrieves information about how the prompt was constructed before being sent to the language model. Args: trace: The trace containing all observations. Returns: ObservationsView: The most recent templating observation, containing prompt construction details. \"\"\" templating_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , name = \"templating\" , ) return max ( templating_observations . data , key = lambda x : x . createdAt ) def _fetch_pre_last_generation_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the second most recent output observation from a trace. Retrieves information about the output generated by the language model, specifically the one before the last one. Args: trace: The trace containing all observations. Returns: ObservationsView: The second most recent output observation. \"\"\" output_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , type = \"GENERATION\" , ) return sorted ( output_observations . data , key = lambda x : x . createdAt )[ - 2 ] @staticmethod def _is_positive ( feedback : Feedback ) -> bool : \"\"\"Determine if the feedback is positive. Classifies feedback as positive if its numeric value is greater than zero. Positive feedback is used to identify high-quality examples for the dataset. Args: feedback: The feedback object containing the user's rating. Returns: bool: True if the feedback value is positive (greater than 0), False otherwise. \"\"\" return feedback . value > 0 __init__ ( langfuse_dataset_service , langfuse_client , feedback_dataset , chainlit_tag_format , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Parameters: langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service for creating and managing Langfuse datasets. langfuse_client ( Langfuse ) \u2013 Client for interacting with the Langfuse API. feedback_dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration for the dataset where positive feedback is stored. chainlit_tag_format ( str ) \u2013 Format string for generating tags to retrieve traces by message ID. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording service activities. Source code in src/augmentation/chainlit/feedback_service.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , langfuse_client : Langfuse , feedback_dataset : LangfuseDatasetConfiguration , chainlit_tag_format : str , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Args: langfuse_dataset_service: Service for creating and managing Langfuse datasets. langfuse_client: Client for interacting with the Langfuse API. feedback_dataset: Configuration for the dataset where positive feedback is stored. chainlit_tag_format: Format string for generating tags to retrieve traces by message ID. logger: Logger instance for recording service activities. \"\"\" self . langfuse_dataset_service = langfuse_dataset_service self . langfuse_client = langfuse_client self . feedback_dataset = feedback_dataset self . chainlit_tag_format = chainlit_tag_format self . logger = logger self . langfuse_dataset_service . create_if_does_not_exist ( feedback_dataset ) upsert ( feedback ) async Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Parameters: feedback ( Feedback ) \u2013 Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool ( bool ) \u2013 True if feedback was successfully processed and stored, False if an error occurred. Source code in src/augmentation/chainlit/feedback_service.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 async def upsert ( self , feedback : Feedback ) -> bool : \"\"\"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Args: feedback: Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool: True if feedback was successfully processed and stored, False if an error occurred. \"\"\" trace = None try : trace = self . _fetch_trace ( feedback . forId ) if self . _is_positive ( feedback ): self . logger . info ( f \"Uploading trace { trace . id } to dataset { self . feedback_dataset . name } .\" ) self . _upload_trace_to_dataset ( trace ) self . langfuse_client . score ( trace_id = trace . id , name = ChainlitFeedbackService . SCORE_NAME , value = feedback . value , comment = feedback . comment , ) self . logger . info ( f \"Upserted feedback for { trace . id } trace with value { feedback . value } .\" ) return True except Exception as e : trace_id = trace . id if trace else None self . logger . warning ( f \"Failed to upsert feedback for { trace_id } trace: { e } \" ) return False ChainlitFeedbackServiceFactory Bases: Factory Factory for creating ChainlitFeedbackService instances. This factory creates properly configured ChainlitFeedbackService instances using the application configuration. It handles initializing dependencies like the Langfuse client and dataset service. Source code in src/augmentation/chainlit/feedback_service.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 class ChainlitFeedbackServiceFactory ( Factory ): \"\"\"Factory for creating ChainlitFeedbackService instances. This factory creates properly configured ChainlitFeedbackService instances using the application configuration. It handles initializing dependencies like the Langfuse client and dataset service. \"\"\" _configuration_class : Type = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> ChainlitFeedbackService : \"\"\"Create a new ChainlitFeedbackService instance. Creates and configures a feedback service with the proper Langfuse client, dataset service, and configuration settings. Args: configuration: Application configuration containing Langfuse settings. Returns: ChainlitFeedbackService: A fully configured feedback service instance. \"\"\" langfuse_client = LangfuseClientFactory . create ( configuration . langfuse ) langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . langfuse ) feedback_dataset = configuration . langfuse . datasets . feedback_dataset chainlit_tag_format = configuration . langfuse . chainlit_tag_format return ChainlitFeedbackService ( langfuse_client = langfuse_client , langfuse_dataset_service = langfuse_dataset_service , feedback_dataset = feedback_dataset , chainlit_tag_format = chainlit_tag_format , )","title":"Feedback_service"},{"location":"src/augmentation/chainlit/feedback_service/#feedback_service","text":"This module contains functionality related to the the feedback_service module for augmentation.chainlit .","title":"Feedback_service"},{"location":"src/augmentation/chainlit/feedback_service/#feedback_service_1","text":"","title":"Feedback_service"},{"location":"src/augmentation/chainlit/feedback_service/#src.augmentation.chainlit.feedback_service.ChainlitFeedbackService","text":"Service for handling Chainlit feedback and Langfuse integration. This service processes user feedback from the Chainlit UI and integrates it with Langfuse for tracking and analysis. It associates feedback with traces in Langfuse, saves positive feedback examples to datasets, and provides mechanisms for retrieving relevant trace information. The service tracks feedback scores, comments, and message content, allowing for detailed analytics in the Langfuse UI and quality improvement of responses over time. Attributes: SCORE_NAME \u2013 The standardized name used for feedback scores in Langfuse. Source code in src/augmentation/chainlit/feedback_service.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class ChainlitFeedbackService : \"\"\"Service for handling Chainlit feedback and Langfuse integration. This service processes user feedback from the Chainlit UI and integrates it with Langfuse for tracking and analysis. It associates feedback with traces in Langfuse, saves positive feedback examples to datasets, and provides mechanisms for retrieving relevant trace information. The service tracks feedback scores, comments, and message content, allowing for detailed analytics in the Langfuse UI and quality improvement of responses over time. Attributes: SCORE_NAME: The standardized name used for feedback scores in Langfuse. \"\"\" SCORE_NAME = \"User Feedback\" def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , langfuse_client : Langfuse , feedback_dataset : LangfuseDatasetConfiguration , chainlit_tag_format : str , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Args: langfuse_dataset_service: Service for creating and managing Langfuse datasets. langfuse_client: Client for interacting with the Langfuse API. feedback_dataset: Configuration for the dataset where positive feedback is stored. chainlit_tag_format: Format string for generating tags to retrieve traces by message ID. logger: Logger instance for recording service activities. \"\"\" self . langfuse_dataset_service = langfuse_dataset_service self . langfuse_client = langfuse_client self . feedback_dataset = feedback_dataset self . chainlit_tag_format = chainlit_tag_format self . logger = logger self . langfuse_dataset_service . create_if_does_not_exist ( feedback_dataset ) async def upsert ( self , feedback : Feedback ) -> bool : \"\"\"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Args: feedback: Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool: True if feedback was successfully processed and stored, False if an error occurred. \"\"\" trace = None try : trace = self . _fetch_trace ( feedback . forId ) if self . _is_positive ( feedback ): self . logger . info ( f \"Uploading trace { trace . id } to dataset { self . feedback_dataset . name } .\" ) self . _upload_trace_to_dataset ( trace ) self . langfuse_client . score ( trace_id = trace . id , name = ChainlitFeedbackService . SCORE_NAME , value = feedback . value , comment = feedback . comment , ) self . logger . info ( f \"Upserted feedback for { trace . id } trace with value { feedback . value } .\" ) return True except Exception as e : trace_id = trace . id if trace else None self . logger . warning ( f \"Failed to upsert feedback for { trace_id } trace: { e } \" ) return False def _fetch_trace ( self , message_id : str ) -> TraceWithDetails : \"\"\"Retrieve Langfuse trace associated with a Chainlit message ID. Uses the configured tag format to locate the trace related to a specific Chainlit message. Args: message_id: The unique identifier of the Chainlit message. Returns: TraceWithDetails: The complete trace data for the message. Raises: TraceNotFoundException: If no trace exists with the tag for this message ID. \"\"\" response = self . langfuse_client . fetch_traces ( tags = [ self . chainlit_tag_format . format ( message_id = message_id )] ) trace = response . data [ 0 ] if response . data else None if trace is None : raise TraceNotFoundException ( message_id ) return trace def _upload_trace_to_dataset ( self , trace : TraceWithDetails ) -> None : \"\"\"Save a trace to the feedback dataset for model improvement. Extracts relevant data from the trace including input query, retrieved nodes, templating information, and the final response, then creates a dataset item that can be used for model evaluation or fine-tuning. Args: trace: The trace containing the complete interaction details. \"\"\" retrieve_observation = self . _fetch_last_retrieve_observation ( trace ) last_templating_observation = self . _fetch_last_templating_observation ( trace ) output_generation_observation = ( self . _fetch_pre_last_generation_observation ( trace ) ) self . langfuse_client . create_dataset_item ( dataset_name = self . feedback_dataset . name , input = { \"query_str\" : trace . input , \"nodes\" : retrieve_observation . output . get ( \"nodes\" ), \"templating\" : last_templating_observation . input , }, expected_output = { \"result\" : output_generation_observation . output [ \"blocks\" ][ 0 ][ \"text\" ], }, source_trace_id = trace . id , metadata = { \"generated_by\" : output_generation_observation . model , }, ) def _fetch_last_retrieve_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the most recent retrieval observation from a trace. Retrieves information about the knowledge retrieval step in the RAG pipeline, including which nodes/documents were retrieved. Args: trace: The trace containing all observations. Returns: ObservationsView: The most recent retrieval observation, containing retrieved nodes. \"\"\" retrieve_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , name = \"retrieve\" , ) return max ( retrieve_observations . data , key = lambda x : x . createdAt ) def _fetch_last_templating_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the most recent templating observation from a trace. Retrieves information about how the prompt was constructed before being sent to the language model. Args: trace: The trace containing all observations. Returns: ObservationsView: The most recent templating observation, containing prompt construction details. \"\"\" templating_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , name = \"templating\" , ) return max ( templating_observations . data , key = lambda x : x . createdAt ) def _fetch_pre_last_generation_observation ( self , trace : TraceWithDetails ) -> ObservationsView : \"\"\"Get the second most recent output observation from a trace. Retrieves information about the output generated by the language model, specifically the one before the last one. Args: trace: The trace containing all observations. Returns: ObservationsView: The second most recent output observation. \"\"\" output_observations = self . langfuse_client . fetch_observations ( trace_id = trace . id , type = \"GENERATION\" , ) return sorted ( output_observations . data , key = lambda x : x . createdAt )[ - 2 ] @staticmethod def _is_positive ( feedback : Feedback ) -> bool : \"\"\"Determine if the feedback is positive. Classifies feedback as positive if its numeric value is greater than zero. Positive feedback is used to identify high-quality examples for the dataset. Args: feedback: The feedback object containing the user's rating. Returns: bool: True if the feedback value is positive (greater than 0), False otherwise. \"\"\" return feedback . value > 0","title":"ChainlitFeedbackService"},{"location":"src/augmentation/chainlit/feedback_service/#src.augmentation.chainlit.feedback_service.ChainlitFeedbackService.__init__","text":"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Parameters: langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service for creating and managing Langfuse datasets. langfuse_client ( Langfuse ) \u2013 Client for interacting with the Langfuse API. feedback_dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration for the dataset where positive feedback is stored. chainlit_tag_format ( str ) \u2013 Format string for generating tags to retrieve traces by message ID. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording service activities. Source code in src/augmentation/chainlit/feedback_service.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , langfuse_client : Langfuse , feedback_dataset : LangfuseDatasetConfiguration , chainlit_tag_format : str , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the ChainlitFeedbackService. Creates the feedback dataset if it doesn't exist. Args: langfuse_dataset_service: Service for creating and managing Langfuse datasets. langfuse_client: Client for interacting with the Langfuse API. feedback_dataset: Configuration for the dataset where positive feedback is stored. chainlit_tag_format: Format string for generating tags to retrieve traces by message ID. logger: Logger instance for recording service activities. \"\"\" self . langfuse_dataset_service = langfuse_dataset_service self . langfuse_client = langfuse_client self . feedback_dataset = feedback_dataset self . chainlit_tag_format = chainlit_tag_format self . logger = logger self . langfuse_dataset_service . create_if_does_not_exist ( feedback_dataset )","title":"__init__"},{"location":"src/augmentation/chainlit/feedback_service/#src.augmentation.chainlit.feedback_service.ChainlitFeedbackService.upsert","text":"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Parameters: feedback ( Feedback ) \u2013 Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool ( bool ) \u2013 True if feedback was successfully processed and stored, False if an error occurred. Source code in src/augmentation/chainlit/feedback_service.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 async def upsert ( self , feedback : Feedback ) -> bool : \"\"\"Process and store Chainlit feedback in Langfuse. Takes a feedback object from Chainlit, associates it with the appropriate trace in Langfuse, and performs two main actions: 1. Records the feedback as a score on the trace 2. For positive feedback, saves the trace data to the feedback dataset for future model training Args: feedback: Chainlit Feedback object containing user feedback value, optional comment, and reference to the message being rated. Returns: bool: True if feedback was successfully processed and stored, False if an error occurred. \"\"\" trace = None try : trace = self . _fetch_trace ( feedback . forId ) if self . _is_positive ( feedback ): self . logger . info ( f \"Uploading trace { trace . id } to dataset { self . feedback_dataset . name } .\" ) self . _upload_trace_to_dataset ( trace ) self . langfuse_client . score ( trace_id = trace . id , name = ChainlitFeedbackService . SCORE_NAME , value = feedback . value , comment = feedback . comment , ) self . logger . info ( f \"Upserted feedback for { trace . id } trace with value { feedback . value } .\" ) return True except Exception as e : trace_id = trace . id if trace else None self . logger . warning ( f \"Failed to upsert feedback for { trace_id } trace: { e } \" ) return False","title":"upsert"},{"location":"src/augmentation/chainlit/feedback_service/#src.augmentation.chainlit.feedback_service.ChainlitFeedbackServiceFactory","text":"Bases: Factory Factory for creating ChainlitFeedbackService instances. This factory creates properly configured ChainlitFeedbackService instances using the application configuration. It handles initializing dependencies like the Langfuse client and dataset service. Source code in src/augmentation/chainlit/feedback_service.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 class ChainlitFeedbackServiceFactory ( Factory ): \"\"\"Factory for creating ChainlitFeedbackService instances. This factory creates properly configured ChainlitFeedbackService instances using the application configuration. It handles initializing dependencies like the Langfuse client and dataset service. \"\"\" _configuration_class : Type = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> ChainlitFeedbackService : \"\"\"Create a new ChainlitFeedbackService instance. Creates and configures a feedback service with the proper Langfuse client, dataset service, and configuration settings. Args: configuration: Application configuration containing Langfuse settings. Returns: ChainlitFeedbackService: A fully configured feedback service instance. \"\"\" langfuse_client = LangfuseClientFactory . create ( configuration . langfuse ) langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . langfuse ) feedback_dataset = configuration . langfuse . datasets . feedback_dataset chainlit_tag_format = configuration . langfuse . chainlit_tag_format return ChainlitFeedbackService ( langfuse_client = langfuse_client , langfuse_dataset_service = langfuse_dataset_service , feedback_dataset = feedback_dataset , chainlit_tag_format = chainlit_tag_format , )","title":"ChainlitFeedbackServiceFactory"},{"location":"src/augmentation/chainlit/service/","text":"Service This module contains functionality related to the the service module for augmentation.chainlit . Service ChainlitService Bases: BaseDataLayer Data layer implementation for Chainlit integration with Langfuse. Handles persistence of feedback and dataset management through Langfuse. Source code in src/augmentation/chainlit/service.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class ChainlitService ( BaseDataLayer ): \"\"\"Data layer implementation for Chainlit integration with Langfuse. Handles persistence of feedback and dataset management through Langfuse. \"\"\" def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , feedback_service : ChainlitFeedbackService , manual_dataset : LangfuseDatasetConfiguration , ): \"\"\"Initialize the Chainlit service. Args: langfuse_dataset_service: Service for managing Langfuse datasets. feedback_service: Service handling Chainlit feedback. manual_dataset: Configuration for manual dataset. \"\"\" self . manual_dataset = manual_dataset self . feedback_service = feedback_service langfuse_dataset_service . create_if_does_not_exist ( self . manual_dataset ) async def upsert_feedback ( self , feedback : Feedback ) -> bool : \"\"\"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Args: feedback: Feedback object containing user feedback details and metadata. Returns: bool: True if feedback was successfully upserted, False otherwise. \"\"\" return await self . feedback_service . upsert ( feedback ) async def build_debug_url ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a UI element in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a user record in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_feedback ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete user feedback from storage. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation step from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_thread_author ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve the author of a conversation thread. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a user record from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def list_threads ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"List all conversation threads in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def update_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def update_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation thread in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass __init__ ( langfuse_dataset_service , feedback_service , manual_dataset ) Initialize the Chainlit service. Parameters: langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service for managing Langfuse datasets. feedback_service ( ChainlitFeedbackService ) \u2013 Service handling Chainlit feedback. manual_dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration for manual dataset. Source code in src/augmentation/chainlit/service.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , feedback_service : ChainlitFeedbackService , manual_dataset : LangfuseDatasetConfiguration , ): \"\"\"Initialize the Chainlit service. Args: langfuse_dataset_service: Service for managing Langfuse datasets. feedback_service: Service handling Chainlit feedback. manual_dataset: Configuration for manual dataset. \"\"\" self . manual_dataset = manual_dataset self . feedback_service = feedback_service langfuse_dataset_service . create_if_does_not_exist ( self . manual_dataset ) build_debug_url ( * args , ** kwargs ) async Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 59 60 61 62 63 64 65 66 67 68 async def build_debug_url ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass create_element ( * args , ** kwargs ) async Create a UI element in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 70 71 72 73 74 75 76 77 78 79 async def create_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a UI element in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass create_step ( * args , ** kwargs ) async Create a conversation step in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 81 82 83 84 85 86 87 88 89 90 async def create_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass create_user ( * args , ** kwargs ) async Create a user record in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 92 93 94 95 96 97 98 99 100 101 async def create_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a user record in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass delete_element ( * args , ** kwargs ) async Delete a UI element from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 103 104 105 106 107 108 109 110 111 112 async def delete_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass delete_feedback ( * args , ** kwargs ) async Delete user feedback from storage. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 114 115 116 117 118 119 120 121 122 123 async def delete_feedback ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete user feedback from storage. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass delete_step ( * args , ** kwargs ) async Delete a conversation step from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 125 126 127 128 129 130 131 132 133 134 async def delete_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation step from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass delete_thread ( * args , ** kwargs ) async Delete a conversation thread from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 136 137 138 139 140 141 142 143 144 145 async def delete_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass get_element ( * args , ** kwargs ) async Retrieve a UI element from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 147 148 149 150 151 152 153 154 155 156 async def get_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass get_thread ( * args , ** kwargs ) async Retrieve a conversation thread from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 158 159 160 161 162 163 164 165 166 167 async def get_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass get_thread_author ( * args , ** kwargs ) async Retrieve the author of a conversation thread. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 169 170 171 172 173 174 175 176 177 178 async def get_thread_author ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve the author of a conversation thread. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass get_user ( * args , ** kwargs ) async Retrieve a user record from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 180 181 182 183 184 185 186 187 188 189 async def get_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a user record from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass list_threads ( * args , ** kwargs ) async List all conversation threads in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 191 192 193 194 195 196 197 198 199 200 async def list_threads ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"List all conversation threads in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass update_step ( * args , ** kwargs ) async Update a conversation step in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 202 203 204 205 206 207 208 209 210 211 async def update_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass update_thread ( * args , ** kwargs ) async Update a conversation thread in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 213 214 215 216 217 218 219 220 221 222 async def update_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation thread in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass upsert_feedback ( feedback ) async Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Parameters: feedback ( Feedback ) \u2013 Feedback object containing user feedback details and metadata. Returns: bool ( bool ) \u2013 True if feedback was successfully upserted, False otherwise. Source code in src/augmentation/chainlit/service.py 45 46 47 48 49 50 51 52 53 54 55 56 57 async def upsert_feedback ( self , feedback : Feedback ) -> bool : \"\"\"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Args: feedback: Feedback object containing user feedback details and metadata. Returns: bool: True if feedback was successfully upserted, False otherwise. \"\"\" return await self . feedback_service . upsert ( feedback ) ChainlitServiceFactory Bases: Factory Factory for creating ChainlitService instances. Creates and configures ChainlitService instances using application configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating ChainlitService instances. In this case, it is _AugmentationConfiguration. Source code in src/augmentation/chainlit/service.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 class ChainlitServiceFactory ( Factory ): \"\"\"Factory for creating ChainlitService instances. Creates and configures ChainlitService instances using application configuration. Attributes: _configuration_class (Type): The configuration class used for creating ChainlitService instances. In this case, it is _AugmentationConfiguration. \"\"\" _configuration_class : Type = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> ChainlitService : \"\"\"Create a configured ChainlitService instance. Args: configuration: Application configuration containing Langfuse settings. Returns: ChainlitService: Configured service instance ready for use. \"\"\" langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . langfuse ) feedback_service = ChainlitFeedbackServiceFactory . create ( configuration ) manual_dataset = configuration . langfuse . datasets . manual_dataset return ChainlitService ( langfuse_dataset_service = langfuse_dataset_service , feedback_service = feedback_service , manual_dataset = manual_dataset , )","title":"Service"},{"location":"src/augmentation/chainlit/service/#service","text":"This module contains functionality related to the the service module for augmentation.chainlit .","title":"Service"},{"location":"src/augmentation/chainlit/service/#service_1","text":"","title":"Service"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService","text":"Bases: BaseDataLayer Data layer implementation for Chainlit integration with Langfuse. Handles persistence of feedback and dataset management through Langfuse. Source code in src/augmentation/chainlit/service.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class ChainlitService ( BaseDataLayer ): \"\"\"Data layer implementation for Chainlit integration with Langfuse. Handles persistence of feedback and dataset management through Langfuse. \"\"\" def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , feedback_service : ChainlitFeedbackService , manual_dataset : LangfuseDatasetConfiguration , ): \"\"\"Initialize the Chainlit service. Args: langfuse_dataset_service: Service for managing Langfuse datasets. feedback_service: Service handling Chainlit feedback. manual_dataset: Configuration for manual dataset. \"\"\" self . manual_dataset = manual_dataset self . feedback_service = feedback_service langfuse_dataset_service . create_if_does_not_exist ( self . manual_dataset ) async def upsert_feedback ( self , feedback : Feedback ) -> bool : \"\"\"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Args: feedback: Feedback object containing user feedback details and metadata. Returns: bool: True if feedback was successfully upserted, False otherwise. \"\"\" return await self . feedback_service . upsert ( feedback ) async def build_debug_url ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a UI element in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def create_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a user record in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_feedback ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete user feedback from storage. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation step from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def delete_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_thread_author ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve the author of a conversation thread. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def get_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a user record from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def list_threads ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"List all conversation threads in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def update_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass async def update_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation thread in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"ChainlitService"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.__init__","text":"Initialize the Chainlit service. Parameters: langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service for managing Langfuse datasets. feedback_service ( ChainlitFeedbackService ) \u2013 Service handling Chainlit feedback. manual_dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration for manual dataset. Source code in src/augmentation/chainlit/service.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , langfuse_dataset_service : LangfuseDatasetService , feedback_service : ChainlitFeedbackService , manual_dataset : LangfuseDatasetConfiguration , ): \"\"\"Initialize the Chainlit service. Args: langfuse_dataset_service: Service for managing Langfuse datasets. feedback_service: Service handling Chainlit feedback. manual_dataset: Configuration for manual dataset. \"\"\" self . manual_dataset = manual_dataset self . feedback_service = feedback_service langfuse_dataset_service . create_if_does_not_exist ( self . manual_dataset )","title":"__init__"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.build_debug_url","text":"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 59 60 61 62 63 64 65 66 67 68 async def build_debug_url ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Build a debug URL for Chainlit thread debugging. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"build_debug_url"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.create_element","text":"Create a UI element in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 70 71 72 73 74 75 76 77 78 79 async def create_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a UI element in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"create_element"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.create_step","text":"Create a conversation step in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 81 82 83 84 85 86 87 88 89 90 async def create_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"create_step"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.create_user","text":"Create a user record in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 92 93 94 95 96 97 98 99 100 101 async def create_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Create a user record in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"create_user"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.delete_element","text":"Delete a UI element from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 103 104 105 106 107 108 109 110 111 112 async def delete_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"delete_element"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.delete_feedback","text":"Delete user feedback from storage. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 114 115 116 117 118 119 120 121 122 123 async def delete_feedback ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete user feedback from storage. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"delete_feedback"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.delete_step","text":"Delete a conversation step from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 125 126 127 128 129 130 131 132 133 134 async def delete_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation step from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"delete_step"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.delete_thread","text":"Delete a conversation thread from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 136 137 138 139 140 141 142 143 144 145 async def delete_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Delete a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"delete_thread"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.get_element","text":"Retrieve a UI element from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 147 148 149 150 151 152 153 154 155 156 async def get_element ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a UI element from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"get_element"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.get_thread","text":"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 158 159 160 161 162 163 164 165 166 167 async def get_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a conversation thread from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"get_thread"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.get_thread_author","text":"Retrieve the author of a conversation thread. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 169 170 171 172 173 174 175 176 177 178 async def get_thread_author ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve the author of a conversation thread. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"get_thread_author"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.get_user","text":"Retrieve a user record from Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 180 181 182 183 184 185 186 187 188 189 async def get_user ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Retrieve a user record from Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"get_user"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.list_threads","text":"List all conversation threads in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 191 192 193 194 195 196 197 198 199 200 async def list_threads ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"List all conversation threads in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"list_threads"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.update_step","text":"Update a conversation step in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 202 203 204 205 206 207 208 209 210 211 async def update_step ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation step in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"update_step"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.update_thread","text":"Update a conversation thread in Chainlit. Not implemented in this integration. Parameters: *args ( Any , default: () ) \u2013 Variable length argument list. **kwargs ( Any , default: {} ) \u2013 Arbitrary keyword arguments. Source code in src/augmentation/chainlit/service.py 213 214 215 216 217 218 219 220 221 222 async def update_thread ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Update a conversation thread in Chainlit. Not implemented in this integration. Args: *args: Variable length argument list. **kwargs: Arbitrary keyword arguments. \"\"\" pass","title":"update_thread"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitService.upsert_feedback","text":"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Parameters: feedback ( Feedback ) \u2013 Feedback object containing user feedback details and metadata. Returns: bool ( bool ) \u2013 True if feedback was successfully upserted, False otherwise. Source code in src/augmentation/chainlit/service.py 45 46 47 48 49 50 51 52 53 54 55 56 57 async def upsert_feedback ( self , feedback : Feedback ) -> bool : \"\"\"Upsert Chainlit feedback to Langfuse database. Processes user feedback from Chainlit UI and stores it in Langfuse for analysis and dataset management. Args: feedback: Feedback object containing user feedback details and metadata. Returns: bool: True if feedback was successfully upserted, False otherwise. \"\"\" return await self . feedback_service . upsert ( feedback )","title":"upsert_feedback"},{"location":"src/augmentation/chainlit/service/#src.augmentation.chainlit.service.ChainlitServiceFactory","text":"Bases: Factory Factory for creating ChainlitService instances. Creates and configures ChainlitService instances using application configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating ChainlitService instances. In this case, it is _AugmentationConfiguration. Source code in src/augmentation/chainlit/service.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 class ChainlitServiceFactory ( Factory ): \"\"\"Factory for creating ChainlitService instances. Creates and configures ChainlitService instances using application configuration. Attributes: _configuration_class (Type): The configuration class used for creating ChainlitService instances. In this case, it is _AugmentationConfiguration. \"\"\" _configuration_class : Type = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> ChainlitService : \"\"\"Create a configured ChainlitService instance. Args: configuration: Application configuration containing Langfuse settings. Returns: ChainlitService: Configured service instance ready for use. \"\"\" langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . langfuse ) feedback_service = ChainlitFeedbackServiceFactory . create ( configuration ) manual_dataset = configuration . langfuse . datasets . manual_dataset return ChainlitService ( langfuse_dataset_service = langfuse_dataset_service , feedback_service = feedback_service , manual_dataset = manual_dataset , )","title":"ChainlitServiceFactory"},{"location":"src/augmentation/chainlit/utils/","text":"Utils This module contains functionality related to the the utils module for augmentation.chainlit . Utils ChainlitUtils Utility class for handling conversation messages. Provides methods for managing welcome messages and reference handling in conversations. Attributes: REFERENCES_TEMPLATE \u2013 Template string for formatting references section. Source code in src/augmentation/chainlit/utils.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class ChainlitUtils : \"\"\"Utility class for handling conversation messages. Provides methods for managing welcome messages and reference handling in conversations. Attributes: REFERENCES_TEMPLATE: Template string for formatting references section. \"\"\" REFERENCES_TEMPLATE = \" \\n\\n **References**: \\n \" \" {references} \\n \" def __init__ ( self , configuration : ChainlitConfiguration ): \"\"\"Initialize ChainlitUtils with configuration. Args: configuration (ChainlitConfiguration): Configuration containing settings like welcome message. \"\"\" self . configuration = configuration def get_disclaimer_message ( self ) -> Message : \"\"\"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message: A chainlit Message object with the Assistant as author and the disclaimer message as content. \"\"\" return Message ( author = \"System\" , content = \"\" , elements = [ Text ( name = self . configuration . disclaimer_title , content = self . configuration . disclaimer_text , ) ], ) def get_welcome_message ( self ) -> Message : \"\"\"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message: A chainlit Message object with the Assistant as author and the welcome message as content. \"\"\" return Message ( author = \"Assistant\" , content = self . configuration . welcome_message ) def add_references ( self , message : Message , response : StreamingResponse ) -> None : \"\"\"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Args: message (Message): The chainlit Message object to modify by adding references. response (StreamingResponse): A response object containing source_nodes with reference information. \"\"\" message . content += self . _get_references_str ( response . source_nodes ) def _get_references_str ( self , nodes : List [ NodeWithScore ]) -> str : \"\"\"Generate a formatted references section from source nodes. Processes a list of source nodes to create a deduplicated, formatted string of references. Args: nodes (List[NodeWithScore]): List of source nodes with relevance scores and metadata containing reference information. Returns: str: A formatted string containing unique references in the structure defined by REFERENCES_TEMPLATE. \"\"\" raw_references = [ self . _get_reference_str ( node ) for node in nodes ] references = \" \\n \" . join ( set ( raw_references )) return self . REFERENCES_TEMPLATE . format ( references = references ) def _get_reference_str ( self , node : NodeWithScore ) -> str : \"\"\"Format a single node's reference as a markdown string. Extracts title and URL from a node's metadata and formats it as a markdown list item, with link formatting if a URL is available. Args: node (NodeWithScore): A source node containing metadata with title and optional URL information. Returns: str: A formatted markdown list item representing the reference, with a clickable link if URL is available. \"\"\" title = node . metadata . get ( \"title\" ) if not title : title = node . metadata . get ( \"Title\" ) url = node . metadata . get ( \"url\" ) if url : return \"- [ {} ]( {} )\" . format ( title , url ) else : return f \"- { title } \" __init__ ( configuration ) Initialize ChainlitUtils with configuration. Parameters: configuration ( ChainlitConfiguration ) \u2013 Configuration containing settings like welcome message. Source code in src/augmentation/chainlit/utils.py 24 25 26 27 28 29 30 31 def __init__ ( self , configuration : ChainlitConfiguration ): \"\"\"Initialize ChainlitUtils with configuration. Args: configuration (ChainlitConfiguration): Configuration containing settings like welcome message. \"\"\" self . configuration = configuration add_references ( message , response ) Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Parameters: message ( Message ) \u2013 The chainlit Message object to modify by adding references. response ( StreamingResponse ) \u2013 A response object containing source_nodes with reference information. Source code in src/augmentation/chainlit/utils.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def add_references ( self , message : Message , response : StreamingResponse ) -> None : \"\"\"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Args: message (Message): The chainlit Message object to modify by adding references. response (StreamingResponse): A response object containing source_nodes with reference information. \"\"\" message . content += self . _get_references_str ( response . source_nodes ) get_disclaimer_message () Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message ( Message ) \u2013 A chainlit Message object with the Assistant as author and the disclaimer message as content. Source code in src/augmentation/chainlit/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def get_disclaimer_message ( self ) -> Message : \"\"\"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message: A chainlit Message object with the Assistant as author and the disclaimer message as content. \"\"\" return Message ( author = \"System\" , content = \"\" , elements = [ Text ( name = self . configuration . disclaimer_title , content = self . configuration . disclaimer_text , ) ], ) get_welcome_message () Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message ( Message ) \u2013 A chainlit Message object with the Assistant as author and the welcome message as content. Source code in src/augmentation/chainlit/utils.py 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_welcome_message ( self ) -> Message : \"\"\"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message: A chainlit Message object with the Assistant as author and the welcome message as content. \"\"\" return Message ( author = \"Assistant\" , content = self . configuration . welcome_message ) ChainlitUtilsFactory Bases: SingletonFactory Factory class for creating ChainlitUtils instances. This class extends the Factory pattern to create instances of ChainlitUtils based on a given ChainlitConfiguration. It ensures that the configuration provided is of the correct type and handles the creation of new instances. Attributes: _configuration_class ( Type ) \u2013 The expected type of configuration objects. Source code in src/augmentation/chainlit/utils.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class ChainlitUtilsFactory ( SingletonFactory ): \"\"\" Factory class for creating ChainlitUtils instances. This class extends the Factory pattern to create instances of ChainlitUtils based on a given ChainlitConfiguration. It ensures that the configuration provided is of the correct type and handles the creation of new instances. Attributes: _configuration_class (Type): The expected type of configuration objects. \"\"\" _configuration_class : Type = ChainlitConfiguration @classmethod def _create_instance ( cls , configuration : ChainlitConfiguration ) -> ChainlitUtils : \"\"\"Create a new ChainlitUtils instance. Args: configuration (ChainlitConfiguration): Configuration object for ChainlitUtils. Returns: ChainlitUtils: A new instance of ChainlitUtils initialized with the given configuration. \"\"\" return ChainlitUtils ( configuration )","title":"Utils"},{"location":"src/augmentation/chainlit/utils/#utils","text":"This module contains functionality related to the the utils module for augmentation.chainlit .","title":"Utils"},{"location":"src/augmentation/chainlit/utils/#utils_1","text":"","title":"Utils"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils","text":"Utility class for handling conversation messages. Provides methods for managing welcome messages and reference handling in conversations. Attributes: REFERENCES_TEMPLATE \u2013 Template string for formatting references section. Source code in src/augmentation/chainlit/utils.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class ChainlitUtils : \"\"\"Utility class for handling conversation messages. Provides methods for managing welcome messages and reference handling in conversations. Attributes: REFERENCES_TEMPLATE: Template string for formatting references section. \"\"\" REFERENCES_TEMPLATE = \" \\n\\n **References**: \\n \" \" {references} \\n \" def __init__ ( self , configuration : ChainlitConfiguration ): \"\"\"Initialize ChainlitUtils with configuration. Args: configuration (ChainlitConfiguration): Configuration containing settings like welcome message. \"\"\" self . configuration = configuration def get_disclaimer_message ( self ) -> Message : \"\"\"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message: A chainlit Message object with the Assistant as author and the disclaimer message as content. \"\"\" return Message ( author = \"System\" , content = \"\" , elements = [ Text ( name = self . configuration . disclaimer_title , content = self . configuration . disclaimer_text , ) ], ) def get_welcome_message ( self ) -> Message : \"\"\"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message: A chainlit Message object with the Assistant as author and the welcome message as content. \"\"\" return Message ( author = \"Assistant\" , content = self . configuration . welcome_message ) def add_references ( self , message : Message , response : StreamingResponse ) -> None : \"\"\"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Args: message (Message): The chainlit Message object to modify by adding references. response (StreamingResponse): A response object containing source_nodes with reference information. \"\"\" message . content += self . _get_references_str ( response . source_nodes ) def _get_references_str ( self , nodes : List [ NodeWithScore ]) -> str : \"\"\"Generate a formatted references section from source nodes. Processes a list of source nodes to create a deduplicated, formatted string of references. Args: nodes (List[NodeWithScore]): List of source nodes with relevance scores and metadata containing reference information. Returns: str: A formatted string containing unique references in the structure defined by REFERENCES_TEMPLATE. \"\"\" raw_references = [ self . _get_reference_str ( node ) for node in nodes ] references = \" \\n \" . join ( set ( raw_references )) return self . REFERENCES_TEMPLATE . format ( references = references ) def _get_reference_str ( self , node : NodeWithScore ) -> str : \"\"\"Format a single node's reference as a markdown string. Extracts title and URL from a node's metadata and formats it as a markdown list item, with link formatting if a URL is available. Args: node (NodeWithScore): A source node containing metadata with title and optional URL information. Returns: str: A formatted markdown list item representing the reference, with a clickable link if URL is available. \"\"\" title = node . metadata . get ( \"title\" ) if not title : title = node . metadata . get ( \"Title\" ) url = node . metadata . get ( \"url\" ) if url : return \"- [ {} ]( {} )\" . format ( title , url ) else : return f \"- { title } \"","title":"ChainlitUtils"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils.__init__","text":"Initialize ChainlitUtils with configuration. Parameters: configuration ( ChainlitConfiguration ) \u2013 Configuration containing settings like welcome message. Source code in src/augmentation/chainlit/utils.py 24 25 26 27 28 29 30 31 def __init__ ( self , configuration : ChainlitConfiguration ): \"\"\"Initialize ChainlitUtils with configuration. Args: configuration (ChainlitConfiguration): Configuration containing settings like welcome message. \"\"\" self . configuration = configuration","title":"__init__"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils.add_references","text":"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Parameters: message ( Message ) \u2013 The chainlit Message object to modify by adding references. response ( StreamingResponse ) \u2013 A response object containing source_nodes with reference information. Source code in src/augmentation/chainlit/utils.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def add_references ( self , message : Message , response : StreamingResponse ) -> None : \"\"\"Append source references to a message from a streaming response. Takes a message object and a streaming response containing source nodes, then appends formatted references to the message content. Args: message (Message): The chainlit Message object to modify by adding references. response (StreamingResponse): A response object containing source_nodes with reference information. \"\"\" message . content += self . _get_references_str ( response . source_nodes )","title":"add_references"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils.get_disclaimer_message","text":"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message ( Message ) \u2013 A chainlit Message object with the Assistant as author and the disclaimer message as content. Source code in src/augmentation/chainlit/utils.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def get_disclaimer_message ( self ) -> Message : \"\"\"Create and return a disclaimer message for the chat interface. Creates a Message object with content from the configuration's disclaimer_text to inform users about the nature of the AI-generated content. Returns: Message: A chainlit Message object with the Assistant as author and the disclaimer message as content. \"\"\" return Message ( author = \"System\" , content = \"\" , elements = [ Text ( name = self . configuration . disclaimer_title , content = self . configuration . disclaimer_text , ) ], )","title":"get_disclaimer_message"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtils.get_welcome_message","text":"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message ( Message ) \u2013 A chainlit Message object with the Assistant as author and the welcome message as content. Source code in src/augmentation/chainlit/utils.py 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_welcome_message ( self ) -> Message : \"\"\"Create and return a welcome message for the chat interface. Creates a Message object with content from the configuration's welcome_message to greet users when they start a conversation. Returns: Message: A chainlit Message object with the Assistant as author and the welcome message as content. \"\"\" return Message ( author = \"Assistant\" , content = self . configuration . welcome_message )","title":"get_welcome_message"},{"location":"src/augmentation/chainlit/utils/#src.augmentation.chainlit.utils.ChainlitUtilsFactory","text":"Bases: SingletonFactory Factory class for creating ChainlitUtils instances. This class extends the Factory pattern to create instances of ChainlitUtils based on a given ChainlitConfiguration. It ensures that the configuration provided is of the correct type and handles the creation of new instances. Attributes: _configuration_class ( Type ) \u2013 The expected type of configuration objects. Source code in src/augmentation/chainlit/utils.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class ChainlitUtilsFactory ( SingletonFactory ): \"\"\" Factory class for creating ChainlitUtils instances. This class extends the Factory pattern to create instances of ChainlitUtils based on a given ChainlitConfiguration. It ensures that the configuration provided is of the correct type and handles the creation of new instances. Attributes: _configuration_class (Type): The expected type of configuration objects. \"\"\" _configuration_class : Type = ChainlitConfiguration @classmethod def _create_instance ( cls , configuration : ChainlitConfiguration ) -> ChainlitUtils : \"\"\"Create a new ChainlitUtils instance. Args: configuration (ChainlitConfiguration): Configuration object for ChainlitUtils. Returns: ChainlitUtils: A new instance of ChainlitUtils initialized with the given configuration. \"\"\" return ChainlitUtils ( configuration )","title":"ChainlitUtilsFactory"},{"location":"src/augmentation/components/chat_engines/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.chat_engines . Registry ChatEngineRegistry Bases: Registry Registry for chat engine components. This registry provides a centralized mechanism for registering, retrieving, and managing chat engine implementations. Chat engines are indexed by their ChatEngineName enum value, allowing for dynamic selection and instantiation of different chat engine strategies. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to ChatEngineName. Source code in src/augmentation/components/chat_engines/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class ChatEngineRegistry ( Registry ): \"\"\" Registry for chat engine components. This registry provides a centralized mechanism for registering, retrieving, and managing chat engine implementations. Chat engines are indexed by their ChatEngineName enum value, allowing for dynamic selection and instantiation of different chat engine strategies. Attributes: _key_class (Type): The class type used for registry keys, set to ChatEngineName. \"\"\" _key_class : Type = ChatEngineName","title":"Registry"},{"location":"src/augmentation/components/chat_engines/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.chat_engines .","title":"Registry"},{"location":"src/augmentation/components/chat_engines/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/chat_engines/registry/#src.augmentation.components.chat_engines.registry.ChatEngineRegistry","text":"Bases: Registry Registry for chat engine components. This registry provides a centralized mechanism for registering, retrieving, and managing chat engine implementations. Chat engines are indexed by their ChatEngineName enum value, allowing for dynamic selection and instantiation of different chat engine strategies. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to ChatEngineName. Source code in src/augmentation/components/chat_engines/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class ChatEngineRegistry ( Registry ): \"\"\" Registry for chat engine components. This registry provides a centralized mechanism for registering, retrieving, and managing chat engine implementations. Chat engines are indexed by their ChatEngineName enum value, allowing for dynamic selection and instantiation of different chat engine strategies. Attributes: _key_class (Type): The class type used for registry keys, set to ChatEngineName. \"\"\" _key_class : Type = ChatEngineName","title":"ChatEngineRegistry"},{"location":"src/augmentation/components/chat_engines/langfuse/callback_manager/","text":"Callback_manager This module contains functionality related to the the callback_manager module for augmentation.components.chat_engines.langfuse . Callback_manager LlamaIndexCallbackManagerFactory Bases: Factory Factory class for creating LlamaIndex callback managers with Langfuse integration. This factory creates a callback manager with a Langfuse handler to enable tracking and observability for LlamaIndex operations. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Source code in src/augmentation/components/chat_engines/langfuse/callback_manager.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class LlamaIndexCallbackManagerFactory ( Factory ): \"\"\"Factory class for creating LlamaIndex callback managers with Langfuse integration. This factory creates a callback manager with a Langfuse handler to enable tracking and observability for LlamaIndex operations. Attributes: _configuration_class (Type): The configuration class used for creating \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration , session_id : str = \"\" ) -> LlamaIndexCallbackHandler : \"\"\"Create a CallbackManager with a LlamaIndexCallbackHandler for Langfuse integration. This method creates a Langfuse callback handler using the provided configuration and wraps it in a CallbackManager. Args: configuration: Langfuse configuration containing API keys and URL session_id: Optional identifier for the session to group related traces Returns: A configured CallbackManager with the Langfuse callback handler \"\"\" handler = LlamaIndexCallbackHandler ( secret_key = configuration . secrets . secret_key . get_secret_value (), public_key = configuration . secrets . public_key . get_secret_value (), host = configuration . url , session_id = session_id , ) return CallbackManager ( handlers = [ handler ])","title":"Callback_manager"},{"location":"src/augmentation/components/chat_engines/langfuse/callback_manager/#callback_manager","text":"This module contains functionality related to the the callback_manager module for augmentation.components.chat_engines.langfuse .","title":"Callback_manager"},{"location":"src/augmentation/components/chat_engines/langfuse/callback_manager/#callback_manager_1","text":"","title":"Callback_manager"},{"location":"src/augmentation/components/chat_engines/langfuse/callback_manager/#src.augmentation.components.chat_engines.langfuse.callback_manager.LlamaIndexCallbackManagerFactory","text":"Bases: Factory Factory class for creating LlamaIndex callback managers with Langfuse integration. This factory creates a callback manager with a Langfuse handler to enable tracking and observability for LlamaIndex operations. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Source code in src/augmentation/components/chat_engines/langfuse/callback_manager.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class LlamaIndexCallbackManagerFactory ( Factory ): \"\"\"Factory class for creating LlamaIndex callback managers with Langfuse integration. This factory creates a callback manager with a Langfuse handler to enable tracking and observability for LlamaIndex operations. Attributes: _configuration_class (Type): The configuration class used for creating \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration , session_id : str = \"\" ) -> LlamaIndexCallbackHandler : \"\"\"Create a CallbackManager with a LlamaIndexCallbackHandler for Langfuse integration. This method creates a Langfuse callback handler using the provided configuration and wraps it in a CallbackManager. Args: configuration: Langfuse configuration containing API keys and URL session_id: Optional identifier for the session to group related traces Returns: A configured CallbackManager with the Langfuse callback handler \"\"\" handler = LlamaIndexCallbackHandler ( secret_key = configuration . secrets . secret_key . get_secret_value (), public_key = configuration . secrets . public_key . get_secret_value (), host = configuration . url , session_id = session_id , ) return CallbackManager ( handlers = [ handler ])","title":"LlamaIndexCallbackManagerFactory"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/","text":"Chat_engine This module contains functionality related to the the chat_engine module for augmentation.components.chat_engines.langfuse . Chat_engine LangfuseChatEngine Bases: CondensePlusContextChatEngine Custom chat engine implementing Retrieval-Augmented Generation (RAG). Coordinates retrieval, post-processing, and response generation for RAG workflow. Integrates with Langfuse for tracing and Chainlit for message tracking. Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 class LangfuseChatEngine ( CondensePlusContextChatEngine ): \"\"\"Custom chat engine implementing Retrieval-Augmented Generation (RAG). Coordinates retrieval, post-processing, and response generation for RAG workflow. Integrates with Langfuse for tracing and Chainlit for message tracking. \"\"\" chainlit_tag_format : str = Field ( description = \"Format of the tag used to retrieve the trace by chainlit message id in Langfuse.\" ) input_guardrail_prompt_template : Optional [ PromptTemplate ] = Field ( description = \"Prompt template for validating user input compliance\" , default = None , ) output_guardrail_prompt_template : Optional [ PromptTemplate ] = Field ( description = \"Prompt template for validating response output compliance\" , default = None , ) def __init__ ( self , retriever : BaseRetriever , llm : LLM , memory : BaseMemory , chainlit_tag_format : str , guardrails_engine : BaseGuardrailsEngine , context_prompt : Optional [ Union [ str , PromptTemplate ]] = None , context_refine_prompt : Optional [ Union [ str , PromptTemplate ]] = None , condense_prompt : Optional [ Union [ str , PromptTemplate ]] = None , system_prompt : Optional [ str ] = None , skip_condense : bool = False , node_postprocessors : Optional [ List [ BaseNodePostprocessor ]] = None , callback_manager : Optional [ CallbackManager ] = None , verbose : bool = False , ): \"\"\" Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Args: retriever: Document retriever for RAG llm: Language model for response generation memory: Memory buffer for chat history chainlit_tag_format: Format for Chainlit message ID in Langfuse guardrails_engine: Guardrail engine for input/output validation context_prompt: Prompt for context generation context_refine_prompt: Prompt for refining context condense_prompt: Prompt for condensing context system_prompt: System prompt for LLM input_guardrail_prompt_template: Prompt template for input validation output_guardrail_prompt_template: Prompt template for output validation skip_condense: Flag to skip context condensing node_postprocessors: List of postprocessors for node processing callback_manager: Callback manager for tracing verbose: Flag for verbose output \"\"\" super () . __init__ ( retriever = retriever , llm = llm , memory = memory , context_prompt = context_prompt , context_refine_prompt = context_refine_prompt , condense_prompt = condense_prompt , system_prompt = system_prompt , skip_condense = skip_condense , node_postprocessors = node_postprocessors , callback_manager = callback_manager , verbose = verbose , ) self . guardrails_engine = guardrails_engine self . chainlit_tag_format = chainlit_tag_format def stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" response = self . _stream_chat ( message = message , chat_history = chat_history , chainlit_message_id = chainlit_message_id , source_process = source_process , ) trace = self . get_current_langfuse_trace () trace . input = message return response @trace_method ( \"chat\" ) def _stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" self . _set_chainlit_message_id ( message_id = chainlit_message_id , source_process = source_process ) guarded_response = self . guardrails_engine . input_guard ( message = message , is_stream = True ) if guarded_response : self . _save_chat_history ( input_message = message , output_message = guarded_response . response ) return guarded_response response = super () . chat ( message = message , chat_history = chat_history ) guarded_response = self . guardrails_engine . output_guard ( message = response . response , is_stream = True ) if guarded_response : self . _save_chat_history ( input_message = message , output_message = guarded_response . response ) return guarded_response response . is_dummy_stream = True return response def get_current_langfuse_trace ( self ) -> StatefulTraceClient : \"\"\"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient: Active Langfuse trace or None if not found \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): return handler . trace return None def set_session_id ( self , session_id : str ) -> None : \"\"\"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Args: session_id: Unique identifier for current user session \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . session_id = session_id def _set_chainlit_message_id ( self , message_id : str , source_process : SourceProcess ) -> None : \"\"\"Configure Chainlit message tracking in Langfuse trace. Links the current Langfuse trace to a Chainlit message ID and tags with the processing source context for traceability in the Langfuse UI. Args: message_id: Chainlit message identifier to reference source_process: Source context enum categorizing the query origin \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . set_trace_params ( tags = [ self . chainlit_tag_format . format ( message_id = message_id ), source_process . name . lower (), ] ) def _save_chat_history ( self , input_message : str , output_message : str ) -> None : \"\"\"Save chat history to memory buffer. Args: input_message: User input message output_message: Generated response message \"\"\" self . _memory . put ( ChatMessage ( role = MessageRole . USER , content = input_message ) ) self . _memory . put ( ChatMessage ( role = MessageRole . ASSISTANT , content = output_message ) ) __init__ ( retriever , llm , memory , chainlit_tag_format , guardrails_engine , context_prompt = None , context_refine_prompt = None , condense_prompt = None , system_prompt = None , skip_condense = False , node_postprocessors = None , callback_manager = None , verbose = False ) Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Parameters: retriever ( BaseRetriever ) \u2013 Document retriever for RAG llm ( LLM ) \u2013 Language model for response generation memory ( BaseMemory ) \u2013 Memory buffer for chat history chainlit_tag_format ( str ) \u2013 Format for Chainlit message ID in Langfuse guardrails_engine ( BaseGuardrailsEngine ) \u2013 Guardrail engine for input/output validation context_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for context generation context_refine_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for refining context condense_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for condensing context system_prompt ( Optional [ str ] , default: None ) \u2013 System prompt for LLM input_guardrail_prompt_template \u2013 Prompt template for input validation output_guardrail_prompt_template \u2013 Prompt template for output validation skip_condense ( bool , default: False ) \u2013 Flag to skip context condensing node_postprocessors ( Optional [ List [ BaseNodePostprocessor ]] , default: None ) \u2013 List of postprocessors for node processing callback_manager ( Optional [ CallbackManager ] , default: None ) \u2013 Callback manager for tracing verbose ( bool , default: False ) \u2013 Flag for verbose output Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , retriever : BaseRetriever , llm : LLM , memory : BaseMemory , chainlit_tag_format : str , guardrails_engine : BaseGuardrailsEngine , context_prompt : Optional [ Union [ str , PromptTemplate ]] = None , context_refine_prompt : Optional [ Union [ str , PromptTemplate ]] = None , condense_prompt : Optional [ Union [ str , PromptTemplate ]] = None , system_prompt : Optional [ str ] = None , skip_condense : bool = False , node_postprocessors : Optional [ List [ BaseNodePostprocessor ]] = None , callback_manager : Optional [ CallbackManager ] = None , verbose : bool = False , ): \"\"\" Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Args: retriever: Document retriever for RAG llm: Language model for response generation memory: Memory buffer for chat history chainlit_tag_format: Format for Chainlit message ID in Langfuse guardrails_engine: Guardrail engine for input/output validation context_prompt: Prompt for context generation context_refine_prompt: Prompt for refining context condense_prompt: Prompt for condensing context system_prompt: System prompt for LLM input_guardrail_prompt_template: Prompt template for input validation output_guardrail_prompt_template: Prompt template for output validation skip_condense: Flag to skip context condensing node_postprocessors: List of postprocessors for node processing callback_manager: Callback manager for tracing verbose: Flag for verbose output \"\"\" super () . __init__ ( retriever = retriever , llm = llm , memory = memory , context_prompt = context_prompt , context_refine_prompt = context_refine_prompt , condense_prompt = condense_prompt , system_prompt = system_prompt , skip_condense = skip_condense , node_postprocessors = node_postprocessors , callback_manager = callback_manager , verbose = verbose , ) self . guardrails_engine = guardrails_engine self . chainlit_tag_format = chainlit_tag_format get_current_langfuse_trace () Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient ( StatefulTraceClient ) \u2013 Active Langfuse trace or None if not found Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 195 196 197 198 199 200 201 202 203 204 205 206 207 def get_current_langfuse_trace ( self ) -> StatefulTraceClient : \"\"\"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient: Active Langfuse trace or None if not found \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): return handler . trace return None set_session_id ( session_id ) Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Parameters: session_id ( str ) \u2013 Unique identifier for current user session Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 209 210 211 212 213 214 215 216 217 218 219 220 def set_session_id ( self , session_id : str ) -> None : \"\"\"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Args: session_id: Unique identifier for current user session \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . session_id = session_id stream_chat ( message , chat_history = None , chainlit_message_id = None , source_process = SourceProcess . CHAT_COMPLETION ) Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Parameters: message ( str ) \u2013 Raw query string to process chat_history ( Optional [ List [ ChatMessage ]] , default: None ) \u2013 Optional chat history for context chainlit_message_id ( str , default: None ) \u2013 Optional ID for linking to Chainlit message in UI source_process ( SourceProcess , default: CHAT_COMPLETION ) \u2013 Context identifier indicating query's origin source Returns: AgentChatResponse ( AgentChatResponse ) \u2013 Generated response from RAG pipeline with dummy streaming Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" response = self . _stream_chat ( message = message , chat_history = chat_history , chainlit_message_id = chainlit_message_id , source_process = source_process , ) trace = self . get_current_langfuse_trace () trace . input = message return response LangfuseChatEngineFactory Bases: Factory Factory for creating configured LangfuseChatEngine instances. Constructs and connects components needed for the RAG pipeline. Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 class LangfuseChatEngineFactory ( Factory ): \"\"\"Factory for creating configured LangfuseChatEngine instances. Constructs and connects components needed for the RAG pipeline. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> LangfuseChatEngine : \"\"\"Create and configure a LangfuseChatEngine instance from configuration. Instantiates all RAG pipeline components based on configuration settings, connects them with a shared callback manager for tracing, and assembles them into a complete chat engine. Args: configuration: Complete augmentation configuration containing settings for all components Returns: LangfuseChatEngine: Fully configured RAG chat engine with tracing \"\"\" chat_engine_configuration = configuration . augmentation . chat_engine llm = LLMRegistry . get ( chat_engine_configuration . llm . provider ) . create ( chat_engine_configuration . llm ) retriever = RetrieverRegistry . get ( chat_engine_configuration . retriever . name ) . create ( configuration ) postprocessors = [ PostprocessorRegistry . get ( postprocessor_configuration . name ) . create ( postprocessor_configuration ) for postprocessor_configuration in chat_engine_configuration . postprocessors ] langfuse_callback_manager = LlamaIndexCallbackManagerFactory . create ( configuration . augmentation . langfuse ) memory = ChatMemoryBuffer ( chat_history = [], token_limit = llm . metadata . context_window - 256 ) ( condense_prompt_template , context_prompt_template , context_refine_prompt_template , system_prompt_template , ) = cls . _get_prompt_templates ( configuration = configuration . augmentation ) guardrails_configuration = ( configuration . augmentation . chat_engine . guardrails ) guardrails_engine = GuardrailsRegistry . get ( guardrails_configuration . name ) . create ( configuration = configuration . augmentation ) retriever . callback_manager = langfuse_callback_manager llm . callback_manager = langfuse_callback_manager for postprocessor in postprocessors : postprocessor . callback_manager = langfuse_callback_manager return LangfuseChatEngine ( retriever = retriever , llm = llm , node_postprocessors = postprocessors , callback_manager = langfuse_callback_manager , memory = memory , context_prompt = context_prompt_template , system_prompt = system_prompt_template , context_refine_prompt = context_refine_prompt_template , condense_prompt = condense_prompt_template , chainlit_tag_format = configuration . augmentation . langfuse . chainlit_tag_format , guardrails_engine = guardrails_engine , ) @staticmethod def _get_prompt_templates ( configuration : _AugmentationConfiguration , ) -> tuple : \"\"\"Retrieves the prompt template for the augmentation process. Args: configuration: Configuration object containing prompt templates settings. Returns: Tuple of prompt templates for condensing, context generation, context refinement, system prompts. \"\"\" langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . langfuse ) condense_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . condense_prompt_name ) context_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . context_prompt_name ) context_refine_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . context_refine_prompt_name ) system_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . system_prompt_name ) return ( condense_prompt_template , context_prompt_template , context_refine_prompt_template , system_prompt_template , ) SourceProcess Bases: Enum Enumeration of possible chat processing sources. Attributes: CHAT_COMPLETION \u2013 Query from interactive chat completion interface DEPLOYMENT_EVALUATION \u2013 Query from automated deployment testing and evaluation Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 37 38 39 40 41 42 43 44 45 46 class SourceProcess ( Enum ): \"\"\"Enumeration of possible chat processing sources. Attributes: CHAT_COMPLETION: Query from interactive chat completion interface DEPLOYMENT_EVALUATION: Query from automated deployment testing and evaluation \"\"\" CHAT_COMPLETION = 1 DEPLOYMENT_EVALUATION = 2","title":"Chat_engine"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#chat_engine","text":"This module contains functionality related to the the chat_engine module for augmentation.components.chat_engines.langfuse .","title":"Chat_engine"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#chat_engine_1","text":"","title":"Chat_engine"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine","text":"Bases: CondensePlusContextChatEngine Custom chat engine implementing Retrieval-Augmented Generation (RAG). Coordinates retrieval, post-processing, and response generation for RAG workflow. Integrates with Langfuse for tracing and Chainlit for message tracking. Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 class LangfuseChatEngine ( CondensePlusContextChatEngine ): \"\"\"Custom chat engine implementing Retrieval-Augmented Generation (RAG). Coordinates retrieval, post-processing, and response generation for RAG workflow. Integrates with Langfuse for tracing and Chainlit for message tracking. \"\"\" chainlit_tag_format : str = Field ( description = \"Format of the tag used to retrieve the trace by chainlit message id in Langfuse.\" ) input_guardrail_prompt_template : Optional [ PromptTemplate ] = Field ( description = \"Prompt template for validating user input compliance\" , default = None , ) output_guardrail_prompt_template : Optional [ PromptTemplate ] = Field ( description = \"Prompt template for validating response output compliance\" , default = None , ) def __init__ ( self , retriever : BaseRetriever , llm : LLM , memory : BaseMemory , chainlit_tag_format : str , guardrails_engine : BaseGuardrailsEngine , context_prompt : Optional [ Union [ str , PromptTemplate ]] = None , context_refine_prompt : Optional [ Union [ str , PromptTemplate ]] = None , condense_prompt : Optional [ Union [ str , PromptTemplate ]] = None , system_prompt : Optional [ str ] = None , skip_condense : bool = False , node_postprocessors : Optional [ List [ BaseNodePostprocessor ]] = None , callback_manager : Optional [ CallbackManager ] = None , verbose : bool = False , ): \"\"\" Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Args: retriever: Document retriever for RAG llm: Language model for response generation memory: Memory buffer for chat history chainlit_tag_format: Format for Chainlit message ID in Langfuse guardrails_engine: Guardrail engine for input/output validation context_prompt: Prompt for context generation context_refine_prompt: Prompt for refining context condense_prompt: Prompt for condensing context system_prompt: System prompt for LLM input_guardrail_prompt_template: Prompt template for input validation output_guardrail_prompt_template: Prompt template for output validation skip_condense: Flag to skip context condensing node_postprocessors: List of postprocessors for node processing callback_manager: Callback manager for tracing verbose: Flag for verbose output \"\"\" super () . __init__ ( retriever = retriever , llm = llm , memory = memory , context_prompt = context_prompt , context_refine_prompt = context_refine_prompt , condense_prompt = condense_prompt , system_prompt = system_prompt , skip_condense = skip_condense , node_postprocessors = node_postprocessors , callback_manager = callback_manager , verbose = verbose , ) self . guardrails_engine = guardrails_engine self . chainlit_tag_format = chainlit_tag_format def stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" response = self . _stream_chat ( message = message , chat_history = chat_history , chainlit_message_id = chainlit_message_id , source_process = source_process , ) trace = self . get_current_langfuse_trace () trace . input = message return response @trace_method ( \"chat\" ) def _stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" self . _set_chainlit_message_id ( message_id = chainlit_message_id , source_process = source_process ) guarded_response = self . guardrails_engine . input_guard ( message = message , is_stream = True ) if guarded_response : self . _save_chat_history ( input_message = message , output_message = guarded_response . response ) return guarded_response response = super () . chat ( message = message , chat_history = chat_history ) guarded_response = self . guardrails_engine . output_guard ( message = response . response , is_stream = True ) if guarded_response : self . _save_chat_history ( input_message = message , output_message = guarded_response . response ) return guarded_response response . is_dummy_stream = True return response def get_current_langfuse_trace ( self ) -> StatefulTraceClient : \"\"\"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient: Active Langfuse trace or None if not found \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): return handler . trace return None def set_session_id ( self , session_id : str ) -> None : \"\"\"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Args: session_id: Unique identifier for current user session \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . session_id = session_id def _set_chainlit_message_id ( self , message_id : str , source_process : SourceProcess ) -> None : \"\"\"Configure Chainlit message tracking in Langfuse trace. Links the current Langfuse trace to a Chainlit message ID and tags with the processing source context for traceability in the Langfuse UI. Args: message_id: Chainlit message identifier to reference source_process: Source context enum categorizing the query origin \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . set_trace_params ( tags = [ self . chainlit_tag_format . format ( message_id = message_id ), source_process . name . lower (), ] ) def _save_chat_history ( self , input_message : str , output_message : str ) -> None : \"\"\"Save chat history to memory buffer. Args: input_message: User input message output_message: Generated response message \"\"\" self . _memory . put ( ChatMessage ( role = MessageRole . USER , content = input_message ) ) self . _memory . put ( ChatMessage ( role = MessageRole . ASSISTANT , content = output_message ) )","title":"LangfuseChatEngine"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine.__init__","text":"Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Parameters: retriever ( BaseRetriever ) \u2013 Document retriever for RAG llm ( LLM ) \u2013 Language model for response generation memory ( BaseMemory ) \u2013 Memory buffer for chat history chainlit_tag_format ( str ) \u2013 Format for Chainlit message ID in Langfuse guardrails_engine ( BaseGuardrailsEngine ) \u2013 Guardrail engine for input/output validation context_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for context generation context_refine_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for refining context condense_prompt ( Optional [ Union [ str , PromptTemplate ]] , default: None ) \u2013 Prompt for condensing context system_prompt ( Optional [ str ] , default: None ) \u2013 System prompt for LLM input_guardrail_prompt_template \u2013 Prompt template for input validation output_guardrail_prompt_template \u2013 Prompt template for output validation skip_condense ( bool , default: False ) \u2013 Flag to skip context condensing node_postprocessors ( Optional [ List [ BaseNodePostprocessor ]] , default: None ) \u2013 List of postprocessors for node processing callback_manager ( Optional [ CallbackManager ] , default: None ) \u2013 Callback manager for tracing verbose ( bool , default: False ) \u2013 Flag for verbose output Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , retriever : BaseRetriever , llm : LLM , memory : BaseMemory , chainlit_tag_format : str , guardrails_engine : BaseGuardrailsEngine , context_prompt : Optional [ Union [ str , PromptTemplate ]] = None , context_refine_prompt : Optional [ Union [ str , PromptTemplate ]] = None , condense_prompt : Optional [ Union [ str , PromptTemplate ]] = None , system_prompt : Optional [ str ] = None , skip_condense : bool = False , node_postprocessors : Optional [ List [ BaseNodePostprocessor ]] = None , callback_manager : Optional [ CallbackManager ] = None , verbose : bool = False , ): \"\"\" Initialize LangfuseChatEngine with retriever, LLM, and optional parameters. Args: retriever: Document retriever for RAG llm: Language model for response generation memory: Memory buffer for chat history chainlit_tag_format: Format for Chainlit message ID in Langfuse guardrails_engine: Guardrail engine for input/output validation context_prompt: Prompt for context generation context_refine_prompt: Prompt for refining context condense_prompt: Prompt for condensing context system_prompt: System prompt for LLM input_guardrail_prompt_template: Prompt template for input validation output_guardrail_prompt_template: Prompt template for output validation skip_condense: Flag to skip context condensing node_postprocessors: List of postprocessors for node processing callback_manager: Callback manager for tracing verbose: Flag for verbose output \"\"\" super () . __init__ ( retriever = retriever , llm = llm , memory = memory , context_prompt = context_prompt , context_refine_prompt = context_refine_prompt , condense_prompt = condense_prompt , system_prompt = system_prompt , skip_condense = skip_condense , node_postprocessors = node_postprocessors , callback_manager = callback_manager , verbose = verbose , ) self . guardrails_engine = guardrails_engine self . chainlit_tag_format = chainlit_tag_format","title":"__init__"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine.get_current_langfuse_trace","text":"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient ( StatefulTraceClient ) \u2013 Active Langfuse trace or None if not found Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 195 196 197 198 199 200 201 202 203 204 205 206 207 def get_current_langfuse_trace ( self ) -> StatefulTraceClient : \"\"\"Retrieve current Langfuse trace from registered callback handler. Searches through callback handlers to find active LlamaIndexCallbackHandler and extract its associated Langfuse trace for monitoring or annotation. Returns: StatefulTraceClient: Active Langfuse trace or None if not found \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): return handler . trace return None","title":"get_current_langfuse_trace"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine.set_session_id","text":"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Parameters: session_id ( str ) \u2013 Unique identifier for current user session Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 209 210 211 212 213 214 215 216 217 218 219 220 def set_session_id ( self , session_id : str ) -> None : \"\"\"Set session ID for Langfuse tracing to group related queries. Updates the session identifier in all registered Langfuse callback handlers to enable session-level analytics and trace grouping. Args: session_id: Unique identifier for current user session \"\"\" for handler in self . callback_manager . handlers : if isinstance ( handler , LlamaIndexCallbackHandler ): handler . session_id = session_id","title":"set_session_id"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngine.stream_chat","text":"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Parameters: message ( str ) \u2013 Raw query string to process chat_history ( Optional [ List [ ChatMessage ]] , default: None ) \u2013 Optional chat history for context chainlit_message_id ( str , default: None ) \u2013 Optional ID for linking to Chainlit message in UI source_process ( SourceProcess , default: CHAT_COMPLETION ) \u2013 Context identifier indicating query's origin source Returns: AgentChatResponse ( AgentChatResponse ) \u2013 Generated response from RAG pipeline with dummy streaming Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def stream_chat ( self , message : str , chat_history : Optional [ List [ ChatMessage ]] = None , chainlit_message_id : str = None , source_process : SourceProcess = SourceProcess . CHAT_COMPLETION , ) -> AgentChatResponse : \"\"\"Process a query using RAG pipeline with Langfuse tracing. Assign current trace input for monitoring. Args: message: Raw query string to process chat_history: Optional chat history for context chainlit_message_id: Optional ID for linking to Chainlit message in UI source_process: Context identifier indicating query's origin source Returns: AgentChatResponse: Generated response from RAG pipeline with dummy streaming \"\"\" response = self . _stream_chat ( message = message , chat_history = chat_history , chainlit_message_id = chainlit_message_id , source_process = source_process , ) trace = self . get_current_langfuse_trace () trace . input = message return response","title":"stream_chat"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.LangfuseChatEngineFactory","text":"Bases: Factory Factory for creating configured LangfuseChatEngine instances. Constructs and connects components needed for the RAG pipeline. Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 class LangfuseChatEngineFactory ( Factory ): \"\"\"Factory for creating configured LangfuseChatEngine instances. Constructs and connects components needed for the RAG pipeline. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> LangfuseChatEngine : \"\"\"Create and configure a LangfuseChatEngine instance from configuration. Instantiates all RAG pipeline components based on configuration settings, connects them with a shared callback manager for tracing, and assembles them into a complete chat engine. Args: configuration: Complete augmentation configuration containing settings for all components Returns: LangfuseChatEngine: Fully configured RAG chat engine with tracing \"\"\" chat_engine_configuration = configuration . augmentation . chat_engine llm = LLMRegistry . get ( chat_engine_configuration . llm . provider ) . create ( chat_engine_configuration . llm ) retriever = RetrieverRegistry . get ( chat_engine_configuration . retriever . name ) . create ( configuration ) postprocessors = [ PostprocessorRegistry . get ( postprocessor_configuration . name ) . create ( postprocessor_configuration ) for postprocessor_configuration in chat_engine_configuration . postprocessors ] langfuse_callback_manager = LlamaIndexCallbackManagerFactory . create ( configuration . augmentation . langfuse ) memory = ChatMemoryBuffer ( chat_history = [], token_limit = llm . metadata . context_window - 256 ) ( condense_prompt_template , context_prompt_template , context_refine_prompt_template , system_prompt_template , ) = cls . _get_prompt_templates ( configuration = configuration . augmentation ) guardrails_configuration = ( configuration . augmentation . chat_engine . guardrails ) guardrails_engine = GuardrailsRegistry . get ( guardrails_configuration . name ) . create ( configuration = configuration . augmentation ) retriever . callback_manager = langfuse_callback_manager llm . callback_manager = langfuse_callback_manager for postprocessor in postprocessors : postprocessor . callback_manager = langfuse_callback_manager return LangfuseChatEngine ( retriever = retriever , llm = llm , node_postprocessors = postprocessors , callback_manager = langfuse_callback_manager , memory = memory , context_prompt = context_prompt_template , system_prompt = system_prompt_template , context_refine_prompt = context_refine_prompt_template , condense_prompt = condense_prompt_template , chainlit_tag_format = configuration . augmentation . langfuse . chainlit_tag_format , guardrails_engine = guardrails_engine , ) @staticmethod def _get_prompt_templates ( configuration : _AugmentationConfiguration , ) -> tuple : \"\"\"Retrieves the prompt template for the augmentation process. Args: configuration: Configuration object containing prompt templates settings. Returns: Tuple of prompt templates for condensing, context generation, context refinement, system prompts. \"\"\" langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . langfuse ) condense_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . condense_prompt_name ) context_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . context_prompt_name ) context_refine_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . context_refine_prompt_name ) system_prompt_template = langfuse_prompt_service . get_prompt_template ( prompt_name = configuration . chat_engine . prompt_templates . system_prompt_name ) return ( condense_prompt_template , context_prompt_template , context_refine_prompt_template , system_prompt_template , )","title":"LangfuseChatEngineFactory"},{"location":"src/augmentation/components/chat_engines/langfuse/chat_engine/#src.augmentation.components.chat_engines.langfuse.chat_engine.SourceProcess","text":"Bases: Enum Enumeration of possible chat processing sources. Attributes: CHAT_COMPLETION \u2013 Query from interactive chat completion interface DEPLOYMENT_EVALUATION \u2013 Query from automated deployment testing and evaluation Source code in src/augmentation/components/chat_engines/langfuse/chat_engine.py 37 38 39 40 41 42 43 44 45 46 class SourceProcess ( Enum ): \"\"\"Enumeration of possible chat processing sources. Attributes: CHAT_COMPLETION: Query from interactive chat completion interface DEPLOYMENT_EVALUATION: Query from automated deployment testing and evaluation \"\"\" CHAT_COMPLETION = 1 DEPLOYMENT_EVALUATION = 2","title":"SourceProcess"},{"location":"src/augmentation/components/chat_engines/langfuse/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.chat_engines.langfuse . Configuration LangfuseChatEngineConfiguration Bases: BaseChatEngineConfiguration Configuration class for the Langfuse chat engine. This class represents the configuration settings required for setting up and operating a chat engine with Langfuse integration. Langfuse provides observability and analytics capabilities for LLM applications. Source code in src/augmentation/components/chat_engines/langfuse/configuration.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class LangfuseChatEngineConfiguration ( BaseChatEngineConfiguration ): \"\"\" Configuration class for the Langfuse chat engine. This class represents the configuration settings required for setting up and operating a chat engine with Langfuse integration. Langfuse provides observability and analytics capabilities for LLM applications. \"\"\" name : ChatEngineName = Field ( ChatEngineName . LANGFUSE , description = \"The name of the chat engine configuration integrated with langfuse.\" , )","title":"Configuration"},{"location":"src/augmentation/components/chat_engines/langfuse/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.chat_engines.langfuse .","title":"Configuration"},{"location":"src/augmentation/components/chat_engines/langfuse/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/chat_engines/langfuse/configuration/#src.augmentation.components.chat_engines.langfuse.configuration.LangfuseChatEngineConfiguration","text":"Bases: BaseChatEngineConfiguration Configuration class for the Langfuse chat engine. This class represents the configuration settings required for setting up and operating a chat engine with Langfuse integration. Langfuse provides observability and analytics capabilities for LLM applications. Source code in src/augmentation/components/chat_engines/langfuse/configuration.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class LangfuseChatEngineConfiguration ( BaseChatEngineConfiguration ): \"\"\" Configuration class for the Langfuse chat engine. This class represents the configuration settings required for setting up and operating a chat engine with Langfuse integration. Langfuse provides observability and analytics capabilities for LLM applications. \"\"\" name : ChatEngineName = Field ( ChatEngineName . LANGFUSE , description = \"The name of the chat engine configuration integrated with langfuse.\" , )","title":"LangfuseChatEngineConfiguration"},{"location":"src/augmentation/components/guardrails/base_guardrails/","text":"Base_guardrails This module contains functionality related to the the base_guardrails module for augmentation.components.guardrails . Base_guardrails BaseGuardrailsEngine Bases: ABC Source code in src/augmentation/components/guardrails/base_guardrails.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class BaseGuardrailsEngine ( ABC ): @abstractmethod def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the input is allowed, otherwise an AgentChatResponse containing a message indicating the input is not allowed. \"\"\" pass @abstractmethod def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the output is allowed, otherwise an AgentChatResponse containing a message indicating the output is not allowed. \"\"\" pass input_guard ( message , is_stream ) abstractmethod Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Parameters: message ( str ) \u2013 Generated response message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: bool ( Optional [ AgentChatResponse ] ) \u2013 None if the input is allowed, otherwise an AgentChatResponse Optional [ AgentChatResponse ] \u2013 containing a message indicating the input is not allowed. Source code in src/augmentation/components/guardrails/base_guardrails.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @abstractmethod def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the input is allowed, otherwise an AgentChatResponse containing a message indicating the input is not allowed. \"\"\" pass output_guard ( message , is_stream ) abstractmethod Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Parameters: message ( str ) \u2013 User input message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: bool ( Optional [ AgentChatResponse ] ) \u2013 None if the output is allowed, otherwise an AgentChatResponse Optional [ AgentChatResponse ] \u2013 containing a message indicating the output is not allowed. Source code in src/augmentation/components/guardrails/base_guardrails.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @abstractmethod def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the output is allowed, otherwise an AgentChatResponse containing a message indicating the output is not allowed. \"\"\" pass","title":"Base_guardrails"},{"location":"src/augmentation/components/guardrails/base_guardrails/#base_guardrails","text":"This module contains functionality related to the the base_guardrails module for augmentation.components.guardrails .","title":"Base_guardrails"},{"location":"src/augmentation/components/guardrails/base_guardrails/#base_guardrails_1","text":"","title":"Base_guardrails"},{"location":"src/augmentation/components/guardrails/base_guardrails/#src.augmentation.components.guardrails.base_guardrails.BaseGuardrailsEngine","text":"Bases: ABC Source code in src/augmentation/components/guardrails/base_guardrails.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class BaseGuardrailsEngine ( ABC ): @abstractmethod def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the input is allowed, otherwise an AgentChatResponse containing a message indicating the input is not allowed. \"\"\" pass @abstractmethod def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the output is allowed, otherwise an AgentChatResponse containing a message indicating the output is not allowed. \"\"\" pass","title":"BaseGuardrailsEngine"},{"location":"src/augmentation/components/guardrails/base_guardrails/#src.augmentation.components.guardrails.base_guardrails.BaseGuardrailsEngine.input_guard","text":"Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Parameters: message ( str ) \u2013 Generated response message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: bool ( Optional [ AgentChatResponse ] ) \u2013 None if the input is allowed, otherwise an AgentChatResponse Optional [ AgentChatResponse ] \u2013 containing a message indicating the input is not allowed. Source code in src/augmentation/components/guardrails/base_guardrails.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @abstractmethod def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the input message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the input is not allowed. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the input is allowed, otherwise an AgentChatResponse containing a message indicating the input is not allowed. \"\"\" pass","title":"input_guard"},{"location":"src/augmentation/components/guardrails/base_guardrails/#src.augmentation.components.guardrails.base_guardrails.BaseGuardrailsEngine.output_guard","text":"Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Parameters: message ( str ) \u2013 User input message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: bool ( Optional [ AgentChatResponse ] ) \u2013 None if the output is allowed, otherwise an AgentChatResponse Optional [ AgentChatResponse ] \u2013 containing a message indicating the output is not allowed. Source code in src/augmentation/components/guardrails/base_guardrails.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @abstractmethod def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Check if the output message is allowed based on guardrail rules. If it does it returns None, otherwise it returns an AgentChatResponse containing a message indicating the output is not allowed. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: bool: None if the output is allowed, otherwise an AgentChatResponse containing a message indicating the output is not allowed. \"\"\" pass","title":"output_guard"},{"location":"src/augmentation/components/guardrails/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.guardrails . Registry","title":"Registry"},{"location":"src/augmentation/components/guardrails/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.guardrails .","title":"Registry"},{"location":"src/augmentation/components/guardrails/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/guardrails/basic/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.guardrails.basic . Configuration","title":"Configuration"},{"location":"src/augmentation/components/guardrails/basic/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.guardrails.basic .","title":"Configuration"},{"location":"src/augmentation/components/guardrails/basic/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/guardrails/basic/guardrails/","text":"Guardrails This module contains functionality related to the the guardrails module for augmentation.components.guardrails.basic . Guardrails BasicGuardrailsEngine Bases: BaseGuardrailsEngine Source code in src/augmentation/components/guardrails/basic/guardrails.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class BasicGuardrailsEngine ( BaseGuardrailsEngine ): def __init__ ( self , llm : LLM , input_prompt_template : PromptTemplate , output_prompt_template : PromptTemplate , ): \"\"\" Initialize GuardrailsEngine with LLM and prompt templates. Args: llm: Language model for response generation input_prompt_template: Prompt template for validating user input compliance output_prompt_template: Prompt template for validating response output compliance \"\"\" self . llm = llm self . input_prompt_template = input_prompt_template self . output_prompt_template = output_prompt_template def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate user input message against guardrail rules. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the input is allowed \"\"\" if not self . _is_input_allowed ( message ): return AgentChatResponse ( response = \"I'm unable to answer this question as it doesn't comply with our usage guidelines.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate generated response message against guardrail rules. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the output is allowed \"\"\" if not self . _is_output_allowed ( message ): return AgentChatResponse ( response = \"I apologize, but I'm unable to provide a response to this question.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None def _is_input_allowed ( self , message : str ) -> bool : \"\"\" Check if the input message is allowed based on guardrail rules. Args: message: User input message to validate Returns: bool: True if the input is allowed, False otherwise \"\"\" prompt = self . input_prompt_template . format ( message ) resp = self . llm . complete ( prompt ) text = resp . text . lower () return not ( \"yes\" in text or \"true\" in text ) def _is_output_allowed ( self , message : str ) -> bool : \"\"\" Check if the output message is allowed based on guardrail rules. Args: message: Generated response message to validate Returns: bool: True if the output is allowed, False otherwise \"\"\" prompt = self . output_prompt_template . format ( message ) response = self . llm . complete ( prompt ) text = response . text . lower () return not ( \"yes\" in text or \"true\" in text ) __init__ ( llm , input_prompt_template , output_prompt_template ) Initialize GuardrailsEngine with LLM and prompt templates. Parameters: llm ( LLM ) \u2013 Language model for response generation input_prompt_template ( PromptTemplate ) \u2013 Prompt template for validating user input compliance output_prompt_template ( PromptTemplate ) \u2013 Prompt template for validating response output compliance Source code in src/augmentation/components/guardrails/basic/guardrails.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , llm : LLM , input_prompt_template : PromptTemplate , output_prompt_template : PromptTemplate , ): \"\"\" Initialize GuardrailsEngine with LLM and prompt templates. Args: llm: Language model for response generation input_prompt_template: Prompt template for validating user input compliance output_prompt_template: Prompt template for validating response output compliance \"\"\" self . llm = llm self . input_prompt_template = input_prompt_template self . output_prompt_template = output_prompt_template input_guard ( message , is_stream ) Validate user input message against guardrail rules. Parameters: message ( str ) \u2013 User input message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: Optional [ AgentChatResponse ] \u2013 Optional[AgentChatResponse]: Response indicating if the input is allowed Source code in src/augmentation/components/guardrails/basic/guardrails.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate user input message against guardrail rules. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the input is allowed \"\"\" if not self . _is_input_allowed ( message ): return AgentChatResponse ( response = \"I'm unable to answer this question as it doesn't comply with our usage guidelines.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None output_guard ( message , is_stream ) Validate generated response message against guardrail rules. Parameters: message ( str ) \u2013 Generated response message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: Optional [ AgentChatResponse ] \u2013 Optional[AgentChatResponse]: Response indicating if the output is allowed Source code in src/augmentation/components/guardrails/basic/guardrails.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate generated response message against guardrail rules. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the output is allowed \"\"\" if not self . _is_output_allowed ( message ): return AgentChatResponse ( response = \"I apologize, but I'm unable to provide a response to this question.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None BasicGuardrailsEngineFactory Bases: Factory Factory for creating a GuardrailsEngine from AugmentationConfiguration. Source code in src/augmentation/components/guardrails/basic/guardrails.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class BasicGuardrailsEngineFactory ( Factory ): \"\"\"Factory for creating a GuardrailsEngine from AugmentationConfiguration.\"\"\" _configuration_class = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> BasicGuardrailsEngine : \"\"\" Create and configure a GuardrailsEngine instance from configuration. Instantiates the LLM and retrieves the prompt templates for input and output guardrails. Args: configuration: Configuration object containing settings for LLM and prompt templates Returns: GuardrailsEngine: Fully configured guardrail engine with LLM and prompt templates \"\"\" llm_configuration = configuration . chat_engine . guardrails . llm llm = LLMRegistry . get ( llm_configuration . provider ) . create ( llm_configuration ) input_guardrail_prompt_template , output_guardrail_prompt_template = ( cls . _get_prompt_templates ( configuration = configuration ) ) return BasicGuardrailsEngine ( llm = llm , input_prompt_template = input_guardrail_prompt_template , output_prompt_template = output_guardrail_prompt_template , ) @staticmethod def _get_prompt_templates ( configuration : _AugmentationConfiguration , ) -> tuple : \"\"\"Retrieves the prompt template for the guardrail process. Args: configuration: Configuration object containing langfuse and prompt templates settings. Returns: Tuple of prompt templates for condensing, context generation, context refinement, system prompts, and guardrail prompts. \"\"\" langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . langfuse ) guardrails_configuration = configuration . chat_engine . guardrails input_guardrail_prompt_template = ( langfuse_prompt_service . get_prompt_template ( prompt_name = guardrails_configuration . input_prompt_name ) ) output_guardrail_prompt_template = ( langfuse_prompt_service . get_prompt_template ( prompt_name = guardrails_configuration . output_prompt_name ) ) return ( input_guardrail_prompt_template , output_guardrail_prompt_template , )","title":"Guardrails"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#guardrails","text":"This module contains functionality related to the the guardrails module for augmentation.components.guardrails.basic .","title":"Guardrails"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#guardrails_1","text":"","title":"Guardrails"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngine","text":"Bases: BaseGuardrailsEngine Source code in src/augmentation/components/guardrails/basic/guardrails.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 class BasicGuardrailsEngine ( BaseGuardrailsEngine ): def __init__ ( self , llm : LLM , input_prompt_template : PromptTemplate , output_prompt_template : PromptTemplate , ): \"\"\" Initialize GuardrailsEngine with LLM and prompt templates. Args: llm: Language model for response generation input_prompt_template: Prompt template for validating user input compliance output_prompt_template: Prompt template for validating response output compliance \"\"\" self . llm = llm self . input_prompt_template = input_prompt_template self . output_prompt_template = output_prompt_template def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate user input message against guardrail rules. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the input is allowed \"\"\" if not self . _is_input_allowed ( message ): return AgentChatResponse ( response = \"I'm unable to answer this question as it doesn't comply with our usage guidelines.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate generated response message against guardrail rules. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the output is allowed \"\"\" if not self . _is_output_allowed ( message ): return AgentChatResponse ( response = \"I apologize, but I'm unable to provide a response to this question.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None def _is_input_allowed ( self , message : str ) -> bool : \"\"\" Check if the input message is allowed based on guardrail rules. Args: message: User input message to validate Returns: bool: True if the input is allowed, False otherwise \"\"\" prompt = self . input_prompt_template . format ( message ) resp = self . llm . complete ( prompt ) text = resp . text . lower () return not ( \"yes\" in text or \"true\" in text ) def _is_output_allowed ( self , message : str ) -> bool : \"\"\" Check if the output message is allowed based on guardrail rules. Args: message: Generated response message to validate Returns: bool: True if the output is allowed, False otherwise \"\"\" prompt = self . output_prompt_template . format ( message ) response = self . llm . complete ( prompt ) text = response . text . lower () return not ( \"yes\" in text or \"true\" in text )","title":"BasicGuardrailsEngine"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngine.__init__","text":"Initialize GuardrailsEngine with LLM and prompt templates. Parameters: llm ( LLM ) \u2013 Language model for response generation input_prompt_template ( PromptTemplate ) \u2013 Prompt template for validating user input compliance output_prompt_template ( PromptTemplate ) \u2013 Prompt template for validating response output compliance Source code in src/augmentation/components/guardrails/basic/guardrails.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , llm : LLM , input_prompt_template : PromptTemplate , output_prompt_template : PromptTemplate , ): \"\"\" Initialize GuardrailsEngine with LLM and prompt templates. Args: llm: Language model for response generation input_prompt_template: Prompt template for validating user input compliance output_prompt_template: Prompt template for validating response output compliance \"\"\" self . llm = llm self . input_prompt_template = input_prompt_template self . output_prompt_template = output_prompt_template","title":"__init__"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngine.input_guard","text":"Validate user input message against guardrail rules. Parameters: message ( str ) \u2013 User input message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: Optional [ AgentChatResponse ] \u2013 Optional[AgentChatResponse]: Response indicating if the input is allowed Source code in src/augmentation/components/guardrails/basic/guardrails.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def input_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate user input message against guardrail rules. Args: message: User input message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the input is allowed \"\"\" if not self . _is_input_allowed ( message ): return AgentChatResponse ( response = \"I'm unable to answer this question as it doesn't comply with our usage guidelines.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None","title":"input_guard"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngine.output_guard","text":"Validate generated response message against guardrail rules. Parameters: message ( str ) \u2013 Generated response message to validate is_stream ( bool ) \u2013 Flag indicating if the response is a stream Returns: Optional [ AgentChatResponse ] \u2013 Optional[AgentChatResponse]: Response indicating if the output is allowed Source code in src/augmentation/components/guardrails/basic/guardrails.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def output_guard ( self , message : str , is_stream : bool ) -> Optional [ AgentChatResponse ]: \"\"\" Validate generated response message against guardrail rules. Args: message: Generated response message to validate is_stream: Flag indicating if the response is a stream Returns: Optional[AgentChatResponse]: Response indicating if the output is allowed \"\"\" if not self . _is_output_allowed ( message ): return AgentChatResponse ( response = \"I apologize, but I'm unable to provide a response to this question.\" , sources = [], source_nodes = [], is_dummy_stream = is_stream , ) return None","title":"output_guard"},{"location":"src/augmentation/components/guardrails/basic/guardrails/#src.augmentation.components.guardrails.basic.guardrails.BasicGuardrailsEngineFactory","text":"Bases: Factory Factory for creating a GuardrailsEngine from AugmentationConfiguration. Source code in src/augmentation/components/guardrails/basic/guardrails.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class BasicGuardrailsEngineFactory ( Factory ): \"\"\"Factory for creating a GuardrailsEngine from AugmentationConfiguration.\"\"\" _configuration_class = _AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : _AugmentationConfiguration ) -> BasicGuardrailsEngine : \"\"\" Create and configure a GuardrailsEngine instance from configuration. Instantiates the LLM and retrieves the prompt templates for input and output guardrails. Args: configuration: Configuration object containing settings for LLM and prompt templates Returns: GuardrailsEngine: Fully configured guardrail engine with LLM and prompt templates \"\"\" llm_configuration = configuration . chat_engine . guardrails . llm llm = LLMRegistry . get ( llm_configuration . provider ) . create ( llm_configuration ) input_guardrail_prompt_template , output_guardrail_prompt_template = ( cls . _get_prompt_templates ( configuration = configuration ) ) return BasicGuardrailsEngine ( llm = llm , input_prompt_template = input_guardrail_prompt_template , output_prompt_template = output_guardrail_prompt_template , ) @staticmethod def _get_prompt_templates ( configuration : _AugmentationConfiguration , ) -> tuple : \"\"\"Retrieves the prompt template for the guardrail process. Args: configuration: Configuration object containing langfuse and prompt templates settings. Returns: Tuple of prompt templates for condensing, context generation, context refinement, system prompts, and guardrail prompts. \"\"\" langfuse_prompt_service = LangfusePromptServiceFactory . create ( configuration = configuration . langfuse ) guardrails_configuration = configuration . chat_engine . guardrails input_guardrail_prompt_template = ( langfuse_prompt_service . get_prompt_template ( prompt_name = guardrails_configuration . input_prompt_name ) ) output_guardrail_prompt_template = ( langfuse_prompt_service . get_prompt_template ( prompt_name = guardrails_configuration . output_prompt_name ) ) return ( input_guardrail_prompt_template , output_guardrail_prompt_template , )","title":"BasicGuardrailsEngineFactory"},{"location":"src/augmentation/components/llms/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.llms . Registry LLMRegistry Bases: Registry Registry for Large Language Model providers. This registry maps LLM provider names (defined in LLMProviderName enum) to their corresponding implementation classes. It allows for dynamic registration and retrieval of LLM provider implementations based on their enumerated types. Attributes: _key_class ( Type ) \u2013 The class used as the key for the registry. Source code in src/augmentation/components/llms/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class LLMRegistry ( Registry ): \"\"\" Registry for Large Language Model providers. This registry maps LLM provider names (defined in LLMProviderName enum) to their corresponding implementation classes. It allows for dynamic registration and retrieval of LLM provider implementations based on their enumerated types. Attributes: _key_class (Type): The class used as the key for the registry. \"\"\" _key_class : Type = LLMProviderName","title":"Registry"},{"location":"src/augmentation/components/llms/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.llms .","title":"Registry"},{"location":"src/augmentation/components/llms/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/llms/registry/#src.augmentation.components.llms.registry.LLMRegistry","text":"Bases: Registry Registry for Large Language Model providers. This registry maps LLM provider names (defined in LLMProviderName enum) to their corresponding implementation classes. It allows for dynamic registration and retrieval of LLM provider implementations based on their enumerated types. Attributes: _key_class ( Type ) \u2013 The class used as the key for the registry. Source code in src/augmentation/components/llms/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class LLMRegistry ( Registry ): \"\"\" Registry for Large Language Model providers. This registry maps LLM provider names (defined in LLMProviderName enum) to their corresponding implementation classes. It allows for dynamic registration and retrieval of LLM provider implementations based on their enumerated types. Attributes: _key_class (Type): The class used as the key for the registry. \"\"\" _key_class : Type = LLMProviderName","title":"LLMRegistry"},{"location":"src/augmentation/components/llms/lite_llm/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.llms.lite_llm . Configuration LiteLLMConfiguration Bases: LLMConfiguration Configuration for LiteLLM language models, which expose multiple providers. To check available providers visit https://docs.litellm.ai/docs/providers. This class extends the base LLMConfiguration to provide LiteLLM-specific configuration options and secrets management. Source code in src/augmentation/components/llms/lite_llm/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LiteLLMConfiguration ( LLMConfiguration ): \"\"\"Configuration for LiteLLM language models, which expose multiple providers. To check available providers visit https://docs.litellm.ai/docs/providers. This class extends the base LLMConfiguration to provide LiteLLM-specific configuration options and secrets management. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix `env_prefix` has to be provided by parent model, so it includes model name. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model provider.\" , ) provider : Literal [ LLMProviderName . LITE_LLM ] = Field ( ... , description = \"The name of the language model provider.\" ) api_base : Optional [ str ] = Field ( None , description = \"Base URL for the API endpoint of the language model provider. If not provided, api_base is determined by LiteLLM.\" , ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) def model_post_init ( self , context : Any ) -> None : \"\"\"Before loading the secrets, its model config's `env_prefix` has to be set to corresponding model. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets_class . model_config [ \"env_prefix\" ] = ( self . _get_secrets_env_prefix () ) super () . model_post_init ( context ) def _get_secrets_env_prefix ( self ) -> str : \"\"\"Returns the environment variable prefix for the secrets. It uses the model name to create a unique prefix for the secrets. All non-alphanumeric characters in model name are replaced with underscores. The prefix is used to load secrets from environment variables. Returns: str: The environment variable prefix for the secrets. \"\"\" model_name = self . name . upper () model_name = \"\" . join ( char if char . isalnum () else \"_\" for char in model_name ) return f \"RAG__LLMS__ { model_name } __\" Secrets Bases: BaseSecrets Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix env_prefix has to be provided by parent model, so it includes model name. Source code in src/augmentation/components/llms/lite_llm/configuration.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Secrets ( BaseSecrets ): \"\"\"Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix `env_prefix` has to be provided by parent model, so it includes model name. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model provider.\" , ) model_post_init ( context ) Before loading the secrets, its model config's env_prefix has to be set to corresponding model. Parameters: context ( Any ) \u2013 The context passed to the pydantic model, must contain 'secrets_file' key. Source code in src/augmentation/components/llms/lite_llm/configuration.py 50 51 52 53 54 55 56 57 58 59 60 def model_post_init ( self , context : Any ) -> None : \"\"\"Before loading the secrets, its model config's `env_prefix` has to be set to corresponding model. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets_class . model_config [ \"env_prefix\" ] = ( self . _get_secrets_env_prefix () ) super () . model_post_init ( context )","title":"Configuration"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.llms.lite_llm .","title":"Configuration"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#src.augmentation.components.llms.lite_llm.configuration.LiteLLMConfiguration","text":"Bases: LLMConfiguration Configuration for LiteLLM language models, which expose multiple providers. To check available providers visit https://docs.litellm.ai/docs/providers. This class extends the base LLMConfiguration to provide LiteLLM-specific configuration options and secrets management. Source code in src/augmentation/components/llms/lite_llm/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LiteLLMConfiguration ( LLMConfiguration ): \"\"\"Configuration for LiteLLM language models, which expose multiple providers. To check available providers visit https://docs.litellm.ai/docs/providers. This class extends the base LLMConfiguration to provide LiteLLM-specific configuration options and secrets management. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix `env_prefix` has to be provided by parent model, so it includes model name. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model provider.\" , ) provider : Literal [ LLMProviderName . LITE_LLM ] = Field ( ... , description = \"The name of the language model provider.\" ) api_base : Optional [ str ] = Field ( None , description = \"Base URL for the API endpoint of the language model provider. If not provided, api_base is determined by LiteLLM.\" , ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) def model_post_init ( self , context : Any ) -> None : \"\"\"Before loading the secrets, its model config's `env_prefix` has to be set to corresponding model. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets_class . model_config [ \"env_prefix\" ] = ( self . _get_secrets_env_prefix () ) super () . model_post_init ( context ) def _get_secrets_env_prefix ( self ) -> str : \"\"\"Returns the environment variable prefix for the secrets. It uses the model name to create a unique prefix for the secrets. All non-alphanumeric characters in model name are replaced with underscores. The prefix is used to load secrets from environment variables. Returns: str: The environment variable prefix for the secrets. \"\"\" model_name = self . name . upper () model_name = \"\" . join ( char if char . isalnum () else \"_\" for char in model_name ) return f \"RAG__LLMS__ { model_name } __\"","title":"LiteLLMConfiguration"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#src.augmentation.components.llms.lite_llm.configuration.LiteLLMConfiguration.Secrets","text":"Bases: BaseSecrets Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix env_prefix has to be provided by parent model, so it includes model name. Source code in src/augmentation/components/llms/lite_llm/configuration.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Secrets ( BaseSecrets ): \"\"\"Secrets configuration for LiteLLM API authentication. Contains sensitive information required to authenticate with the LiteLLM API. Environment prefix `env_prefix` has to be provided by parent model, so it includes model name. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model provider.\" , )","title":"Secrets"},{"location":"src/augmentation/components/llms/lite_llm/configuration/#src.augmentation.components.llms.lite_llm.configuration.LiteLLMConfiguration.model_post_init","text":"Before loading the secrets, its model config's env_prefix has to be set to corresponding model. Parameters: context ( Any ) \u2013 The context passed to the pydantic model, must contain 'secrets_file' key. Source code in src/augmentation/components/llms/lite_llm/configuration.py 50 51 52 53 54 55 56 57 58 59 60 def model_post_init ( self , context : Any ) -> None : \"\"\"Before loading the secrets, its model config's `env_prefix` has to be set to corresponding model. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets_class . model_config [ \"env_prefix\" ] = ( self . _get_secrets_env_prefix () ) super () . model_post_init ( context )","title":"model_post_init"},{"location":"src/augmentation/components/llms/lite_llm/llm/","text":"Llm This module contains functionality related to the the llm module for augmentation.components.llms.lite_llm . Llm LiteLLMFactory Bases: SingletonFactory Factory class for creating LiteLLM language model instances. This class implements the Singleton pattern to ensure only one instance of an LiteLLM model with a specific configuration exists in the application. It uses LiteLLMConfiguration to configure the model parameters. Attributes: _configuration_class ( Type ) \u2013 Type of the configuration class used for creating LiteLLM model instances. Source code in src/augmentation/components/llms/lite_llm/llm.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class LiteLLMFactory ( SingletonFactory ): \"\"\" Factory class for creating LiteLLM language model instances. This class implements the Singleton pattern to ensure only one instance of an LiteLLM model with a specific configuration exists in the application. It uses LiteLLMConfiguration to configure the model parameters. Attributes: _configuration_class: Type of the configuration class used for creating LiteLLM model instances. \"\"\" _configuration_class : Type = LiteLLMConfiguration @classmethod def _create_instance ( cls , configuration : LiteLLMConfiguration ) -> LiteLLM : \"\"\" Creates a new instance of the LiteLLM language model. Args: configuration (LiteLLMConfiguration): Configuration object containing settings for the LiteLLM model, including API key, model name, maximum tokens, and retry settings. Returns: LiteLLM: An instance of the LiteLLM language model configured with the provided settings. \"\"\" return LiteLLM ( api_key = configuration . secrets . api_key . get_secret_value (), api_base = configuration . api_base , model = configuration . name , max_tokens = configuration . max_tokens , max_retries = configuration . max_retries , )","title":"Llm"},{"location":"src/augmentation/components/llms/lite_llm/llm/#llm","text":"This module contains functionality related to the the llm module for augmentation.components.llms.lite_llm .","title":"Llm"},{"location":"src/augmentation/components/llms/lite_llm/llm/#llm_1","text":"","title":"Llm"},{"location":"src/augmentation/components/llms/lite_llm/llm/#src.augmentation.components.llms.lite_llm.llm.LiteLLMFactory","text":"Bases: SingletonFactory Factory class for creating LiteLLM language model instances. This class implements the Singleton pattern to ensure only one instance of an LiteLLM model with a specific configuration exists in the application. It uses LiteLLMConfiguration to configure the model parameters. Attributes: _configuration_class ( Type ) \u2013 Type of the configuration class used for creating LiteLLM model instances. Source code in src/augmentation/components/llms/lite_llm/llm.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class LiteLLMFactory ( SingletonFactory ): \"\"\" Factory class for creating LiteLLM language model instances. This class implements the Singleton pattern to ensure only one instance of an LiteLLM model with a specific configuration exists in the application. It uses LiteLLMConfiguration to configure the model parameters. Attributes: _configuration_class: Type of the configuration class used for creating LiteLLM model instances. \"\"\" _configuration_class : Type = LiteLLMConfiguration @classmethod def _create_instance ( cls , configuration : LiteLLMConfiguration ) -> LiteLLM : \"\"\" Creates a new instance of the LiteLLM language model. Args: configuration (LiteLLMConfiguration): Configuration object containing settings for the LiteLLM model, including API key, model name, maximum tokens, and retry settings. Returns: LiteLLM: An instance of the LiteLLM language model configured with the provided settings. \"\"\" return LiteLLM ( api_key = configuration . secrets . api_key . get_secret_value (), api_base = configuration . api_base , model = configuration . name , max_tokens = configuration . max_tokens , max_retries = configuration . max_retries , )","title":"LiteLLMFactory"},{"location":"src/augmentation/components/postprocessors/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.postprocessors . Registry PostprocessorRegistry Bases: Registry Registry for managing postprocessors in the RAG pipeline. This registry uses PostProcessorName as the key class to identify and retrieve different postprocessor implementations. Postprocessors are components that perform final transformations on data after the main processing is complete. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to PostProcessorName. Source code in src/augmentation/components/postprocessors/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class PostprocessorRegistry ( Registry ): \"\"\" Registry for managing postprocessors in the RAG pipeline. This registry uses PostProcessorName as the key class to identify and retrieve different postprocessor implementations. Postprocessors are components that perform final transformations on data after the main processing is complete. Attributes: _key_class (Type): The class type used for registry keys, set to PostProcessorName. \"\"\" _key_class : Type = PostProcessorName","title":"Registry"},{"location":"src/augmentation/components/postprocessors/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.postprocessors .","title":"Registry"},{"location":"src/augmentation/components/postprocessors/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/postprocessors/registry/#src.augmentation.components.postprocessors.registry.PostprocessorRegistry","text":"Bases: Registry Registry for managing postprocessors in the RAG pipeline. This registry uses PostProcessorName as the key class to identify and retrieve different postprocessor implementations. Postprocessors are components that perform final transformations on data after the main processing is complete. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to PostProcessorName. Source code in src/augmentation/components/postprocessors/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class PostprocessorRegistry ( Registry ): \"\"\" Registry for managing postprocessors in the RAG pipeline. This registry uses PostProcessorName as the key class to identify and retrieve different postprocessor implementations. Postprocessors are components that perform final transformations on data after the main processing is complete. Attributes: _key_class (Type): The class type used for registry keys, set to PostProcessorName. \"\"\" _key_class : Type = PostProcessorName","title":"PostprocessorRegistry"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/","text":"Configuraiton This module contains functionality related to the the configuraiton module for augmentation.components.postprocessors.colbert_rerank . Configuraiton ColbertRerankConfiguration Bases: PostProcessorConfiguration Configuration for the ColBERT reranking postprocessor. This class defines the settings needed to perform document reranking using the ColBERT retrieval model, which improves search quality through late interaction between queries and documents. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class ColbertRerankConfiguration ( PostProcessorConfiguration ): \"\"\" Configuration for the ColBERT reranking postprocessor. This class defines the settings needed to perform document reranking using the ColBERT retrieval model, which improves search quality through late interaction between queries and documents. \"\"\" class Models ( str , Enum ): \"\"\"Supported ColBERT models for reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" class Tokenizers ( str , Enum ): \"\"\"Supported tokenizers for ColBERT reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" name : Literal [ PostProcessorName . COLBERT_RERANK ] = Field ( ... , description = \"The name of the postprocessor.\" ) model : Models = Field ( Models . COLBERTV2 , description = \"Model used for reranking\" ) tokenizer : Tokenizers = Field ( Tokenizers . COLBERTV2 , description = \"Tokenizer used for reranking\" ) top_n : int = Field ( 5 , description = \"Number of documents to be reranked\" ) keep_retrieval_score : bool = Field ( True , description = \"Toggle to keep the retrieval score after the reranking\" , ) Models Bases: str , Enum Supported ColBERT models for reranking. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 21 22 23 24 class Models ( str , Enum ): \"\"\"Supported ColBERT models for reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" Tokenizers Bases: str , Enum Supported tokenizers for ColBERT reranking. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 26 27 28 29 class Tokenizers ( str , Enum ): \"\"\"Supported tokenizers for ColBERT reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\"","title":"Configuraiton"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#configuraiton","text":"This module contains functionality related to the the configuraiton module for augmentation.components.postprocessors.colbert_rerank .","title":"Configuraiton"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#configuraiton_1","text":"","title":"Configuraiton"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#src.augmentation.components.postprocessors.colbert_rerank.configuraiton.ColbertRerankConfiguration","text":"Bases: PostProcessorConfiguration Configuration for the ColBERT reranking postprocessor. This class defines the settings needed to perform document reranking using the ColBERT retrieval model, which improves search quality through late interaction between queries and documents. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class ColbertRerankConfiguration ( PostProcessorConfiguration ): \"\"\" Configuration for the ColBERT reranking postprocessor. This class defines the settings needed to perform document reranking using the ColBERT retrieval model, which improves search quality through late interaction between queries and documents. \"\"\" class Models ( str , Enum ): \"\"\"Supported ColBERT models for reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" class Tokenizers ( str , Enum ): \"\"\"Supported tokenizers for ColBERT reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\" name : Literal [ PostProcessorName . COLBERT_RERANK ] = Field ( ... , description = \"The name of the postprocessor.\" ) model : Models = Field ( Models . COLBERTV2 , description = \"Model used for reranking\" ) tokenizer : Tokenizers = Field ( Tokenizers . COLBERTV2 , description = \"Tokenizer used for reranking\" ) top_n : int = Field ( 5 , description = \"Number of documents to be reranked\" ) keep_retrieval_score : bool = Field ( True , description = \"Toggle to keep the retrieval score after the reranking\" , )","title":"ColbertRerankConfiguration"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#src.augmentation.components.postprocessors.colbert_rerank.configuraiton.ColbertRerankConfiguration.Models","text":"Bases: str , Enum Supported ColBERT models for reranking. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 21 22 23 24 class Models ( str , Enum ): \"\"\"Supported ColBERT models for reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\"","title":"Models"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/configuraiton/#src.augmentation.components.postprocessors.colbert_rerank.configuraiton.ColbertRerankConfiguration.Tokenizers","text":"Bases: str , Enum Supported tokenizers for ColBERT reranking. Source code in src/augmentation/components/postprocessors/colbert_rerank/configuraiton.py 26 27 28 29 class Tokenizers ( str , Enum ): \"\"\"Supported tokenizers for ColBERT reranking.\"\"\" COLBERTV2 = \"colbert-ir/colbertv2.0\"","title":"Tokenizers"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/postprocessor/","text":"Postprocessor This module contains functionality related to the the postprocessor module for augmentation.components.postprocessors.colbert_rerank . Postprocessor ColbertRerankFactory Bases: Factory Factory for creating ColbertRerank instances. ColbertRerank is a postprocessor that reranks retrieved nodes using ColBERT, a neural information retrieval model that uses contextualized late interaction. This factory handles the creation of ColbertRerank objects based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the ColbertRerank. Source code in src/augmentation/components/postprocessors/colbert_rerank/postprocessor.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class ColbertRerankFactory ( Factory ): \"\"\" Factory for creating ColbertRerank instances. ColbertRerank is a postprocessor that reranks retrieved nodes using ColBERT, a neural information retrieval model that uses contextualized late interaction. This factory handles the creation of ColbertRerank objects based on the provided configuration. Attributes: _configuration_class (Type): The configuration class for the ColbertRerank. \"\"\" _configuration_class : Type = ColbertRerankConfiguration @classmethod def _create_instance ( cls , configuration : ColbertRerankConfiguration ) -> ColbertRerank : \"\"\" Creates a ColbertRerank instance based on the provided configuration. Args: configuration (ColbertRerankConfiguration): Configuration object containing parameters for the ColbertRerank instance. Returns: ColbertRerank: An initialized ColbertRerank postprocessor that can be used to rerank retrieved documents based on their relevance to the query. \"\"\" return ColbertRerank ( top_n = configuration . top_n , model = configuration . model . value , tokenizer = configuration . tokenizer . value , keep_retrieval_score = configuration . keep_retrieval_score , )","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/postprocessor/#postprocessor","text":"This module contains functionality related to the the postprocessor module for augmentation.components.postprocessors.colbert_rerank .","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/postprocessor/#postprocessor_1","text":"","title":"Postprocessor"},{"location":"src/augmentation/components/postprocessors/colbert_rerank/postprocessor/#src.augmentation.components.postprocessors.colbert_rerank.postprocessor.ColbertRerankFactory","text":"Bases: Factory Factory for creating ColbertRerank instances. ColbertRerank is a postprocessor that reranks retrieved nodes using ColBERT, a neural information retrieval model that uses contextualized late interaction. This factory handles the creation of ColbertRerank objects based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the ColbertRerank. Source code in src/augmentation/components/postprocessors/colbert_rerank/postprocessor.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class ColbertRerankFactory ( Factory ): \"\"\" Factory for creating ColbertRerank instances. ColbertRerank is a postprocessor that reranks retrieved nodes using ColBERT, a neural information retrieval model that uses contextualized late interaction. This factory handles the creation of ColbertRerank objects based on the provided configuration. Attributes: _configuration_class (Type): The configuration class for the ColbertRerank. \"\"\" _configuration_class : Type = ColbertRerankConfiguration @classmethod def _create_instance ( cls , configuration : ColbertRerankConfiguration ) -> ColbertRerank : \"\"\" Creates a ColbertRerank instance based on the provided configuration. Args: configuration (ColbertRerankConfiguration): Configuration object containing parameters for the ColbertRerank instance. Returns: ColbertRerank: An initialized ColbertRerank postprocessor that can be used to rerank retrieved documents based on their relevance to the query. \"\"\" return ColbertRerank ( top_n = configuration . top_n , model = configuration . model . value , tokenizer = configuration . tokenizer . value , keep_retrieval_score = configuration . keep_retrieval_score , )","title":"ColbertRerankFactory"},{"location":"src/augmentation/components/retrievers/registry/","text":"Registry This module contains functionality related to the the registry module for augmentation.components.retrievers . Registry RetrieverRegistry Bases: Registry Registry for managing retriever components in the RAG system. This registry maps RetrieverName enum values to their corresponding retriever implementations, facilitating the creation and management of retriever instances based on configuration. It inherits from the base Registry class and specifies RetrieverName as the key type for registration and lookup operations. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to RetrieverName enum. Source code in src/augmentation/components/retrievers/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class RetrieverRegistry ( Registry ): \"\"\"Registry for managing retriever components in the RAG system. This registry maps RetrieverName enum values to their corresponding retriever implementations, facilitating the creation and management of retriever instances based on configuration. It inherits from the base Registry class and specifies RetrieverName as the key type for registration and lookup operations. Attributes: _key_class (Type): The class type used for registry keys, set to RetrieverName enum. \"\"\" _key_class : Type = RetrieverName","title":"Registry"},{"location":"src/augmentation/components/retrievers/registry/#registry","text":"This module contains functionality related to the the registry module for augmentation.components.retrievers .","title":"Registry"},{"location":"src/augmentation/components/retrievers/registry/#registry_1","text":"","title":"Registry"},{"location":"src/augmentation/components/retrievers/registry/#src.augmentation.components.retrievers.registry.RetrieverRegistry","text":"Bases: Registry Registry for managing retriever components in the RAG system. This registry maps RetrieverName enum values to their corresponding retriever implementations, facilitating the creation and management of retriever instances based on configuration. It inherits from the base Registry class and specifies RetrieverName as the key type for registration and lookup operations. Attributes: _key_class ( Type ) \u2013 The class type used for registry keys, set to RetrieverName enum. Source code in src/augmentation/components/retrievers/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 class RetrieverRegistry ( Registry ): \"\"\"Registry for managing retriever components in the RAG system. This registry maps RetrieverName enum values to their corresponding retriever implementations, facilitating the creation and management of retriever instances based on configuration. It inherits from the base Registry class and specifies RetrieverName as the key type for registration and lookup operations. Attributes: _key_class (Type): The class type used for registry keys, set to RetrieverName enum. \"\"\" _key_class : Type = RetrieverName","title":"RetrieverRegistry"},{"location":"src/augmentation/components/retrievers/auto/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.retrievers.auto . Configuration AutoRetrieverConfiguration Bases: RetrieverConfiguration Configuration for the Auto Retriever component. The Auto Retriever automatically determines the most appropriate retrieval strategy based on the query content, using an LLM to analyze and extract relevant metadata from the query. Source code in src/augmentation/components/retrievers/auto/configuration.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class AutoRetrieverConfiguration ( RetrieverConfiguration ): \"\"\" Configuration for the Auto Retriever component. The Auto Retriever automatically determines the most appropriate retrieval strategy based on the query content, using an LLM to analyze and extract relevant metadata from the query. \"\"\" name : Literal [ RetrieverName . AUTO ] = Field ( ... , description = \"The name of the retriever.\" ) llm : Any = Field ( ... , description = \"The LLM configuration used to extract metadata from the query.\" , ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\" Validates the LLM configuration using the LLMConfigurationRegistry. This validator ensures that the provided LLM configuration is valid according to the registered LLM configuration classes. Args: value: The LLM configuration value to validate. info: ValidationInfo object containing context about the validation. Returns: The validated LLM configuration object. \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , )","title":"Configuration"},{"location":"src/augmentation/components/retrievers/auto/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.retrievers.auto .","title":"Configuration"},{"location":"src/augmentation/components/retrievers/auto/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/retrievers/auto/configuration/#src.augmentation.components.retrievers.auto.configuration.AutoRetrieverConfiguration","text":"Bases: RetrieverConfiguration Configuration for the Auto Retriever component. The Auto Retriever automatically determines the most appropriate retrieval strategy based on the query content, using an LLM to analyze and extract relevant metadata from the query. Source code in src/augmentation/components/retrievers/auto/configuration.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class AutoRetrieverConfiguration ( RetrieverConfiguration ): \"\"\" Configuration for the Auto Retriever component. The Auto Retriever automatically determines the most appropriate retrieval strategy based on the query content, using an LLM to analyze and extract relevant metadata from the query. \"\"\" name : Literal [ RetrieverName . AUTO ] = Field ( ... , description = \"The name of the retriever.\" ) llm : Any = Field ( ... , description = \"The LLM configuration used to extract metadata from the query.\" , ) @field_validator ( \"llm\" ) @classmethod def _validate_llm ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\" Validates the LLM configuration using the LLMConfigurationRegistry. This validator ensures that the provided LLM configuration is valid according to the registered LLM configuration classes. Args: value: The LLM configuration value to validate. info: ValidationInfo object containing context about the validation. Returns: The validated LLM configuration object. \"\"\" return super () . _validate ( value , info = info , registry = LLMConfigurationRegistry , )","title":"AutoRetrieverConfiguration"},{"location":"src/augmentation/components/retrievers/auto/retriever/","text":"Retriever This module contains functionality related to the the retriever module for augmentation.components.retrievers.auto . Retriever AutoRetrieverFactory Bases: Factory Factory class for creating VectorIndexAutoRetriever instances. This factory builds auto-retriever components that utilize LLMs to dynamically construct queries for vector store retrieval based on user inputs. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the auto retriever. Source code in src/augmentation/components/retrievers/auto/retriever.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class AutoRetrieverFactory ( Factory ): \"\"\" Factory class for creating VectorIndexAutoRetriever instances. This factory builds auto-retriever components that utilize LLMs to dynamically construct queries for vector store retrieval based on user inputs. Attributes: _configuration_class: The configuration class for the auto retriever. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> VectorIndexAutoRetriever : \"\"\" Creates a VectorIndexAutoRetriever instance based on the provided configuration. This method: 1. Sets up the vector store using the configuration 2. Initializes the embedding model 3. Creates a VectorStoreIndex from the vector store and embedding model 4. Configures the LLM for the retriever 5. Returns a fully configured VectorIndexAutoRetriever Args: configuration: AugmentationConfiguration object containing all necessary settings for creating the retriever component Returns: VectorIndexAutoRetriever: A configured auto-retriever for dynamic query processing \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) llm = LLMRegistry . get ( retriever_configuration . llm . provider ) . create ( retriever_configuration . llm ) return VectorIndexAutoRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , llm = llm , vector_store_info = VectorStoreInfo ( content_info = \"Knowledge base of FELD M company used for retrieval process in RAG system.\" , metadata_info = [ MetadataInfo ( name = \"creation_date\" , type = \"date\" , description = ( \"Date of creation of the chunk's document\" ), ), MetadataInfo ( name = \"last_update_date\" , type = \"date\" , description = ( \"Date of the last update of the chunk's document.\" ), ), ], ), )","title":"Retriever"},{"location":"src/augmentation/components/retrievers/auto/retriever/#retriever","text":"This module contains functionality related to the the retriever module for augmentation.components.retrievers.auto .","title":"Retriever"},{"location":"src/augmentation/components/retrievers/auto/retriever/#retriever_1","text":"","title":"Retriever"},{"location":"src/augmentation/components/retrievers/auto/retriever/#src.augmentation.components.retrievers.auto.retriever.AutoRetrieverFactory","text":"Bases: Factory Factory class for creating VectorIndexAutoRetriever instances. This factory builds auto-retriever components that utilize LLMs to dynamically construct queries for vector store retrieval based on user inputs. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the auto retriever. Source code in src/augmentation/components/retrievers/auto/retriever.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class AutoRetrieverFactory ( Factory ): \"\"\" Factory class for creating VectorIndexAutoRetriever instances. This factory builds auto-retriever components that utilize LLMs to dynamically construct queries for vector store retrieval based on user inputs. Attributes: _configuration_class: The configuration class for the auto retriever. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> VectorIndexAutoRetriever : \"\"\" Creates a VectorIndexAutoRetriever instance based on the provided configuration. This method: 1. Sets up the vector store using the configuration 2. Initializes the embedding model 3. Creates a VectorStoreIndex from the vector store and embedding model 4. Configures the LLM for the retriever 5. Returns a fully configured VectorIndexAutoRetriever Args: configuration: AugmentationConfiguration object containing all necessary settings for creating the retriever component Returns: VectorIndexAutoRetriever: A configured auto-retriever for dynamic query processing \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) llm = LLMRegistry . get ( retriever_configuration . llm . provider ) . create ( retriever_configuration . llm ) return VectorIndexAutoRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , llm = llm , vector_store_info = VectorStoreInfo ( content_info = \"Knowledge base of FELD M company used for retrieval process in RAG system.\" , metadata_info = [ MetadataInfo ( name = \"creation_date\" , type = \"date\" , description = ( \"Date of creation of the chunk's document\" ), ), MetadataInfo ( name = \"last_update_date\" , type = \"date\" , description = ( \"Date of the last update of the chunk's document.\" ), ), ], ), )","title":"AutoRetrieverFactory"},{"location":"src/augmentation/components/retrievers/basic/configuration/","text":"Configuration This module contains functionality related to the the configuration module for augmentation.components.retrievers.basic . Configuration BasicRetrieverConfiguration Bases: RetrieverConfiguration Configuration for the Basic Retriever component. This class defines the configuration parameters needed for initializing and operating the basic retriever, extending the base RetrieverConfiguration. Source code in src/augmentation/components/retrievers/basic/configuration.py 11 12 13 14 15 16 17 18 19 20 21 class BasicRetrieverConfiguration ( RetrieverConfiguration ): \"\"\" Configuration for the Basic Retriever component. This class defines the configuration parameters needed for initializing and operating the basic retriever, extending the base RetrieverConfiguration. \"\"\" name : Literal [ RetrieverName . BASIC ] = Field ( ... , description = \"The name of the retriever.\" )","title":"Configuration"},{"location":"src/augmentation/components/retrievers/basic/configuration/#configuration","text":"This module contains functionality related to the the configuration module for augmentation.components.retrievers.basic .","title":"Configuration"},{"location":"src/augmentation/components/retrievers/basic/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/augmentation/components/retrievers/basic/configuration/#src.augmentation.components.retrievers.basic.configuration.BasicRetrieverConfiguration","text":"Bases: RetrieverConfiguration Configuration for the Basic Retriever component. This class defines the configuration parameters needed for initializing and operating the basic retriever, extending the base RetrieverConfiguration. Source code in src/augmentation/components/retrievers/basic/configuration.py 11 12 13 14 15 16 17 18 19 20 21 class BasicRetrieverConfiguration ( RetrieverConfiguration ): \"\"\" Configuration for the Basic Retriever component. This class defines the configuration parameters needed for initializing and operating the basic retriever, extending the base RetrieverConfiguration. \"\"\" name : Literal [ RetrieverName . BASIC ] = Field ( ... , description = \"The name of the retriever.\" )","title":"BasicRetrieverConfiguration"},{"location":"src/augmentation/components/retrievers/basic/retriever/","text":"Retriever This module contains functionality related to the the retriever module for augmentation.components.retrievers.basic . Retriever BasicRetrieverFactory Bases: Factory Factory class for creating VectorIndexRetriever instances. This factory implements the Factory design pattern to create a basic retriever component that uses vector similarity search to retrieve relevant context from a vector store. Source code in src/augmentation/components/retrievers/basic/retriever.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class BasicRetrieverFactory ( Factory ): \"\"\" Factory class for creating VectorIndexRetriever instances. This factory implements the Factory design pattern to create a basic retriever component that uses vector similarity search to retrieve relevant context from a vector store. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> VectorIndexRetriever : \"\"\" Creates a VectorIndexRetriever instance based on the provided configuration. This method: 1. Initializes the vector store from configuration 2. Creates the embedding model 3. Sets up the vector store index 4. Configures and returns the retriever with specified parameters Args: configuration: An AugmentationConfiguration object containing settings for the vector store, embedding model, and retriever parameters. Returns: VectorIndexRetriever: Configured retriever instance ready for similarity searches. \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) return VectorIndexRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , )","title":"Retriever"},{"location":"src/augmentation/components/retrievers/basic/retriever/#retriever","text":"This module contains functionality related to the the retriever module for augmentation.components.retrievers.basic .","title":"Retriever"},{"location":"src/augmentation/components/retrievers/basic/retriever/#retriever_1","text":"","title":"Retriever"},{"location":"src/augmentation/components/retrievers/basic/retriever/#src.augmentation.components.retrievers.basic.retriever.BasicRetrieverFactory","text":"Bases: Factory Factory class for creating VectorIndexRetriever instances. This factory implements the Factory design pattern to create a basic retriever component that uses vector similarity search to retrieve relevant context from a vector store. Source code in src/augmentation/components/retrievers/basic/retriever.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class BasicRetrieverFactory ( Factory ): \"\"\" Factory class for creating VectorIndexRetriever instances. This factory implements the Factory design pattern to create a basic retriever component that uses vector similarity search to retrieve relevant context from a vector store. \"\"\" _configuration_class : Type = AugmentationConfiguration @classmethod def _create_instance ( cls , configuration : AugmentationConfiguration ) -> VectorIndexRetriever : \"\"\" Creates a VectorIndexRetriever instance based on the provided configuration. This method: 1. Initializes the vector store from configuration 2. Creates the embedding model 3. Sets up the vector store index 4. Configures and returns the retriever with specified parameters Args: configuration: An AugmentationConfiguration object containing settings for the vector store, embedding model, and retriever parameters. Returns: VectorIndexRetriever: Configured retriever instance ready for similarity searches. \"\"\" vector_store_configuration = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_configuration . name ) . create ( vector_store_configuration ) embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) index = VectorStoreIndex . from_vector_store ( vector_store = vector_store , embed_model = embedding_model , ) retriever_configuration = ( configuration . augmentation . chat_engine . retriever ) return VectorIndexRetriever ( index = index , similarity_top_k = retriever_configuration . similarity_top_k , )","title":"BasicRetrieverFactory"},{"location":"src/augmentation/langfuse/client/","text":"Client This module contains functionality related to the the client module for augmentation.langfuse . Client LangfuseClientFactory Bases: SingletonFactory Factory class for creating and managing Langfuse client instances. This class implements the Singleton pattern through inheriting from SingletonFactory, ensuring only one Langfuse client instance exists throughout the application. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Langfuse client instances. In this case, it is LangfuseConfiguration. Source code in src/augmentation/langfuse/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class LangfuseClientFactory ( SingletonFactory ): \"\"\" Factory class for creating and managing Langfuse client instances. This class implements the Singleton pattern through inheriting from SingletonFactory, ensuring only one Langfuse client instance exists throughout the application. Attributes: _configuration_class (Type): The configuration class used for creating Langfuse client instances. In this case, it is LangfuseConfiguration. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> Langfuse : \"\"\" Creates a new Langfuse client instance. Args: configuration (LangfuseConfiguration): Configuration object containing Langfuse API credentials and URL settings. Returns: Langfuse: A configured Langfuse client instance ready for use with the provided credentials and host. \"\"\" return Langfuse ( secret_key = configuration . secrets . secret_key . get_secret_value (), public_key = configuration . secrets . public_key . get_secret_value (), host = configuration . url , )","title":"Client"},{"location":"src/augmentation/langfuse/client/#client","text":"This module contains functionality related to the the client module for augmentation.langfuse .","title":"Client"},{"location":"src/augmentation/langfuse/client/#client_1","text":"","title":"Client"},{"location":"src/augmentation/langfuse/client/#src.augmentation.langfuse.client.LangfuseClientFactory","text":"Bases: SingletonFactory Factory class for creating and managing Langfuse client instances. This class implements the Singleton pattern through inheriting from SingletonFactory, ensuring only one Langfuse client instance exists throughout the application. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Langfuse client instances. In this case, it is LangfuseConfiguration. Source code in src/augmentation/langfuse/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class LangfuseClientFactory ( SingletonFactory ): \"\"\" Factory class for creating and managing Langfuse client instances. This class implements the Singleton pattern through inheriting from SingletonFactory, ensuring only one Langfuse client instance exists throughout the application. Attributes: _configuration_class (Type): The configuration class used for creating Langfuse client instances. In this case, it is LangfuseConfiguration. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> Langfuse : \"\"\" Creates a new Langfuse client instance. Args: configuration (LangfuseConfiguration): Configuration object containing Langfuse API credentials and URL settings. Returns: Langfuse: A configured Langfuse client instance ready for use with the provided credentials and host. \"\"\" return Langfuse ( secret_key = configuration . secrets . secret_key . get_secret_value (), public_key = configuration . secrets . public_key . get_secret_value (), host = configuration . url , )","title":"LangfuseClientFactory"},{"location":"src/augmentation/langfuse/dataset_service/","text":"Dataset_service This module contains functionality related to the the dataset_service module for augmentation.langfuse . Dataset_service LangfuseDatasetService Service for managing Langfuse datasets. This service provides methods to create, manage, and retrieve datasets within the Langfuse platform. It handles the communication with the Langfuse API for all dataset-related operations. Source code in src/augmentation/langfuse/dataset_service.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class LangfuseDatasetService : \"\"\"Service for managing Langfuse datasets. This service provides methods to create, manage, and retrieve datasets within the Langfuse platform. It handles the communication with the Langfuse API for all dataset-related operations. \"\"\" def __init__ ( self , langfuse_client : Langfuse , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Langfuse dataset service. Args: langfuse_client: Authenticated client for Langfuse API interactions. logger: Logger instance for recording operations. Defaults to module logger. \"\"\" self . langfuse_client = langfuse_client self . logger = logger def create_if_does_not_exist ( self , dataset : LangfuseDatasetConfiguration ) -> None : \"\"\"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Args: dataset: Configuration object containing dataset name, description, and metadata for creation. Note: The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. \"\"\" try : self . langfuse_client . get_dataset ( dataset . name ) self . logger . info ( f \"Dataset { dataset . name } exists.\" ) except NotFoundError : self . logger . info ( f \"Dataset { dataset . name } does not exist. Creating...\" ) self . langfuse_client . create_dataset ( name = dataset . name , description = dataset . description , metadata = dataset . metadata , ) def get_dataset ( self , dataset_name : str ) -> DatasetClient : \"\"\"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Args: dataset_name: The unique name identifier of the dataset to retrieve. Returns: DatasetClient: A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError: If a dataset with the specified name doesn't exist. \"\"\" return self . langfuse_client . get_dataset ( dataset_name ) __init__ ( langfuse_client , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the Langfuse dataset service. Parameters: langfuse_client ( Langfuse ) \u2013 Authenticated client for Langfuse API interactions. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operations. Defaults to module logger. Source code in src/augmentation/langfuse/dataset_service.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , langfuse_client : Langfuse , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Langfuse dataset service. Args: langfuse_client: Authenticated client for Langfuse API interactions. logger: Logger instance for recording operations. Defaults to module logger. \"\"\" self . langfuse_client = langfuse_client self . logger = logger create_if_does_not_exist ( dataset ) Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Parameters: dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration object containing dataset name, description, and metadata for creation. Note The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. Source code in src/augmentation/langfuse/dataset_service.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_if_does_not_exist ( self , dataset : LangfuseDatasetConfiguration ) -> None : \"\"\"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Args: dataset: Configuration object containing dataset name, description, and metadata for creation. Note: The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. \"\"\" try : self . langfuse_client . get_dataset ( dataset . name ) self . logger . info ( f \"Dataset { dataset . name } exists.\" ) except NotFoundError : self . logger . info ( f \"Dataset { dataset . name } does not exist. Creating...\" ) self . langfuse_client . create_dataset ( name = dataset . name , description = dataset . description , metadata = dataset . metadata , ) get_dataset ( dataset_name ) Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Parameters: dataset_name ( str ) \u2013 The unique name identifier of the dataset to retrieve. Returns: DatasetClient ( DatasetClient ) \u2013 A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError \u2013 If a dataset with the specified name doesn't exist. Source code in src/augmentation/langfuse/dataset_service.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_dataset ( self , dataset_name : str ) -> DatasetClient : \"\"\"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Args: dataset_name: The unique name identifier of the dataset to retrieve. Returns: DatasetClient: A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError: If a dataset with the specified name doesn't exist. \"\"\" return self . langfuse_client . get_dataset ( dataset_name ) LangfuseDatasetServiceFactory Bases: Factory Factory for creating LangfuseDatasetService instances. Creates and configures LangfuseDatasetService instances with the appropriate client based on provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class type used by this factory. Source code in src/augmentation/langfuse/dataset_service.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class LangfuseDatasetServiceFactory ( Factory ): \"\"\"Factory for creating LangfuseDatasetService instances. Creates and configures LangfuseDatasetService instances with the appropriate client based on provided configuration. Attributes: _configuration_class: The configuration class type used by this factory. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> LangfuseDatasetService : \"\"\"Create a configured LangfuseDatasetService instance. Args: configuration: The Langfuse configuration containing API credentials and other settings. Returns: A fully initialized LangfuseDatasetService instance with an authenticated client. \"\"\" client = LangfuseClientFactory . create ( configuration ) return LangfuseDatasetService ( client )","title":"Dataset_service"},{"location":"src/augmentation/langfuse/dataset_service/#dataset_service","text":"This module contains functionality related to the the dataset_service module for augmentation.langfuse .","title":"Dataset_service"},{"location":"src/augmentation/langfuse/dataset_service/#dataset_service_1","text":"","title":"Dataset_service"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetService","text":"Service for managing Langfuse datasets. This service provides methods to create, manage, and retrieve datasets within the Langfuse platform. It handles the communication with the Langfuse API for all dataset-related operations. Source code in src/augmentation/langfuse/dataset_service.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class LangfuseDatasetService : \"\"\"Service for managing Langfuse datasets. This service provides methods to create, manage, and retrieve datasets within the Langfuse platform. It handles the communication with the Langfuse API for all dataset-related operations. \"\"\" def __init__ ( self , langfuse_client : Langfuse , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Langfuse dataset service. Args: langfuse_client: Authenticated client for Langfuse API interactions. logger: Logger instance for recording operations. Defaults to module logger. \"\"\" self . langfuse_client = langfuse_client self . logger = logger def create_if_does_not_exist ( self , dataset : LangfuseDatasetConfiguration ) -> None : \"\"\"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Args: dataset: Configuration object containing dataset name, description, and metadata for creation. Note: The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. \"\"\" try : self . langfuse_client . get_dataset ( dataset . name ) self . logger . info ( f \"Dataset { dataset . name } exists.\" ) except NotFoundError : self . logger . info ( f \"Dataset { dataset . name } does not exist. Creating...\" ) self . langfuse_client . create_dataset ( name = dataset . name , description = dataset . description , metadata = dataset . metadata , ) def get_dataset ( self , dataset_name : str ) -> DatasetClient : \"\"\"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Args: dataset_name: The unique name identifier of the dataset to retrieve. Returns: DatasetClient: A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError: If a dataset with the specified name doesn't exist. \"\"\" return self . langfuse_client . get_dataset ( dataset_name )","title":"LangfuseDatasetService"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetService.__init__","text":"Initialize the Langfuse dataset service. Parameters: langfuse_client ( Langfuse ) \u2013 Authenticated client for Langfuse API interactions. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operations. Defaults to module logger. Source code in src/augmentation/langfuse/dataset_service.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , langfuse_client : Langfuse , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Langfuse dataset service. Args: langfuse_client: Authenticated client for Langfuse API interactions. logger: Logger instance for recording operations. Defaults to module logger. \"\"\" self . langfuse_client = langfuse_client self . logger = logger","title":"__init__"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetService.create_if_does_not_exist","text":"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Parameters: dataset ( LangfuseDatasetConfiguration ) \u2013 Configuration object containing dataset name, description, and metadata for creation. Note The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. Source code in src/augmentation/langfuse/dataset_service.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_if_does_not_exist ( self , dataset : LangfuseDatasetConfiguration ) -> None : \"\"\"Create a dataset in Langfuse if it doesn't already exist. Checks if a dataset with the specified name exists in Langfuse. If not found, creates a new dataset with the provided configuration. Args: dataset: Configuration object containing dataset name, description, and metadata for creation. Note: The NotFoundError exception from Langfuse is caught and used as an indicator to create a new dataset, but is still logged due to Langfuse implementation details. \"\"\" try : self . langfuse_client . get_dataset ( dataset . name ) self . logger . info ( f \"Dataset { dataset . name } exists.\" ) except NotFoundError : self . logger . info ( f \"Dataset { dataset . name } does not exist. Creating...\" ) self . langfuse_client . create_dataset ( name = dataset . name , description = dataset . description , metadata = dataset . metadata , )","title":"create_if_does_not_exist"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetService.get_dataset","text":"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Parameters: dataset_name ( str ) \u2013 The unique name identifier of the dataset to retrieve. Returns: DatasetClient ( DatasetClient ) \u2013 A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError \u2013 If a dataset with the specified name doesn't exist. Source code in src/augmentation/langfuse/dataset_service.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def get_dataset ( self , dataset_name : str ) -> DatasetClient : \"\"\"Retrieve a dataset client by name. Provides a client instance for interacting with a specific dataset in the Langfuse platform. Args: dataset_name: The unique name identifier of the dataset to retrieve. Returns: DatasetClient: A client object for performing operations on the specified dataset (such as adding examples, querying data). Raises: NotFoundError: If a dataset with the specified name doesn't exist. \"\"\" return self . langfuse_client . get_dataset ( dataset_name )","title":"get_dataset"},{"location":"src/augmentation/langfuse/dataset_service/#src.augmentation.langfuse.dataset_service.LangfuseDatasetServiceFactory","text":"Bases: Factory Factory for creating LangfuseDatasetService instances. Creates and configures LangfuseDatasetService instances with the appropriate client based on provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class type used by this factory. Source code in src/augmentation/langfuse/dataset_service.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class LangfuseDatasetServiceFactory ( Factory ): \"\"\"Factory for creating LangfuseDatasetService instances. Creates and configures LangfuseDatasetService instances with the appropriate client based on provided configuration. Attributes: _configuration_class: The configuration class type used by this factory. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> LangfuseDatasetService : \"\"\"Create a configured LangfuseDatasetService instance. Args: configuration: The Langfuse configuration containing API credentials and other settings. Returns: A fully initialized LangfuseDatasetService instance with an authenticated client. \"\"\" client = LangfuseClientFactory . create ( configuration ) return LangfuseDatasetService ( client )","title":"LangfuseDatasetServiceFactory"},{"location":"src/augmentation/langfuse/prompt_service/","text":"Prompt_service This module contains functionality related to the the prompt_service module for augmentation.langfuse . Prompt_service LangfusePromptService Source code in src/augmentation/langfuse/prompt_service.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class LangfusePromptService : def __init__ ( self , client : Langfuse , logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initializes the LangfusePromptService with a Langfuse client and a logger. Args: client (Langfuse): The Langfuse client instance. logger (Logger): The logger instance. \"\"\" self . client = client self . logger = logger def create_prompt_if_not_exists ( self , prompt_name : str , prompt_template : str , ) -> None : \"\"\" Creates a new prompt in Langfuse if it does not already exist. Args: prompt_name (str): The name of the prompt. prompt_template (str): The template of the prompt. \"\"\" if self . prompt_exists ( prompt_name ): return self . logger . info ( f \"Creating { prompt_name } prompt in Langfuse\" ) self . client . create_prompt ( name = prompt_name , prompt = prompt_template , labels = [ \"production\" ] ) def prompt_exists ( self , prompt_name : str , ) -> bool : \"\"\" Checks if a prompt exists in Langfuse. Args: prompt_name (str): The name of the prompt. Returns: bool: True if the prompt exists, False otherwise. \"\"\" try : self . client . get_prompt ( prompt_name ) return True except Exception : return False def get_prompt_template ( self , prompt_name : str ) -> str : \"\"\" Retrieves the prompt template from Langfuse. Args: prompt_name (str): The name of the prompt. Returns: str: The prompt template. \"\"\" prompt = self . client . get_prompt ( prompt_name ) return prompt . prompt __init__ ( client , logger = LoggerConfiguration . get_logger ( __name__ )) Initializes the LangfusePromptService with a Langfuse client and a logger. Parameters: client ( Langfuse ) \u2013 The Langfuse client instance. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 The logger instance. Source code in src/augmentation/langfuse/prompt_service.py 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , client : Langfuse , logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initializes the LangfusePromptService with a Langfuse client and a logger. Args: client (Langfuse): The Langfuse client instance. logger (Logger): The logger instance. \"\"\" self . client = client self . logger = logger create_prompt_if_not_exists ( prompt_name , prompt_template ) Creates a new prompt in Langfuse if it does not already exist. Parameters: prompt_name ( str ) \u2013 The name of the prompt. prompt_template ( str ) \u2013 The template of the prompt. Source code in src/augmentation/langfuse/prompt_service.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def create_prompt_if_not_exists ( self , prompt_name : str , prompt_template : str , ) -> None : \"\"\" Creates a new prompt in Langfuse if it does not already exist. Args: prompt_name (str): The name of the prompt. prompt_template (str): The template of the prompt. \"\"\" if self . prompt_exists ( prompt_name ): return self . logger . info ( f \"Creating { prompt_name } prompt in Langfuse\" ) self . client . create_prompt ( name = prompt_name , prompt = prompt_template , labels = [ \"production\" ] ) get_prompt_template ( prompt_name ) Retrieves the prompt template from Langfuse. Parameters: prompt_name ( str ) \u2013 The name of the prompt. Returns: str ( str ) \u2013 The prompt template. Source code in src/augmentation/langfuse/prompt_service.py 67 68 69 70 71 72 73 74 75 76 77 78 def get_prompt_template ( self , prompt_name : str ) -> str : \"\"\" Retrieves the prompt template from Langfuse. Args: prompt_name (str): The name of the prompt. Returns: str: The prompt template. \"\"\" prompt = self . client . get_prompt ( prompt_name ) return prompt . prompt prompt_exists ( prompt_name ) Checks if a prompt exists in Langfuse. Parameters: prompt_name ( str ) \u2013 The name of the prompt. Returns: bool ( bool ) \u2013 True if the prompt exists, False otherwise. Source code in src/augmentation/langfuse/prompt_service.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def prompt_exists ( self , prompt_name : str , ) -> bool : \"\"\" Checks if a prompt exists in Langfuse. Args: prompt_name (str): The name of the prompt. Returns: bool: True if the prompt exists, False otherwise. \"\"\" try : self . client . get_prompt ( prompt_name ) return True except Exception : return False LangfusePromptServiceFactory Bases: Factory Factory class for creating and managing Langfuse prompt service instances. This class implements the Singleton pattern through inheriting from Factory, ensuring only one Langfuse prompt service instance exists throughout the application. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Langfuse prompt service instances. In this case, it is LangfuseConfiguration. Source code in src/augmentation/langfuse/prompt_service.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class LangfusePromptServiceFactory ( Factory ): \"\"\" Factory class for creating and managing Langfuse prompt service instances. This class implements the Singleton pattern through inheriting from Factory, ensuring only one Langfuse prompt service instance exists throughout the application. Attributes: _configuration_class (Type): The configuration class used for creating Langfuse prompt service instances. In this case, it is LangfuseConfiguration. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> Langfuse : \"\"\" Creates a new Langfuse prompt service instance. Args: configuration (LangfuseConfiguration): Configuration object containing Langfuse API credentials and URL settings. Returns: LangfusePromptService: A configured Langfuse prompt service instance ready for use with the provided credentials and host. \"\"\" client = LangfuseClientFactory . create ( configuration ) return LangfusePromptService ( client = client )","title":"Prompt_service"},{"location":"src/augmentation/langfuse/prompt_service/#prompt_service","text":"This module contains functionality related to the the prompt_service module for augmentation.langfuse .","title":"Prompt_service"},{"location":"src/augmentation/langfuse/prompt_service/#prompt_service_1","text":"","title":"Prompt_service"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService","text":"Source code in src/augmentation/langfuse/prompt_service.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class LangfusePromptService : def __init__ ( self , client : Langfuse , logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initializes the LangfusePromptService with a Langfuse client and a logger. Args: client (Langfuse): The Langfuse client instance. logger (Logger): The logger instance. \"\"\" self . client = client self . logger = logger def create_prompt_if_not_exists ( self , prompt_name : str , prompt_template : str , ) -> None : \"\"\" Creates a new prompt in Langfuse if it does not already exist. Args: prompt_name (str): The name of the prompt. prompt_template (str): The template of the prompt. \"\"\" if self . prompt_exists ( prompt_name ): return self . logger . info ( f \"Creating { prompt_name } prompt in Langfuse\" ) self . client . create_prompt ( name = prompt_name , prompt = prompt_template , labels = [ \"production\" ] ) def prompt_exists ( self , prompt_name : str , ) -> bool : \"\"\" Checks if a prompt exists in Langfuse. Args: prompt_name (str): The name of the prompt. Returns: bool: True if the prompt exists, False otherwise. \"\"\" try : self . client . get_prompt ( prompt_name ) return True except Exception : return False def get_prompt_template ( self , prompt_name : str ) -> str : \"\"\" Retrieves the prompt template from Langfuse. Args: prompt_name (str): The name of the prompt. Returns: str: The prompt template. \"\"\" prompt = self . client . get_prompt ( prompt_name ) return prompt . prompt","title":"LangfusePromptService"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService.__init__","text":"Initializes the LangfusePromptService with a Langfuse client and a logger. Parameters: client ( Langfuse ) \u2013 The Langfuse client instance. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 The logger instance. Source code in src/augmentation/langfuse/prompt_service.py 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , client : Langfuse , logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initializes the LangfusePromptService with a Langfuse client and a logger. Args: client (Langfuse): The Langfuse client instance. logger (Logger): The logger instance. \"\"\" self . client = client self . logger = logger","title":"__init__"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService.create_prompt_if_not_exists","text":"Creates a new prompt in Langfuse if it does not already exist. Parameters: prompt_name ( str ) \u2013 The name of the prompt. prompt_template ( str ) \u2013 The template of the prompt. Source code in src/augmentation/langfuse/prompt_service.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def create_prompt_if_not_exists ( self , prompt_name : str , prompt_template : str , ) -> None : \"\"\" Creates a new prompt in Langfuse if it does not already exist. Args: prompt_name (str): The name of the prompt. prompt_template (str): The template of the prompt. \"\"\" if self . prompt_exists ( prompt_name ): return self . logger . info ( f \"Creating { prompt_name } prompt in Langfuse\" ) self . client . create_prompt ( name = prompt_name , prompt = prompt_template , labels = [ \"production\" ] )","title":"create_prompt_if_not_exists"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService.get_prompt_template","text":"Retrieves the prompt template from Langfuse. Parameters: prompt_name ( str ) \u2013 The name of the prompt. Returns: str ( str ) \u2013 The prompt template. Source code in src/augmentation/langfuse/prompt_service.py 67 68 69 70 71 72 73 74 75 76 77 78 def get_prompt_template ( self , prompt_name : str ) -> str : \"\"\" Retrieves the prompt template from Langfuse. Args: prompt_name (str): The name of the prompt. Returns: str: The prompt template. \"\"\" prompt = self . client . get_prompt ( prompt_name ) return prompt . prompt","title":"get_prompt_template"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptService.prompt_exists","text":"Checks if a prompt exists in Langfuse. Parameters: prompt_name ( str ) \u2013 The name of the prompt. Returns: bool ( bool ) \u2013 True if the prompt exists, False otherwise. Source code in src/augmentation/langfuse/prompt_service.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def prompt_exists ( self , prompt_name : str , ) -> bool : \"\"\" Checks if a prompt exists in Langfuse. Args: prompt_name (str): The name of the prompt. Returns: bool: True if the prompt exists, False otherwise. \"\"\" try : self . client . get_prompt ( prompt_name ) return True except Exception : return False","title":"prompt_exists"},{"location":"src/augmentation/langfuse/prompt_service/#src.augmentation.langfuse.prompt_service.LangfusePromptServiceFactory","text":"Bases: Factory Factory class for creating and managing Langfuse prompt service instances. This class implements the Singleton pattern through inheriting from Factory, ensuring only one Langfuse prompt service instance exists throughout the application. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating Langfuse prompt service instances. In this case, it is LangfuseConfiguration. Source code in src/augmentation/langfuse/prompt_service.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class LangfusePromptServiceFactory ( Factory ): \"\"\" Factory class for creating and managing Langfuse prompt service instances. This class implements the Singleton pattern through inheriting from Factory, ensuring only one Langfuse prompt service instance exists throughout the application. Attributes: _configuration_class (Type): The configuration class used for creating Langfuse prompt service instances. In this case, it is LangfuseConfiguration. \"\"\" _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> Langfuse : \"\"\" Creates a new Langfuse prompt service instance. Args: configuration (LangfuseConfiguration): Configuration object containing Langfuse API credentials and URL settings. Returns: LangfusePromptService: A configured Langfuse prompt service instance ready for use with the provided credentials and host. \"\"\" client = LangfuseClientFactory . create ( configuration ) return LangfusePromptService ( client = client )","title":"LangfusePromptServiceFactory"},{"location":"src/core/base_configuration/","text":"Base_configuration This module contains functionality related to the the base_configuration module for core . Base_configuration BaseConfiguration Bases: BaseModel , ABC Base abstract class for all configuration models. Provides common functionality like hashing for configuration objects. Extend this class to create specific configuration types. Source code in src/core/base_configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BaseConfiguration ( BaseModel , ABC ): \"\"\" Base abstract class for all configuration models. Provides common functionality like hashing for configuration objects. Extend this class to create specific configuration types. \"\"\" def __hash__ ( self ) -> int : \"\"\"Not the most efficient way of hashing, but it works for now.\"\"\" json = self . model_dump_json () return hash ( json ) @classmethod def _validate ( cls , value : Any , info : ValidationInfo , registry : Type [ ConfigurationRegistry ], ) -> Any : \"\"\" Validates the value against the type defined in the registry. Args: value (Any): The value of the field. info (ValidationInfo): The information about the field. registry (Type[ConfigurationRegistry]): The registry to use for validation. Returns: Any: The validated value. \"\"\" type_adapter = TypeAdapter ( registry . get_union_type ()) return type_adapter . validate_python ( value , context = info . context , ) __hash__ () Not the most efficient way of hashing, but it works for now. Source code in src/core/base_configuration.py 29 30 31 32 def __hash__ ( self ) -> int : \"\"\"Not the most efficient way of hashing, but it works for now.\"\"\" json = self . model_dump_json () return hash ( json ) BaseConfigurationWithSecrets Bases: BaseConfiguration Abstract model for configuration's secrets handling. Provides functionality to automatically load and validate secrets from environment variables or files. Extending class has to implement secrets field with corresponding type. Source code in src/core/base_configuration.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class BaseConfigurationWithSecrets ( BaseConfiguration ): \"\"\" Abstract model for configuration's secrets handling. Provides functionality to automatically load and validate secrets from environment variables or files. Extending class has to implement `secrets` field with corresponding type. \"\"\" secrets : BaseSecrets = Field ( None , description = \"`BaseSecrets` is meant for the the configuration that does not require secrets.\" \"In other case `BaseSecrets` should be replaced with the corresponding secrets class.\" , ) def model_post_init ( self , context : Any ) -> None : \"\"\" Function is invoked after the model is initialized. It is used to initialize secrets. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" self . secrets = self . _get_secrets ( secrets_file = context [ \"secrets_file\" ]) def _get_secrets ( self , secrets_file : str ) -> BaseSettings : \"\"\" Function to initialize secrets from the specified file. Args: secrets_file (str): The path to the secrets file. Returns: BaseSettings: The initialized secrets object. Raises: ValueError: If secrets are not found or cannot be loaded. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets = secrets_class ( _env_file = secrets_file ) if secrets is None : raise ValueError ( f \"Secrets for { self . name } not found.\" ) return secrets model_post_init ( context ) Function is invoked after the model is initialized. It is used to initialize secrets. Parameters: context ( Any ) \u2013 The context passed to the pydantic model, must contain 'secrets_file' key. Source code in src/core/base_configuration.py 88 89 90 91 92 93 94 95 def model_post_init ( self , context : Any ) -> None : \"\"\" Function is invoked after the model is initialized. It is used to initialize secrets. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" self . secrets = self . _get_secrets ( secrets_file = context [ \"secrets_file\" ]) BaseSecrets Bases: BaseConfiguration , BaseSettings Base class for secrets management. Extends both BaseConfiguration and BaseSettings to handle sensitive configuration data like API keys, passwords, etc. Uses Pydantic's BaseSettings for environment variable and file-based loading. Source code in src/core/base_configuration.py 59 60 61 62 63 64 65 66 67 68 69 70 class BaseSecrets ( BaseConfiguration , BaseSettings ): \"\"\" Base class for secrets management. Extends both BaseConfiguration and BaseSettings to handle sensitive configuration data like API keys, passwords, etc. Uses Pydantic's BaseSettings for environment variable and file-based loading. \"\"\" model_config = ConfigDict ( extra = \"ignore\" , ) BasicConfiguration Bases: BaseConfiguration Standard configuration class with metadata support. Includes a metadata field for storing application-specific settings. Source code in src/core/base_configuration.py 248 249 250 251 252 253 254 255 256 257 class BasicConfiguration ( BaseConfiguration ): \"\"\" Standard configuration class with metadata support. Includes a metadata field for storing application-specific settings. \"\"\" metadata : Optional [ MetadataConfiguration ] = Field ( None , description = \"Metadata of the run.\" ) EnvironmentName Bases: str , Enum Enumeration of available environments. Defines the possible runtime environments for the application. Source code in src/core/base_configuration.py 137 138 139 140 141 142 143 144 145 146 147 148 class EnvironmentName ( str , Enum ): \"\"\" Enumeration of available environments. Defines the possible runtime environments for the application. \"\"\" DEFAULT = \"default\" LOCAL = \"local\" DEV = \"dev\" TEST = \"test\" PROD = \"prod\" LogLevelName Bases: str , Enum Enumeration of available logging levels. Provides a mapping between string log levels and their corresponding logging constants. Source code in src/core/base_configuration.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class LogLevelName ( str , Enum ): \"\"\" Enumeration of available logging levels. Provides a mapping between string log levels and their corresponding logging constants. \"\"\" DEBUG = \"debug\" INFO = \"info\" WARNING = \"warning\" ERROR = \"error\" CRITICAL = \"critical\" @property def value_as_int ( self ) -> int : \"\"\"Convert string log level to the corresponding logging constant.\"\"\" return logging . _nameToLevel [ self . value . upper ()] value_as_int property Convert string log level to the corresponding logging constant. MetadataConfiguration Bases: BaseConfiguration Configuration for application metadata. Fields are read from the command line arguments. Source code in src/core/base_configuration.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class MetadataConfiguration ( BaseConfiguration ): \"\"\" Configuration for application metadata. Fields are read from the command line arguments. \"\"\" environment : EnvironmentName = Field ( EnvironmentName . LOCAL , description = \"The environment of the application.\" , ) build_name : Optional [ str ] = Field ( None , description = \"The name of the build.\" , ) log_level : LogLevelName = Field ( LogLevelName . INFO , description = \"The log level of the application.\" ) on_prem_config : bool = Field ( False , description = \"If set, then the configuration will be read from on premise configuration.\" \"Otherwise from the orchestrator service.\" , ) @model_validator ( mode = \"before\" ) @classmethod def validate_from_args ( cls , data : dict ) -> dict : \"\"\" Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Args: data (dict): The configuration data. Returns: dict: The validated configuration data. \"\"\" parser = cls . _get_parser () args , _ = parser . parse_known_args () return cls . _get_data ( data = data , args = args ) @classmethod def _get_data ( cls , data : dict , args : argparse . Namespace ) -> dict : \"\"\" Function to parse the arguments. Returns: argparse.Namespace: The parsed arguments. \"\"\" if args . env : data [ \"environment\" ] = EnvironmentName ( args . env ) if args . build_name : data [ \"build_name\" ] = args . build_name if args . on_prem_config : data [ \"on_prem_config\" ] = args . on_prem_config if args . log_level : data [ \"log_level\" ] = args . log_level return data @classmethod def _get_parser ( cls ) -> argparse . ArgumentParser : \"\"\" Function to initialize the argument parser to read arguments from command line. Returns: argparse.ArgumentParser: The argument parser. \"\"\" parser = argparse . ArgumentParser () parser . add_argument ( \"--env\" , type = EnvironmentName , help = \"Runtime environment.\" , default = EnvironmentName . LOCAL , choices = [ env . value for env in EnvironmentName ], ) parser . add_argument ( \"--build-name\" , type = str , help = \"The name of the build.\" , default = f \"build-local- { time . time () } \" , ) parser . add_argument ( \"--on-prem-config\" , help = \"If set, then the configuration will be read from on premise configuration.\" \"Otherwise from the orchestrator service.\" , action = \"store_true\" , ) parser . add_argument ( \"--log-level\" , type = LogLevelName , help = \"Log level.\" , default = LogLevelName . INFO , choices = [ level . value for level in LogLevelName ], ) return parser validate_from_args ( data ) classmethod Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Parameters: data ( dict ) \u2013 The configuration data. Returns: dict ( dict ) \u2013 The validated configuration data. Source code in src/core/base_configuration.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @model_validator ( mode = \"before\" ) @classmethod def validate_from_args ( cls , data : dict ) -> dict : \"\"\" Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Args: data (dict): The configuration data. Returns: dict: The validated configuration data. \"\"\" parser = cls . _get_parser () args , _ = parser . parse_known_args () return cls . _get_data ( data = data , args = args )","title":"Base_configuration"},{"location":"src/core/base_configuration/#base_configuration","text":"This module contains functionality related to the the base_configuration module for core .","title":"Base_configuration"},{"location":"src/core/base_configuration/#base_configuration_1","text":"","title":"Base_configuration"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseConfiguration","text":"Bases: BaseModel , ABC Base abstract class for all configuration models. Provides common functionality like hashing for configuration objects. Extend this class to create specific configuration types. Source code in src/core/base_configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class BaseConfiguration ( BaseModel , ABC ): \"\"\" Base abstract class for all configuration models. Provides common functionality like hashing for configuration objects. Extend this class to create specific configuration types. \"\"\" def __hash__ ( self ) -> int : \"\"\"Not the most efficient way of hashing, but it works for now.\"\"\" json = self . model_dump_json () return hash ( json ) @classmethod def _validate ( cls , value : Any , info : ValidationInfo , registry : Type [ ConfigurationRegistry ], ) -> Any : \"\"\" Validates the value against the type defined in the registry. Args: value (Any): The value of the field. info (ValidationInfo): The information about the field. registry (Type[ConfigurationRegistry]): The registry to use for validation. Returns: Any: The validated value. \"\"\" type_adapter = TypeAdapter ( registry . get_union_type ()) return type_adapter . validate_python ( value , context = info . context , )","title":"BaseConfiguration"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseConfiguration.__hash__","text":"Not the most efficient way of hashing, but it works for now. Source code in src/core/base_configuration.py 29 30 31 32 def __hash__ ( self ) -> int : \"\"\"Not the most efficient way of hashing, but it works for now.\"\"\" json = self . model_dump_json () return hash ( json )","title":"__hash__"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseConfigurationWithSecrets","text":"Bases: BaseConfiguration Abstract model for configuration's secrets handling. Provides functionality to automatically load and validate secrets from environment variables or files. Extending class has to implement secrets field with corresponding type. Source code in src/core/base_configuration.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class BaseConfigurationWithSecrets ( BaseConfiguration ): \"\"\" Abstract model for configuration's secrets handling. Provides functionality to automatically load and validate secrets from environment variables or files. Extending class has to implement `secrets` field with corresponding type. \"\"\" secrets : BaseSecrets = Field ( None , description = \"`BaseSecrets` is meant for the the configuration that does not require secrets.\" \"In other case `BaseSecrets` should be replaced with the corresponding secrets class.\" , ) def model_post_init ( self , context : Any ) -> None : \"\"\" Function is invoked after the model is initialized. It is used to initialize secrets. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" self . secrets = self . _get_secrets ( secrets_file = context [ \"secrets_file\" ]) def _get_secrets ( self , secrets_file : str ) -> BaseSettings : \"\"\" Function to initialize secrets from the specified file. Args: secrets_file (str): The path to the secrets file. Returns: BaseSettings: The initialized secrets object. Raises: ValueError: If secrets are not found or cannot be loaded. \"\"\" secrets_class = type ( self ) . model_fields [ \"secrets\" ] . annotation secrets = secrets_class ( _env_file = secrets_file ) if secrets is None : raise ValueError ( f \"Secrets for { self . name } not found.\" ) return secrets","title":"BaseConfigurationWithSecrets"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseConfigurationWithSecrets.model_post_init","text":"Function is invoked after the model is initialized. It is used to initialize secrets. Parameters: context ( Any ) \u2013 The context passed to the pydantic model, must contain 'secrets_file' key. Source code in src/core/base_configuration.py 88 89 90 91 92 93 94 95 def model_post_init ( self , context : Any ) -> None : \"\"\" Function is invoked after the model is initialized. It is used to initialize secrets. Args: context (Any): The context passed to the pydantic model, must contain 'secrets_file' key. \"\"\" self . secrets = self . _get_secrets ( secrets_file = context [ \"secrets_file\" ])","title":"model_post_init"},{"location":"src/core/base_configuration/#src.core.base_configuration.BaseSecrets","text":"Bases: BaseConfiguration , BaseSettings Base class for secrets management. Extends both BaseConfiguration and BaseSettings to handle sensitive configuration data like API keys, passwords, etc. Uses Pydantic's BaseSettings for environment variable and file-based loading. Source code in src/core/base_configuration.py 59 60 61 62 63 64 65 66 67 68 69 70 class BaseSecrets ( BaseConfiguration , BaseSettings ): \"\"\" Base class for secrets management. Extends both BaseConfiguration and BaseSettings to handle sensitive configuration data like API keys, passwords, etc. Uses Pydantic's BaseSettings for environment variable and file-based loading. \"\"\" model_config = ConfigDict ( extra = \"ignore\" , )","title":"BaseSecrets"},{"location":"src/core/base_configuration/#src.core.base_configuration.BasicConfiguration","text":"Bases: BaseConfiguration Standard configuration class with metadata support. Includes a metadata field for storing application-specific settings. Source code in src/core/base_configuration.py 248 249 250 251 252 253 254 255 256 257 class BasicConfiguration ( BaseConfiguration ): \"\"\" Standard configuration class with metadata support. Includes a metadata field for storing application-specific settings. \"\"\" metadata : Optional [ MetadataConfiguration ] = Field ( None , description = \"Metadata of the run.\" )","title":"BasicConfiguration"},{"location":"src/core/base_configuration/#src.core.base_configuration.EnvironmentName","text":"Bases: str , Enum Enumeration of available environments. Defines the possible runtime environments for the application. Source code in src/core/base_configuration.py 137 138 139 140 141 142 143 144 145 146 147 148 class EnvironmentName ( str , Enum ): \"\"\" Enumeration of available environments. Defines the possible runtime environments for the application. \"\"\" DEFAULT = \"default\" LOCAL = \"local\" DEV = \"dev\" TEST = \"test\" PROD = \"prod\"","title":"EnvironmentName"},{"location":"src/core/base_configuration/#src.core.base_configuration.LogLevelName","text":"Bases: str , Enum Enumeration of available logging levels. Provides a mapping between string log levels and their corresponding logging constants. Source code in src/core/base_configuration.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class LogLevelName ( str , Enum ): \"\"\" Enumeration of available logging levels. Provides a mapping between string log levels and their corresponding logging constants. \"\"\" DEBUG = \"debug\" INFO = \"info\" WARNING = \"warning\" ERROR = \"error\" CRITICAL = \"critical\" @property def value_as_int ( self ) -> int : \"\"\"Convert string log level to the corresponding logging constant.\"\"\" return logging . _nameToLevel [ self . value . upper ()]","title":"LogLevelName"},{"location":"src/core/base_configuration/#src.core.base_configuration.LogLevelName.value_as_int","text":"Convert string log level to the corresponding logging constant.","title":"value_as_int"},{"location":"src/core/base_configuration/#src.core.base_configuration.MetadataConfiguration","text":"Bases: BaseConfiguration Configuration for application metadata. Fields are read from the command line arguments. Source code in src/core/base_configuration.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class MetadataConfiguration ( BaseConfiguration ): \"\"\" Configuration for application metadata. Fields are read from the command line arguments. \"\"\" environment : EnvironmentName = Field ( EnvironmentName . LOCAL , description = \"The environment of the application.\" , ) build_name : Optional [ str ] = Field ( None , description = \"The name of the build.\" , ) log_level : LogLevelName = Field ( LogLevelName . INFO , description = \"The log level of the application.\" ) on_prem_config : bool = Field ( False , description = \"If set, then the configuration will be read from on premise configuration.\" \"Otherwise from the orchestrator service.\" , ) @model_validator ( mode = \"before\" ) @classmethod def validate_from_args ( cls , data : dict ) -> dict : \"\"\" Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Args: data (dict): The configuration data. Returns: dict: The validated configuration data. \"\"\" parser = cls . _get_parser () args , _ = parser . parse_known_args () return cls . _get_data ( data = data , args = args ) @classmethod def _get_data ( cls , data : dict , args : argparse . Namespace ) -> dict : \"\"\" Function to parse the arguments. Returns: argparse.Namespace: The parsed arguments. \"\"\" if args . env : data [ \"environment\" ] = EnvironmentName ( args . env ) if args . build_name : data [ \"build_name\" ] = args . build_name if args . on_prem_config : data [ \"on_prem_config\" ] = args . on_prem_config if args . log_level : data [ \"log_level\" ] = args . log_level return data @classmethod def _get_parser ( cls ) -> argparse . ArgumentParser : \"\"\" Function to initialize the argument parser to read arguments from command line. Returns: argparse.ArgumentParser: The argument parser. \"\"\" parser = argparse . ArgumentParser () parser . add_argument ( \"--env\" , type = EnvironmentName , help = \"Runtime environment.\" , default = EnvironmentName . LOCAL , choices = [ env . value for env in EnvironmentName ], ) parser . add_argument ( \"--build-name\" , type = str , help = \"The name of the build.\" , default = f \"build-local- { time . time () } \" , ) parser . add_argument ( \"--on-prem-config\" , help = \"If set, then the configuration will be read from on premise configuration.\" \"Otherwise from the orchestrator service.\" , action = \"store_true\" , ) parser . add_argument ( \"--log-level\" , type = LogLevelName , help = \"Log level.\" , default = LogLevelName . INFO , choices = [ level . value for level in LogLevelName ], ) return parser","title":"MetadataConfiguration"},{"location":"src/core/base_configuration/#src.core.base_configuration.MetadataConfiguration.validate_from_args","text":"Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Parameters: data ( dict ) \u2013 The configuration data. Returns: dict ( dict ) \u2013 The validated configuration data. Source code in src/core/base_configuration.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @model_validator ( mode = \"before\" ) @classmethod def validate_from_args ( cls , data : dict ) -> dict : \"\"\" Validate configuration data from command-line arguments. Parses known arguments and validates them against the model fields. Args: data (dict): The configuration data. Returns: dict: The validated configuration data. \"\"\" parser = cls . _get_parser () args , _ = parser . parse_known_args () return cls . _get_data ( data = data , args = args )","title":"validate_from_args"},{"location":"src/core/base_factory/","text":"Base_factory This module contains functionality related to the the base_factory module for core . Base_factory ConfigurationRegistry Bases: Registry A specialized registry for configuration classes. This class extends the Registry to provide additional functionality specific to managing configuration classes, particularly the ability to create Union types from all registered configurations. The type is used by pydantic to validate the given configuration. Source code in src/core/base_factory.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class ConfigurationRegistry ( Registry ): \"\"\" A specialized registry for configuration classes. This class extends the Registry to provide additional functionality specific to managing configuration classes, particularly the ability to create Union types from all registered configurations. The type is used by pydantic to validate the given configuration. \"\"\" @classmethod def get_union_type ( cls ) -> Any : \"\"\" Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any: A Union type containing all registered configuration classes. \"\"\" return Union [ tuple ( cls . _objects . values ())] get_union_type () classmethod Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any ( Any ) \u2013 A Union type containing all registered configuration classes. Source code in src/core/base_factory.py 211 212 213 214 215 216 217 218 219 220 221 222 @classmethod def get_union_type ( cls ) -> Any : \"\"\" Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any: A Union type containing all registered configuration classes. \"\"\" return Union [ tuple ( cls . _objects . values ())] Factory Bases: ABC Abstract Factory base class for creating instances based on configurations. This class implements the Factory pattern and provides a standardized way to create instances of objects based on their configuration. Attributes: _configuration_class ( Type ) \u2013 The expected type of configuration objects. Source code in src/core/base_factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Factory ( ABC ): \"\"\" Abstract Factory base class for creating instances based on configurations. This class implements the Factory pattern and provides a standardized way to create instances of objects based on their configuration. Attributes: _configuration_class (Type): The expected type of configuration objects. \"\"\" _configuration_class : Type = None @classmethod @abstractmethod def _create_instance ( cls , configuration : Any ) -> Any : \"\"\" Abstract method that must be implemented by subclasses to create instances. Args: configuration (Any): The configuration object used to create the instance. Returns: Any: A new instance created based on the provided configuration. \"\"\" pass @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create an instance based on the given configuration. Args: configuration (Any): The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any: A new instance created based on the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_instance ( configuration ) create ( configuration ) classmethod Create an instance based on the given configuration. Parameters: configuration ( Any ) \u2013 The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any ( Any ) \u2013 A new instance created based on the provided configuration. Raises: ValueError \u2013 If the configuration is not an instance of the expected type. Source code in src/core/base_factory.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create an instance based on the given configuration. Args: configuration (Any): The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any: A new instance created based on the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_instance ( configuration ) Registry A registry for storing and retrieving objects by key. This class implements a simple registry pattern that allows objects to be registered with a key and retrieved later using that key. It is designed to be subclassed, with each subclass defining its own expected key type. It is used for registering different plugins or components in a system. Attributes: _key_class ( Type ) \u2013 The expected type for registry keys. _objects ( Dict [ Any , Any ] ) \u2013 Dictionary storing registered objects by their keys. Source code in src/core/base_factory.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 class Registry : \"\"\" A registry for storing and retrieving objects by key. This class implements a simple registry pattern that allows objects to be registered with a key and retrieved later using that key. It is designed to be subclassed, with each subclass defining its own expected key type. It is used for registering different plugins or components in a system. Attributes: _key_class (Type): The expected type for registry keys. _objects (Dict[Any, Any]): Dictionary storing registered objects by their keys. \"\"\" _key_class : Type = None _objects : Dict [ Any , Any ] = {} def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _objects = {} @classmethod def register ( cls , key : Any , Any : Any ) -> None : \"\"\" Register an object with the specified key. Args: key (Any): The key to associate with the object. Must be of type _key_class. Any (Any): The object to register. Raises: ValueError: If the key is not an instance of the expected type. \"\"\" if not isinstance ( key , cls . _key_class ): raise ValueError ( f \"Key must be of type: { cls . _key_class } \" ) cls . _objects [ key ] = Any @classmethod def get ( cls , key : Any ) -> Any : \"\"\" Retrieve an object by its key. Args: key (Any): The key associated with the object to retrieve. Returns: Any: The object associated with the key. Raises: ValueError: If no object is registered with the given key. \"\"\" if key not in cls . _objects : raise ValueError ( f \"Factory for ' { key } ' key is not registered.\" ) return cls . _objects [ key ] @classmethod def get_all ( cls ) -> Dict [ Any , Any ]: \"\"\" Get all registered objects. Returns: Dict[Any, Any]: A dictionary containing all registered objects with their keys. \"\"\" return cls . _objects __init_subclass__ ( ** kwargs ) Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Parameters: **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in src/core/base_factory.py 143 144 145 146 147 148 149 150 151 152 153 154 def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _objects = {} get ( key ) classmethod Retrieve an object by its key. Parameters: key ( Any ) \u2013 The key associated with the object to retrieve. Returns: Any ( Any ) \u2013 The object associated with the key. Raises: ValueError \u2013 If no object is registered with the given key. Source code in src/core/base_factory.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 @classmethod def get ( cls , key : Any ) -> Any : \"\"\" Retrieve an object by its key. Args: key (Any): The key associated with the object to retrieve. Returns: Any: The object associated with the key. Raises: ValueError: If no object is registered with the given key. \"\"\" if key not in cls . _objects : raise ValueError ( f \"Factory for ' { key } ' key is not registered.\" ) return cls . _objects [ key ] get_all () classmethod Get all registered objects. Returns: Dict [ Any , Any ] \u2013 Dict[Any, Any]: A dictionary containing all registered objects with their keys. Source code in src/core/base_factory.py 190 191 192 193 194 195 196 197 198 @classmethod def get_all ( cls ) -> Dict [ Any , Any ]: \"\"\" Get all registered objects. Returns: Dict[Any, Any]: A dictionary containing all registered objects with their keys. \"\"\" return cls . _objects register ( key , Any ) classmethod Register an object with the specified key. Parameters: key ( Any ) \u2013 The key to associate with the object. Must be of type _key_class. Any ( Any ) \u2013 The object to register. Raises: ValueError \u2013 If the key is not an instance of the expected type. Source code in src/core/base_factory.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 @classmethod def register ( cls , key : Any , Any : Any ) -> None : \"\"\" Register an object with the specified key. Args: key (Any): The key to associate with the object. Must be of type _key_class. Any (Any): The object to register. Raises: ValueError: If the key is not an instance of the expected type. \"\"\" if not isinstance ( key , cls . _key_class ): raise ValueError ( f \"Key must be of type: { cls . _key_class } \" ) cls . _objects [ key ] = Any SingletonFactory Bases: Factory This class extends the Factory pattern with Singleton functionality, ensuring that only one instance is created for each unique configuration during a single runtime. Attributes: _cache ( dict ) \u2013 Dictionary storing instances by their configurations. Source code in src/core/base_factory.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class SingletonFactory ( Factory ): \"\"\" This class extends the Factory pattern with Singleton functionality, ensuring that only one instance is created for each unique configuration during a single runtime. Attributes: _cache (dict): Dictionary storing instances by their configurations. \"\"\" _cache : dict = {} def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _cache = {} @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create or retrieve a singleton instance based on the given configuration. Args: configuration (Any): The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any: A singleton instance associated with the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_singleton ( configuration ) @classmethod def _create_singleton ( cls , configuration : Any ) -> Any : \"\"\" Create a new instance or return an existing one from the cache. Args: configuration (Any): The configuration object used to create/retrieve the instance. Returns: Any: A singleton instance associated with the provided configuration. \"\"\" if configuration in cls . _cache : return cls . _cache [ configuration ] instance = cls . _create_instance ( configuration ) cls . _cache [ configuration ] = instance return instance @classmethod def clear_cache ( cls ): \"\"\" Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. \"\"\" cls . _cache . clear () __init_subclass__ ( ** kwargs ) Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Parameters: **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in src/core/base_factory.py 65 66 67 68 69 70 71 72 73 74 75 76 def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _cache = {} clear_cache () classmethod Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. Source code in src/core/base_factory.py 115 116 117 118 119 120 121 122 @classmethod def clear_cache ( cls ): \"\"\" Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. \"\"\" cls . _cache . clear () create ( configuration ) classmethod Create or retrieve a singleton instance based on the given configuration. Parameters: configuration ( Any ) \u2013 The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any ( Any ) \u2013 A singleton instance associated with the provided configuration. Raises: ValueError \u2013 If the configuration is not an instance of the expected type. Source code in src/core/base_factory.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create or retrieve a singleton instance based on the given configuration. Args: configuration (Any): The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any: A singleton instance associated with the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_singleton ( configuration )","title":"Base_factory"},{"location":"src/core/base_factory/#base_factory","text":"This module contains functionality related to the the base_factory module for core .","title":"Base_factory"},{"location":"src/core/base_factory/#base_factory_1","text":"","title":"Base_factory"},{"location":"src/core/base_factory/#src.core.base_factory.ConfigurationRegistry","text":"Bases: Registry A specialized registry for configuration classes. This class extends the Registry to provide additional functionality specific to managing configuration classes, particularly the ability to create Union types from all registered configurations. The type is used by pydantic to validate the given configuration. Source code in src/core/base_factory.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 class ConfigurationRegistry ( Registry ): \"\"\" A specialized registry for configuration classes. This class extends the Registry to provide additional functionality specific to managing configuration classes, particularly the ability to create Union types from all registered configurations. The type is used by pydantic to validate the given configuration. \"\"\" @classmethod def get_union_type ( cls ) -> Any : \"\"\" Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any: A Union type containing all registered configuration classes. \"\"\" return Union [ tuple ( cls . _objects . values ())]","title":"ConfigurationRegistry"},{"location":"src/core/base_factory/#src.core.base_factory.ConfigurationRegistry.get_union_type","text":"Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any ( Any ) \u2013 A Union type containing all registered configuration classes. Source code in src/core/base_factory.py 211 212 213 214 215 216 217 218 219 220 221 222 @classmethod def get_union_type ( cls ) -> Any : \"\"\" Create a Union type of all registered configuration classes. This is useful for type hints where any of the registered configuration types can be accepted. Returns: Any: A Union type containing all registered configuration classes. \"\"\" return Union [ tuple ( cls . _objects . values ())]","title":"get_union_type"},{"location":"src/core/base_factory/#src.core.base_factory.Factory","text":"Bases: ABC Abstract Factory base class for creating instances based on configurations. This class implements the Factory pattern and provides a standardized way to create instances of objects based on their configuration. Attributes: _configuration_class ( Type ) \u2013 The expected type of configuration objects. Source code in src/core/base_factory.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Factory ( ABC ): \"\"\" Abstract Factory base class for creating instances based on configurations. This class implements the Factory pattern and provides a standardized way to create instances of objects based on their configuration. Attributes: _configuration_class (Type): The expected type of configuration objects. \"\"\" _configuration_class : Type = None @classmethod @abstractmethod def _create_instance ( cls , configuration : Any ) -> Any : \"\"\" Abstract method that must be implemented by subclasses to create instances. Args: configuration (Any): The configuration object used to create the instance. Returns: Any: A new instance created based on the provided configuration. \"\"\" pass @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create an instance based on the given configuration. Args: configuration (Any): The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any: A new instance created based on the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_instance ( configuration )","title":"Factory"},{"location":"src/core/base_factory/#src.core.base_factory.Factory.create","text":"Create an instance based on the given configuration. Parameters: configuration ( Any ) \u2013 The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any ( Any ) \u2013 A new instance created based on the provided configuration. Raises: ValueError \u2013 If the configuration is not an instance of the expected type. Source code in src/core/base_factory.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create an instance based on the given configuration. Args: configuration (Any): The configuration object used to create the instance. It has to be of the type specified by _configuration_class. Returns: Any: A new instance created based on the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_instance ( configuration )","title":"create"},{"location":"src/core/base_factory/#src.core.base_factory.Registry","text":"A registry for storing and retrieving objects by key. This class implements a simple registry pattern that allows objects to be registered with a key and retrieved later using that key. It is designed to be subclassed, with each subclass defining its own expected key type. It is used for registering different plugins or components in a system. Attributes: _key_class ( Type ) \u2013 The expected type for registry keys. _objects ( Dict [ Any , Any ] ) \u2013 Dictionary storing registered objects by their keys. Source code in src/core/base_factory.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 class Registry : \"\"\" A registry for storing and retrieving objects by key. This class implements a simple registry pattern that allows objects to be registered with a key and retrieved later using that key. It is designed to be subclassed, with each subclass defining its own expected key type. It is used for registering different plugins or components in a system. Attributes: _key_class (Type): The expected type for registry keys. _objects (Dict[Any, Any]): Dictionary storing registered objects by their keys. \"\"\" _key_class : Type = None _objects : Dict [ Any , Any ] = {} def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _objects = {} @classmethod def register ( cls , key : Any , Any : Any ) -> None : \"\"\" Register an object with the specified key. Args: key (Any): The key to associate with the object. Must be of type _key_class. Any (Any): The object to register. Raises: ValueError: If the key is not an instance of the expected type. \"\"\" if not isinstance ( key , cls . _key_class ): raise ValueError ( f \"Key must be of type: { cls . _key_class } \" ) cls . _objects [ key ] = Any @classmethod def get ( cls , key : Any ) -> Any : \"\"\" Retrieve an object by its key. Args: key (Any): The key associated with the object to retrieve. Returns: Any: The object associated with the key. Raises: ValueError: If no object is registered with the given key. \"\"\" if key not in cls . _objects : raise ValueError ( f \"Factory for ' { key } ' key is not registered.\" ) return cls . _objects [ key ] @classmethod def get_all ( cls ) -> Dict [ Any , Any ]: \"\"\" Get all registered objects. Returns: Dict[Any, Any]: A dictionary containing all registered objects with their keys. \"\"\" return cls . _objects","title":"Registry"},{"location":"src/core/base_factory/#src.core.base_factory.Registry.__init_subclass__","text":"Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Parameters: **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in src/core/base_factory.py 143 144 145 146 147 148 149 150 151 152 153 154 def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty objects dictionary. This method is called when a subclass of Registry is created, ensuring each subclass has its own independent registry. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _objects = {}","title":"__init_subclass__"},{"location":"src/core/base_factory/#src.core.base_factory.Registry.get","text":"Retrieve an object by its key. Parameters: key ( Any ) \u2013 The key associated with the object to retrieve. Returns: Any ( Any ) \u2013 The object associated with the key. Raises: ValueError \u2013 If no object is registered with the given key. Source code in src/core/base_factory.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 @classmethod def get ( cls , key : Any ) -> Any : \"\"\" Retrieve an object by its key. Args: key (Any): The key associated with the object to retrieve. Returns: Any: The object associated with the key. Raises: ValueError: If no object is registered with the given key. \"\"\" if key not in cls . _objects : raise ValueError ( f \"Factory for ' { key } ' key is not registered.\" ) return cls . _objects [ key ]","title":"get"},{"location":"src/core/base_factory/#src.core.base_factory.Registry.get_all","text":"Get all registered objects. Returns: Dict [ Any , Any ] \u2013 Dict[Any, Any]: A dictionary containing all registered objects with their keys. Source code in src/core/base_factory.py 190 191 192 193 194 195 196 197 198 @classmethod def get_all ( cls ) -> Dict [ Any , Any ]: \"\"\" Get all registered objects. Returns: Dict[Any, Any]: A dictionary containing all registered objects with their keys. \"\"\" return cls . _objects","title":"get_all"},{"location":"src/core/base_factory/#src.core.base_factory.Registry.register","text":"Register an object with the specified key. Parameters: key ( Any ) \u2013 The key to associate with the object. Must be of type _key_class. Any ( Any ) \u2013 The object to register. Raises: ValueError \u2013 If the key is not an instance of the expected type. Source code in src/core/base_factory.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 @classmethod def register ( cls , key : Any , Any : Any ) -> None : \"\"\" Register an object with the specified key. Args: key (Any): The key to associate with the object. Must be of type _key_class. Any (Any): The object to register. Raises: ValueError: If the key is not an instance of the expected type. \"\"\" if not isinstance ( key , cls . _key_class ): raise ValueError ( f \"Key must be of type: { cls . _key_class } \" ) cls . _objects [ key ] = Any","title":"register"},{"location":"src/core/base_factory/#src.core.base_factory.SingletonFactory","text":"Bases: Factory This class extends the Factory pattern with Singleton functionality, ensuring that only one instance is created for each unique configuration during a single runtime. Attributes: _cache ( dict ) \u2013 Dictionary storing instances by their configurations. Source code in src/core/base_factory.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class SingletonFactory ( Factory ): \"\"\" This class extends the Factory pattern with Singleton functionality, ensuring that only one instance is created for each unique configuration during a single runtime. Attributes: _cache (dict): Dictionary storing instances by their configurations. \"\"\" _cache : dict = {} def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _cache = {} @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create or retrieve a singleton instance based on the given configuration. Args: configuration (Any): The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any: A singleton instance associated with the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_singleton ( configuration ) @classmethod def _create_singleton ( cls , configuration : Any ) -> Any : \"\"\" Create a new instance or return an existing one from the cache. Args: configuration (Any): The configuration object used to create/retrieve the instance. Returns: Any: A singleton instance associated with the provided configuration. \"\"\" if configuration in cls . _cache : return cls . _cache [ configuration ] instance = cls . _create_instance ( configuration ) cls . _cache [ configuration ] = instance return instance @classmethod def clear_cache ( cls ): \"\"\" Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. \"\"\" cls . _cache . clear ()","title":"SingletonFactory"},{"location":"src/core/base_factory/#src.core.base_factory.SingletonFactory.__init_subclass__","text":"Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Parameters: **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in src/core/base_factory.py 65 66 67 68 69 70 71 72 73 74 75 76 def __init_subclass__ ( cls , ** kwargs : Any ): \"\"\" Initialize a new subclass with an empty cache. This method is called when a subclass of SingletonFactory is created, ensuring each subclass has its own independent cache. Args: **kwargs: Additional keyword arguments. \"\"\" super () . __init_subclass__ ( ** kwargs ) cls . _cache = {}","title":"__init_subclass__"},{"location":"src/core/base_factory/#src.core.base_factory.SingletonFactory.clear_cache","text":"Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. Source code in src/core/base_factory.py 115 116 117 118 119 120 121 122 @classmethod def clear_cache ( cls ): \"\"\" Clear the singleton instance cache. This method removes all cached instances, allowing them to be garbage collected. \"\"\" cls . _cache . clear ()","title":"clear_cache"},{"location":"src/core/base_factory/#src.core.base_factory.SingletonFactory.create","text":"Create or retrieve a singleton instance based on the given configuration. Parameters: configuration ( Any ) \u2013 The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any ( Any ) \u2013 A singleton instance associated with the provided configuration. Raises: ValueError \u2013 If the configuration is not an instance of the expected type. Source code in src/core/base_factory.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 @classmethod def create ( cls , configuration : Any ) -> Any : \"\"\" Create or retrieve a singleton instance based on the given configuration. Args: configuration (Any): The configuration object used to create/retrieve the instance. It has to be of the type specified by _configuration_class. Returns: Any: A singleton instance associated with the provided configuration. Raises: ValueError: If the configuration is not an instance of the expected type. \"\"\" if not isinstance ( configuration , cls . _configuration_class ): raise ValueError ( f \"Given configuration is { type ( configuration ) } , but must be of type type: { cls . _configuration_class } \" ) return cls . _create_singleton ( configuration )","title":"create"},{"location":"src/core/base_initializer/","text":"Base_initializer This module contains functionality related to the the base_initializer module for core . Base_initializer BaseInitializer Bases: ABC Abstract base class for configuration initializers. This class defines the interface for initializers that are responsible for retrieving and initializing configuration objects. Subclasses must implement the get_configuration method to provide a concrete initialization strategy. Attributes: _configuration_class \u2013 The class of the configuration object to be initialized. Source code in src/core/base_initializer.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class BaseInitializer ( ABC ): \"\"\"Abstract base class for configuration initializers. This class defines the interface for initializers that are responsible for retrieving and initializing configuration objects. Subclasses must implement the get_configuration method to provide a concrete initialization strategy. Attributes: _configuration_class: The class of the configuration object to be initialized. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ]): \"\"\"Initialize the BaseInitializer. Args: configuration_class: The configuration class to use for initialization. \"\"\" self . _configuration_class = configuration_class @abstractmethod def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass __init__ ( configuration_class ) Initialize the BaseInitializer. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 The configuration class to use for initialization. Source code in src/core/base_initializer.py 26 27 28 29 30 31 32 def __init__ ( self , configuration_class : Type [ BaseConfiguration ]): \"\"\"Initialize the BaseInitializer. Args: configuration_class: The configuration class to use for initialization. \"\"\" self . _configuration_class = configuration_class get_configuration () abstractmethod Retrieve the configuration instance. Returns: BaseConfiguration \u2013 The initialized configuration object. Raises: NotImplementedError \u2013 This method must be implemented by subclasses. Source code in src/core/base_initializer.py 34 35 36 37 38 39 40 41 42 43 44 @abstractmethod def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass BasePackageLoader Bases: ABC Abstract base class for dynamic package loading. This class defines the interface for package loaders that dynamically discover and load packages from specified locations. Subclasses must implement the load_packages method to provide a concrete package loading strategy. The class provides a helper method _load_packages that handles the common logic of dynamically importing modules. Attributes: logger \u2013 Logger instance used for logging package loading activities. Source code in src/core/base_initializer.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class BasePackageLoader ( ABC ): \"\"\"Abstract base class for dynamic package loading. This class defines the interface for package loaders that dynamically discover and load packages from specified locations. Subclasses must implement the load_packages method to provide a concrete package loading strategy. The class provides a helper method _load_packages that handles the common logic of dynamically importing modules. Attributes: logger: Logger instance used for logging package loading activities. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the BasePackageLoader. Args: logger: Logger instance for logging package loading activities. \"\"\" self . logger = logger @abstractmethod def load_packages ( self ) -> None : \"\"\"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass def _load_packages ( self , parent_packages : List [ str ]) -> None : \"\"\"Load packages from the specified parent packages. This method dynamically imports modules from the provided parent packages and calls their register() method to register components with the system. It skips the 'core' package and handles import errors gracefully. Args: parent_packages: List of parent package names to load modules from. \"\"\" for parent_package in parent_packages : self . logger . info ( f \"Loading { parent_package } packages...\" ) package_path = parent_package . replace ( \".\" , \"/\" ) for _ , name , is_package in pkgutil . iter_modules ([ package_path ]): if is_package and name != \"core\" : try : module_path = f \" { parent_package } . { name } \" module = importlib . import_module ( module_path ) module . register () self . logger . info ( f \"Loaded package: { name } .\" ) except ImportError as e : self . logger . error ( f \"Failed to load datasource package { name } : { e } .\" ) except Exception as e : self . logger . error ( f \"Failed to register package { name } : { e } .\" ) __init__ ( logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the BasePackageLoader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging package loading activities. Source code in src/core/base_initializer.py 59 60 61 62 63 64 65 66 67 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the BasePackageLoader. Args: logger: Logger instance for logging package loading activities. \"\"\" self . logger = logger load_packages () abstractmethod Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError \u2013 This method must be implemented by subclasses. Source code in src/core/base_initializer.py 69 70 71 72 73 74 75 76 77 78 79 @abstractmethod def load_packages ( self ) -> None : \"\"\"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass BasicInitializer Bases: BaseInitializer Common initializer for embedding, augmentation and evaluation processes. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. Source code in src/core/base_initializer.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class BasicInitializer ( BaseInitializer ): \"\"\"Common initializer for embedding, augmentation and evaluation processes. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ], package_loader : BasePackageLoader , ): \"\"\"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Args: configuration_class: The configuration class to use for initialization. package_loader: The package loader to use for loading packages. \"\"\" super () . __init__ ( configuration_class ) self . configuration_retriever : BaseConfigurationRetriever = None package_loader . load_packages () self . _init_configuration_retriever () def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. \"\"\" return self . configuration_retriever . get () def _init_configuration_retriever ( self ) -> None : \"\"\"Initialize the configuration retriever. Creates an appropriate configuration retriever based on the metadata configuration and assigns it to the configuration_retriever attribute. \"\"\" metadata = MetadataConfiguration () configuration_retriever_class = ConfiguratioRetriverRegistry . get ( on_prem = metadata . on_prem_config ) self . configuration_retriever = configuration_retriever_class ( configuration_class = self . _configuration_class , metadata = metadata ) __init__ ( configuration_class , package_loader ) Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 The configuration class to use for initialization. package_loader ( BasePackageLoader ) \u2013 The package loader to use for loading packages. Source code in src/core/base_initializer.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __init__ ( self , configuration_class : Type [ BaseConfiguration ], package_loader : BasePackageLoader , ): \"\"\"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Args: configuration_class: The configuration class to use for initialization. package_loader: The package loader to use for loading packages. \"\"\" super () . __init__ ( configuration_class ) self . configuration_retriever : BaseConfigurationRetriever = None package_loader . load_packages () self . _init_configuration_retriever () get_configuration () Retrieve the configuration instance. Returns: BaseConfiguration \u2013 The initialized configuration object. Source code in src/core/base_initializer.py 139 140 141 142 143 144 145 def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. \"\"\" return self . configuration_retriever . get ()","title":"Base_initializer"},{"location":"src/core/base_initializer/#base_initializer","text":"This module contains functionality related to the the base_initializer module for core .","title":"Base_initializer"},{"location":"src/core/base_initializer/#base_initializer_1","text":"","title":"Base_initializer"},{"location":"src/core/base_initializer/#src.core.base_initializer.BaseInitializer","text":"Bases: ABC Abstract base class for configuration initializers. This class defines the interface for initializers that are responsible for retrieving and initializing configuration objects. Subclasses must implement the get_configuration method to provide a concrete initialization strategy. Attributes: _configuration_class \u2013 The class of the configuration object to be initialized. Source code in src/core/base_initializer.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class BaseInitializer ( ABC ): \"\"\"Abstract base class for configuration initializers. This class defines the interface for initializers that are responsible for retrieving and initializing configuration objects. Subclasses must implement the get_configuration method to provide a concrete initialization strategy. Attributes: _configuration_class: The class of the configuration object to be initialized. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ]): \"\"\"Initialize the BaseInitializer. Args: configuration_class: The configuration class to use for initialization. \"\"\" self . _configuration_class = configuration_class @abstractmethod def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass","title":"BaseInitializer"},{"location":"src/core/base_initializer/#src.core.base_initializer.BaseInitializer.__init__","text":"Initialize the BaseInitializer. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 The configuration class to use for initialization. Source code in src/core/base_initializer.py 26 27 28 29 30 31 32 def __init__ ( self , configuration_class : Type [ BaseConfiguration ]): \"\"\"Initialize the BaseInitializer. Args: configuration_class: The configuration class to use for initialization. \"\"\" self . _configuration_class = configuration_class","title":"__init__"},{"location":"src/core/base_initializer/#src.core.base_initializer.BaseInitializer.get_configuration","text":"Retrieve the configuration instance. Returns: BaseConfiguration \u2013 The initialized configuration object. Raises: NotImplementedError \u2013 This method must be implemented by subclasses. Source code in src/core/base_initializer.py 34 35 36 37 38 39 40 41 42 43 44 @abstractmethod def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass","title":"get_configuration"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasePackageLoader","text":"Bases: ABC Abstract base class for dynamic package loading. This class defines the interface for package loaders that dynamically discover and load packages from specified locations. Subclasses must implement the load_packages method to provide a concrete package loading strategy. The class provides a helper method _load_packages that handles the common logic of dynamically importing modules. Attributes: logger \u2013 Logger instance used for logging package loading activities. Source code in src/core/base_initializer.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class BasePackageLoader ( ABC ): \"\"\"Abstract base class for dynamic package loading. This class defines the interface for package loaders that dynamically discover and load packages from specified locations. Subclasses must implement the load_packages method to provide a concrete package loading strategy. The class provides a helper method _load_packages that handles the common logic of dynamically importing modules. Attributes: logger: Logger instance used for logging package loading activities. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the BasePackageLoader. Args: logger: Logger instance for logging package loading activities. \"\"\" self . logger = logger @abstractmethod def load_packages ( self ) -> None : \"\"\"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass def _load_packages ( self , parent_packages : List [ str ]) -> None : \"\"\"Load packages from the specified parent packages. This method dynamically imports modules from the provided parent packages and calls their register() method to register components with the system. It skips the 'core' package and handles import errors gracefully. Args: parent_packages: List of parent package names to load modules from. \"\"\" for parent_package in parent_packages : self . logger . info ( f \"Loading { parent_package } packages...\" ) package_path = parent_package . replace ( \".\" , \"/\" ) for _ , name , is_package in pkgutil . iter_modules ([ package_path ]): if is_package and name != \"core\" : try : module_path = f \" { parent_package } . { name } \" module = importlib . import_module ( module_path ) module . register () self . logger . info ( f \"Loaded package: { name } .\" ) except ImportError as e : self . logger . error ( f \"Failed to load datasource package { name } : { e } .\" ) except Exception as e : self . logger . error ( f \"Failed to register package { name } : { e } .\" )","title":"BasePackageLoader"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasePackageLoader.__init__","text":"Initialize the BasePackageLoader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging package loading activities. Source code in src/core/base_initializer.py 59 60 61 62 63 64 65 66 67 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\"Initialize the BasePackageLoader. Args: logger: Logger instance for logging package loading activities. \"\"\" self . logger = logger","title":"__init__"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasePackageLoader.load_packages","text":"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError \u2013 This method must be implemented by subclasses. Source code in src/core/base_initializer.py 69 70 71 72 73 74 75 76 77 78 79 @abstractmethod def load_packages ( self ) -> None : \"\"\"Load packages dynamically. This method should be implemented by subclasses to provide the specific package loading logic needed for their application context. Raises: NotImplementedError: This method must be implemented by subclasses. \"\"\" pass","title":"load_packages"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasicInitializer","text":"Bases: BaseInitializer Common initializer for embedding, augmentation and evaluation processes. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. Source code in src/core/base_initializer.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class BasicInitializer ( BaseInitializer ): \"\"\"Common initializer for embedding, augmentation and evaluation processes. Multiple components are used in the embedding, augmentation and evaluation processes. To avoid code duplication, this initializer is used to bind the components to the injector. It is intended to be subclassed by the specific initializers for each process. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ], package_loader : BasePackageLoader , ): \"\"\"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Args: configuration_class: The configuration class to use for initialization. package_loader: The package loader to use for loading packages. \"\"\" super () . __init__ ( configuration_class ) self . configuration_retriever : BaseConfigurationRetriever = None package_loader . load_packages () self . _init_configuration_retriever () def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. \"\"\" return self . configuration_retriever . get () def _init_configuration_retriever ( self ) -> None : \"\"\"Initialize the configuration retriever. Creates an appropriate configuration retriever based on the metadata configuration and assigns it to the configuration_retriever attribute. \"\"\" metadata = MetadataConfiguration () configuration_retriever_class = ConfiguratioRetriverRegistry . get ( on_prem = metadata . on_prem_config ) self . configuration_retriever = configuration_retriever_class ( configuration_class = self . _configuration_class , metadata = metadata )","title":"BasicInitializer"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasicInitializer.__init__","text":"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 The configuration class to use for initialization. package_loader ( BasePackageLoader ) \u2013 The package loader to use for loading packages. Source code in src/core/base_initializer.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __init__ ( self , configuration_class : Type [ BaseConfiguration ], package_loader : BasePackageLoader , ): \"\"\"Initialize the BasicInitializer. Loads packages and initializes the configuration retriever. Args: configuration_class: The configuration class to use for initialization. package_loader: The package loader to use for loading packages. \"\"\" super () . __init__ ( configuration_class ) self . configuration_retriever : BaseConfigurationRetriever = None package_loader . load_packages () self . _init_configuration_retriever ()","title":"__init__"},{"location":"src/core/base_initializer/#src.core.base_initializer.BasicInitializer.get_configuration","text":"Retrieve the configuration instance. Returns: BaseConfiguration \u2013 The initialized configuration object. Source code in src/core/base_initializer.py 139 140 141 142 143 144 145 def get_configuration ( self ) -> BaseConfiguration : \"\"\"Retrieve the configuration instance. Returns: The initialized configuration object. \"\"\" return self . configuration_retriever . get ()","title":"get_configuration"},{"location":"src/core/configuration_retrievers/","text":"Configuration_retrievers This module contains functionality related to the the configuration_retrievers module for core . Configuration_retrievers BaseConfigurationRetriever Bases: ABC Abstract base class for configuration retrieval. This class defines the common interface and functionality for retrieving application configurations from various sources. Concrete implementations should extend this class to provide specific retrieval mechanisms. Attributes: CONFIGURATIONS_DIR ( str ) \u2013 Directory where configuration files are stored. Source code in src/core/configuration_retrievers.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class BaseConfigurationRetriever ( ABC ): \"\"\"Abstract base class for configuration retrieval. This class defines the common interface and functionality for retrieving application configurations from various sources. Concrete implementations should extend this class to provide specific retrieval mechanisms. Attributes: CONFIGURATIONS_DIR (str): Directory where configuration files are stored. \"\"\" CONFIGURATIONS_DIR = \"configurations\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ], metadata : MetadataConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the configuration retriever. Args: configuration_class: Class of the configuration object to be retrieved. metadata: Applicaton metadata. logger: Logger instance for this class. \"\"\" self . metadata = metadata self . logger = logger self . _configuration_class = configuration_class self . configuration = None @abstractmethod def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve the configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object of `_configuration_class` type. \"\"\" pass def _parse_configuration ( self , configuration_json : dict , verbose = True ): \"\"\"Parse JSON configuration into a configuration object. Args: configuration_json: Dictionary containing the configuration data. verbose: Whether to log the parsed configuration. Returns: Parsed configuration object of `_configuration_class` type.. \"\"\" secrets_filepath = self . _get_secrets_filepath () configuration = self . _configuration_class . model_validate_json ( configuration_json , context = { \"secrets_file\" : secrets_filepath } ) configuration . metadata = self . metadata if verbose : self . logger . info ( f \"::Environment: { self . metadata . environment } \" ) self . logger . info ( configuration . model_dump_json ( indent = 4 )) return configuration def _get_secrets_filepath ( self ) -> str : \"\"\"Get the path to the secrets file based on environment. Returns: Path to the secrets file. \"\"\" return f \" { OnPremConfigurationRetriever . CONFIGURATIONS_DIR } /secrets. { self . metadata . environment . value } .env\" __init__ ( configuration_class , metadata , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the configuration retriever. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 Class of the configuration object to be retrieved. metadata ( MetadataConfiguration ) \u2013 Applicaton metadata. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for this class. Source code in src/core/configuration_retrievers.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , configuration_class : Type [ BaseConfiguration ], metadata : MetadataConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the configuration retriever. Args: configuration_class: Class of the configuration object to be retrieved. metadata: Applicaton metadata. logger: Logger instance for this class. \"\"\" self . metadata = metadata self . logger = logger self . _configuration_class = configuration_class self . configuration = None get ( verbose = True ) abstractmethod Retrieve the configuration. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Returns: BaseConfiguration \u2013 The parsed configuration object of _configuration_class type. Source code in src/core/configuration_retrievers.py 40 41 42 43 44 45 46 47 48 49 50 @abstractmethod def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve the configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object of `_configuration_class` type. \"\"\" pass ConfiguratioRetriverRegistry Registry for configuration retrievers. Provides factory methods to get the appropriate configuration retriever based on context. Source code in src/core/configuration_retrievers.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class ConfiguratioRetriverRegistry : \"\"\"Registry for configuration retrievers. Provides factory methods to get the appropriate configuration retriever based on context. \"\"\" def get ( on_prem : bool ) -> BaseConfigurationRetriever : \"\"\"Get the appropriate configuration retriever class. Args: on_prem: Whether to use on-premise configuration or remote. Returns: The appropriate configuration retriever class. \"\"\" if on_prem : return OnPremConfigurationRetriever else : return RemoteConfigurationRetriever get ( on_prem ) Get the appropriate configuration retriever class. Parameters: on_prem ( bool ) \u2013 Whether to use on-premise configuration or remote. Returns: BaseConfigurationRetriever \u2013 The appropriate configuration retriever class. Source code in src/core/configuration_retrievers.py 159 160 161 162 163 164 165 166 167 168 169 170 171 def get ( on_prem : bool ) -> BaseConfigurationRetriever : \"\"\"Get the appropriate configuration retriever class. Args: on_prem: Whether to use on-premise configuration or remote. Returns: The appropriate configuration retriever class. \"\"\" if on_prem : return OnPremConfigurationRetriever else : return RemoteConfigurationRetriever OnPremConfigurationRetriever Bases: BaseConfigurationRetriever Configuration retriever for on-premise file sources. Retrieves configurations from local JSON files. Source code in src/core/configuration_retrievers.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class OnPremConfigurationRetriever ( BaseConfigurationRetriever ): \"\"\"Configuration retriever for on-premise file sources. Retrieves configurations from local JSON files. \"\"\" def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" if not self . configuration : self . configuration = self . _get ( verbose ) return self . configuration def _get ( self , verbose ) -> BaseConfiguration : \"\"\"Internal method to retrieve and parse configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" configuration_filepath = self . _get_configuration_filepath () with open ( configuration_filepath ) as f : configuration_json = f . read () return self . _parse_configuration ( configuration_json = configuration_json , verbose = verbose ) def _get_configuration_filepath ( self ) -> str : \"\"\"Get the path to the configuration file based on environment. Returns: Path to the configuration JSON file. \"\"\" return f \" { OnPremConfigurationRetriever . CONFIGURATIONS_DIR } /configuration. { self . metadata . environment . value } .json\" get ( verbose = True ) Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Returns: BaseConfiguration \u2013 The parsed configuration object. Source code in src/core/configuration_retrievers.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" if not self . configuration : self . configuration = self . _get ( verbose ) return self . configuration RemoteConfigurationRetriever Bases: BaseConfigurationRetriever Configuration retriever for remote sources. Retrieves configuration from remote sources like config servers or cloud storage. Currently not implemented. Source code in src/core/configuration_retrievers.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class RemoteConfigurationRetriever ( BaseConfigurationRetriever ): \"\"\"Configuration retriever for remote sources. Retrieves configuration from remote sources like config servers or cloud storage. Currently not implemented. \"\"\" def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from a remote source. Args: verbose: Whether to log detailed information during retrieval. Raises: NotImplementedError: This functionality is not yet implemented. Returns: The configuration object. \"\"\" raise NotImplementedError ( \"Remote configuration is not implemented yet.\" ) get ( verbose = True ) Retrieve configuration from a remote source. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Raises: NotImplementedError \u2013 This functionality is not yet implemented. Returns: BaseConfiguration \u2013 The configuration object. Source code in src/core/configuration_retrievers.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from a remote source. Args: verbose: Whether to log detailed information during retrieval. Raises: NotImplementedError: This functionality is not yet implemented. Returns: The configuration object. \"\"\" raise NotImplementedError ( \"Remote configuration is not implemented yet.\" )","title":"Configuration_retrievers"},{"location":"src/core/configuration_retrievers/#configuration_retrievers","text":"This module contains functionality related to the the configuration_retrievers module for core .","title":"Configuration_retrievers"},{"location":"src/core/configuration_retrievers/#configuration_retrievers_1","text":"","title":"Configuration_retrievers"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.BaseConfigurationRetriever","text":"Bases: ABC Abstract base class for configuration retrieval. This class defines the common interface and functionality for retrieving application configurations from various sources. Concrete implementations should extend this class to provide specific retrieval mechanisms. Attributes: CONFIGURATIONS_DIR ( str ) \u2013 Directory where configuration files are stored. Source code in src/core/configuration_retrievers.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class BaseConfigurationRetriever ( ABC ): \"\"\"Abstract base class for configuration retrieval. This class defines the common interface and functionality for retrieving application configurations from various sources. Concrete implementations should extend this class to provide specific retrieval mechanisms. Attributes: CONFIGURATIONS_DIR (str): Directory where configuration files are stored. \"\"\" CONFIGURATIONS_DIR = \"configurations\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ], metadata : MetadataConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the configuration retriever. Args: configuration_class: Class of the configuration object to be retrieved. metadata: Applicaton metadata. logger: Logger instance for this class. \"\"\" self . metadata = metadata self . logger = logger self . _configuration_class = configuration_class self . configuration = None @abstractmethod def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve the configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object of `_configuration_class` type. \"\"\" pass def _parse_configuration ( self , configuration_json : dict , verbose = True ): \"\"\"Parse JSON configuration into a configuration object. Args: configuration_json: Dictionary containing the configuration data. verbose: Whether to log the parsed configuration. Returns: Parsed configuration object of `_configuration_class` type.. \"\"\" secrets_filepath = self . _get_secrets_filepath () configuration = self . _configuration_class . model_validate_json ( configuration_json , context = { \"secrets_file\" : secrets_filepath } ) configuration . metadata = self . metadata if verbose : self . logger . info ( f \"::Environment: { self . metadata . environment } \" ) self . logger . info ( configuration . model_dump_json ( indent = 4 )) return configuration def _get_secrets_filepath ( self ) -> str : \"\"\"Get the path to the secrets file based on environment. Returns: Path to the secrets file. \"\"\" return f \" { OnPremConfigurationRetriever . CONFIGURATIONS_DIR } /secrets. { self . metadata . environment . value } .env\"","title":"BaseConfigurationRetriever"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.BaseConfigurationRetriever.__init__","text":"Initialize the configuration retriever. Parameters: configuration_class ( Type [ BaseConfiguration ] ) \u2013 Class of the configuration object to be retrieved. metadata ( MetadataConfiguration ) \u2013 Applicaton metadata. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for this class. Source code in src/core/configuration_retrievers.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , configuration_class : Type [ BaseConfiguration ], metadata : MetadataConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the configuration retriever. Args: configuration_class: Class of the configuration object to be retrieved. metadata: Applicaton metadata. logger: Logger instance for this class. \"\"\" self . metadata = metadata self . logger = logger self . _configuration_class = configuration_class self . configuration = None","title":"__init__"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.BaseConfigurationRetriever.get","text":"Retrieve the configuration. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Returns: BaseConfiguration \u2013 The parsed configuration object of _configuration_class type. Source code in src/core/configuration_retrievers.py 40 41 42 43 44 45 46 47 48 49 50 @abstractmethod def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve the configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object of `_configuration_class` type. \"\"\" pass","title":"get"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.ConfiguratioRetriverRegistry","text":"Registry for configuration retrievers. Provides factory methods to get the appropriate configuration retriever based on context. Source code in src/core/configuration_retrievers.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 class ConfiguratioRetriverRegistry : \"\"\"Registry for configuration retrievers. Provides factory methods to get the appropriate configuration retriever based on context. \"\"\" def get ( on_prem : bool ) -> BaseConfigurationRetriever : \"\"\"Get the appropriate configuration retriever class. Args: on_prem: Whether to use on-premise configuration or remote. Returns: The appropriate configuration retriever class. \"\"\" if on_prem : return OnPremConfigurationRetriever else : return RemoteConfigurationRetriever","title":"ConfiguratioRetriverRegistry"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.ConfiguratioRetriverRegistry.get","text":"Get the appropriate configuration retriever class. Parameters: on_prem ( bool ) \u2013 Whether to use on-premise configuration or remote. Returns: BaseConfigurationRetriever \u2013 The appropriate configuration retriever class. Source code in src/core/configuration_retrievers.py 159 160 161 162 163 164 165 166 167 168 169 170 171 def get ( on_prem : bool ) -> BaseConfigurationRetriever : \"\"\"Get the appropriate configuration retriever class. Args: on_prem: Whether to use on-premise configuration or remote. Returns: The appropriate configuration retriever class. \"\"\" if on_prem : return OnPremConfigurationRetriever else : return RemoteConfigurationRetriever","title":"get"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.OnPremConfigurationRetriever","text":"Bases: BaseConfigurationRetriever Configuration retriever for on-premise file sources. Retrieves configurations from local JSON files. Source code in src/core/configuration_retrievers.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class OnPremConfigurationRetriever ( BaseConfigurationRetriever ): \"\"\"Configuration retriever for on-premise file sources. Retrieves configurations from local JSON files. \"\"\" def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" if not self . configuration : self . configuration = self . _get ( verbose ) return self . configuration def _get ( self , verbose ) -> BaseConfiguration : \"\"\"Internal method to retrieve and parse configuration. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" configuration_filepath = self . _get_configuration_filepath () with open ( configuration_filepath ) as f : configuration_json = f . read () return self . _parse_configuration ( configuration_json = configuration_json , verbose = verbose ) def _get_configuration_filepath ( self ) -> str : \"\"\"Get the path to the configuration file based on environment. Returns: Path to the configuration JSON file. \"\"\" return f \" { OnPremConfigurationRetriever . CONFIGURATIONS_DIR } /configuration. { self . metadata . environment . value } .json\"","title":"OnPremConfigurationRetriever"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.OnPremConfigurationRetriever.get","text":"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Returns: BaseConfiguration \u2013 The parsed configuration object. Source code in src/core/configuration_retrievers.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from local files. This method implements caching to avoid re-reading configuration files. Args: verbose: Whether to log detailed information during retrieval. Returns: The parsed configuration object. \"\"\" if not self . configuration : self . configuration = self . _get ( verbose ) return self . configuration","title":"get"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.RemoteConfigurationRetriever","text":"Bases: BaseConfigurationRetriever Configuration retriever for remote sources. Retrieves configuration from remote sources like config servers or cloud storage. Currently not implemented. Source code in src/core/configuration_retrievers.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class RemoteConfigurationRetriever ( BaseConfigurationRetriever ): \"\"\"Configuration retriever for remote sources. Retrieves configuration from remote sources like config servers or cloud storage. Currently not implemented. \"\"\" def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from a remote source. Args: verbose: Whether to log detailed information during retrieval. Raises: NotImplementedError: This functionality is not yet implemented. Returns: The configuration object. \"\"\" raise NotImplementedError ( \"Remote configuration is not implemented yet.\" )","title":"RemoteConfigurationRetriever"},{"location":"src/core/configuration_retrievers/#src.core.configuration_retrievers.RemoteConfigurationRetriever.get","text":"Retrieve configuration from a remote source. Parameters: verbose ( bool , default: True ) \u2013 Whether to log detailed information during retrieval. Raises: NotImplementedError \u2013 This functionality is not yet implemented. Returns: BaseConfiguration \u2013 The configuration object. Source code in src/core/configuration_retrievers.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def get ( self , verbose : bool = True ) -> BaseConfiguration : \"\"\"Retrieve configuration from a remote source. Args: verbose: Whether to log detailed information during retrieval. Raises: NotImplementedError: This functionality is not yet implemented. Returns: The configuration object. \"\"\" raise NotImplementedError ( \"Remote configuration is not implemented yet.\" )","title":"get"},{"location":"src/core/logger/","text":"Logger This module contains functionality related to the the logger module for core . Logger LoggerConfiguration Utility class for configuring application-wide logging. This class provides methods to create consistently configured loggers, manage log levels, and control the behavior of third-party loggers. It uses the log level settings from MetadataConfiguration by default. Attributes: LOG_FORMAT \u2013 The format string used for log messages log_level ( LogLevelName ) \u2013 The application-wide log level (cached from configuration) Source code in src/core/logger.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LoggerConfiguration : \"\"\"Utility class for configuring application-wide logging. This class provides methods to create consistently configured loggers, manage log levels, and control the behavior of third-party loggers. It uses the log level settings from MetadataConfiguration by default. Attributes: LOG_FORMAT: The format string used for log messages log_level: The application-wide log level (cached from configuration) \"\"\" LOG_FORMAT = \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" log_level : LogLevelName = None @classmethod def get_logger ( cls , name : str , log_level : LogLevelName = None , propagate : bool = False ) -> logging . Logger : \"\"\"Create a logger with specified configuration. Args: name: Name for the logger (typically __name__) log_level: Logging level propagate: Whether to propagate to parent loggers Returns: logging.Logger: Configured logger instance \"\"\" if not log_level : log_level = cls . get_log_level () logger = logging . getLogger ( name ) logger . setLevel ( cls . log_level . value_as_int ) logger . propagate = propagate if not logger . handlers : formatter = logging . Formatter ( cls . LOG_FORMAT ) handler = logging . StreamHandler ( sys . stdout ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger @classmethod def get_log_level ( cls ) -> LogLevelName : \"\"\"Get the log level for the logger. Returns: LogLevelName: The log level for the logger \"\"\" if not cls . log_level : metadata = MetadataConfiguration () cls . log_level = metadata . log_level return cls . log_level @staticmethod def mute_logs (): \"\"\"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level \"\"\" # Warning logs logging . getLogger ( \"httpx\" ) . setLevel ( logging . WARNING ) # Error logs logging . getLogger ( \"pdfminer\" ) . setLevel ( logging . ERROR ) get_log_level () classmethod Get the log level for the logger. Returns: LogLevelName ( LogLevelName ) \u2013 The log level for the logger Source code in src/core/logger.py 52 53 54 55 56 57 58 59 60 61 62 @classmethod def get_log_level ( cls ) -> LogLevelName : \"\"\"Get the log level for the logger. Returns: LogLevelName: The log level for the logger \"\"\" if not cls . log_level : metadata = MetadataConfiguration () cls . log_level = metadata . log_level return cls . log_level get_logger ( name , log_level = None , propagate = False ) classmethod Create a logger with specified configuration. Parameters: name ( str ) \u2013 Name for the logger (typically name ) log_level ( LogLevelName , default: None ) \u2013 Logging level propagate ( bool , default: False ) \u2013 Whether to propagate to parent loggers Returns: Logger \u2013 logging.Logger: Configured logger instance Source code in src/core/logger.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @classmethod def get_logger ( cls , name : str , log_level : LogLevelName = None , propagate : bool = False ) -> logging . Logger : \"\"\"Create a logger with specified configuration. Args: name: Name for the logger (typically __name__) log_level: Logging level propagate: Whether to propagate to parent loggers Returns: logging.Logger: Configured logger instance \"\"\" if not log_level : log_level = cls . get_log_level () logger = logging . getLogger ( name ) logger . setLevel ( cls . log_level . value_as_int ) logger . propagate = propagate if not logger . handlers : formatter = logging . Formatter ( cls . LOG_FORMAT ) handler = logging . StreamHandler ( sys . stdout ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger mute_logs () staticmethod Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level Source code in src/core/logger.py 64 65 66 67 68 69 70 71 72 73 74 75 76 @staticmethod def mute_logs (): \"\"\"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level \"\"\" # Warning logs logging . getLogger ( \"httpx\" ) . setLevel ( logging . WARNING ) # Error logs logging . getLogger ( \"pdfminer\" ) . setLevel ( logging . ERROR )","title":"Logger"},{"location":"src/core/logger/#logger","text":"This module contains functionality related to the the logger module for core .","title":"Logger"},{"location":"src/core/logger/#logger_1","text":"","title":"Logger"},{"location":"src/core/logger/#src.core.logger.LoggerConfiguration","text":"Utility class for configuring application-wide logging. This class provides methods to create consistently configured loggers, manage log levels, and control the behavior of third-party loggers. It uses the log level settings from MetadataConfiguration by default. Attributes: LOG_FORMAT \u2013 The format string used for log messages log_level ( LogLevelName ) \u2013 The application-wide log level (cached from configuration) Source code in src/core/logger.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class LoggerConfiguration : \"\"\"Utility class for configuring application-wide logging. This class provides methods to create consistently configured loggers, manage log levels, and control the behavior of third-party loggers. It uses the log level settings from MetadataConfiguration by default. Attributes: LOG_FORMAT: The format string used for log messages log_level: The application-wide log level (cached from configuration) \"\"\" LOG_FORMAT = \" %(asctime)s - %(name)s - %(levelname)s - %(message)s \" log_level : LogLevelName = None @classmethod def get_logger ( cls , name : str , log_level : LogLevelName = None , propagate : bool = False ) -> logging . Logger : \"\"\"Create a logger with specified configuration. Args: name: Name for the logger (typically __name__) log_level: Logging level propagate: Whether to propagate to parent loggers Returns: logging.Logger: Configured logger instance \"\"\" if not log_level : log_level = cls . get_log_level () logger = logging . getLogger ( name ) logger . setLevel ( cls . log_level . value_as_int ) logger . propagate = propagate if not logger . handlers : formatter = logging . Formatter ( cls . LOG_FORMAT ) handler = logging . StreamHandler ( sys . stdout ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger @classmethod def get_log_level ( cls ) -> LogLevelName : \"\"\"Get the log level for the logger. Returns: LogLevelName: The log level for the logger \"\"\" if not cls . log_level : metadata = MetadataConfiguration () cls . log_level = metadata . log_level return cls . log_level @staticmethod def mute_logs (): \"\"\"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level \"\"\" # Warning logs logging . getLogger ( \"httpx\" ) . setLevel ( logging . WARNING ) # Error logs logging . getLogger ( \"pdfminer\" ) . setLevel ( logging . ERROR )","title":"LoggerConfiguration"},{"location":"src/core/logger/#src.core.logger.LoggerConfiguration.get_log_level","text":"Get the log level for the logger. Returns: LogLevelName ( LogLevelName ) \u2013 The log level for the logger Source code in src/core/logger.py 52 53 54 55 56 57 58 59 60 61 62 @classmethod def get_log_level ( cls ) -> LogLevelName : \"\"\"Get the log level for the logger. Returns: LogLevelName: The log level for the logger \"\"\" if not cls . log_level : metadata = MetadataConfiguration () cls . log_level = metadata . log_level return cls . log_level","title":"get_log_level"},{"location":"src/core/logger/#src.core.logger.LoggerConfiguration.get_logger","text":"Create a logger with specified configuration. Parameters: name ( str ) \u2013 Name for the logger (typically name ) log_level ( LogLevelName , default: None ) \u2013 Logging level propagate ( bool , default: False ) \u2013 Whether to propagate to parent loggers Returns: Logger \u2013 logging.Logger: Configured logger instance Source code in src/core/logger.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @classmethod def get_logger ( cls , name : str , log_level : LogLevelName = None , propagate : bool = False ) -> logging . Logger : \"\"\"Create a logger with specified configuration. Args: name: Name for the logger (typically __name__) log_level: Logging level propagate: Whether to propagate to parent loggers Returns: logging.Logger: Configured logger instance \"\"\" if not log_level : log_level = cls . get_log_level () logger = logging . getLogger ( name ) logger . setLevel ( cls . log_level . value_as_int ) logger . propagate = propagate if not logger . handlers : formatter = logging . Formatter ( cls . LOG_FORMAT ) handler = logging . StreamHandler ( sys . stdout ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) return logger","title":"get_logger"},{"location":"src/core/logger/#src.core.logger.LoggerConfiguration.mute_logs","text":"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level Source code in src/core/logger.py 64 65 66 67 68 69 70 71 72 73 74 75 76 @staticmethod def mute_logs (): \"\"\"Suppress verbose logs from third-party libraries. Sets appropriate log levels for various libraries to reduce log noise: - Sets 'httpx' to WARNING level - Sets 'pdfminer' to ERROR level \"\"\" # Warning logs logging . getLogger ( \"httpx\" ) . setLevel ( logging . WARNING ) # Error logs logging . getLogger ( \"pdfminer\" ) . setLevel ( logging . ERROR )","title":"mute_logs"},{"location":"src/embedding/bootstrap/initializer/","text":"Initializer This module contains functionality related to the the initializer module for embedding.bootstrap . Initializer EmbeddingInitializer Bases: ExtractionInitializer Initializer for embedding, augmentation and evaluation processes. Extends the ExtractionInitializer to provide specialized initialization for embedding-related functionality. Responsible for binding embedding components to the dependency injection container and preparing the environment for embedding, augmentation, and evaluation processes. Attributes: configuration \u2013 EmbeddingConfiguration object containing configuration parameters configuration_json \u2013 JSON string representation of the EmbeddingConfiguration package_loader \u2013 Loader responsible for importing required packages Source code in src/embedding/bootstrap/initializer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class EmbeddingInitializer ( ExtractionInitializer ): \"\"\"Initializer for embedding, augmentation and evaluation processes. Extends the ExtractionInitializer to provide specialized initialization for embedding-related functionality. Responsible for binding embedding components to the dependency injection container and preparing the environment for embedding, augmentation, and evaluation processes. Attributes: configuration: EmbeddingConfiguration object containing configuration parameters configuration_json: JSON string representation of the EmbeddingConfiguration package_loader: Loader responsible for importing required packages \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EmbeddingConfiguration , package_loader : BasePackageLoader = EmbeddingPackageLoader (), ): super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) EmbeddingPackageLoader Bases: ExtractionPackageLoader Package loader for embedding components. Extends the ExtractionPackageLoader to include embedding-specific packages. Responsible for dynamically loading embedding-related modules into the application. Attributes: logger \u2013 Logger instance for logging package loading operations Source code in src/embedding/bootstrap/initializer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class EmbeddingPackageLoader ( ExtractionPackageLoader ): \"\"\"Package loader for embedding components. Extends the ExtractionPackageLoader to include embedding-specific packages. Responsible for dynamically loading embedding-related modules into the application. Attributes: logger: Logger instance for logging package loading operations \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.embedding.vector_stores\" , \"src.embedding.embedding_models\" , \"src.embedding.splitters\" , \"src.embedding.embedders\" , \"src.embedding.orchestrators\" , ] ) load_packages () Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. Source code in src/embedding/bootstrap/initializer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def load_packages ( self ) -> None : \"\"\"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.embedding.vector_stores\" , \"src.embedding.embedding_models\" , \"src.embedding.splitters\" , \"src.embedding.embedders\" , \"src.embedding.orchestrators\" , ] )","title":"Initializer"},{"location":"src/embedding/bootstrap/initializer/#initializer","text":"This module contains functionality related to the the initializer module for embedding.bootstrap .","title":"Initializer"},{"location":"src/embedding/bootstrap/initializer/#initializer_1","text":"","title":"Initializer"},{"location":"src/embedding/bootstrap/initializer/#src.embedding.bootstrap.initializer.EmbeddingInitializer","text":"Bases: ExtractionInitializer Initializer for embedding, augmentation and evaluation processes. Extends the ExtractionInitializer to provide specialized initialization for embedding-related functionality. Responsible for binding embedding components to the dependency injection container and preparing the environment for embedding, augmentation, and evaluation processes. Attributes: configuration \u2013 EmbeddingConfiguration object containing configuration parameters configuration_json \u2013 JSON string representation of the EmbeddingConfiguration package_loader \u2013 Loader responsible for importing required packages Source code in src/embedding/bootstrap/initializer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class EmbeddingInitializer ( ExtractionInitializer ): \"\"\"Initializer for embedding, augmentation and evaluation processes. Extends the ExtractionInitializer to provide specialized initialization for embedding-related functionality. Responsible for binding embedding components to the dependency injection container and preparing the environment for embedding, augmentation, and evaluation processes. Attributes: configuration: EmbeddingConfiguration object containing configuration parameters configuration_json: JSON string representation of the EmbeddingConfiguration package_loader: Loader responsible for importing required packages \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EmbeddingConfiguration , package_loader : BasePackageLoader = EmbeddingPackageLoader (), ): super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , )","title":"EmbeddingInitializer"},{"location":"src/embedding/bootstrap/initializer/#src.embedding.bootstrap.initializer.EmbeddingPackageLoader","text":"Bases: ExtractionPackageLoader Package loader for embedding components. Extends the ExtractionPackageLoader to include embedding-specific packages. Responsible for dynamically loading embedding-related modules into the application. Attributes: logger \u2013 Logger instance for logging package loading operations Source code in src/embedding/bootstrap/initializer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class EmbeddingPackageLoader ( ExtractionPackageLoader ): \"\"\"Package loader for embedding components. Extends the ExtractionPackageLoader to include embedding-specific packages. Responsible for dynamically loading embedding-related modules into the application. Attributes: logger: Logger instance for logging package loading operations \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.embedding.vector_stores\" , \"src.embedding.embedding_models\" , \"src.embedding.splitters\" , \"src.embedding.embedders\" , \"src.embedding.orchestrators\" , ] )","title":"EmbeddingPackageLoader"},{"location":"src/embedding/bootstrap/initializer/#src.embedding.bootstrap.initializer.EmbeddingPackageLoader.load_packages","text":"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. Source code in src/embedding/bootstrap/initializer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def load_packages ( self ) -> None : \"\"\"Loads all required packages for embedding functionality. First loads extraction packages via the parent class, then loads embedding-specific packages including vector stores, embedding models, text splitters, embedders, and orchestrators. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.embedding.vector_stores\" , \"src.embedding.embedding_models\" , \"src.embedding.splitters\" , \"src.embedding.embedders\" , \"src.embedding.orchestrators\" , ] )","title":"load_packages"},{"location":"src/embedding/bootstrap/configuration/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.bootstrap.configuration . Configuration EmbedderName Bases: str , Enum Enumeration of supported embedder implementations. Currently supports: - BASIC: The default basic embedder Source code in src/embedding/bootstrap/configuration/configuration.py 29 30 31 32 33 34 35 36 37 class EmbedderName ( str , Enum ): \"\"\" Enumeration of supported embedder implementations. Currently supports: - BASIC: The default basic embedder \"\"\" BASIC = \"basic\" EmbeddingConfiguration Bases: ExtractionConfiguration Complete configuration for the embedding process. Extends the extraction configuration with embedding-specific settings, creating a comprehensive configuration for the entire embedding pipeline. Source code in src/embedding/bootstrap/configuration/configuration.py 103 104 105 106 107 108 109 110 111 112 113 class EmbeddingConfiguration ( ExtractionConfiguration ): \"\"\" Complete configuration for the embedding process. Extends the extraction configuration with embedding-specific settings, creating a comprehensive configuration for the entire embedding pipeline. \"\"\" embedding : _EmbeddingConfiguration = Field ( ... , description = \"Configuration of the embedding process.\" ) EmbeddingOrchestratorName Bases: str , Enum Enumeration of supported embedding orchestrator types. Currently supports: - BASIC: The default basic orchestration strategy Source code in src/embedding/bootstrap/configuration/configuration.py 18 19 20 21 22 23 24 25 26 class EmbeddingOrchestratorName ( str , Enum ): \"\"\" Enumeration of supported embedding orchestrator types. Currently supports: - BASIC: The default basic orchestration strategy \"\"\" BASIC = \"basic\"","title":"Configuration"},{"location":"src/embedding/bootstrap/configuration/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.bootstrap.configuration .","title":"Configuration"},{"location":"src/embedding/bootstrap/configuration/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/bootstrap/configuration/configuration/#src.embedding.bootstrap.configuration.configuration.EmbedderName","text":"Bases: str , Enum Enumeration of supported embedder implementations. Currently supports: - BASIC: The default basic embedder Source code in src/embedding/bootstrap/configuration/configuration.py 29 30 31 32 33 34 35 36 37 class EmbedderName ( str , Enum ): \"\"\" Enumeration of supported embedder implementations. Currently supports: - BASIC: The default basic embedder \"\"\" BASIC = \"basic\"","title":"EmbedderName"},{"location":"src/embedding/bootstrap/configuration/configuration/#src.embedding.bootstrap.configuration.configuration.EmbeddingConfiguration","text":"Bases: ExtractionConfiguration Complete configuration for the embedding process. Extends the extraction configuration with embedding-specific settings, creating a comprehensive configuration for the entire embedding pipeline. Source code in src/embedding/bootstrap/configuration/configuration.py 103 104 105 106 107 108 109 110 111 112 113 class EmbeddingConfiguration ( ExtractionConfiguration ): \"\"\" Complete configuration for the embedding process. Extends the extraction configuration with embedding-specific settings, creating a comprehensive configuration for the entire embedding pipeline. \"\"\" embedding : _EmbeddingConfiguration = Field ( ... , description = \"Configuration of the embedding process.\" )","title":"EmbeddingConfiguration"},{"location":"src/embedding/bootstrap/configuration/configuration/#src.embedding.bootstrap.configuration.configuration.EmbeddingOrchestratorName","text":"Bases: str , Enum Enumeration of supported embedding orchestrator types. Currently supports: - BASIC: The default basic orchestration strategy Source code in src/embedding/bootstrap/configuration/configuration.py 18 19 20 21 22 23 24 25 26 class EmbeddingOrchestratorName ( str , Enum ): \"\"\" Enumeration of supported embedding orchestrator types. Currently supports: - BASIC: The default basic orchestration strategy \"\"\" BASIC = \"basic\"","title":"EmbeddingOrchestratorName"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/","text":"Embedding_model_configuration This module contains functionality related to the the embedding_model_configuration module for embedding.bootstrap.configuration . Embedding_model_configuration EmbeddingModelConfiguration Bases: BaseConfigurationWithSecrets Configuration class for embedding models. This class defines the necessary parameters and settings for configuring an embedding model, including provider information, model name, and tokenization settings. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class EmbeddingModelConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Configuration class for embedding models. This class defines the necessary parameters and settings for configuring an embedding model, including provider information, model name, and tokenization settings. \"\"\" provider : EmbeddingModelProviderName = Field ( ... , description = \"The provider of the embedding model.\" ) name : str = Field ( ... , description = \"The name of the embedding model.\" ) tokenizer_name : str = Field ( ... , description = \"The name of the tokenizer used by the embedding model.\" , ) batch_size : int = Field ( 64 , description = \"The batch size for embedding.\" ) splitter : Any = Field ( None , description = \"The splitter configuration for the embedding model.\" ) @field_validator ( \"splitter\" ) @classmethod def _validate_splitter ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate the splitter configuration. This method ensures that the provided splitter configuration is valid according to the SplitterConfigurationRegistry. Args: value: The splitter configuration value to validate. info: Validation context information. Returns: The validated splitter configuration. \"\"\" return super () . _validate ( value , info = info , registry = SplitterConfigurationRegistry , ) EmbeddingModelConfigurationRegistry Bases: ConfigurationRegistry Registry for embedding model configurations. This registry maps embedding model provider names to their respective configuration classes. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 69 70 71 72 73 74 75 76 class EmbeddingModelConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for embedding model configurations. This registry maps embedding model provider names to their respective configuration classes. \"\"\" _key_class : Type = EmbeddingModelProviderName EmbeddingModelProviderName Bases: str , Enum Enumeration of supported embedding model providers. This enum lists all the providers that can be used for embedding models. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 13 14 15 16 17 18 19 20 21 class EmbeddingModelProviderName ( str , Enum ): \"\"\"Enumeration of supported embedding model providers. This enum lists all the providers that can be used for embedding models. \"\"\" HUGGING_FACE = \"hugging_face\" OPENAI = \"openai\" VOYAGE = \"voyage\"","title":"Embedding_model_configuration"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#embedding_model_configuration","text":"This module contains functionality related to the the embedding_model_configuration module for embedding.bootstrap.configuration .","title":"Embedding_model_configuration"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#embedding_model_configuration_1","text":"","title":"Embedding_model_configuration"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#src.embedding.bootstrap.configuration.embedding_model_configuration.EmbeddingModelConfiguration","text":"Bases: BaseConfigurationWithSecrets Configuration class for embedding models. This class defines the necessary parameters and settings for configuring an embedding model, including provider information, model name, and tokenization settings. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class EmbeddingModelConfiguration ( BaseConfigurationWithSecrets ): \"\"\"Configuration class for embedding models. This class defines the necessary parameters and settings for configuring an embedding model, including provider information, model name, and tokenization settings. \"\"\" provider : EmbeddingModelProviderName = Field ( ... , description = \"The provider of the embedding model.\" ) name : str = Field ( ... , description = \"The name of the embedding model.\" ) tokenizer_name : str = Field ( ... , description = \"The name of the tokenizer used by the embedding model.\" , ) batch_size : int = Field ( 64 , description = \"The batch size for embedding.\" ) splitter : Any = Field ( None , description = \"The splitter configuration for the embedding model.\" ) @field_validator ( \"splitter\" ) @classmethod def _validate_splitter ( cls , value : Any , info : ValidationInfo ) -> Any : \"\"\"Validate the splitter configuration. This method ensures that the provided splitter configuration is valid according to the SplitterConfigurationRegistry. Args: value: The splitter configuration value to validate. info: Validation context information. Returns: The validated splitter configuration. \"\"\" return super () . _validate ( value , info = info , registry = SplitterConfigurationRegistry , )","title":"EmbeddingModelConfiguration"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#src.embedding.bootstrap.configuration.embedding_model_configuration.EmbeddingModelConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for embedding model configurations. This registry maps embedding model provider names to their respective configuration classes. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 69 70 71 72 73 74 75 76 class EmbeddingModelConfigurationRegistry ( ConfigurationRegistry ): \"\"\"Registry for embedding model configurations. This registry maps embedding model provider names to their respective configuration classes. \"\"\" _key_class : Type = EmbeddingModelProviderName","title":"EmbeddingModelConfigurationRegistry"},{"location":"src/embedding/bootstrap/configuration/embedding_model_configuration/#src.embedding.bootstrap.configuration.embedding_model_configuration.EmbeddingModelProviderName","text":"Bases: str , Enum Enumeration of supported embedding model providers. This enum lists all the providers that can be used for embedding models. Source code in src/embedding/bootstrap/configuration/embedding_model_configuration.py 13 14 15 16 17 18 19 20 21 class EmbeddingModelProviderName ( str , Enum ): \"\"\"Enumeration of supported embedding model providers. This enum lists all the providers that can be used for embedding models. \"\"\" HUGGING_FACE = \"hugging_face\" OPENAI = \"openai\" VOYAGE = \"voyage\"","title":"EmbeddingModelProviderName"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/","text":"Splitting_configuration This module contains functionality related to the the splitting_configuration module for embedding.bootstrap.configuration . Splitting_configuration SplitterConfiguration Bases: BaseConfiguration Base configuration class for text splitters. This class provides the foundation for defining specific configurations required by different text splitting algorithms. Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 19 20 21 22 23 24 25 26 27 class SplitterConfiguration ( BaseConfiguration ): \"\"\" Base configuration class for text splitters. This class provides the foundation for defining specific configurations required by different text splitting algorithms. \"\"\" pass SplitterConfigurationRegistry Bases: ConfigurationRegistry Registry for managing different splitter configurations. This registry maps SplitterName enum values to their corresponding configuration classes, allowing for dynamic configuration selection based on the chosen splitter type. Attributes: _key_class ( Type ) \u2013 The type used as keys in the registry (SplitterName). Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 31 32 33 34 35 36 37 38 39 40 41 42 43 class SplitterConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for managing different splitter configurations. This registry maps SplitterName enum values to their corresponding configuration classes, allowing for dynamic configuration selection based on the chosen splitter type. Attributes: _key_class: The type used as keys in the registry (SplitterName). \"\"\" _key_class : Type = SplitterName SplitterName Bases: str , Enum Enumeration of available text splitter types. Attributes: BASIC_MARKDOWN \u2013 A basic splitter for markdown documents. Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 8 9 10 11 12 13 14 15 16 class SplitterName ( str , Enum ): \"\"\" Enumeration of available text splitter types. Attributes: BASIC_MARKDOWN: A basic splitter for markdown documents. \"\"\" BASIC_MARKDOWN = \"basic-markdown\"","title":"Splitting_configuration"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#splitting_configuration","text":"This module contains functionality related to the the splitting_configuration module for embedding.bootstrap.configuration .","title":"Splitting_configuration"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#splitting_configuration_1","text":"","title":"Splitting_configuration"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#src.embedding.bootstrap.configuration.splitting_configuration.SplitterConfiguration","text":"Bases: BaseConfiguration Base configuration class for text splitters. This class provides the foundation for defining specific configurations required by different text splitting algorithms. Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 19 20 21 22 23 24 25 26 27 class SplitterConfiguration ( BaseConfiguration ): \"\"\" Base configuration class for text splitters. This class provides the foundation for defining specific configurations required by different text splitting algorithms. \"\"\" pass","title":"SplitterConfiguration"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#src.embedding.bootstrap.configuration.splitting_configuration.SplitterConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for managing different splitter configurations. This registry maps SplitterName enum values to their corresponding configuration classes, allowing for dynamic configuration selection based on the chosen splitter type. Attributes: _key_class ( Type ) \u2013 The type used as keys in the registry (SplitterName). Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 31 32 33 34 35 36 37 38 39 40 41 42 43 class SplitterConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for managing different splitter configurations. This registry maps SplitterName enum values to their corresponding configuration classes, allowing for dynamic configuration selection based on the chosen splitter type. Attributes: _key_class: The type used as keys in the registry (SplitterName). \"\"\" _key_class : Type = SplitterName","title":"SplitterConfigurationRegistry"},{"location":"src/embedding/bootstrap/configuration/splitting_configuration/#src.embedding.bootstrap.configuration.splitting_configuration.SplitterName","text":"Bases: str , Enum Enumeration of available text splitter types. Attributes: BASIC_MARKDOWN \u2013 A basic splitter for markdown documents. Source code in src/embedding/bootstrap/configuration/splitting_configuration.py 8 9 10 11 12 13 14 15 16 class SplitterName ( str , Enum ): \"\"\" Enumeration of available text splitter types. Attributes: BASIC_MARKDOWN: A basic splitter for markdown documents. \"\"\" BASIC_MARKDOWN = \"basic-markdown\"","title":"SplitterName"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/","text":"Vector_store_configuration This module contains functionality related to the the vector_store_configuration module for embedding.bootstrap.configuration . Vector_store_configuration VectorStoreConfiguration Bases: BaseConfigurationWithSecrets , ABC Abstract base configuration class for vector stores. Inherits from BaseConfigurationWithSecrets to handle secure configuration settings. All specific vector store configurations should inherit from this class. Attributes: port ( int ) \u2013 The port number for connecting to the vector store server. collection_name ( str ) \u2013 Name of the collection in the vector store. host ( str ) \u2013 Hostname or IP address of the vector store server. protocol ( Union [ Literal ['http'], Literal ['https']] ) \u2013 Connection protocol (http or https). Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class VectorStoreConfiguration ( BaseConfigurationWithSecrets , ABC ): \"\"\" Abstract base configuration class for vector stores. Inherits from BaseConfigurationWithSecrets to handle secure configuration settings. All specific vector store configurations should inherit from this class. Attributes: port: The port number for connecting to the vector store server. collection_name: Name of the collection in the vector store. host: Hostname or IP address of the vector store server. protocol: Connection protocol (http or https). \"\"\" port : int = Field ( ... , description = \"The port for the vector store.\" ) collection_name : str = Field ( ... , description = \"The collection name in the vector store.\" ) host : str = Field ( \"127.0.0.1\" , description = \"Host of the vector store server\" ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"The protocol for the vector store.\" ) VectorStoreConfigurationRegistry Bases: ConfigurationRegistry Registry for vector store configurations. Maps VectorStoreName enum values to their corresponding configuration classes. Used to retrieve the appropriate configuration based on the selected vector store. Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 50 51 52 53 54 55 56 57 58 class VectorStoreConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for vector store configurations. Maps VectorStoreName enum values to their corresponding configuration classes. Used to retrieve the appropriate configuration based on the selected vector store. \"\"\" _key_class = VectorStoreName VectorStoreName Bases: str , Enum Enumeration of supported vector store providers. Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 12 13 14 15 16 17 18 19 class VectorStoreName ( str , Enum ): \"\"\" Enumeration of supported vector store providers. \"\"\" QDRANT = \"qdrant\" CHROMA = \"chroma\" PGVECTOR = \"pgvector\"","title":"Vector_store_configuration"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#vector_store_configuration","text":"This module contains functionality related to the the vector_store_configuration module for embedding.bootstrap.configuration .","title":"Vector_store_configuration"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#vector_store_configuration_1","text":"","title":"Vector_store_configuration"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#src.embedding.bootstrap.configuration.vector_store_configuration.VectorStoreConfiguration","text":"Bases: BaseConfigurationWithSecrets , ABC Abstract base configuration class for vector stores. Inherits from BaseConfigurationWithSecrets to handle secure configuration settings. All specific vector store configurations should inherit from this class. Attributes: port ( int ) \u2013 The port number for connecting to the vector store server. collection_name ( str ) \u2013 Name of the collection in the vector store. host ( str ) \u2013 Hostname or IP address of the vector store server. protocol ( Union [ Literal ['http'], Literal ['https']] ) \u2013 Connection protocol (http or https). Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class VectorStoreConfiguration ( BaseConfigurationWithSecrets , ABC ): \"\"\" Abstract base configuration class for vector stores. Inherits from BaseConfigurationWithSecrets to handle secure configuration settings. All specific vector store configurations should inherit from this class. Attributes: port: The port number for connecting to the vector store server. collection_name: Name of the collection in the vector store. host: Hostname or IP address of the vector store server. protocol: Connection protocol (http or https). \"\"\" port : int = Field ( ... , description = \"The port for the vector store.\" ) collection_name : str = Field ( ... , description = \"The collection name in the vector store.\" ) host : str = Field ( \"127.0.0.1\" , description = \"Host of the vector store server\" ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"The protocol for the vector store.\" )","title":"VectorStoreConfiguration"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#src.embedding.bootstrap.configuration.vector_store_configuration.VectorStoreConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for vector store configurations. Maps VectorStoreName enum values to their corresponding configuration classes. Used to retrieve the appropriate configuration based on the selected vector store. Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 50 51 52 53 54 55 56 57 58 class VectorStoreConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for vector store configurations. Maps VectorStoreName enum values to their corresponding configuration classes. Used to retrieve the appropriate configuration based on the selected vector store. \"\"\" _key_class = VectorStoreName","title":"VectorStoreConfigurationRegistry"},{"location":"src/embedding/bootstrap/configuration/vector_store_configuration/#src.embedding.bootstrap.configuration.vector_store_configuration.VectorStoreName","text":"Bases: str , Enum Enumeration of supported vector store providers. Source code in src/embedding/bootstrap/configuration/vector_store_configuration.py 12 13 14 15 16 17 18 19 class VectorStoreName ( str , Enum ): \"\"\" Enumeration of supported vector store providers. \"\"\" QDRANT = \"qdrant\" CHROMA = \"chroma\" PGVECTOR = \"pgvector\"","title":"VectorStoreName"},{"location":"src/embedding/embedders/base_embedder/","text":"Base_embedder This module contains functionality related to the the base_embedder module for embedding.embedders . Base_embedder BaseEmbedder Bases: ABC Abstract base class for text node embedding operations. This class provides core functionality for embedding text nodes, with derived classes implementing specific embedding strategies. Source code in src/embedding/embedders/base_embedder.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class BaseEmbedder ( ABC ): \"\"\"Abstract base class for text node embedding operations. This class provides core functionality for embedding text nodes, with derived classes implementing specific embedding strategies. \"\"\" def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , ): \"\"\"Initialize embedder with configuration, model and storage. Args: configuration: Configuration parameters for the embedding process embedding_model: Model to generate text embeddings vector_store: Storage system for persisting embedding vectors \"\"\" super () . __init__ () self . configuration = configuration self . embedding_model = embedding_model self . vector_store = vector_store @abstractmethod def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Args: nodes: Collection of text nodes to embed Note: Implementation should modify nodes in-place by adding embeddings \"\"\" pass @abstractmethod def embed_flush ( self ) -> None : \"\"\"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note: Should be called at the end of processing to ensure no nodes remain unembedded in the buffer \"\"\" pass __init__ ( configuration , embedding_model , vector_store ) Initialize embedder with configuration, model and storage. Parameters: configuration ( EmbeddingConfiguration ) \u2013 Configuration parameters for the embedding process embedding_model ( BaseEmbedding ) \u2013 Model to generate text embeddings vector_store ( VectorStore ) \u2013 Storage system for persisting embedding vectors Source code in src/embedding/embedders/base_embedder.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , ): \"\"\"Initialize embedder with configuration, model and storage. Args: configuration: Configuration parameters for the embedding process embedding_model: Model to generate text embeddings vector_store: Storage system for persisting embedding vectors \"\"\" super () . __init__ () self . configuration = configuration self . embedding_model = embedding_model self . vector_store = vector_store embed ( nodes ) abstractmethod Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Parameters: nodes ( List [ TextNode ] ) \u2013 Collection of text nodes to embed Note Implementation should modify nodes in-place by adding embeddings Source code in src/embedding/embedders/base_embedder.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @abstractmethod def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Args: nodes: Collection of text nodes to embed Note: Implementation should modify nodes in-place by adding embeddings \"\"\" pass embed_flush () abstractmethod Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note Should be called at the end of processing to ensure no nodes remain unembedded in the buffer Source code in src/embedding/embedders/base_embedder.py 53 54 55 56 57 58 59 60 61 62 63 @abstractmethod def embed_flush ( self ) -> None : \"\"\"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note: Should be called at the end of processing to ensure no nodes remain unembedded in the buffer \"\"\" pass","title":"Base_embedder"},{"location":"src/embedding/embedders/base_embedder/#base_embedder","text":"This module contains functionality related to the the base_embedder module for embedding.embedders .","title":"Base_embedder"},{"location":"src/embedding/embedders/base_embedder/#base_embedder_1","text":"","title":"Base_embedder"},{"location":"src/embedding/embedders/base_embedder/#src.embedding.embedders.base_embedder.BaseEmbedder","text":"Bases: ABC Abstract base class for text node embedding operations. This class provides core functionality for embedding text nodes, with derived classes implementing specific embedding strategies. Source code in src/embedding/embedders/base_embedder.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class BaseEmbedder ( ABC ): \"\"\"Abstract base class for text node embedding operations. This class provides core functionality for embedding text nodes, with derived classes implementing specific embedding strategies. \"\"\" def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , ): \"\"\"Initialize embedder with configuration, model and storage. Args: configuration: Configuration parameters for the embedding process embedding_model: Model to generate text embeddings vector_store: Storage system for persisting embedding vectors \"\"\" super () . __init__ () self . configuration = configuration self . embedding_model = embedding_model self . vector_store = vector_store @abstractmethod def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Args: nodes: Collection of text nodes to embed Note: Implementation should modify nodes in-place by adding embeddings \"\"\" pass @abstractmethod def embed_flush ( self ) -> None : \"\"\"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note: Should be called at the end of processing to ensure no nodes remain unembedded in the buffer \"\"\" pass","title":"BaseEmbedder"},{"location":"src/embedding/embedders/base_embedder/#src.embedding.embedders.base_embedder.BaseEmbedder.__init__","text":"Initialize embedder with configuration, model and storage. Parameters: configuration ( EmbeddingConfiguration ) \u2013 Configuration parameters for the embedding process embedding_model ( BaseEmbedding ) \u2013 Model to generate text embeddings vector_store ( VectorStore ) \u2013 Storage system for persisting embedding vectors Source code in src/embedding/embedders/base_embedder.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , ): \"\"\"Initialize embedder with configuration, model and storage. Args: configuration: Configuration parameters for the embedding process embedding_model: Model to generate text embeddings vector_store: Storage system for persisting embedding vectors \"\"\" super () . __init__ () self . configuration = configuration self . embedding_model = embedding_model self . vector_store = vector_store","title":"__init__"},{"location":"src/embedding/embedders/base_embedder/#src.embedding.embedders.base_embedder.BaseEmbedder.embed","text":"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Parameters: nodes ( List [ TextNode ] ) \u2013 Collection of text nodes to embed Note Implementation should modify nodes in-place by adding embeddings Source code in src/embedding/embedders/base_embedder.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @abstractmethod def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes using batch processing. This method should implement a strategy for processing the provided nodes, potentially splitting them into batches for efficient embedding generation. Args: nodes: Collection of text nodes to embed Note: Implementation should modify nodes in-place by adding embeddings \"\"\" pass","title":"embed"},{"location":"src/embedding/embedders/base_embedder/#src.embedding.embedders.base_embedder.BaseEmbedder.embed_flush","text":"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note Should be called at the end of processing to ensure no nodes remain unembedded in the buffer Source code in src/embedding/embedders/base_embedder.py 53 54 55 56 57 58 59 60 61 62 63 @abstractmethod def embed_flush ( self ) -> None : \"\"\"Process and generate embeddings for any remaining nodes. This method should handle any nodes that remain in the buffer, ensuring all nodes receive embeddings. Note: Should be called at the end of processing to ensure no nodes remain unembedded in the buffer \"\"\" pass","title":"embed_flush"},{"location":"src/embedding/embedders/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.embedders . Registry EmbedderRegistry Bases: Registry Registry for managing embedding models. This registry stores and provides access to embedding model implementations based on their corresponding EmbedderName enumeration values. It extends the base Registry class to provide type-safe access to embedders. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, which is EmbedderName enum in this case. Source code in src/embedding/embedders/registry.py 7 8 9 10 11 12 13 14 15 16 17 18 19 class EmbedderRegistry ( Registry ): \"\"\"Registry for managing embedding models. This registry stores and provides access to embedding model implementations based on their corresponding EmbedderName enumeration values. It extends the base Registry class to provide type-safe access to embedders. Attributes: _key_class (Type): The class type used as keys in the registry, which is EmbedderName enum in this case. \"\"\" _key_class : Type = EmbedderName","title":"Registry"},{"location":"src/embedding/embedders/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.embedders .","title":"Registry"},{"location":"src/embedding/embedders/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/embedders/registry/#src.embedding.embedders.registry.EmbedderRegistry","text":"Bases: Registry Registry for managing embedding models. This registry stores and provides access to embedding model implementations based on their corresponding EmbedderName enumeration values. It extends the base Registry class to provide type-safe access to embedders. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, which is EmbedderName enum in this case. Source code in src/embedding/embedders/registry.py 7 8 9 10 11 12 13 14 15 16 17 18 19 class EmbedderRegistry ( Registry ): \"\"\"Registry for managing embedding models. This registry stores and provides access to embedding model implementations based on their corresponding EmbedderName enumeration values. It extends the base Registry class to provide type-safe access to embedders. Attributes: _key_class (Type): The class type used as keys in the registry, which is EmbedderName enum in this case. \"\"\" _key_class : Type = EmbedderName","title":"EmbedderRegistry"},{"location":"src/embedding/embedders/basic/embedder/","text":"Embedder This module contains functionality related to the the embedder module for embedding.embedders.basic . Embedder BasicEmbedder Bases: BaseEmbedder Implementation of text node embedding operations. Handles batch embedding generation and vector store persistence for text nodes. Source code in src/embedding/embedders/basic/embedder.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class BasicEmbedder ( BaseEmbedder ): \"\"\"Implementation of text node embedding operations. Handles batch embedding generation and vector store persistence for text nodes. \"\"\" def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize BasicEmbedder with model and storage. Args: configuration: Configuration for embedding process embedding_model: Model to generate embeddings vector_store: Storage for embedding vectors logger: Logger instance for tracking operations \"\"\" super () . __init__ ( configuration , embedding_model , vector_store ) self . logger = logger self . batch_size = configuration . embedding . embedding_model . batch_size self . current_nodes_batch = [] def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Args: nodes: Collection of text nodes to embed Note: Modifies nodes in-place by setting embedding attribute \"\"\" self . current_nodes_batch . extend ( nodes ) while len ( self . current_nodes_batch ) >= self . batch_size : batch = self . current_nodes_batch [: self . batch_size ] self . _embed_nodes_batch ( batch ) self . _save_nodes_batch ( batch ) self . current_nodes_batch = self . current_nodes_batch [ self . batch_size : ] def embed_flush ( self ) -> None : \"\"\"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. \"\"\" if self . current_nodes_batch : self . _embed_nodes_batch ( self . current_nodes_batch ) self . _save_nodes_batch ( self . current_nodes_batch ) self . current_nodes_batch = [] def _embed_nodes_batch ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for a batch of text nodes. Extracts content from each node, generates embeddings using the embedding model, and assigns the resulting embeddings back to each node. Args: nodes: Batch of nodes to generate embeddings for \"\"\" self . logger . info ( f \"Embedding batch of { len ( nodes ) } nodes.\" ) nodes_contents = [ node . get_content ( metadata_mode = MetadataMode . EMBED ) for node in nodes ] nodes_embeddings = self . embedding_model . get_text_embedding_batch ( nodes_contents , ) for node , node_embedding in zip ( nodes , nodes_embeddings ): node . embedding = node_embedding def _save_nodes_batch ( self , nodes : List [ TextNode ]) -> None : \"\"\"Save batch of text nodes to vector store. Creates a storage context with the configured vector store and stores the nodes with their embeddings using VectorStoreIndex. Args: nodes: Batch of nodes to save to the vector store \"\"\" self . logger . info ( f \"Saving batch of { len ( nodes ) } nodes to vector store.\" ) self . vector_store . add ( nodes ) __init__ ( configuration , embedding_model , vector_store , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize BasicEmbedder with model and storage. Parameters: configuration ( EmbeddingConfiguration ) \u2013 Configuration for embedding process embedding_model ( BaseEmbedding ) \u2013 Model to generate embeddings vector_store ( VectorStore ) \u2013 Storage for embedding vectors logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for tracking operations Source code in src/embedding/embedders/basic/embedder.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize BasicEmbedder with model and storage. Args: configuration: Configuration for embedding process embedding_model: Model to generate embeddings vector_store: Storage for embedding vectors logger: Logger instance for tracking operations \"\"\" super () . __init__ ( configuration , embedding_model , vector_store ) self . logger = logger self . batch_size = configuration . embedding . embedding_model . batch_size self . current_nodes_batch = [] embed ( nodes ) Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Parameters: nodes ( List [ TextNode ] ) \u2013 Collection of text nodes to embed Note Modifies nodes in-place by setting embedding attribute Source code in src/embedding/embedders/basic/embedder.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Args: nodes: Collection of text nodes to embed Note: Modifies nodes in-place by setting embedding attribute \"\"\" self . current_nodes_batch . extend ( nodes ) while len ( self . current_nodes_batch ) >= self . batch_size : batch = self . current_nodes_batch [: self . batch_size ] self . _embed_nodes_batch ( batch ) self . _save_nodes_batch ( batch ) self . current_nodes_batch = self . current_nodes_batch [ self . batch_size : ] embed_flush () Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. Source code in src/embedding/embedders/basic/embedder.py 68 69 70 71 72 73 74 75 76 77 78 79 def embed_flush ( self ) -> None : \"\"\"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. \"\"\" if self . current_nodes_batch : self . _embed_nodes_batch ( self . current_nodes_batch ) self . _save_nodes_batch ( self . current_nodes_batch ) self . current_nodes_batch = [] BasicEmbedderFactory Bases: Factory Source code in src/embedding/embedders/basic/embedder.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class BasicEmbedderFactory ( Factory ): _configuration_class : Type = EmbeddingConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingConfiguration ) -> BasicEmbedder : \"\"\"Creates a configured BasicEmbedder instance. Initializes embedding model and vector store components based on the provided configuration settings. Args: configuration: Settings for embedding process configuration Returns: BasicEmbedder: Configured embedder instance ready for document processing \"\"\" embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) vector_store_config = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_config . name ) . create ( vector_store_config ) return BasicEmbedder ( configuration = configuration , embedding_model = embedding_model , vector_store = vector_store , )","title":"Embedder"},{"location":"src/embedding/embedders/basic/embedder/#embedder","text":"This module contains functionality related to the the embedder module for embedding.embedders.basic .","title":"Embedder"},{"location":"src/embedding/embedders/basic/embedder/#embedder_1","text":"","title":"Embedder"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedder","text":"Bases: BaseEmbedder Implementation of text node embedding operations. Handles batch embedding generation and vector store persistence for text nodes. Source code in src/embedding/embedders/basic/embedder.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class BasicEmbedder ( BaseEmbedder ): \"\"\"Implementation of text node embedding operations. Handles batch embedding generation and vector store persistence for text nodes. \"\"\" def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize BasicEmbedder with model and storage. Args: configuration: Configuration for embedding process embedding_model: Model to generate embeddings vector_store: Storage for embedding vectors logger: Logger instance for tracking operations \"\"\" super () . __init__ ( configuration , embedding_model , vector_store ) self . logger = logger self . batch_size = configuration . embedding . embedding_model . batch_size self . current_nodes_batch = [] def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Args: nodes: Collection of text nodes to embed Note: Modifies nodes in-place by setting embedding attribute \"\"\" self . current_nodes_batch . extend ( nodes ) while len ( self . current_nodes_batch ) >= self . batch_size : batch = self . current_nodes_batch [: self . batch_size ] self . _embed_nodes_batch ( batch ) self . _save_nodes_batch ( batch ) self . current_nodes_batch = self . current_nodes_batch [ self . batch_size : ] def embed_flush ( self ) -> None : \"\"\"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. \"\"\" if self . current_nodes_batch : self . _embed_nodes_batch ( self . current_nodes_batch ) self . _save_nodes_batch ( self . current_nodes_batch ) self . current_nodes_batch = [] def _embed_nodes_batch ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for a batch of text nodes. Extracts content from each node, generates embeddings using the embedding model, and assigns the resulting embeddings back to each node. Args: nodes: Batch of nodes to generate embeddings for \"\"\" self . logger . info ( f \"Embedding batch of { len ( nodes ) } nodes.\" ) nodes_contents = [ node . get_content ( metadata_mode = MetadataMode . EMBED ) for node in nodes ] nodes_embeddings = self . embedding_model . get_text_embedding_batch ( nodes_contents , ) for node , node_embedding in zip ( nodes , nodes_embeddings ): node . embedding = node_embedding def _save_nodes_batch ( self , nodes : List [ TextNode ]) -> None : \"\"\"Save batch of text nodes to vector store. Creates a storage context with the configured vector store and stores the nodes with their embeddings using VectorStoreIndex. Args: nodes: Batch of nodes to save to the vector store \"\"\" self . logger . info ( f \"Saving batch of { len ( nodes ) } nodes to vector store.\" ) self . vector_store . add ( nodes )","title":"BasicEmbedder"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedder.__init__","text":"Initialize BasicEmbedder with model and storage. Parameters: configuration ( EmbeddingConfiguration ) \u2013 Configuration for embedding process embedding_model ( BaseEmbedding ) \u2013 Model to generate embeddings vector_store ( VectorStore ) \u2013 Storage for embedding vectors logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for tracking operations Source code in src/embedding/embedders/basic/embedder.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , configuration : EmbeddingConfiguration , embedding_model : BaseEmbedding , vector_store : VectorStore , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize BasicEmbedder with model and storage. Args: configuration: Configuration for embedding process embedding_model: Model to generate embeddings vector_store: Storage for embedding vectors logger: Logger instance for tracking operations \"\"\" super () . __init__ ( configuration , embedding_model , vector_store ) self . logger = logger self . batch_size = configuration . embedding . embedding_model . batch_size self . current_nodes_batch = []","title":"__init__"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedder.embed","text":"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Parameters: nodes ( List [ TextNode ] ) \u2013 Collection of text nodes to embed Note Modifies nodes in-place by setting embedding attribute Source code in src/embedding/embedders/basic/embedder.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def embed ( self , nodes : List [ TextNode ]) -> None : \"\"\"Generate embeddings for text nodes in batches. Adds nodes to the current batch and processes complete batches when the batch size threshold is reached. Nodes are embedded and then saved to the vector store. Args: nodes: Collection of text nodes to embed Note: Modifies nodes in-place by setting embedding attribute \"\"\" self . current_nodes_batch . extend ( nodes ) while len ( self . current_nodes_batch ) >= self . batch_size : batch = self . current_nodes_batch [: self . batch_size ] self . _embed_nodes_batch ( batch ) self . _save_nodes_batch ( batch ) self . current_nodes_batch = self . current_nodes_batch [ self . batch_size : ]","title":"embed"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedder.embed_flush","text":"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. Source code in src/embedding/embedders/basic/embedder.py 68 69 70 71 72 73 74 75 76 77 78 79 def embed_flush ( self ) -> None : \"\"\"Process any remaining nodes in the current batch. Ensures all nodes that haven't reached the batch size threshold are embedded and saved to the vector store before clearing the batch. Should be called after processing all documents to avoid losing the final incomplete batch. \"\"\" if self . current_nodes_batch : self . _embed_nodes_batch ( self . current_nodes_batch ) self . _save_nodes_batch ( self . current_nodes_batch ) self . current_nodes_batch = []","title":"embed_flush"},{"location":"src/embedding/embedders/basic/embedder/#src.embedding.embedders.basic.embedder.BasicEmbedderFactory","text":"Bases: Factory Source code in src/embedding/embedders/basic/embedder.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 class BasicEmbedderFactory ( Factory ): _configuration_class : Type = EmbeddingConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingConfiguration ) -> BasicEmbedder : \"\"\"Creates a configured BasicEmbedder instance. Initializes embedding model and vector store components based on the provided configuration settings. Args: configuration: Settings for embedding process configuration Returns: BasicEmbedder: Configured embedder instance ready for document processing \"\"\" embedding_model_config = configuration . embedding . embedding_model embedding_model = EmbeddingModelRegistry . get ( embedding_model_config . provider ) . create ( embedding_model_config ) vector_store_config = configuration . embedding . vector_store vector_store = VectorStoreRegistry . get ( vector_store_config . name ) . create ( vector_store_config ) return BasicEmbedder ( configuration = configuration , embedding_model = embedding_model , vector_store = vector_store , )","title":"BasicEmbedderFactory"},{"location":"src/embedding/embedding_models/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.embedding_models . Registry EmbeddingModelRegistry Bases: Registry Registry for embedding models that maps provider names to their implementations. This registry uses EmbeddingModelProviderName as keys to store and retrieve embedding model implementations, allowing the system to support multiple embedding model providers while providing a uniform interface for registration and lookup. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingModelProviderName enum. Source code in src/embedding/embedding_models/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class EmbeddingModelRegistry ( Registry ): \"\"\" Registry for embedding models that maps provider names to their implementations. This registry uses EmbeddingModelProviderName as keys to store and retrieve embedding model implementations, allowing the system to support multiple embedding model providers while providing a uniform interface for registration and lookup. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingModelProviderName enum. \"\"\" _key_class : Type = EmbeddingModelProviderName EmbeddingModelTokenizerRegistry Bases: Registry Registry for embedding model tokenizers that maps provider names to their respective tokenizers. This registry uses EmbeddingModelProviderName as keys to store and retrieve tokenizer implementations that are compatible with specific embedding models. Tokenizers are used to preprocess text before embedding generation. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingModelProviderName enum. Source code in src/embedding/embedding_models/registry.py 25 26 27 28 29 30 31 32 33 34 35 36 37 class EmbeddingModelTokenizerRegistry ( Registry ): \"\"\" Registry for embedding model tokenizers that maps provider names to their respective tokenizers. This registry uses EmbeddingModelProviderName as keys to store and retrieve tokenizer implementations that are compatible with specific embedding models. Tokenizers are used to preprocess text before embedding generation. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingModelProviderName enum. \"\"\" _key_class : Type = EmbeddingModelProviderName","title":"Registry"},{"location":"src/embedding/embedding_models/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.embedding_models .","title":"Registry"},{"location":"src/embedding/embedding_models/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/embedding_models/registry/#src.embedding.embedding_models.registry.EmbeddingModelRegistry","text":"Bases: Registry Registry for embedding models that maps provider names to their implementations. This registry uses EmbeddingModelProviderName as keys to store and retrieve embedding model implementations, allowing the system to support multiple embedding model providers while providing a uniform interface for registration and lookup. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingModelProviderName enum. Source code in src/embedding/embedding_models/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class EmbeddingModelRegistry ( Registry ): \"\"\" Registry for embedding models that maps provider names to their implementations. This registry uses EmbeddingModelProviderName as keys to store and retrieve embedding model implementations, allowing the system to support multiple embedding model providers while providing a uniform interface for registration and lookup. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingModelProviderName enum. \"\"\" _key_class : Type = EmbeddingModelProviderName","title":"EmbeddingModelRegistry"},{"location":"src/embedding/embedding_models/registry/#src.embedding.embedding_models.registry.EmbeddingModelTokenizerRegistry","text":"Bases: Registry Registry for embedding model tokenizers that maps provider names to their respective tokenizers. This registry uses EmbeddingModelProviderName as keys to store and retrieve tokenizer implementations that are compatible with specific embedding models. Tokenizers are used to preprocess text before embedding generation. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingModelProviderName enum. Source code in src/embedding/embedding_models/registry.py 25 26 27 28 29 30 31 32 33 34 35 36 37 class EmbeddingModelTokenizerRegistry ( Registry ): \"\"\" Registry for embedding model tokenizers that maps provider names to their respective tokenizers. This registry uses EmbeddingModelProviderName as keys to store and retrieve tokenizer implementations that are compatible with specific embedding models. Tokenizers are used to preprocess text before embedding generation. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingModelProviderName enum. \"\"\" _key_class : Type = EmbeddingModelProviderName","title":"EmbeddingModelTokenizerRegistry"},{"location":"src/embedding/embedding_models/hugging_face/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.embedding_models.hugging_face . Configuration HuggingFaceEmbeddingModelConfiguration Bases: EmbeddingModelConfiguration Configuration class for Hugging Face embedding models. This class extends the base EmbeddingModelConfiguration to provide specific configuration options for Hugging Face embedding models. Source code in src/embedding/embedding_models/hugging_face/configuration.py 11 12 13 14 15 16 17 18 19 20 21 class HuggingFaceEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\" Configuration class for Hugging Face embedding models. This class extends the base EmbeddingModelConfiguration to provide specific configuration options for Hugging Face embedding models. \"\"\" provider : Literal [ EmbeddingModelProviderName . HUGGING_FACE ] = Field ( ... , description = \"The provider of the embedding model.\" )","title":"Configuration"},{"location":"src/embedding/embedding_models/hugging_face/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.embedding_models.hugging_face .","title":"Configuration"},{"location":"src/embedding/embedding_models/hugging_face/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/embedding_models/hugging_face/configuration/#src.embedding.embedding_models.hugging_face.configuration.HuggingFaceEmbeddingModelConfiguration","text":"Bases: EmbeddingModelConfiguration Configuration class for Hugging Face embedding models. This class extends the base EmbeddingModelConfiguration to provide specific configuration options for Hugging Face embedding models. Source code in src/embedding/embedding_models/hugging_face/configuration.py 11 12 13 14 15 16 17 18 19 20 21 class HuggingFaceEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\" Configuration class for Hugging Face embedding models. This class extends the base EmbeddingModelConfiguration to provide specific configuration options for Hugging Face embedding models. \"\"\" provider : Literal [ EmbeddingModelProviderName . HUGGING_FACE ] = Field ( ... , description = \"The provider of the embedding model.\" )","title":"HuggingFaceEmbeddingModelConfiguration"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/","text":"Embedding_model This module contains functionality related to the the embedding_model module for embedding.embedding_models.hugging_face . Embedding_model HuggingFaceEmbeddingModelFactory Bases: SingletonFactory Factory for creating configured HuggingFace embedding models. This singleton factory creates and configures HuggingFaceEmbedding instances based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating instances. Source code in src/embedding/embedding_models/hugging_face/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class HuggingFaceEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured HuggingFace embedding models. This singleton factory creates and configures HuggingFaceEmbedding instances based on the provided configuration. Attributes: _configuration_class (Type): The configuration class used for creating instances. \"\"\" _configuration_class : Type = HuggingFaceEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : HuggingFaceEmbeddingModelConfiguration ) -> HuggingFaceEmbedding : \"\"\"Creates a HuggingFaceEmbedding instance based on provided configuration. Args: configuration: HuggingFace embedding model configuration. Returns: HuggingFaceEmbedding: Configured embedding model instance. \"\"\" return HuggingFaceEmbedding ( model_name = configuration . name , embed_batch_size = configuration . batch_size , ) HuggingFaceEmbeddingModelTokenizerFactory Bases: SingletonFactory Factory for creating HuggingFace tokenizer functions. This singleton factory creates and configures tokenizer functions for HuggingFace models based on the provided configuration. Source code in src/embedding/embedding_models/hugging_face/embedding_model.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class HuggingFaceEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating HuggingFace tokenizer functions. This singleton factory creates and configures tokenizer functions for HuggingFace models based on the provided configuration. \"\"\" _configuration_class : Type = HuggingFaceEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : HuggingFaceEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function based on provided configuration. Args: configuration: HuggingFace embedding model configuration. Returns: Callable: A tokenize function from the configured tokenizer. \"\"\" return AutoTokenizer . from_pretrained ( configuration . tokenizer_name ) . tokenize","title":"Embedding_model"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/#embedding_model","text":"This module contains functionality related to the the embedding_model module for embedding.embedding_models.hugging_face .","title":"Embedding_model"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/#embedding_model_1","text":"","title":"Embedding_model"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/#src.embedding.embedding_models.hugging_face.embedding_model.HuggingFaceEmbeddingModelFactory","text":"Bases: SingletonFactory Factory for creating configured HuggingFace embedding models. This singleton factory creates and configures HuggingFaceEmbedding instances based on the provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class used for creating instances. Source code in src/embedding/embedding_models/hugging_face/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class HuggingFaceEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured HuggingFace embedding models. This singleton factory creates and configures HuggingFaceEmbedding instances based on the provided configuration. Attributes: _configuration_class (Type): The configuration class used for creating instances. \"\"\" _configuration_class : Type = HuggingFaceEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : HuggingFaceEmbeddingModelConfiguration ) -> HuggingFaceEmbedding : \"\"\"Creates a HuggingFaceEmbedding instance based on provided configuration. Args: configuration: HuggingFace embedding model configuration. Returns: HuggingFaceEmbedding: Configured embedding model instance. \"\"\" return HuggingFaceEmbedding ( model_name = configuration . name , embed_batch_size = configuration . batch_size , )","title":"HuggingFaceEmbeddingModelFactory"},{"location":"src/embedding/embedding_models/hugging_face/embedding_model/#src.embedding.embedding_models.hugging_face.embedding_model.HuggingFaceEmbeddingModelTokenizerFactory","text":"Bases: SingletonFactory Factory for creating HuggingFace tokenizer functions. This singleton factory creates and configures tokenizer functions for HuggingFace models based on the provided configuration. Source code in src/embedding/embedding_models/hugging_face/embedding_model.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class HuggingFaceEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating HuggingFace tokenizer functions. This singleton factory creates and configures tokenizer functions for HuggingFace models based on the provided configuration. \"\"\" _configuration_class : Type = HuggingFaceEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : HuggingFaceEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function based on provided configuration. Args: configuration: HuggingFace embedding model configuration. Returns: Callable: A tokenize function from the configured tokenizer. \"\"\" return AutoTokenizer . from_pretrained ( configuration . tokenizer_name ) . tokenize","title":"HuggingFaceEmbeddingModelTokenizerFactory"},{"location":"src/embedding/embedding_models/openai/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.embedding_models.openai . Configuration OpenAIEmbeddingModelConfiguration Bases: EmbeddingModelConfiguration Configuration for OpenAI embedding models. This class defines the configuration parameters needed to use OpenAI embedding models, including API credentials, model parameters, and request size limitations. Source code in src/embedding/embedding_models/openai/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class OpenAIEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\" Configuration for OpenAI embedding models. This class defines the configuration parameters needed to use OpenAI embedding models, including API credentials, model parameters, and request size limitations. \"\"\" class Secrets ( BaseSecrets ): \"\"\" Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__OPEN_AI__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) provider : Literal [ EmbeddingModelProviderName . OPENAI ] = Field ( ... , description = \"The provider of the embedding model.\" ) max_request_size_in_tokens : int = Field ( 8191 , description = \"Maximum size of the request in tokens.\" , ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) def model_post_init ( self , __context : dict ): \"\"\" Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Args: __context: Context information provided by Pydantic during initialization \"\"\" super () . model_post_init ( __context ) if self . splitter : self . batch_size = ( self . max_request_size_in_tokens // self . splitter . chunk_size_in_tokens ) Secrets Bases: BaseSecrets Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. Source code in src/embedding/embedding_models/openai/configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Secrets ( BaseSecrets ): \"\"\" Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__OPEN_AI__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) model_post_init ( __context ) Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Parameters: __context ( dict ) \u2013 Context information provided by Pydantic during initialization Source code in src/embedding/embedding_models/openai/configuration.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def model_post_init ( self , __context : dict ): \"\"\" Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Args: __context: Context information provided by Pydantic during initialization \"\"\" super () . model_post_init ( __context ) if self . splitter : self . batch_size = ( self . max_request_size_in_tokens // self . splitter . chunk_size_in_tokens )","title":"Configuration"},{"location":"src/embedding/embedding_models/openai/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.embedding_models.openai .","title":"Configuration"},{"location":"src/embedding/embedding_models/openai/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/embedding_models/openai/configuration/#src.embedding.embedding_models.openai.configuration.OpenAIEmbeddingModelConfiguration","text":"Bases: EmbeddingModelConfiguration Configuration for OpenAI embedding models. This class defines the configuration parameters needed to use OpenAI embedding models, including API credentials, model parameters, and request size limitations. Source code in src/embedding/embedding_models/openai/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class OpenAIEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\" Configuration for OpenAI embedding models. This class defines the configuration parameters needed to use OpenAI embedding models, including API credentials, model parameters, and request size limitations. \"\"\" class Secrets ( BaseSecrets ): \"\"\" Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__OPEN_AI__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) provider : Literal [ EmbeddingModelProviderName . OPENAI ] = Field ( ... , description = \"The provider of the embedding model.\" ) max_request_size_in_tokens : int = Field ( 8191 , description = \"Maximum size of the request in tokens.\" , ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) def model_post_init ( self , __context : dict ): \"\"\" Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Args: __context: Context information provided by Pydantic during initialization \"\"\" super () . model_post_init ( __context ) if self . splitter : self . batch_size = ( self . max_request_size_in_tokens // self . splitter . chunk_size_in_tokens )","title":"OpenAIEmbeddingModelConfiguration"},{"location":"src/embedding/embedding_models/openai/configuration/#src.embedding.embedding_models.openai.configuration.OpenAIEmbeddingModelConfiguration.Secrets","text":"Bases: BaseSecrets Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. Source code in src/embedding/embedding_models/openai/configuration.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Secrets ( BaseSecrets ): \"\"\" Secrets configuration for OpenAI embedding models. Contains sensitive credentials required for API authentication with appropriate environment variable mappings. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__OPEN_AI__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" )","title":"Secrets"},{"location":"src/embedding/embedding_models/openai/configuration/#src.embedding.embedding_models.openai.configuration.OpenAIEmbeddingModelConfiguration.model_post_init","text":"Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Parameters: __context ( dict ) \u2013 Context information provided by Pydantic during initialization Source code in src/embedding/embedding_models/openai/configuration.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def model_post_init ( self , __context : dict ): \"\"\" Post-initialization processing for the model configuration. Calculates the appropriate batch size based on the maximum request size and the configured text splitter's chunk size if a splitter is defined. Args: __context: Context information provided by Pydantic during initialization \"\"\" super () . model_post_init ( __context ) if self . splitter : self . batch_size = ( self . max_request_size_in_tokens // self . splitter . chunk_size_in_tokens )","title":"model_post_init"},{"location":"src/embedding/embedding_models/openai/embedding_model/","text":"Embedding_model This module contains functionality related to the the embedding_model module for embedding.embedding_models.openai . Embedding_model OpenAIEmbeddingModelFactory Bases: SingletonFactory Factory for creating configured OpenAI embedding models. This factory creates singleton instances of OpenAI embedding models based on the provided configuration. Source code in src/embedding/embedding_models/openai/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class OpenAIEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured OpenAI embedding models. This factory creates singleton instances of OpenAI embedding models based on the provided configuration. \"\"\" _configuration_class : Type = OpenAIEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : OpenAIEmbeddingModelConfiguration ) -> OpenAIEmbedding : \"\"\"Creates an OpenAI embedding model based on provided configuration. Args: configuration: OpenAI embedding model configuration. Returns: OpenAIEmbedding: Configured OpenAI embedding model instance. \"\"\" return OpenAIEmbedding ( api_key = configuration . secrets . api_key . get_secret_value (), model_name = configuration . name , embed_batch_size = configuration . batch_size , ) OpenAIEmbeddingModelTokenizerFactory Bases: SingletonFactory Factory for creating OpenAI tokenizer functions. This factory creates singleton instances of OpenAI tokenizer functions based on the provided configuration. Source code in src/embedding/embedding_models/openai/embedding_model.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class OpenAIEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating OpenAI tokenizer functions. This factory creates singleton instances of OpenAI tokenizer functions based on the provided configuration. \"\"\" _configuration_class : Type = OpenAIEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : OpenAIEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function for OpenAI models based on provided configuration. Args: configuration: OpenAI embedding model configuration. Returns: Callable: A tokenizer function that converts text to token IDs. \"\"\" return tiktoken . encoding_for_model ( configuration . tokenizer_name ) . encode","title":"Embedding_model"},{"location":"src/embedding/embedding_models/openai/embedding_model/#embedding_model","text":"This module contains functionality related to the the embedding_model module for embedding.embedding_models.openai .","title":"Embedding_model"},{"location":"src/embedding/embedding_models/openai/embedding_model/#embedding_model_1","text":"","title":"Embedding_model"},{"location":"src/embedding/embedding_models/openai/embedding_model/#src.embedding.embedding_models.openai.embedding_model.OpenAIEmbeddingModelFactory","text":"Bases: SingletonFactory Factory for creating configured OpenAI embedding models. This factory creates singleton instances of OpenAI embedding models based on the provided configuration. Source code in src/embedding/embedding_models/openai/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class OpenAIEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured OpenAI embedding models. This factory creates singleton instances of OpenAI embedding models based on the provided configuration. \"\"\" _configuration_class : Type = OpenAIEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : OpenAIEmbeddingModelConfiguration ) -> OpenAIEmbedding : \"\"\"Creates an OpenAI embedding model based on provided configuration. Args: configuration: OpenAI embedding model configuration. Returns: OpenAIEmbedding: Configured OpenAI embedding model instance. \"\"\" return OpenAIEmbedding ( api_key = configuration . secrets . api_key . get_secret_value (), model_name = configuration . name , embed_batch_size = configuration . batch_size , )","title":"OpenAIEmbeddingModelFactory"},{"location":"src/embedding/embedding_models/openai/embedding_model/#src.embedding.embedding_models.openai.embedding_model.OpenAIEmbeddingModelTokenizerFactory","text":"Bases: SingletonFactory Factory for creating OpenAI tokenizer functions. This factory creates singleton instances of OpenAI tokenizer functions based on the provided configuration. Source code in src/embedding/embedding_models/openai/embedding_model.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class OpenAIEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating OpenAI tokenizer functions. This factory creates singleton instances of OpenAI tokenizer functions based on the provided configuration. \"\"\" _configuration_class : Type = OpenAIEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : OpenAIEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function for OpenAI models based on provided configuration. Args: configuration: OpenAI embedding model configuration. Returns: Callable: A tokenizer function that converts text to token IDs. \"\"\" return tiktoken . encoding_for_model ( configuration . tokenizer_name ) . encode","title":"OpenAIEmbeddingModelTokenizerFactory"},{"location":"src/embedding/embedding_models/voyage/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.embedding_models.voyage . Configuration VoyageEmbeddingModelConfiguration Bases: EmbeddingModelConfiguration Configuration for Voyage embedding models. This class represents the configuration needed for using Voyage AI embedding models, including authentication credentials and model settings. Source code in src/embedding/embedding_models/voyage/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VoyageEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\"Configuration for Voyage embedding models. This class represents the configuration needed for using Voyage AI embedding models, including authentication credentials and model settings. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__VOYAGE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) provider : Literal [ EmbeddingModelProviderName . VOYAGE ] = Field ( ... , description = \"The provider of the embedding model.\" ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" ) Secrets Bases: BaseSecrets Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. Source code in src/embedding/embedding_models/voyage/configuration.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Secrets ( BaseSecrets ): \"\"\"Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__VOYAGE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" )","title":"Configuration"},{"location":"src/embedding/embedding_models/voyage/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.embedding_models.voyage .","title":"Configuration"},{"location":"src/embedding/embedding_models/voyage/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/embedding_models/voyage/configuration/#src.embedding.embedding_models.voyage.configuration.VoyageEmbeddingModelConfiguration","text":"Bases: EmbeddingModelConfiguration Configuration for Voyage embedding models. This class represents the configuration needed for using Voyage AI embedding models, including authentication credentials and model settings. Source code in src/embedding/embedding_models/voyage/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VoyageEmbeddingModelConfiguration ( EmbeddingModelConfiguration ): \"\"\"Configuration for Voyage embedding models. This class represents the configuration needed for using Voyage AI embedding models, including authentication credentials and model settings. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__VOYAGE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" ) provider : Literal [ EmbeddingModelProviderName . VOYAGE ] = Field ( ... , description = \"The provider of the embedding model.\" ) secrets : Secrets = Field ( None , description = \"The secrets for the language model.\" )","title":"VoyageEmbeddingModelConfiguration"},{"location":"src/embedding/embedding_models/voyage/configuration/#src.embedding.embedding_models.voyage.configuration.VoyageEmbeddingModelConfiguration.Secrets","text":"Bases: BaseSecrets Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. Source code in src/embedding/embedding_models/voyage/configuration.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Secrets ( BaseSecrets ): \"\"\"Secrets for Voyage embedding model authentication. Contains API key and other sensitive information required to authenticate with the Voyage AI API for embedding generation. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAGKB__EMBEDDING_MODELS__VOYAGE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) api_key : SecretStr = Field ( ... , description = \"API key for the model\" )","title":"Secrets"},{"location":"src/embedding/embedding_models/voyage/embedding_model/","text":"Embedding_model This module contains functionality related to the the embedding_model module for embedding.embedding_models.voyage . Embedding_model VoyageEmbeddingModelFactory Bases: SingletonFactory Factory for creating configured Voyage embedding models. This factory ensures only one instance of a Voyage embedding model is created for each configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the Voyage embedding model. Source code in src/embedding/embedding_models/voyage/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VoyageEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured Voyage embedding models. This factory ensures only one instance of a Voyage embedding model is created for each configuration. Attributes: _configuration_class (Type): The configuration class for the Voyage embedding model. \"\"\" _configuration_class : Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : VoyageEmbeddingModelConfiguration ) -> VoyageEmbedding : \"\"\"Creates a Voyage embedding model based on provided configuration. Args: configuration: Voyage embedding model configuration with API key and settings. Returns: VoyageEmbedding: Configured Voyage embedding model instance. \"\"\" return VoyageEmbedding ( voyage_api_key = configuration . secrets . api_key . get_secret_value (), model_name = configuration . name , embed_batch_size = configuration . batch_size , ) VoyageEmbeddingModelTokenizerFactory Bases: SingletonFactory Factory for creating tokenizers for Voyage embedding models. Provides a singleton tokenizer function based on the configuration. Source code in src/embedding/embedding_models/voyage/embedding_model.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class VoyageEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating tokenizers for Voyage embedding models. Provides a singleton tokenizer function based on the configuration. \"\"\" _configuration_class : Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : VoyageEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function based on provided configuration. Args: configuration: Voyage embedding model configuration containing tokenizer name. Returns: Callable: A tokenizer function that can be used to tokenize input text. \"\"\" return AutoTokenizer . from_pretrained ( configuration . tokenizer_name ) . tokenize","title":"Embedding_model"},{"location":"src/embedding/embedding_models/voyage/embedding_model/#embedding_model","text":"This module contains functionality related to the the embedding_model module for embedding.embedding_models.voyage .","title":"Embedding_model"},{"location":"src/embedding/embedding_models/voyage/embedding_model/#embedding_model_1","text":"","title":"Embedding_model"},{"location":"src/embedding/embedding_models/voyage/embedding_model/#src.embedding.embedding_models.voyage.embedding_model.VoyageEmbeddingModelFactory","text":"Bases: SingletonFactory Factory for creating configured Voyage embedding models. This factory ensures only one instance of a Voyage embedding model is created for each configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for the Voyage embedding model. Source code in src/embedding/embedding_models/voyage/embedding_model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class VoyageEmbeddingModelFactory ( SingletonFactory ): \"\"\"Factory for creating configured Voyage embedding models. This factory ensures only one instance of a Voyage embedding model is created for each configuration. Attributes: _configuration_class (Type): The configuration class for the Voyage embedding model. \"\"\" _configuration_class : Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : VoyageEmbeddingModelConfiguration ) -> VoyageEmbedding : \"\"\"Creates a Voyage embedding model based on provided configuration. Args: configuration: Voyage embedding model configuration with API key and settings. Returns: VoyageEmbedding: Configured Voyage embedding model instance. \"\"\" return VoyageEmbedding ( voyage_api_key = configuration . secrets . api_key . get_secret_value (), model_name = configuration . name , embed_batch_size = configuration . batch_size , )","title":"VoyageEmbeddingModelFactory"},{"location":"src/embedding/embedding_models/voyage/embedding_model/#src.embedding.embedding_models.voyage.embedding_model.VoyageEmbeddingModelTokenizerFactory","text":"Bases: SingletonFactory Factory for creating tokenizers for Voyage embedding models. Provides a singleton tokenizer function based on the configuration. Source code in src/embedding/embedding_models/voyage/embedding_model.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class VoyageEmbeddingModelTokenizerFactory ( SingletonFactory ): \"\"\"Factory for creating tokenizers for Voyage embedding models. Provides a singleton tokenizer function based on the configuration. \"\"\" _configuration_class : Type = VoyageEmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : VoyageEmbeddingModelConfiguration ) -> Callable : \"\"\"Creates a tokenizer function based on provided configuration. Args: configuration: Voyage embedding model configuration containing tokenizer name. Returns: Callable: A tokenizer function that can be used to tokenize input text. \"\"\" return AutoTokenizer . from_pretrained ( configuration . tokenizer_name ) . tokenize","title":"VoyageEmbeddingModelTokenizerFactory"},{"location":"src/embedding/orchestrators/base_orchestrator/","text":"Base_orchestrator This module contains functionality related to the the base_orchestrator module for embedding.orchestrators . Base_orchestrator BaseEmbeddingOrchestrator Bases: ABC Abstract base class for embedding orchestration. This class defines the interface for embedding orchestrators that coordinate data extraction, splitting, and embedding processes. Source code in src/embedding/orchestrators/base_orchestrator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class BaseEmbeddingOrchestrator ( ABC ): \"\"\" Abstract base class for embedding orchestration. This class defines the interface for embedding orchestrators that coordinate data extraction, splitting, and embedding processes. \"\"\" def __init__ ( self , datasource_orchestrator : BaseDatasourceOrchestrator , splitter : BaseSplitter , embedder : BaseEmbedder , ) -> None : \"\"\" Initialize a new embedding orchestrator. Args: datasource_orchestrator: Orchestrator for extracting data from sources splitter: Component responsible for splitting documents into nodes embedder: Component that generates embeddings for nodes \"\"\" self . datasource_orchestrator = datasource_orchestrator self . splitter = splitter self . embedder = embedder @abstractmethod async def embed ( self ) -> None : \"\"\" Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. \"\"\" pass __init__ ( datasource_orchestrator , splitter , embedder ) Initialize a new embedding orchestrator. Parameters: datasource_orchestrator ( BaseDatasourceOrchestrator ) \u2013 Orchestrator for extracting data from sources splitter ( BaseSplitter ) \u2013 Component responsible for splitting documents into nodes embedder ( BaseEmbedder ) \u2013 Component that generates embeddings for nodes Source code in src/embedding/orchestrators/base_orchestrator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , datasource_orchestrator : BaseDatasourceOrchestrator , splitter : BaseSplitter , embedder : BaseEmbedder , ) -> None : \"\"\" Initialize a new embedding orchestrator. Args: datasource_orchestrator: Orchestrator for extracting data from sources splitter: Component responsible for splitting documents into nodes embedder: Component that generates embeddings for nodes \"\"\" self . datasource_orchestrator = datasource_orchestrator self . splitter = splitter self . embedder = embedder embed () abstractmethod async Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. Source code in src/embedding/orchestrators/base_orchestrator.py 34 35 36 37 38 39 40 41 42 @abstractmethod async def embed ( self ) -> None : \"\"\" Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. \"\"\" pass BasicEmbeddingOrchestrator Bases: BaseEmbeddingOrchestrator Basic implementation of embedding orchestration. This orchestrator implements a simple workflow that: 1. Retrieves documents from the datasource 2. Splits each document into nodes 3. Embeds the nodes 4. Flushes any remaining embeddings Source code in src/embedding/orchestrators/base_orchestrator.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class BasicEmbeddingOrchestrator ( BaseEmbeddingOrchestrator ): \"\"\" Basic implementation of embedding orchestration. This orchestrator implements a simple workflow that: 1. Retrieves documents from the datasource 2. Splits each document into nodes 3. Embeds the nodes 4. Flushes any remaining embeddings \"\"\" async def embed ( self ) -> None : \"\"\" Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush () embed () async Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. Source code in src/embedding/orchestrators/base_orchestrator.py 56 57 58 59 60 61 62 63 64 65 66 67 async def embed ( self ) -> None : \"\"\" Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"Base_orchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#base_orchestrator","text":"This module contains functionality related to the the base_orchestrator module for embedding.orchestrators .","title":"Base_orchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#base_orchestrator_1","text":"","title":"Base_orchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BaseEmbeddingOrchestrator","text":"Bases: ABC Abstract base class for embedding orchestration. This class defines the interface for embedding orchestrators that coordinate data extraction, splitting, and embedding processes. Source code in src/embedding/orchestrators/base_orchestrator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class BaseEmbeddingOrchestrator ( ABC ): \"\"\" Abstract base class for embedding orchestration. This class defines the interface for embedding orchestrators that coordinate data extraction, splitting, and embedding processes. \"\"\" def __init__ ( self , datasource_orchestrator : BaseDatasourceOrchestrator , splitter : BaseSplitter , embedder : BaseEmbedder , ) -> None : \"\"\" Initialize a new embedding orchestrator. Args: datasource_orchestrator: Orchestrator for extracting data from sources splitter: Component responsible for splitting documents into nodes embedder: Component that generates embeddings for nodes \"\"\" self . datasource_orchestrator = datasource_orchestrator self . splitter = splitter self . embedder = embedder @abstractmethod async def embed ( self ) -> None : \"\"\" Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. \"\"\" pass","title":"BaseEmbeddingOrchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BaseEmbeddingOrchestrator.__init__","text":"Initialize a new embedding orchestrator. Parameters: datasource_orchestrator ( BaseDatasourceOrchestrator ) \u2013 Orchestrator for extracting data from sources splitter ( BaseSplitter ) \u2013 Component responsible for splitting documents into nodes embedder ( BaseEmbedder ) \u2013 Component that generates embeddings for nodes Source code in src/embedding/orchestrators/base_orchestrator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , datasource_orchestrator : BaseDatasourceOrchestrator , splitter : BaseSplitter , embedder : BaseEmbedder , ) -> None : \"\"\" Initialize a new embedding orchestrator. Args: datasource_orchestrator: Orchestrator for extracting data from sources splitter: Component responsible for splitting documents into nodes embedder: Component that generates embeddings for nodes \"\"\" self . datasource_orchestrator = datasource_orchestrator self . splitter = splitter self . embedder = embedder","title":"__init__"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BaseEmbeddingOrchestrator.embed","text":"Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. Source code in src/embedding/orchestrators/base_orchestrator.py 34 35 36 37 38 39 40 41 42 @abstractmethod async def embed ( self ) -> None : \"\"\" Execute the embedding process. This method must be implemented by concrete subclasses to define the specific embedding workflow. \"\"\" pass","title":"embed"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BasicEmbeddingOrchestrator","text":"Bases: BaseEmbeddingOrchestrator Basic implementation of embedding orchestration. This orchestrator implements a simple workflow that: 1. Retrieves documents from the datasource 2. Splits each document into nodes 3. Embeds the nodes 4. Flushes any remaining embeddings Source code in src/embedding/orchestrators/base_orchestrator.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class BasicEmbeddingOrchestrator ( BaseEmbeddingOrchestrator ): \"\"\" Basic implementation of embedding orchestration. This orchestrator implements a simple workflow that: 1. Retrieves documents from the datasource 2. Splits each document into nodes 3. Embeds the nodes 4. Flushes any remaining embeddings \"\"\" async def embed ( self ) -> None : \"\"\" Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"BasicEmbeddingOrchestrator"},{"location":"src/embedding/orchestrators/base_orchestrator/#src.embedding.orchestrators.base_orchestrator.BasicEmbeddingOrchestrator.embed","text":"Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. Source code in src/embedding/orchestrators/base_orchestrator.py 56 57 58 59 60 61 62 63 64 65 66 67 async def embed ( self ) -> None : \"\"\" Execute the basic embedding process. Retrieves documents asynchronously from the datasource orchestrator, splits each document into nodes, embeds the nodes, and finally flushes any remaining embeddings to ensure all data is processed. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"embed"},{"location":"src/embedding/orchestrators/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.orchestrators . Registry EmbeddingOrchestratorRegistry Bases: Registry Registry for managing embedding orchestrators. This registry uses EmbeddingOrchestratorName as keys to store and retrieve orchestrator implementations. It extends the base Registry class to provide specialized functionality for embedding orchestration components. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingOrchestratorName enum. Source code in src/embedding/orchestrators/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 class EmbeddingOrchestratorRegistry ( Registry ): \"\"\"Registry for managing embedding orchestrators. This registry uses EmbeddingOrchestratorName as keys to store and retrieve orchestrator implementations. It extends the base Registry class to provide specialized functionality for embedding orchestration components. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingOrchestratorName enum. \"\"\" _key_class : Type = EmbeddingOrchestratorName","title":"Registry"},{"location":"src/embedding/orchestrators/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.orchestrators .","title":"Registry"},{"location":"src/embedding/orchestrators/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/orchestrators/registry/#src.embedding.orchestrators.registry.EmbeddingOrchestratorRegistry","text":"Bases: Registry Registry for managing embedding orchestrators. This registry uses EmbeddingOrchestratorName as keys to store and retrieve orchestrator implementations. It extends the base Registry class to provide specialized functionality for embedding orchestration components. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to EmbeddingOrchestratorName enum. Source code in src/embedding/orchestrators/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 class EmbeddingOrchestratorRegistry ( Registry ): \"\"\"Registry for managing embedding orchestrators. This registry uses EmbeddingOrchestratorName as keys to store and retrieve orchestrator implementations. It extends the base Registry class to provide specialized functionality for embedding orchestration components. Attributes: _key_class: The class type used as keys in the registry, set to EmbeddingOrchestratorName enum. \"\"\" _key_class : Type = EmbeddingOrchestratorName","title":"EmbeddingOrchestratorRegistry"},{"location":"src/embedding/orchestrators/basic/orchestrator/","text":"Orchestrator This module contains functionality related to the the orchestrator module for embedding.orchestrators.basic . Orchestrator BasicEmbeddingOrchestrator Bases: BaseEmbeddingOrchestrator A basic orchestrator for embedding pipeline processing. This orchestrator implements a straightforward process that: 1. Fetches documents from a datasource 2. Splits documents into nodes 3. Embeds those nodes Source code in src/embedding/orchestrators/basic/orchestrator.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class BasicEmbeddingOrchestrator ( BaseEmbeddingOrchestrator ): \"\"\" A basic orchestrator for embedding pipeline processing. This orchestrator implements a straightforward process that: 1. Fetches documents from a datasource 2. Splits documents into nodes 3. Embeds those nodes \"\"\" async def embed ( self ) -> None : \"\"\" Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush () embed () async Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. Source code in src/embedding/orchestrators/basic/orchestrator.py 23 24 25 26 27 28 29 30 31 32 33 34 35 async def embed ( self ) -> None : \"\"\" Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush () BasicEmbeddingOrchestratorFactory Bases: Factory Factory for creating BasicEmbeddingOrchestrator instances. Creates and configures orchestrators with appropriate datasources, splitters, and embedders based on the provided configuration. Source code in src/embedding/orchestrators/basic/orchestrator.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class BasicEmbeddingOrchestratorFactory ( Factory ): \"\"\" Factory for creating BasicEmbeddingOrchestrator instances. Creates and configures orchestrators with appropriate datasources, splitters, and embedders based on the provided configuration. \"\"\" _configuration_class : Type = EmbeddingConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingConfiguration ) -> BasicEmbeddingOrchestrator : \"\"\" Create a configured BasicEmbeddingOrchestrator instance. Args: configuration: Complete embedding configuration containing datasource, splitter, and embedder specifications Returns: A configured BasicEmbeddingOrchestrator ready for use Raises: ValueError: If splitter configuration is missing \"\"\" datasource_orchestrator = DatasourceOrchestratorRegistry . get ( configuration . extraction . orchestrator_name ) . create ( configuration ) embedding_model_configuration = configuration . embedding . embedding_model splitter_configuration = embedding_model_configuration . splitter if not splitter_configuration : raise ValueError ( \"Splitter configuration is required for embedding process.\" ) splitter = SplitterRegistry . get ( splitter_configuration . name ) . create ( embedding_model_configuration ) embedder = EmbedderRegistry . get ( configuration . embedding . embedder_name ) . create ( configuration ) return BasicEmbeddingOrchestrator ( datasource_orchestrator = datasource_orchestrator , splitter = splitter , embedder = embedder , )","title":"Orchestrator"},{"location":"src/embedding/orchestrators/basic/orchestrator/#orchestrator","text":"This module contains functionality related to the the orchestrator module for embedding.orchestrators.basic .","title":"Orchestrator"},{"location":"src/embedding/orchestrators/basic/orchestrator/#orchestrator_1","text":"","title":"Orchestrator"},{"location":"src/embedding/orchestrators/basic/orchestrator/#src.embedding.orchestrators.basic.orchestrator.BasicEmbeddingOrchestrator","text":"Bases: BaseEmbeddingOrchestrator A basic orchestrator for embedding pipeline processing. This orchestrator implements a straightforward process that: 1. Fetches documents from a datasource 2. Splits documents into nodes 3. Embeds those nodes Source code in src/embedding/orchestrators/basic/orchestrator.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class BasicEmbeddingOrchestrator ( BaseEmbeddingOrchestrator ): \"\"\" A basic orchestrator for embedding pipeline processing. This orchestrator implements a straightforward process that: 1. Fetches documents from a datasource 2. Splits documents into nodes 3. Embeds those nodes \"\"\" async def embed ( self ) -> None : \"\"\" Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"BasicEmbeddingOrchestrator"},{"location":"src/embedding/orchestrators/basic/orchestrator/#src.embedding.orchestrators.basic.orchestrator.BasicEmbeddingOrchestrator.embed","text":"Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. Source code in src/embedding/orchestrators/basic/orchestrator.py 23 24 25 26 27 28 29 30 31 32 33 34 35 async def embed ( self ) -> None : \"\"\" Execute the embedding process. Asynchronously retrieves documents from the datasource, splits them into nodes using the configured splitter, and embeds those nodes with the configured embedder. Finally flushes any remaining embeddings. \"\"\" async for doc in self . datasource_orchestrator . full_refresh_sync (): nodes = self . splitter . split ( doc ) self . embedder . embed ( nodes ) self . embedder . embed_flush ()","title":"embed"},{"location":"src/embedding/orchestrators/basic/orchestrator/#src.embedding.orchestrators.basic.orchestrator.BasicEmbeddingOrchestratorFactory","text":"Bases: Factory Factory for creating BasicEmbeddingOrchestrator instances. Creates and configures orchestrators with appropriate datasources, splitters, and embedders based on the provided configuration. Source code in src/embedding/orchestrators/basic/orchestrator.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class BasicEmbeddingOrchestratorFactory ( Factory ): \"\"\" Factory for creating BasicEmbeddingOrchestrator instances. Creates and configures orchestrators with appropriate datasources, splitters, and embedders based on the provided configuration. \"\"\" _configuration_class : Type = EmbeddingConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingConfiguration ) -> BasicEmbeddingOrchestrator : \"\"\" Create a configured BasicEmbeddingOrchestrator instance. Args: configuration: Complete embedding configuration containing datasource, splitter, and embedder specifications Returns: A configured BasicEmbeddingOrchestrator ready for use Raises: ValueError: If splitter configuration is missing \"\"\" datasource_orchestrator = DatasourceOrchestratorRegistry . get ( configuration . extraction . orchestrator_name ) . create ( configuration ) embedding_model_configuration = configuration . embedding . embedding_model splitter_configuration = embedding_model_configuration . splitter if not splitter_configuration : raise ValueError ( \"Splitter configuration is required for embedding process.\" ) splitter = SplitterRegistry . get ( splitter_configuration . name ) . create ( embedding_model_configuration ) embedder = EmbedderRegistry . get ( configuration . embedding . embedder_name ) . create ( configuration ) return BasicEmbeddingOrchestrator ( datasource_orchestrator = datasource_orchestrator , splitter = splitter , embedder = embedder , )","title":"BasicEmbeddingOrchestratorFactory"},{"location":"src/embedding/splitters/base_splitter/","text":"Base_splitter This module contains functionality related to the the base_splitter module for embedding.splitters . Base_splitter BaseSplitter Bases: ABC , Generic [ DocType ] Abstract base class for document splitter. This class defines a common interface for document splitters that transform various document types into text nodes for further processing. It leverages generic typing to support different document formats while maintaining type safety. Implementations should handle the specific logic required to parse and split different document types into meaningful text chunks. Source code in src/embedding/splitters/base_splitter.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class BaseSplitter ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document splitter. This class defines a common interface for document splitters that transform various document types into text nodes for further processing. It leverages generic typing to support different document formats while maintaining type safety. Implementations should handle the specific logic required to parse and split different document types into meaningful text chunks. \"\"\" @abstractmethod def split ( self , document : DocType ) -> TextNode : \"\"\"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Args: document: The document to split or process Returns: TextNode: The processed text node generated from the document \"\"\" pass split ( document ) abstractmethod Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Parameters: document ( DocType ) \u2013 The document to split or process Returns: TextNode ( TextNode ) \u2013 The processed text node generated from the document Source code in src/embedding/splitters/base_splitter.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @abstractmethod def split ( self , document : DocType ) -> TextNode : \"\"\"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Args: document: The document to split or process Returns: TextNode: The processed text node generated from the document \"\"\" pass","title":"Base_splitter"},{"location":"src/embedding/splitters/base_splitter/#base_splitter","text":"This module contains functionality related to the the base_splitter module for embedding.splitters .","title":"Base_splitter"},{"location":"src/embedding/splitters/base_splitter/#base_splitter_1","text":"","title":"Base_splitter"},{"location":"src/embedding/splitters/base_splitter/#src.embedding.splitters.base_splitter.BaseSplitter","text":"Bases: ABC , Generic [ DocType ] Abstract base class for document splitter. This class defines a common interface for document splitters that transform various document types into text nodes for further processing. It leverages generic typing to support different document formats while maintaining type safety. Implementations should handle the specific logic required to parse and split different document types into meaningful text chunks. Source code in src/embedding/splitters/base_splitter.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class BaseSplitter ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document splitter. This class defines a common interface for document splitters that transform various document types into text nodes for further processing. It leverages generic typing to support different document formats while maintaining type safety. Implementations should handle the specific logic required to parse and split different document types into meaningful text chunks. \"\"\" @abstractmethod def split ( self , document : DocType ) -> TextNode : \"\"\"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Args: document: The document to split or process Returns: TextNode: The processed text node generated from the document \"\"\" pass","title":"BaseSplitter"},{"location":"src/embedding/splitters/base_splitter/#src.embedding.splitters.base_splitter.BaseSplitter.split","text":"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Parameters: document ( DocType ) \u2013 The document to split or process Returns: TextNode ( TextNode ) \u2013 The processed text node generated from the document Source code in src/embedding/splitters/base_splitter.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @abstractmethod def split ( self , document : DocType ) -> TextNode : \"\"\"Split a document into a text node. This method processes a single document and converts it into a TextNode representation suitable for embedding or other processing. Implementing classes should define the specific logic for parsing different document types. Args: document: The document to split or process Returns: TextNode: The processed text node generated from the document \"\"\" pass","title":"split"},{"location":"src/embedding/splitters/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.splitters . Registry SplitterRegistry Bases: Registry Registry for document splitters that manages the registration and retrieval of different splitter implementations. This registry maps SplitterName enum values to their corresponding splitter implementations, allowing for a centralized way to access different text splitting strategies throughout the application. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to SplitterName enum. Source code in src/embedding/splitters/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 class SplitterRegistry ( Registry ): \"\"\" Registry for document splitters that manages the registration and retrieval of different splitter implementations. This registry maps SplitterName enum values to their corresponding splitter implementations, allowing for a centralized way to access different text splitting strategies throughout the application. Attributes: _key_class: The class type used as keys in the registry, set to SplitterName enum. \"\"\" _key_class : Type = SplitterName","title":"Registry"},{"location":"src/embedding/splitters/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.splitters .","title":"Registry"},{"location":"src/embedding/splitters/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/splitters/registry/#src.embedding.splitters.registry.SplitterRegistry","text":"Bases: Registry Registry for document splitters that manages the registration and retrieval of different splitter implementations. This registry maps SplitterName enum values to their corresponding splitter implementations, allowing for a centralized way to access different text splitting strategies throughout the application. Attributes: _key_class ( Type ) \u2013 The class type used as keys in the registry, set to SplitterName enum. Source code in src/embedding/splitters/registry.py 9 10 11 12 13 14 15 16 17 18 19 20 class SplitterRegistry ( Registry ): \"\"\" Registry for document splitters that manages the registration and retrieval of different splitter implementations. This registry maps SplitterName enum values to their corresponding splitter implementations, allowing for a centralized way to access different text splitting strategies throughout the application. Attributes: _key_class: The class type used as keys in the registry, set to SplitterName enum. \"\"\" _key_class : Type = SplitterName","title":"SplitterRegistry"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/","text":"Basic_markdown_splitter This module contains functionality related to the the basic_markdown_splitter module for embedding.splitters.basic_markdown . Basic_markdown_splitter BasicMarkdownSplitter Bases: BaseSplitter , Generic [ DocType ] Splitter for markdown documents with token-based chunking. Splits markdown content into nodes based on document structure and token limits. Supports node merging and splitting to maintain consistent chunk sizes. Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class BasicMarkdownSplitter ( BaseSplitter , Generic [ DocType ]): \"\"\"Splitter for markdown documents with token-based chunking. Splits markdown content into nodes based on document structure and token limits. Supports node merging and splitting to maintain consistent chunk sizes. \"\"\" def __init__ ( self , chunk_size_in_tokens : int , chunk_overlap_in_tokens : int , tokenize_func : Callable , ): \"\"\"Initialize markdown splitter. Args: chunk_size_in_tokens: Maximum tokens per chunk chunk_overlap_in_tokens: Token overlap between chunks tokenize_func: Function to tokenize text for token counting \"\"\" self . chunk_size_in_tokens = chunk_size_in_tokens self . tokenize_func = tokenize_func self . markdown_node_parser = MarkdownNodeParser () self . sentence_splitter = SentenceSplitter ( chunk_size = chunk_size_in_tokens , chunk_overlap = chunk_overlap_in_tokens , tokenizer = tokenize_func , ) def split ( self , document : DocType ) -> TextNode : \"\"\"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Args: document: Markdown document to be processed Returns: List[TextNode]: Collection of processed text nodes with optimized sizes \"\"\" document_nodes = self . markdown_node_parser . get_nodes_from_documents ( [ document ] ) document_nodes = self . _split_big_nodes ( document_nodes ) document_nodes = self . _merge_small_nodes ( document_nodes ) return document_nodes def _split_big_nodes ( self , document_nodes : List [ TextNode ] ) -> List [ TextNode ]: \"\"\"Split oversized nodes into smaller chunks. Identifies nodes exceeding the token limit and processes them through the sentence splitter to create smaller, semantically coherent chunks. Args: document_nodes: Collection of nodes to process Returns: List[TextNode]: Processed nodes within token size limits \"\"\" new_document_nodes = [] for document_node in document_nodes : text = document_node . text document_node_size = len ( self . tokenize_func ( text )) if document_node_size > self . chunk_size_in_tokens : document_sub_nodes = self . _split_big_node ( document_node ) new_document_nodes . extend ( document_sub_nodes ) else : new_document_nodes . append ( document_node ) return new_document_nodes def _split_big_node ( self , document_node : TextNode ) -> List [ TextNode ]: \"\"\"Split single oversized node into smaller nodes. Uses sentence boundary detection to create semantically meaningful smaller chunks from a large node, preserving metadata from the original node. Args: document_node: Node exceeding token size limit Returns: List[TextNode]: Collection of smaller nodes derived from original \"\"\" text = document_node . text sub_texts = self . sentence_splitter . split_text ( text ) sub_nodes = [] for sub_text in sub_texts : sub_node = document_node . model_copy () sub_node . id_ = str ( uuid . uuid4 ()) sub_node . text = sub_text sub_nodes . append ( sub_node ) return sub_nodes def _merge_small_nodes ( self , document_nodes : List [ TextNode ] ) -> List [ TextNode ]: \"\"\"Merge adjacent small nodes into larger chunks. Combines consecutive nodes when their combined token count remains under the maximum limit, optimizing for fewer, larger chunks while respecting token boundaries. Args: document_nodes: Collection of nodes to potentially merge Returns: List[TextNode]: Optimized collection with merged nodes \"\"\" new_document_nodes = [] current_node = document_nodes [ 0 ] for node in document_nodes [ 1 :]: current_text = current_node . text current_node_size = len ( self . tokenize_func ( current_text )) node_text = node . text node_size = len ( self . tokenize_func ( node_text )) if current_node_size + node_size <= self . chunk_size_in_tokens : current_node . text += node . text else : new_document_nodes . append ( current_node ) current_node = node new_document_nodes . append ( current_node ) return new_document_nodes __init__ ( chunk_size_in_tokens , chunk_overlap_in_tokens , tokenize_func ) Initialize markdown splitter. Parameters: chunk_size_in_tokens ( int ) \u2013 Maximum tokens per chunk chunk_overlap_in_tokens ( int ) \u2013 Token overlap between chunks tokenize_func ( Callable ) \u2013 Function to tokenize text for token counting Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , chunk_size_in_tokens : int , chunk_overlap_in_tokens : int , tokenize_func : Callable , ): \"\"\"Initialize markdown splitter. Args: chunk_size_in_tokens: Maximum tokens per chunk chunk_overlap_in_tokens: Token overlap between chunks tokenize_func: Function to tokenize text for token counting \"\"\" self . chunk_size_in_tokens = chunk_size_in_tokens self . tokenize_func = tokenize_func self . markdown_node_parser = MarkdownNodeParser () self . sentence_splitter = SentenceSplitter ( chunk_size = chunk_size_in_tokens , chunk_overlap = chunk_overlap_in_tokens , tokenizer = tokenize_func , ) split ( document ) Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Parameters: document ( DocType ) \u2013 Markdown document to be processed Returns: TextNode \u2013 List[TextNode]: Collection of processed text nodes with optimized sizes Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def split ( self , document : DocType ) -> TextNode : \"\"\"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Args: document: Markdown document to be processed Returns: List[TextNode]: Collection of processed text nodes with optimized sizes \"\"\" document_nodes = self . markdown_node_parser . get_nodes_from_documents ( [ document ] ) document_nodes = self . _split_big_nodes ( document_nodes ) document_nodes = self . _merge_small_nodes ( document_nodes ) return document_nodes BasicMarkdownSplitterFactory Bases: Factory Factory for creating BasicMarkdownSplitter instances. Creates splitter instances configured according to the provided embedding model configuration. Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class BasicMarkdownSplitterFactory ( Factory ): \"\"\"Factory for creating BasicMarkdownSplitter instances. Creates splitter instances configured according to the provided embedding model configuration. \"\"\" _configuration_class : Type = EmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingModelConfiguration ) -> BasicMarkdownSplitter : \"\"\"Create a BasicMarkdownSplitter instance from configuration. Validates the configuration, retrieves the appropriate tokenizer, and instantiates a properly configured splitter. Args: configuration: Embedding model configuration containing splitter settings Returns: BasicMarkdownSplitter: Configured splitter instance Raises: ValueError: If the configuration lacks proper splitter settings \"\"\" if not configuration . splitter or not isinstance ( configuration . splitter , BasicMarkdownSplitterConfiguration ): raise ValueError ( \"`BasicMarkdownSplitterConfiguration` configuration is required for `BasicMarkdownSplitter`.\" ) tokenizer_func = EmbeddingModelTokenizerRegistry . get ( configuration . provider ) . create ( configuration ) return BasicMarkdownSplitter ( chunk_size_in_tokens = configuration . splitter . chunk_size_in_tokens , chunk_overlap_in_tokens = configuration . splitter . chunk_overlap_in_tokens , tokenize_func = tokenizer_func , )","title":"Basic_markdown_splitter"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#basic_markdown_splitter","text":"This module contains functionality related to the the basic_markdown_splitter module for embedding.splitters.basic_markdown .","title":"Basic_markdown_splitter"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#basic_markdown_splitter_1","text":"","title":"Basic_markdown_splitter"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#src.embedding.splitters.basic_markdown.basic_markdown_splitter.BasicMarkdownSplitter","text":"Bases: BaseSplitter , Generic [ DocType ] Splitter for markdown documents with token-based chunking. Splits markdown content into nodes based on document structure and token limits. Supports node merging and splitting to maintain consistent chunk sizes. Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class BasicMarkdownSplitter ( BaseSplitter , Generic [ DocType ]): \"\"\"Splitter for markdown documents with token-based chunking. Splits markdown content into nodes based on document structure and token limits. Supports node merging and splitting to maintain consistent chunk sizes. \"\"\" def __init__ ( self , chunk_size_in_tokens : int , chunk_overlap_in_tokens : int , tokenize_func : Callable , ): \"\"\"Initialize markdown splitter. Args: chunk_size_in_tokens: Maximum tokens per chunk chunk_overlap_in_tokens: Token overlap between chunks tokenize_func: Function to tokenize text for token counting \"\"\" self . chunk_size_in_tokens = chunk_size_in_tokens self . tokenize_func = tokenize_func self . markdown_node_parser = MarkdownNodeParser () self . sentence_splitter = SentenceSplitter ( chunk_size = chunk_size_in_tokens , chunk_overlap = chunk_overlap_in_tokens , tokenizer = tokenize_func , ) def split ( self , document : DocType ) -> TextNode : \"\"\"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Args: document: Markdown document to be processed Returns: List[TextNode]: Collection of processed text nodes with optimized sizes \"\"\" document_nodes = self . markdown_node_parser . get_nodes_from_documents ( [ document ] ) document_nodes = self . _split_big_nodes ( document_nodes ) document_nodes = self . _merge_small_nodes ( document_nodes ) return document_nodes def _split_big_nodes ( self , document_nodes : List [ TextNode ] ) -> List [ TextNode ]: \"\"\"Split oversized nodes into smaller chunks. Identifies nodes exceeding the token limit and processes them through the sentence splitter to create smaller, semantically coherent chunks. Args: document_nodes: Collection of nodes to process Returns: List[TextNode]: Processed nodes within token size limits \"\"\" new_document_nodes = [] for document_node in document_nodes : text = document_node . text document_node_size = len ( self . tokenize_func ( text )) if document_node_size > self . chunk_size_in_tokens : document_sub_nodes = self . _split_big_node ( document_node ) new_document_nodes . extend ( document_sub_nodes ) else : new_document_nodes . append ( document_node ) return new_document_nodes def _split_big_node ( self , document_node : TextNode ) -> List [ TextNode ]: \"\"\"Split single oversized node into smaller nodes. Uses sentence boundary detection to create semantically meaningful smaller chunks from a large node, preserving metadata from the original node. Args: document_node: Node exceeding token size limit Returns: List[TextNode]: Collection of smaller nodes derived from original \"\"\" text = document_node . text sub_texts = self . sentence_splitter . split_text ( text ) sub_nodes = [] for sub_text in sub_texts : sub_node = document_node . model_copy () sub_node . id_ = str ( uuid . uuid4 ()) sub_node . text = sub_text sub_nodes . append ( sub_node ) return sub_nodes def _merge_small_nodes ( self , document_nodes : List [ TextNode ] ) -> List [ TextNode ]: \"\"\"Merge adjacent small nodes into larger chunks. Combines consecutive nodes when their combined token count remains under the maximum limit, optimizing for fewer, larger chunks while respecting token boundaries. Args: document_nodes: Collection of nodes to potentially merge Returns: List[TextNode]: Optimized collection with merged nodes \"\"\" new_document_nodes = [] current_node = document_nodes [ 0 ] for node in document_nodes [ 1 :]: current_text = current_node . text current_node_size = len ( self . tokenize_func ( current_text )) node_text = node . text node_size = len ( self . tokenize_func ( node_text )) if current_node_size + node_size <= self . chunk_size_in_tokens : current_node . text += node . text else : new_document_nodes . append ( current_node ) current_node = node new_document_nodes . append ( current_node ) return new_document_nodes","title":"BasicMarkdownSplitter"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#src.embedding.splitters.basic_markdown.basic_markdown_splitter.BasicMarkdownSplitter.__init__","text":"Initialize markdown splitter. Parameters: chunk_size_in_tokens ( int ) \u2013 Maximum tokens per chunk chunk_overlap_in_tokens ( int ) \u2013 Token overlap between chunks tokenize_func ( Callable ) \u2013 Function to tokenize text for token counting Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , chunk_size_in_tokens : int , chunk_overlap_in_tokens : int , tokenize_func : Callable , ): \"\"\"Initialize markdown splitter. Args: chunk_size_in_tokens: Maximum tokens per chunk chunk_overlap_in_tokens: Token overlap between chunks tokenize_func: Function to tokenize text for token counting \"\"\" self . chunk_size_in_tokens = chunk_size_in_tokens self . tokenize_func = tokenize_func self . markdown_node_parser = MarkdownNodeParser () self . sentence_splitter = SentenceSplitter ( chunk_size = chunk_size_in_tokens , chunk_overlap = chunk_overlap_in_tokens , tokenizer = tokenize_func , )","title":"__init__"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#src.embedding.splitters.basic_markdown.basic_markdown_splitter.BasicMarkdownSplitter.split","text":"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Parameters: document ( DocType ) \u2013 Markdown document to be processed Returns: TextNode \u2013 List[TextNode]: Collection of processed text nodes with optimized sizes Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def split ( self , document : DocType ) -> TextNode : \"\"\"Split markdown documents into text nodes. Split markdown document by markdown tags, then adjusts node sizes through splitting large nodes and merging small nodes to optimize for the target chunk size. Args: document: Markdown document to be processed Returns: List[TextNode]: Collection of processed text nodes with optimized sizes \"\"\" document_nodes = self . markdown_node_parser . get_nodes_from_documents ( [ document ] ) document_nodes = self . _split_big_nodes ( document_nodes ) document_nodes = self . _merge_small_nodes ( document_nodes ) return document_nodes","title":"split"},{"location":"src/embedding/splitters/basic_markdown/basic_markdown_splitter/#src.embedding.splitters.basic_markdown.basic_markdown_splitter.BasicMarkdownSplitterFactory","text":"Bases: Factory Factory for creating BasicMarkdownSplitter instances. Creates splitter instances configured according to the provided embedding model configuration. Source code in src/embedding/splitters/basic_markdown/basic_markdown_splitter.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class BasicMarkdownSplitterFactory ( Factory ): \"\"\"Factory for creating BasicMarkdownSplitter instances. Creates splitter instances configured according to the provided embedding model configuration. \"\"\" _configuration_class : Type = EmbeddingModelConfiguration @classmethod def _create_instance ( cls , configuration : EmbeddingModelConfiguration ) -> BasicMarkdownSplitter : \"\"\"Create a BasicMarkdownSplitter instance from configuration. Validates the configuration, retrieves the appropriate tokenizer, and instantiates a properly configured splitter. Args: configuration: Embedding model configuration containing splitter settings Returns: BasicMarkdownSplitter: Configured splitter instance Raises: ValueError: If the configuration lacks proper splitter settings \"\"\" if not configuration . splitter or not isinstance ( configuration . splitter , BasicMarkdownSplitterConfiguration ): raise ValueError ( \"`BasicMarkdownSplitterConfiguration` configuration is required for `BasicMarkdownSplitter`.\" ) tokenizer_func = EmbeddingModelTokenizerRegistry . get ( configuration . provider ) . create ( configuration ) return BasicMarkdownSplitter ( chunk_size_in_tokens = configuration . splitter . chunk_size_in_tokens , chunk_overlap_in_tokens = configuration . splitter . chunk_overlap_in_tokens , tokenize_func = tokenizer_func , )","title":"BasicMarkdownSplitterFactory"},{"location":"src/embedding/splitters/basic_markdown/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.splitters.basic_markdown . Configuration BasicMarkdownSplitterConfiguration Bases: SplitterConfiguration Configuration for the BasicMarkdownSplitter. This class defines the parameters needed to split markdown documents into chunks with specific token sizes and overlaps. Source code in src/embedding/splitters/basic_markdown/configuration.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class BasicMarkdownSplitterConfiguration ( SplitterConfiguration ): \"\"\" Configuration for the BasicMarkdownSplitter. This class defines the parameters needed to split markdown documents into chunks with specific token sizes and overlaps. \"\"\" chunk_overlap_in_tokens : int = Field ( ... , description = \"The number of tokens that overlap between chunks.\" ) chunk_size_in_tokens : int = Field ( ... , description = \"The size of each chunk in tokens.\" ) name : SplitterName = Field ( SplitterName . BASIC_MARKDOWN , description = \"The name of the splitter.\" )","title":"Configuration"},{"location":"src/embedding/splitters/basic_markdown/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.splitters.basic_markdown .","title":"Configuration"},{"location":"src/embedding/splitters/basic_markdown/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/splitters/basic_markdown/configuration/#src.embedding.splitters.basic_markdown.configuration.BasicMarkdownSplitterConfiguration","text":"Bases: SplitterConfiguration Configuration for the BasicMarkdownSplitter. This class defines the parameters needed to split markdown documents into chunks with specific token sizes and overlaps. Source code in src/embedding/splitters/basic_markdown/configuration.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class BasicMarkdownSplitterConfiguration ( SplitterConfiguration ): \"\"\" Configuration for the BasicMarkdownSplitter. This class defines the parameters needed to split markdown documents into chunks with specific token sizes and overlaps. \"\"\" chunk_overlap_in_tokens : int = Field ( ... , description = \"The number of tokens that overlap between chunks.\" ) chunk_size_in_tokens : int = Field ( ... , description = \"The size of each chunk in tokens.\" ) name : SplitterName = Field ( SplitterName . BASIC_MARKDOWN , description = \"The name of the splitter.\" )","title":"BasicMarkdownSplitterConfiguration"},{"location":"src/embedding/vector_stores/registry/","text":"Registry This module contains functionality related to the the registry module for embedding.vector_stores . Registry VectorStoreRegistry Bases: Registry Registry for vector store implementations. Maps VectorStoreName values to vector store implementation classes. Used for registering and retrieving different vector store backends that can be configured in the system. Source code in src/embedding/vector_stores/registry.py 9 10 11 12 13 14 15 16 17 18 class VectorStoreRegistry ( Registry ): \"\"\" Registry for vector store implementations. Maps VectorStoreName values to vector store implementation classes. Used for registering and retrieving different vector store backends that can be configured in the system. \"\"\" _key_class : Type = VectorStoreName VectorStoreValidatorRegistry Bases: Registry Registry for vector store validator implementations. Maps VectorStoreName values to validator classes responsible for validating vector store configurations before use. Validators ensure proper setup and connection parameters. Source code in src/embedding/vector_stores/registry.py 21 22 23 24 25 26 27 28 29 30 class VectorStoreValidatorRegistry ( Registry ): \"\"\" Registry for vector store validator implementations. Maps VectorStoreName values to validator classes responsible for validating vector store configurations before use. Validators ensure proper setup and connection parameters. \"\"\" _key_class : Type = VectorStoreName","title":"Registry"},{"location":"src/embedding/vector_stores/registry/#registry","text":"This module contains functionality related to the the registry module for embedding.vector_stores .","title":"Registry"},{"location":"src/embedding/vector_stores/registry/#registry_1","text":"","title":"Registry"},{"location":"src/embedding/vector_stores/registry/#src.embedding.vector_stores.registry.VectorStoreRegistry","text":"Bases: Registry Registry for vector store implementations. Maps VectorStoreName values to vector store implementation classes. Used for registering and retrieving different vector store backends that can be configured in the system. Source code in src/embedding/vector_stores/registry.py 9 10 11 12 13 14 15 16 17 18 class VectorStoreRegistry ( Registry ): \"\"\" Registry for vector store implementations. Maps VectorStoreName values to vector store implementation classes. Used for registering and retrieving different vector store backends that can be configured in the system. \"\"\" _key_class : Type = VectorStoreName","title":"VectorStoreRegistry"},{"location":"src/embedding/vector_stores/registry/#src.embedding.vector_stores.registry.VectorStoreValidatorRegistry","text":"Bases: Registry Registry for vector store validator implementations. Maps VectorStoreName values to validator classes responsible for validating vector store configurations before use. Validators ensure proper setup and connection parameters. Source code in src/embedding/vector_stores/registry.py 21 22 23 24 25 26 27 28 29 30 class VectorStoreValidatorRegistry ( Registry ): \"\"\" Registry for vector store validator implementations. Maps VectorStoreName values to validator classes responsible for validating vector store configurations before use. Validators ensure proper setup and connection parameters. \"\"\" _key_class : Type = VectorStoreName","title":"VectorStoreValidatorRegistry"},{"location":"src/embedding/vector_stores/chroma/client/","text":"Client This module contains functionality related to the the client module for embedding.vector_stores.chroma . Client ChromaVectorStoreClientFactory Bases: SingletonFactory Factory for creating and managing Chroma vector store client instances. This factory implements the Singleton pattern, ensuring only one client instance exists per unique configuration. It creates HTTP clients for connecting to Chroma DB vector store services. Source code in src/embedding/vector_stores/chroma/client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ChromaVectorStoreClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Chroma vector store client instances. This factory implements the Singleton pattern, ensuring only one client instance exists per unique configuration. It creates HTTP clients for connecting to Chroma DB vector store services. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaClient : \"\"\" Create a new Chroma client instance based on the provided configuration. Args: configuration (ChromaVectorStoreConfiguration): Configuration containing connection details for the Chroma vector store. Returns: ChromaClient: An HTTP client connected to the specified Chroma service. \"\"\" return ChromaHttpClient ( host = configuration . host , port = configuration . port , )","title":"Client"},{"location":"src/embedding/vector_stores/chroma/client/#client","text":"This module contains functionality related to the the client module for embedding.vector_stores.chroma .","title":"Client"},{"location":"src/embedding/vector_stores/chroma/client/#client_1","text":"","title":"Client"},{"location":"src/embedding/vector_stores/chroma/client/#src.embedding.vector_stores.chroma.client.ChromaVectorStoreClientFactory","text":"Bases: SingletonFactory Factory for creating and managing Chroma vector store client instances. This factory implements the Singleton pattern, ensuring only one client instance exists per unique configuration. It creates HTTP clients for connecting to Chroma DB vector store services. Source code in src/embedding/vector_stores/chroma/client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ChromaVectorStoreClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Chroma vector store client instances. This factory implements the Singleton pattern, ensuring only one client instance exists per unique configuration. It creates HTTP clients for connecting to Chroma DB vector store services. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaClient : \"\"\" Create a new Chroma client instance based on the provided configuration. Args: configuration (ChromaVectorStoreConfiguration): Configuration containing connection details for the Chroma vector store. Returns: ChromaClient: An HTTP client connected to the specified Chroma service. \"\"\" return ChromaHttpClient ( host = configuration . host , port = configuration . port , )","title":"ChromaVectorStoreClientFactory"},{"location":"src/embedding/vector_stores/chroma/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.vector_stores.chroma . Configuration ChromaVectorStoreConfiguration Bases: VectorStoreConfiguration Configuration for the ChromaDB vector store. Source code in src/embedding/vector_stores/chroma/configuration.py 11 12 13 14 15 16 class ChromaVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\"Configuration for the ChromaDB vector store.\"\"\" name : Literal [ VectorStoreName . CHROMA ] = Field ( ... , description = \"The name of the vector store.\" )","title":"Configuration"},{"location":"src/embedding/vector_stores/chroma/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.vector_stores.chroma .","title":"Configuration"},{"location":"src/embedding/vector_stores/chroma/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/vector_stores/chroma/configuration/#src.embedding.vector_stores.chroma.configuration.ChromaVectorStoreConfiguration","text":"Bases: VectorStoreConfiguration Configuration for the ChromaDB vector store. Source code in src/embedding/vector_stores/chroma/configuration.py 11 12 13 14 15 16 class ChromaVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\"Configuration for the ChromaDB vector store.\"\"\" name : Literal [ VectorStoreName . CHROMA ] = Field ( ... , description = \"The name of the vector store.\" )","title":"ChromaVectorStoreConfiguration"},{"location":"src/embedding/vector_stores/chroma/validator/","text":"Validator This module contains functionality related to the the validator module for embedding.vector_stores.chroma . Validator ChromaVectorStoreValidator Bases: BaseVectorStoreValidator Validator for Chroma vector store configuration. Validates collection settings and existence for Chroma vector store backend. Ensures proper configuration before operations are performed against the vector store. Source code in src/embedding/vector_stores/chroma/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class ChromaVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Chroma vector store configuration. Validates collection settings and existence for Chroma vector store backend. Ensures proper configuration before operations are performed against the vector store. \"\"\" def __init__ ( self , configuration : ChromaVectorStoreConfiguration , client : ChromaClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Chroma vector store settings client: Client for Chroma operations \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException: If collection already exists \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException: If collection already exists \"\"\" collection_name = self . configuration . collection_name if collection_name in self . client . list_collections (): raise CollectionExistsException ( collection_name ) __init__ ( configuration , client ) Initialize validator with configuration and client. Parameters: configuration ( ChromaVectorStoreConfiguration ) \u2013 Chroma vector store settings client ( ClientAPI ) \u2013 Client for Chroma operations Source code in src/embedding/vector_stores/chroma/validator.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration : ChromaVectorStoreConfiguration , client : ChromaClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Chroma vector store settings client: Client for Chroma operations \"\"\" self . configuration = configuration self . client = client validate () Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException \u2013 If collection already exists Source code in src/embedding/vector_stores/chroma/validator.py 36 37 38 39 40 41 42 43 44 45 def validate ( self ) -> None : \"\"\"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException: If collection already exists \"\"\" self . validate_collection () validate_collection () Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException \u2013 If collection already exists Source code in src/embedding/vector_stores/chroma/validator.py 47 48 49 50 51 52 53 54 55 56 57 58 def validate_collection ( self ) -> None : \"\"\"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException: If collection already exists \"\"\" collection_name = self . configuration . collection_name if collection_name in self . client . list_collections (): raise CollectionExistsException ( collection_name ) ChromaVectorStoreValidatorFactory Bases: SingletonFactory Factory for creating configured Chroma validator instances. Manages the creation and caching of ChromaVectorStoreValidator instances based on provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for Chroma vector store. Source code in src/embedding/vector_stores/chroma/validator.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class ChromaVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating configured Chroma validator instances. Manages the creation and caching of ChromaVectorStoreValidator instances based on provided configuration. Attributes: _configuration_class (Type): The configuration class for Chroma vector store. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaVectorStoreValidator : \"\"\"Creates a Chroma validator based on provided configuration. Args: configuration: Chroma connection configuration. Returns: ChromaVectorStoreValidator: Configured validator instance. \"\"\" client = ChromaVectorStoreClientFactory . create ( configuration ) return ChromaVectorStoreValidator ( configuration = configuration , client = client )","title":"Validator"},{"location":"src/embedding/vector_stores/chroma/validator/#validator","text":"This module contains functionality related to the the validator module for embedding.vector_stores.chroma .","title":"Validator"},{"location":"src/embedding/vector_stores/chroma/validator/#validator_1","text":"","title":"Validator"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidator","text":"Bases: BaseVectorStoreValidator Validator for Chroma vector store configuration. Validates collection settings and existence for Chroma vector store backend. Ensures proper configuration before operations are performed against the vector store. Source code in src/embedding/vector_stores/chroma/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class ChromaVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Chroma vector store configuration. Validates collection settings and existence for Chroma vector store backend. Ensures proper configuration before operations are performed against the vector store. \"\"\" def __init__ ( self , configuration : ChromaVectorStoreConfiguration , client : ChromaClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Chroma vector store settings client: Client for Chroma operations \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException: If collection already exists \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException: If collection already exists \"\"\" collection_name = self . configuration . collection_name if collection_name in self . client . list_collections (): raise CollectionExistsException ( collection_name )","title":"ChromaVectorStoreValidator"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidator.__init__","text":"Initialize validator with configuration and client. Parameters: configuration ( ChromaVectorStoreConfiguration ) \u2013 Chroma vector store settings client ( ClientAPI ) \u2013 Client for Chroma operations Source code in src/embedding/vector_stores/chroma/validator.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration : ChromaVectorStoreConfiguration , client : ChromaClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Chroma vector store settings client: Client for Chroma operations \"\"\" self . configuration = configuration self . client = client","title":"__init__"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidator.validate","text":"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException \u2013 If collection already exists Source code in src/embedding/vector_stores/chroma/validator.py 36 37 38 39 40 41 42 43 44 45 def validate ( self ) -> None : \"\"\"Validate the Chroma vector store settings. Performs all required validation steps for the Chroma vector store, including collection validation. Raises: CollectionExistsException: If collection already exists \"\"\" self . validate_collection ()","title":"validate"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidator.validate_collection","text":"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException \u2013 If collection already exists Source code in src/embedding/vector_stores/chroma/validator.py 47 48 49 50 51 52 53 54 55 56 57 58 def validate_collection ( self ) -> None : \"\"\"Validate Chroma collection existence. Checks if a collection with the specified name already exists in the Chroma database. Raises: CollectionExistsException: If collection already exists \"\"\" collection_name = self . configuration . collection_name if collection_name in self . client . list_collections (): raise CollectionExistsException ( collection_name )","title":"validate_collection"},{"location":"src/embedding/vector_stores/chroma/validator/#src.embedding.vector_stores.chroma.validator.ChromaVectorStoreValidatorFactory","text":"Bases: SingletonFactory Factory for creating configured Chroma validator instances. Manages the creation and caching of ChromaVectorStoreValidator instances based on provided configuration. Attributes: _configuration_class ( Type ) \u2013 The configuration class for Chroma vector store. Source code in src/embedding/vector_stores/chroma/validator.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class ChromaVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating configured Chroma validator instances. Manages the creation and caching of ChromaVectorStoreValidator instances based on provided configuration. Attributes: _configuration_class (Type): The configuration class for Chroma vector store. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaVectorStoreValidator : \"\"\"Creates a Chroma validator based on provided configuration. Args: configuration: Chroma connection configuration. Returns: ChromaVectorStoreValidator: Configured validator instance. \"\"\" client = ChromaVectorStoreClientFactory . create ( configuration ) return ChromaVectorStoreValidator ( configuration = configuration , client = client )","title":"ChromaVectorStoreValidatorFactory"},{"location":"src/embedding/vector_stores/chroma/vector_store/","text":"Vector_store This module contains functionality related to the the vector_store module for embedding.vector_stores.chroma . Vector_store ChromaVectorStoreFactory Bases: SingletonFactory Factory for creating configured Chroma vector stores. This singleton factory creates and manages ChromaVectorStore instances based on the provided configuration. It ensures that only one instance is created for each unique configuration. Source code in src/embedding/vector_stores/chroma/vector_store.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class ChromaVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured Chroma vector stores. This singleton factory creates and manages ChromaVectorStore instances based on the provided configuration. It ensures that only one instance is created for each unique configuration. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaVectorStore : \"\"\"Creates a Chroma vector store based on provided configuration. Args: configuration: Chroma vector store connection configuration containing host, port, and collection name. Returns: ChromaVectorStore: Configured Chroma vector store instance. \"\"\" return ChromaVectorStore ( host = configuration . host , port = str ( configuration . port ), collection_name = configuration . collection_name , )","title":"Vector_store"},{"location":"src/embedding/vector_stores/chroma/vector_store/#vector_store","text":"This module contains functionality related to the the vector_store module for embedding.vector_stores.chroma .","title":"Vector_store"},{"location":"src/embedding/vector_stores/chroma/vector_store/#vector_store_1","text":"","title":"Vector_store"},{"location":"src/embedding/vector_stores/chroma/vector_store/#src.embedding.vector_stores.chroma.vector_store.ChromaVectorStoreFactory","text":"Bases: SingletonFactory Factory for creating configured Chroma vector stores. This singleton factory creates and manages ChromaVectorStore instances based on the provided configuration. It ensures that only one instance is created for each unique configuration. Source code in src/embedding/vector_stores/chroma/vector_store.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class ChromaVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured Chroma vector stores. This singleton factory creates and manages ChromaVectorStore instances based on the provided configuration. It ensures that only one instance is created for each unique configuration. \"\"\" _configuration_class : Type = ChromaVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : ChromaVectorStoreConfiguration ) -> ChromaVectorStore : \"\"\"Creates a Chroma vector store based on provided configuration. Args: configuration: Chroma vector store connection configuration containing host, port, and collection name. Returns: ChromaVectorStore: Configured Chroma vector store instance. \"\"\" return ChromaVectorStore ( host = configuration . host , port = str ( configuration . port ), collection_name = configuration . collection_name , )","title":"ChromaVectorStoreFactory"},{"location":"src/embedding/vector_stores/core/exceptions/","text":"Exceptions This module contains functionality related to the the exceptions module for embedding.vector_stores.core . Exceptions CollectionExistsException Bases: Exception Exception raised when attempting to create a vector store collection that already exists. This exception helps prevent accidental overwriting of existing collections and provides clear error messaging about the conflicting collection name. Source code in src/embedding/vector_stores/core/exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class CollectionExistsException ( Exception ): \"\"\"Exception raised when attempting to create a vector store collection that already exists. This exception helps prevent accidental overwriting of existing collections and provides clear error messaging about the conflicting collection name. \"\"\" def __init__ ( self , collection_name : str ) -> None : \"\"\"Initialize the CollectionExistsException with the conflicting collection name. Args: collection_name: Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. \"\"\" self . collection_name = collection_name self . message = f \"Collection with name { collection_name } already exists\" super () . __init__ ( self . message ) __init__ ( collection_name ) Initialize the CollectionExistsException with the conflicting collection name. Parameters: collection_name ( str ) \u2013 Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. Source code in src/embedding/vector_stores/core/exceptions.py 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , collection_name : str ) -> None : \"\"\"Initialize the CollectionExistsException with the conflicting collection name. Args: collection_name: Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. \"\"\" self . collection_name = collection_name self . message = f \"Collection with name { collection_name } already exists\" super () . __init__ ( self . message )","title":"Exceptions"},{"location":"src/embedding/vector_stores/core/exceptions/#exceptions","text":"This module contains functionality related to the the exceptions module for embedding.vector_stores.core .","title":"Exceptions"},{"location":"src/embedding/vector_stores/core/exceptions/#exceptions_1","text":"","title":"Exceptions"},{"location":"src/embedding/vector_stores/core/exceptions/#src.embedding.vector_stores.core.exceptions.CollectionExistsException","text":"Bases: Exception Exception raised when attempting to create a vector store collection that already exists. This exception helps prevent accidental overwriting of existing collections and provides clear error messaging about the conflicting collection name. Source code in src/embedding/vector_stores/core/exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class CollectionExistsException ( Exception ): \"\"\"Exception raised when attempting to create a vector store collection that already exists. This exception helps prevent accidental overwriting of existing collections and provides clear error messaging about the conflicting collection name. \"\"\" def __init__ ( self , collection_name : str ) -> None : \"\"\"Initialize the CollectionExistsException with the conflicting collection name. Args: collection_name: Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. \"\"\" self . collection_name = collection_name self . message = f \"Collection with name { collection_name } already exists\" super () . __init__ ( self . message )","title":"CollectionExistsException"},{"location":"src/embedding/vector_stores/core/exceptions/#src.embedding.vector_stores.core.exceptions.CollectionExistsException.__init__","text":"Initialize the CollectionExistsException with the conflicting collection name. Parameters: collection_name ( str ) \u2013 Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. Source code in src/embedding/vector_stores/core/exceptions.py 8 9 10 11 12 13 14 15 16 17 def __init__ ( self , collection_name : str ) -> None : \"\"\"Initialize the CollectionExistsException with the conflicting collection name. Args: collection_name: Name of the collection that already exists in the vector store. This name caused the creation attempt to fail. \"\"\" self . collection_name = collection_name self . message = f \"Collection with name { collection_name } already exists\" super () . __init__ ( self . message )","title":"__init__"},{"location":"src/embedding/vector_stores/core/validator/","text":"Validator This module contains functionality related to the the validator module for embedding.vector_stores.core . Validator BaseVectorStoreValidator Bases: ABC Abstract base class for vector store validators. This class defines the interface for vector store validators which are responsible for verifying that vector store configurations are valid before attempting operations. All vector store validator implementations should inherit from this class and implement the validate method. Source code in src/embedding/vector_stores/core/validator.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class BaseVectorStoreValidator ( ABC ): \"\"\" Abstract base class for vector store validators. This class defines the interface for vector store validators which are responsible for verifying that vector store configurations are valid before attempting operations. All vector store validator implementations should inherit from this class and implement the validate method. \"\"\" @abstractmethod def validate ( self ) -> None : \"\"\" Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception: If validation fails, implementations should raise appropriate exceptions with descriptive messages. \"\"\" pass validate () abstractmethod Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception \u2013 If validation fails, implementations should raise appropriate exceptions with descriptive messages. Source code in src/embedding/vector_stores/core/validator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @abstractmethod def validate ( self ) -> None : \"\"\" Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception: If validation fails, implementations should raise appropriate exceptions with descriptive messages. \"\"\" pass","title":"Validator"},{"location":"src/embedding/vector_stores/core/validator/#validator","text":"This module contains functionality related to the the validator module for embedding.vector_stores.core .","title":"Validator"},{"location":"src/embedding/vector_stores/core/validator/#validator_1","text":"","title":"Validator"},{"location":"src/embedding/vector_stores/core/validator/#src.embedding.vector_stores.core.validator.BaseVectorStoreValidator","text":"Bases: ABC Abstract base class for vector store validators. This class defines the interface for vector store validators which are responsible for verifying that vector store configurations are valid before attempting operations. All vector store validator implementations should inherit from this class and implement the validate method. Source code in src/embedding/vector_stores/core/validator.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class BaseVectorStoreValidator ( ABC ): \"\"\" Abstract base class for vector store validators. This class defines the interface for vector store validators which are responsible for verifying that vector store configurations are valid before attempting operations. All vector store validator implementations should inherit from this class and implement the validate method. \"\"\" @abstractmethod def validate ( self ) -> None : \"\"\" Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception: If validation fails, implementations should raise appropriate exceptions with descriptive messages. \"\"\" pass","title":"BaseVectorStoreValidator"},{"location":"src/embedding/vector_stores/core/validator/#src.embedding.vector_stores.core.validator.BaseVectorStoreValidator.validate","text":"Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception \u2013 If validation fails, implementations should raise appropriate exceptions with descriptive messages. Source code in src/embedding/vector_stores/core/validator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @abstractmethod def validate ( self ) -> None : \"\"\" Validate the vector store settings. This method should check if all required settings are provided and have valid values. It should also verify any connections or permissions required for the vector store operation. Raises: Exception: If validation fails, implementations should raise appropriate exceptions with descriptive messages. \"\"\" pass","title":"validate"},{"location":"src/embedding/vector_stores/pgvector/client/","text":"Client This module contains functionality related to the the client module for embedding.vector_stores.pgvector . Client PGVectorStoreClientFactory Bases: SingletonFactory Factory class for creating and managing a singleton instance of the PGVector database client. This factory ensures that only one connection to the PGVector database is maintained throughout the application lifecycle, following the Singleton pattern. Source code in src/embedding/vector_stores/pgvector/client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class PGVectorStoreClientFactory ( SingletonFactory ): \"\"\" Factory class for creating and managing a singleton instance of the PGVector database client. This factory ensures that only one connection to the PGVector database is maintained throughout the application lifecycle, following the Singleton pattern. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorClient : \"\"\" Creates a new connection to the PGVector database using the provided configuration. Args: configuration: Configuration object containing connection details and credentials for the PGVector database. Returns: A connection object to the PGVector database that can be used for database operations. \"\"\" return connect ( host = configuration . host , port = configuration . port , database = configuration . database_name , user = configuration . secrets . username . get_secret_value (), password = configuration . secrets . password . get_secret_value (), )","title":"Client"},{"location":"src/embedding/vector_stores/pgvector/client/#client","text":"This module contains functionality related to the the client module for embedding.vector_stores.pgvector .","title":"Client"},{"location":"src/embedding/vector_stores/pgvector/client/#client_1","text":"","title":"Client"},{"location":"src/embedding/vector_stores/pgvector/client/#src.embedding.vector_stores.pgvector.client.PGVectorStoreClientFactory","text":"Bases: SingletonFactory Factory class for creating and managing a singleton instance of the PGVector database client. This factory ensures that only one connection to the PGVector database is maintained throughout the application lifecycle, following the Singleton pattern. Source code in src/embedding/vector_stores/pgvector/client.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class PGVectorStoreClientFactory ( SingletonFactory ): \"\"\" Factory class for creating and managing a singleton instance of the PGVector database client. This factory ensures that only one connection to the PGVector database is maintained throughout the application lifecycle, following the Singleton pattern. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorClient : \"\"\" Creates a new connection to the PGVector database using the provided configuration. Args: configuration: Configuration object containing connection details and credentials for the PGVector database. Returns: A connection object to the PGVector database that can be used for database operations. \"\"\" return connect ( host = configuration . host , port = configuration . port , database = configuration . database_name , user = configuration . secrets . username . get_secret_value (), password = configuration . secrets . password . get_secret_value (), )","title":"PGVectorStoreClientFactory"},{"location":"src/embedding/vector_stores/pgvector/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.vector_stores.pgvector . Configuration PGVectorStoreConfiguration Bases: VectorStoreConfiguration Configuration for PostgreSQL with pgvector extension as a vector store. This class provides all necessary configuration parameters to connect to and operate with a PostgreSQL database using the pgvector extension. Source code in src/embedding/vector_stores/pgvector/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class PGVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\"Configuration for PostgreSQL with pgvector extension as a vector store. This class provides all necessary configuration parameters to connect to and operate with a PostgreSQL database using the pgvector extension. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__VECTOR_STORE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username for PostgreSQL database authentication.\" ) password : SecretStr = Field ( ... , description = \"Password for PostgreSQL database authentication.\" ) name : Literal [ VectorStoreName . PGVECTOR ] = Field ( ... , description = \"The identifier of the vector store, must be PGVECTOR.\" ) database_name : str = Field ( ... , description = \"Name of the PostgreSQL database to connect to.\" ) embed_dim : int = Field ( 384 , description = \"Dimension of the vector embeddings stored in pgvector.\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials for the PostgreSQL database.\" , ) Secrets Bases: BaseSecrets Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. Source code in src/embedding/vector_stores/pgvector/configuration.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Secrets ( BaseSecrets ): \"\"\"Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__VECTOR_STORE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username for PostgreSQL database authentication.\" ) password : SecretStr = Field ( ... , description = \"Password for PostgreSQL database authentication.\" )","title":"Configuration"},{"location":"src/embedding/vector_stores/pgvector/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.vector_stores.pgvector .","title":"Configuration"},{"location":"src/embedding/vector_stores/pgvector/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/vector_stores/pgvector/configuration/#src.embedding.vector_stores.pgvector.configuration.PGVectorStoreConfiguration","text":"Bases: VectorStoreConfiguration Configuration for PostgreSQL with pgvector extension as a vector store. This class provides all necessary configuration parameters to connect to and operate with a PostgreSQL database using the pgvector extension. Source code in src/embedding/vector_stores/pgvector/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class PGVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\"Configuration for PostgreSQL with pgvector extension as a vector store. This class provides all necessary configuration parameters to connect to and operate with a PostgreSQL database using the pgvector extension. \"\"\" class Secrets ( BaseSecrets ): \"\"\"Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__VECTOR_STORE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username for PostgreSQL database authentication.\" ) password : SecretStr = Field ( ... , description = \"Password for PostgreSQL database authentication.\" ) name : Literal [ VectorStoreName . PGVECTOR ] = Field ( ... , description = \"The identifier of the vector store, must be PGVECTOR.\" ) database_name : str = Field ( ... , description = \"Name of the PostgreSQL database to connect to.\" ) embed_dim : int = Field ( 384 , description = \"Dimension of the vector embeddings stored in pgvector.\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials for the PostgreSQL database.\" , )","title":"PGVectorStoreConfiguration"},{"location":"src/embedding/vector_stores/pgvector/configuration/#src.embedding.vector_stores.pgvector.configuration.PGVectorStoreConfiguration.Secrets","text":"Bases: BaseSecrets Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. Source code in src/embedding/vector_stores/pgvector/configuration.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Secrets ( BaseSecrets ): \"\"\"Secret credentials required for authenticating with the PostgreSQL database. Handles environment variable loading with the RAG__VECTOR_STORE__ prefix. \"\"\" model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__VECTOR_STORE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username for PostgreSQL database authentication.\" ) password : SecretStr = Field ( ... , description = \"Password for PostgreSQL database authentication.\" )","title":"Secrets"},{"location":"src/embedding/vector_stores/pgvector/validator/","text":"Validator This module contains functionality related to the the validator module for embedding.vector_stores.pgvector . Validator PGVectorStoreValidator Bases: BaseVectorStoreValidator Validator for Postgres vector store configuration. Validates the existence of a table (treated as a collection) in the Postgres vector store backend. This validator ensures we don't create duplicate tables in the database. Source code in src/embedding/vector_stores/pgvector/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class PGVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Postgres vector store configuration. Validates the existence of a table (treated as a collection) in the Postgres vector store backend. This validator ensures we don't create duplicate tables in the database. \"\"\" def __init__ ( self , configuration : PGVectorStoreConfiguration , client : PGVectorClient , ): \"\"\"Initialize the PGVector validator with configuration and client. Args: configuration: Configuration for the PGVector store client: PostgreSQL connection client \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException: If the collection already exists in database \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException: If the table (collection) already exists. \"\"\" collection_name = self . configuration . collection_name with self . client . cursor () as cursor : query = f \"SELECT to_regclass('data_ { collection_name } ');\" cursor . execute ( query ) result = cursor . fetchone ()[ 0 ] if result is not None : raise CollectionExistsException ( collection_name ) __init__ ( configuration , client ) Initialize the PGVector validator with configuration and client. Parameters: configuration ( PGVectorStoreConfiguration ) \u2013 Configuration for the PGVector store client ( connection ) \u2013 PostgreSQL connection client Source code in src/embedding/vector_stores/pgvector/validator.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration : PGVectorStoreConfiguration , client : PGVectorClient , ): \"\"\"Initialize the PGVector validator with configuration and client. Args: configuration: Configuration for the PGVector store client: PostgreSQL connection client \"\"\" self . configuration = configuration self . client = client validate () Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException \u2013 If the collection already exists in database Source code in src/embedding/vector_stores/pgvector/validator.py 36 37 38 39 40 41 42 43 44 def validate ( self ) -> None : \"\"\"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException: If the collection already exists in database \"\"\" self . validate_collection () validate_collection () Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException \u2013 If the table (collection) already exists. Source code in src/embedding/vector_stores/pgvector/validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def validate_collection ( self ) -> None : \"\"\"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException: If the table (collection) already exists. \"\"\" collection_name = self . configuration . collection_name with self . client . cursor () as cursor : query = f \"SELECT to_regclass('data_ { collection_name } ');\" cursor . execute ( query ) result = cursor . fetchone ()[ 0 ] if result is not None : raise CollectionExistsException ( collection_name ) PGVectorStoreValidatorFactory Bases: SingletonFactory Factory for creating configured PGVector store validators. Creates and manages singleton instances of validators for PostgreSQL vector store backends. Attributes: _configuration_class ( Type ) \u2013 Type of the configuration class used for validation. Source code in src/embedding/vector_stores/pgvector/validator.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class PGVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating configured PGVector store validators. Creates and manages singleton instances of validators for PostgreSQL vector store backends. Attributes: _configuration_class: Type of the configuration class used for validation. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorStoreValidator : \"\"\"Creates a PGVector validator based on provided configuration. Args: configuration: PostgreSQL vector store connection configuration. Returns: PGVectorStoreValidator: Configured validator instance. \"\"\" client = PGVectorStoreClientFactory . create ( configuration ) return PGVectorStoreValidator ( configuration = configuration , client = client )","title":"Validator"},{"location":"src/embedding/vector_stores/pgvector/validator/#validator","text":"This module contains functionality related to the the validator module for embedding.vector_stores.pgvector .","title":"Validator"},{"location":"src/embedding/vector_stores/pgvector/validator/#validator_1","text":"","title":"Validator"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidator","text":"Bases: BaseVectorStoreValidator Validator for Postgres vector store configuration. Validates the existence of a table (treated as a collection) in the Postgres vector store backend. This validator ensures we don't create duplicate tables in the database. Source code in src/embedding/vector_stores/pgvector/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class PGVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Postgres vector store configuration. Validates the existence of a table (treated as a collection) in the Postgres vector store backend. This validator ensures we don't create duplicate tables in the database. \"\"\" def __init__ ( self , configuration : PGVectorStoreConfiguration , client : PGVectorClient , ): \"\"\"Initialize the PGVector validator with configuration and client. Args: configuration: Configuration for the PGVector store client: PostgreSQL connection client \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException: If the collection already exists in database \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException: If the table (collection) already exists. \"\"\" collection_name = self . configuration . collection_name with self . client . cursor () as cursor : query = f \"SELECT to_regclass('data_ { collection_name } ');\" cursor . execute ( query ) result = cursor . fetchone ()[ 0 ] if result is not None : raise CollectionExistsException ( collection_name )","title":"PGVectorStoreValidator"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidator.__init__","text":"Initialize the PGVector validator with configuration and client. Parameters: configuration ( PGVectorStoreConfiguration ) \u2013 Configuration for the PGVector store client ( connection ) \u2013 PostgreSQL connection client Source code in src/embedding/vector_stores/pgvector/validator.py 22 23 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration : PGVectorStoreConfiguration , client : PGVectorClient , ): \"\"\"Initialize the PGVector validator with configuration and client. Args: configuration: Configuration for the PGVector store client: PostgreSQL connection client \"\"\" self . configuration = configuration self . client = client","title":"__init__"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidator.validate","text":"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException \u2013 If the collection already exists in database Source code in src/embedding/vector_stores/pgvector/validator.py 36 37 38 39 40 41 42 43 44 def validate ( self ) -> None : \"\"\"Validate the PGVector configuration settings. Runs all validation checks to ensure the vector store can be properly initialized. Raises: CollectionExistsException: If the collection already exists in database \"\"\" self . validate_collection ()","title":"validate"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidator.validate_collection","text":"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException \u2013 If the table (collection) already exists. Source code in src/embedding/vector_stores/pgvector/validator.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def validate_collection ( self ) -> None : \"\"\"Validate that the PGVector collection (table) doesn't already exist. Checks if a table with the same name exists in the PostgreSQL database to prevent duplicate collections. Raises: CollectionExistsException: If the table (collection) already exists. \"\"\" collection_name = self . configuration . collection_name with self . client . cursor () as cursor : query = f \"SELECT to_regclass('data_ { collection_name } ');\" cursor . execute ( query ) result = cursor . fetchone ()[ 0 ] if result is not None : raise CollectionExistsException ( collection_name )","title":"validate_collection"},{"location":"src/embedding/vector_stores/pgvector/validator/#src.embedding.vector_stores.pgvector.validator.PGVectorStoreValidatorFactory","text":"Bases: SingletonFactory Factory for creating configured PGVector store validators. Creates and manages singleton instances of validators for PostgreSQL vector store backends. Attributes: _configuration_class ( Type ) \u2013 Type of the configuration class used for validation. Source code in src/embedding/vector_stores/pgvector/validator.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class PGVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating configured PGVector store validators. Creates and manages singleton instances of validators for PostgreSQL vector store backends. Attributes: _configuration_class: Type of the configuration class used for validation. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorStoreValidator : \"\"\"Creates a PGVector validator based on provided configuration. Args: configuration: PostgreSQL vector store connection configuration. Returns: PGVectorStoreValidator: Configured validator instance. \"\"\" client = PGVectorStoreClientFactory . create ( configuration ) return PGVectorStoreValidator ( configuration = configuration , client = client )","title":"PGVectorStoreValidatorFactory"},{"location":"src/embedding/vector_stores/pgvector/vector_store/","text":"Vector_store This module contains functionality related to the the vector_store module for embedding.vector_stores.pgvector . Vector_store PGVectorStoreFactory Bases: SingletonFactory Factory for creating configured PostgreSQL vector store clients. This factory creates and manages a singleton instance of PGVectorStore for vector similarity search using the pgvector extension. Source code in src/embedding/vector_stores/pgvector/vector_store.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class PGVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured PostgreSQL vector store clients. This factory creates and manages a singleton instance of PGVectorStore for vector similarity search using the pgvector extension. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorStore : \"\"\"Creates a PostgreSQL vector store client based on provided configuration. Args: configuration: PostgreSQL vector store connection configuration. Returns: PGVectorStore: Configured PostgreSQL vector store instance. \"\"\" return PGVectorStore . from_params ( database = configuration . database_name , host = configuration . host , password = configuration . secrets . password . get_secret_value (), port = configuration . port , user = configuration . secrets . username . get_secret_value (), table_name = configuration . collection_name , embed_dim = configuration . embed_dim , )","title":"Vector_store"},{"location":"src/embedding/vector_stores/pgvector/vector_store/#vector_store","text":"This module contains functionality related to the the vector_store module for embedding.vector_stores.pgvector .","title":"Vector_store"},{"location":"src/embedding/vector_stores/pgvector/vector_store/#vector_store_1","text":"","title":"Vector_store"},{"location":"src/embedding/vector_stores/pgvector/vector_store/#src.embedding.vector_stores.pgvector.vector_store.PGVectorStoreFactory","text":"Bases: SingletonFactory Factory for creating configured PostgreSQL vector store clients. This factory creates and manages a singleton instance of PGVectorStore for vector similarity search using the pgvector extension. Source code in src/embedding/vector_stores/pgvector/vector_store.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class PGVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured PostgreSQL vector store clients. This factory creates and manages a singleton instance of PGVectorStore for vector similarity search using the pgvector extension. \"\"\" _configuration_class : Type = PGVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : PGVectorStoreConfiguration ) -> PGVectorStore : \"\"\"Creates a PostgreSQL vector store client based on provided configuration. Args: configuration: PostgreSQL vector store connection configuration. Returns: PGVectorStore: Configured PostgreSQL vector store instance. \"\"\" return PGVectorStore . from_params ( database = configuration . database_name , host = configuration . host , password = configuration . secrets . password . get_secret_value (), port = configuration . port , user = configuration . secrets . username . get_secret_value (), table_name = configuration . collection_name , embed_dim = configuration . embed_dim , )","title":"PGVectorStoreFactory"},{"location":"src/embedding/vector_stores/qdrant/client/","text":"Client This module contains functionality related to the the client module for embedding.vector_stores.qdrant . Client QdrantClientFactory Bases: SingletonFactory Singleton factory for creating and managing Qdrant client instances. This factory ensures that only one Qdrant client instance is created for each unique configuration, promoting resource efficiency. Source code in src/embedding/vector_stores/qdrant/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class QdrantClientFactory ( SingletonFactory ): \"\"\" Singleton factory for creating and managing Qdrant client instances. This factory ensures that only one Qdrant client instance is created for each unique configuration, promoting resource efficiency. \"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantClient : \"\"\" Create a new QdrantClient instance based on the provided configuration. Args: configuration (QDrantVectorStoreConfiguration): Configuration containing connection parameters for the Qdrant server. Returns: QdrantClient: A configured client instance for interacting with the Qdrant vector database. \"\"\" return QdrantClient ( url = configuration . url , port = configuration . port , check_compatibility = False , )","title":"Client"},{"location":"src/embedding/vector_stores/qdrant/client/#client","text":"This module contains functionality related to the the client module for embedding.vector_stores.qdrant .","title":"Client"},{"location":"src/embedding/vector_stores/qdrant/client/#client_1","text":"","title":"Client"},{"location":"src/embedding/vector_stores/qdrant/client/#src.embedding.vector_stores.qdrant.client.QdrantClientFactory","text":"Bases: SingletonFactory Singleton factory for creating and managing Qdrant client instances. This factory ensures that only one Qdrant client instance is created for each unique configuration, promoting resource efficiency. Source code in src/embedding/vector_stores/qdrant/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class QdrantClientFactory ( SingletonFactory ): \"\"\" Singleton factory for creating and managing Qdrant client instances. This factory ensures that only one Qdrant client instance is created for each unique configuration, promoting resource efficiency. \"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantClient : \"\"\" Create a new QdrantClient instance based on the provided configuration. Args: configuration (QDrantVectorStoreConfiguration): Configuration containing connection parameters for the Qdrant server. Returns: QdrantClient: A configured client instance for interacting with the Qdrant vector database. \"\"\" return QdrantClient ( url = configuration . url , port = configuration . port , check_compatibility = False , )","title":"QdrantClientFactory"},{"location":"src/embedding/vector_stores/qdrant/configuration/","text":"Configuration This module contains functionality related to the the configuration module for embedding.vector_stores.qdrant . Configuration QDrantVectorStoreConfiguration Bases: VectorStoreConfiguration Configuration settings specific to QDrant vector store. Extends the base VectorStoreConfiguration with QDrant-specific properties and functionality. Source code in src/embedding/vector_stores/qdrant/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class QDrantVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\" Configuration settings specific to QDrant vector store. Extends the base VectorStoreConfiguration with QDrant-specific properties and functionality. \"\"\" name : Literal [ VectorStoreName . QDRANT ] = Field ( ... , description = \"The name of the vector store.\" ) @property def url ( self ) -> str : \"\"\" Constructs the full URL for connecting to the QDrant service. Returns: str: The complete URL with protocol, host, and port. \"\"\" return f \" { self . protocol } :// { self . host } : { self . port } \" url property Constructs the full URL for connecting to the QDrant service. Returns: str ( str ) \u2013 The complete URL with protocol, host, and port.","title":"Configuration"},{"location":"src/embedding/vector_stores/qdrant/configuration/#configuration","text":"This module contains functionality related to the the configuration module for embedding.vector_stores.qdrant .","title":"Configuration"},{"location":"src/embedding/vector_stores/qdrant/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/embedding/vector_stores/qdrant/configuration/#src.embedding.vector_stores.qdrant.configuration.QDrantVectorStoreConfiguration","text":"Bases: VectorStoreConfiguration Configuration settings specific to QDrant vector store. Extends the base VectorStoreConfiguration with QDrant-specific properties and functionality. Source code in src/embedding/vector_stores/qdrant/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class QDrantVectorStoreConfiguration ( VectorStoreConfiguration ): \"\"\" Configuration settings specific to QDrant vector store. Extends the base VectorStoreConfiguration with QDrant-specific properties and functionality. \"\"\" name : Literal [ VectorStoreName . QDRANT ] = Field ( ... , description = \"The name of the vector store.\" ) @property def url ( self ) -> str : \"\"\" Constructs the full URL for connecting to the QDrant service. Returns: str: The complete URL with protocol, host, and port. \"\"\" return f \" { self . protocol } :// { self . host } : { self . port } \"","title":"QDrantVectorStoreConfiguration"},{"location":"src/embedding/vector_stores/qdrant/configuration/#src.embedding.vector_stores.qdrant.configuration.QDrantVectorStoreConfiguration.url","text":"Constructs the full URL for connecting to the QDrant service. Returns: str ( str ) \u2013 The complete URL with protocol, host, and port.","title":"url"},{"location":"src/embedding/vector_stores/qdrant/validator/","text":"Validator This module contains functionality related to the the validator module for embedding.vector_stores.qdrant . Validator QdrantVectorStoreValidator Bases: BaseVectorStoreValidator Validator for Qdrant vector store configuration. Validates collection settings and existence for Qdrant vector store backend. Attributes: configuration \u2013 Settings for vector store client \u2013 Client for Qdrant interactions Source code in src/embedding/vector_stores/qdrant/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class QdrantVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Qdrant vector store configuration. Validates collection settings and existence for Qdrant vector store backend. Attributes: configuration: Settings for vector store client: Client for Qdrant interactions \"\"\" def __init__ ( self , configuration : QDrantVectorStoreConfiguration , client : QdrantClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Qdrant vector store settings client: Client for Qdrant operations \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException: If collection already exists in the Qdrant database with the specified name \"\"\" collection_name = self . configuration . collection_name if self . client . collection_exists ( collection_name ): raise CollectionExistsException ( collection_name ) __init__ ( configuration , client ) Initialize validator with configuration and client. Parameters: configuration ( QDrantVectorStoreConfiguration ) \u2013 Qdrant vector store settings client ( QdrantClient ) \u2013 Client for Qdrant operations Source code in src/embedding/vector_stores/qdrant/validator.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , configuration : QDrantVectorStoreConfiguration , client : QdrantClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Qdrant vector store settings client: Client for Qdrant operations \"\"\" self . configuration = configuration self . client = client validate () Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. Source code in src/embedding/vector_stores/qdrant/validator.py 39 40 41 42 43 44 45 def validate ( self ) -> None : \"\"\"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. \"\"\" self . validate_collection () validate_collection () Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException \u2013 If collection already exists in the Qdrant database with the specified name Source code in src/embedding/vector_stores/qdrant/validator.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def validate_collection ( self ) -> None : \"\"\"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException: If collection already exists in the Qdrant database with the specified name \"\"\" collection_name = self . configuration . collection_name if self . client . collection_exists ( collection_name ): raise CollectionExistsException ( collection_name ) QdrantVectorStoreValidatorFactory Bases: SingletonFactory Factory for creating Qdrant vector store validators. Implements the singleton pattern to ensure only one validator instance exists for each unique configuration. Source code in src/embedding/vector_stores/qdrant/validator.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class QdrantVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating Qdrant vector store validators. Implements the singleton pattern to ensure only one validator instance exists for each unique configuration. \"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantVectorStoreValidator : \"\"\"Creates a Qdrant validator based on provided configuration. Instantiates a new validator with the appropriate client for the given configuration. Args: configuration: QDrant connection configuration. Returns: QdrantVectorStoreValidator: Configured validator instance. \"\"\" client = QdrantClientFactory . create ( configuration ) return QdrantVectorStoreValidator ( configuration = configuration , client = client )","title":"Validator"},{"location":"src/embedding/vector_stores/qdrant/validator/#validator","text":"This module contains functionality related to the the validator module for embedding.vector_stores.qdrant .","title":"Validator"},{"location":"src/embedding/vector_stores/qdrant/validator/#validator_1","text":"","title":"Validator"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidator","text":"Bases: BaseVectorStoreValidator Validator for Qdrant vector store configuration. Validates collection settings and existence for Qdrant vector store backend. Attributes: configuration \u2013 Settings for vector store client \u2013 Client for Qdrant interactions Source code in src/embedding/vector_stores/qdrant/validator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class QdrantVectorStoreValidator ( BaseVectorStoreValidator ): \"\"\"Validator for Qdrant vector store configuration. Validates collection settings and existence for Qdrant vector store backend. Attributes: configuration: Settings for vector store client: Client for Qdrant interactions \"\"\" def __init__ ( self , configuration : QDrantVectorStoreConfiguration , client : QdrantClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Qdrant vector store settings client: Client for Qdrant operations \"\"\" self . configuration = configuration self . client = client def validate ( self ) -> None : \"\"\"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. \"\"\" self . validate_collection () def validate_collection ( self ) -> None : \"\"\"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException: If collection already exists in the Qdrant database with the specified name \"\"\" collection_name = self . configuration . collection_name if self . client . collection_exists ( collection_name ): raise CollectionExistsException ( collection_name )","title":"QdrantVectorStoreValidator"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidator.__init__","text":"Initialize validator with configuration and client. Parameters: configuration ( QDrantVectorStoreConfiguration ) \u2013 Qdrant vector store settings client ( QdrantClient ) \u2013 Client for Qdrant operations Source code in src/embedding/vector_stores/qdrant/validator.py 25 26 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , configuration : QDrantVectorStoreConfiguration , client : QdrantClient , ): \"\"\"Initialize validator with configuration and client. Args: configuration: Qdrant vector store settings client: Client for Qdrant operations \"\"\" self . configuration = configuration self . client = client","title":"__init__"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidator.validate","text":"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. Source code in src/embedding/vector_stores/qdrant/validator.py 39 40 41 42 43 44 45 def validate ( self ) -> None : \"\"\"Validate the Qdrant vector store settings. Performs validation checks on the provided configuration to ensure compatibility with Qdrant backend requirements. \"\"\" self . validate_collection ()","title":"validate"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidator.validate_collection","text":"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException \u2013 If collection already exists in the Qdrant database with the specified name Source code in src/embedding/vector_stores/qdrant/validator.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def validate_collection ( self ) -> None : \"\"\"Validate Qdrant collection existence. Checks if the specified collection already exists in the Qdrant database and raises an exception if it does, preventing unintentional overwriting of existing collections. Raises: CollectionExistsException: If collection already exists in the Qdrant database with the specified name \"\"\" collection_name = self . configuration . collection_name if self . client . collection_exists ( collection_name ): raise CollectionExistsException ( collection_name )","title":"validate_collection"},{"location":"src/embedding/vector_stores/qdrant/validator/#src.embedding.vector_stores.qdrant.validator.QdrantVectorStoreValidatorFactory","text":"Bases: SingletonFactory Factory for creating Qdrant vector store validators. Implements the singleton pattern to ensure only one validator instance exists for each unique configuration. Source code in src/embedding/vector_stores/qdrant/validator.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class QdrantVectorStoreValidatorFactory ( SingletonFactory ): \"\"\"Factory for creating Qdrant vector store validators. Implements the singleton pattern to ensure only one validator instance exists for each unique configuration. \"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantVectorStoreValidator : \"\"\"Creates a Qdrant validator based on provided configuration. Instantiates a new validator with the appropriate client for the given configuration. Args: configuration: QDrant connection configuration. Returns: QdrantVectorStoreValidator: Configured validator instance. \"\"\" client = QdrantClientFactory . create ( configuration ) return QdrantVectorStoreValidator ( configuration = configuration , client = client )","title":"QdrantVectorStoreValidatorFactory"},{"location":"src/embedding/vector_stores/qdrant/vector_store/","text":"Vector_store This module contains functionality related to the the vector_store module for embedding.vector_stores.qdrant . Vector_store QdrantVectorStoreFactory Bases: SingletonFactory Factory for creating configured Qdrant vector store instances using the Singleton pattern. Source code in src/embedding/vector_stores/qdrant/vector_store.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class QdrantVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured Qdrant vector store instances using the Singleton pattern.\"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantVectorStore : \"\"\"Creates a Qdrant vector store based on provided configuration. This method instantiates a Qdrant client using the QdrantClientFactory and uses it to create a QdrantVectorStore instance with the specified collection name from the configuration. Args: configuration: QDrant connection configuration containing connection parameters and collection name. Returns: QdrantVectorStore: Configured vector store instance ready for embedding storage and retrieval operations. \"\"\" client = QdrantClientFactory . create ( configuration ) return QdrantVectorStore ( client = client , collection_name = configuration . collection_name )","title":"Vector_store"},{"location":"src/embedding/vector_stores/qdrant/vector_store/#vector_store","text":"This module contains functionality related to the the vector_store module for embedding.vector_stores.qdrant .","title":"Vector_store"},{"location":"src/embedding/vector_stores/qdrant/vector_store/#vector_store_1","text":"","title":"Vector_store"},{"location":"src/embedding/vector_stores/qdrant/vector_store/#src.embedding.vector_stores.qdrant.vector_store.QdrantVectorStoreFactory","text":"Bases: SingletonFactory Factory for creating configured Qdrant vector store instances using the Singleton pattern. Source code in src/embedding/vector_stores/qdrant/vector_store.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class QdrantVectorStoreFactory ( SingletonFactory ): \"\"\"Factory for creating configured Qdrant vector store instances using the Singleton pattern.\"\"\" _configuration_class : Type = QDrantVectorStoreConfiguration @classmethod def _create_instance ( cls , configuration : QDrantVectorStoreConfiguration ) -> QdrantVectorStore : \"\"\"Creates a Qdrant vector store based on provided configuration. This method instantiates a Qdrant client using the QdrantClientFactory and uses it to create a QdrantVectorStore instance with the specified collection name from the configuration. Args: configuration: QDrant connection configuration containing connection parameters and collection name. Returns: QdrantVectorStore: Configured vector store instance ready for embedding storage and retrieval operations. \"\"\" client = QdrantClientFactory . create ( configuration ) return QdrantVectorStore ( client = client , collection_name = configuration . collection_name )","title":"QdrantVectorStoreFactory"},{"location":"src/evaluation/bootstrap/initializer/","text":"Initializer This module contains functionality related to the the initializer module for evaluation.bootstrap . Initializer EvaluationInitializer Bases: AugmentationInitializer Initializer for evaluation processes that extends augmentation capabilities. This initializer inherits from AugmentationInitializer to reuse augmentation functionality while setting up evaluation-specific components. It handles the configuration binding and initialization of services required for evaluating RAG systems. The class bridges augmentation and evaluation processes by providing a consistent initialization pattern across the application. It's responsible for: - Loading evaluation configuration - Setting up evaluation metrics and benchmarks - Binding evaluation-specific dependencies Source code in src/evaluation/bootstrap/initializer.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EvaluationInitializer ( AugmentationInitializer ): \"\"\"Initializer for evaluation processes that extends augmentation capabilities. This initializer inherits from AugmentationInitializer to reuse augmentation functionality while setting up evaluation-specific components. It handles the configuration binding and initialization of services required for evaluating RAG systems. The class bridges augmentation and evaluation processes by providing a consistent initialization pattern across the application. It's responsible for: - Loading evaluation configuration - Setting up evaluation metrics and benchmarks - Binding evaluation-specific dependencies \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EvaluationConfiguration , ): \"\"\"Initialize the evaluation components with the specified configuration. Args: configuration_class: Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. \"\"\" super () . __init__ ( configuration_class ) __init__ ( configuration_class = EvaluationConfiguration ) Initialize the evaluation components with the specified configuration. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: EvaluationConfiguration ) \u2013 Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. Source code in src/evaluation/bootstrap/initializer.py 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EvaluationConfiguration , ): \"\"\"Initialize the evaluation components with the specified configuration. Args: configuration_class: Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. \"\"\" super () . __init__ ( configuration_class )","title":"Initializer"},{"location":"src/evaluation/bootstrap/initializer/#initializer","text":"This module contains functionality related to the the initializer module for evaluation.bootstrap .","title":"Initializer"},{"location":"src/evaluation/bootstrap/initializer/#initializer_1","text":"","title":"Initializer"},{"location":"src/evaluation/bootstrap/initializer/#src.evaluation.bootstrap.initializer.EvaluationInitializer","text":"Bases: AugmentationInitializer Initializer for evaluation processes that extends augmentation capabilities. This initializer inherits from AugmentationInitializer to reuse augmentation functionality while setting up evaluation-specific components. It handles the configuration binding and initialization of services required for evaluating RAG systems. The class bridges augmentation and evaluation processes by providing a consistent initialization pattern across the application. It's responsible for: - Loading evaluation configuration - Setting up evaluation metrics and benchmarks - Binding evaluation-specific dependencies Source code in src/evaluation/bootstrap/initializer.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class EvaluationInitializer ( AugmentationInitializer ): \"\"\"Initializer for evaluation processes that extends augmentation capabilities. This initializer inherits from AugmentationInitializer to reuse augmentation functionality while setting up evaluation-specific components. It handles the configuration binding and initialization of services required for evaluating RAG systems. The class bridges augmentation and evaluation processes by providing a consistent initialization pattern across the application. It's responsible for: - Loading evaluation configuration - Setting up evaluation metrics and benchmarks - Binding evaluation-specific dependencies \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EvaluationConfiguration , ): \"\"\"Initialize the evaluation components with the specified configuration. Args: configuration_class: Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. \"\"\" super () . __init__ ( configuration_class )","title":"EvaluationInitializer"},{"location":"src/evaluation/bootstrap/initializer/#src.evaluation.bootstrap.initializer.EvaluationInitializer.__init__","text":"Initialize the evaluation components with the specified configuration. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: EvaluationConfiguration ) \u2013 Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. Source code in src/evaluation/bootstrap/initializer.py 24 25 26 27 28 29 30 31 32 33 34 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = EvaluationConfiguration , ): \"\"\"Initialize the evaluation components with the specified configuration. Args: configuration_class: Class to use for loading the evaluation configuration. Defaults to EvaluationConfiguration. \"\"\" super () . __init__ ( configuration_class )","title":"__init__"},{"location":"src/evaluation/bootstrap/configuration/configuration/","text":"Configuration This module contains functionality related to the the configuration module for evaluation.bootstrap.configuration . Configuration EvaluationConfiguration Bases: AugmentationConfiguration Main configuration class for the evaluation process. This class extends the AugmentationConfiguration to include evaluation-specific settings, allowing the evaluation pipeline to be properly configured. Source code in src/evaluation/bootstrap/configuration/configuration.py 75 76 77 78 79 80 81 82 83 84 85 86 class EvaluationConfiguration ( AugmentationConfiguration ): \"\"\" Main configuration class for the evaluation process. This class extends the AugmentationConfiguration to include evaluation-specific settings, allowing the evaluation pipeline to be properly configured. \"\"\" evaluation : _EvaluationConfiguration = Field ( ... , description = \"Configuration of the augmentation process.\" )","title":"Configuration"},{"location":"src/evaluation/bootstrap/configuration/configuration/#configuration","text":"This module contains functionality related to the the configuration module for evaluation.bootstrap.configuration .","title":"Configuration"},{"location":"src/evaluation/bootstrap/configuration/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/evaluation/bootstrap/configuration/configuration/#src.evaluation.bootstrap.configuration.configuration.EvaluationConfiguration","text":"Bases: AugmentationConfiguration Main configuration class for the evaluation process. This class extends the AugmentationConfiguration to include evaluation-specific settings, allowing the evaluation pipeline to be properly configured. Source code in src/evaluation/bootstrap/configuration/configuration.py 75 76 77 78 79 80 81 82 83 84 85 86 class EvaluationConfiguration ( AugmentationConfiguration ): \"\"\" Main configuration class for the evaluation process. This class extends the AugmentationConfiguration to include evaluation-specific settings, allowing the evaluation pipeline to be properly configured. \"\"\" evaluation : _EvaluationConfiguration = Field ( ... , description = \"Configuration of the augmentation process.\" )","title":"EvaluationConfiguration"},{"location":"src/evaluation/evaluators/langfuse/","text":"Langfuse This module contains functionality related to the the langfuse module for evaluation.evaluators . Langfuse LangfuseEvaluator Evaluator that tracks RAG performance metrics in Langfuse. Integrates chat engine execution with RAGAS evaluation and publishes quality metrics to Langfuse for monitoring and analysis. Source code in src/evaluation/evaluators/langfuse.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class LangfuseEvaluator : \"\"\"Evaluator that tracks RAG performance metrics in Langfuse. Integrates chat engine execution with RAGAS evaluation and publishes quality metrics to Langfuse for monitoring and analysis. \"\"\" def __init__ ( self , chat_engine : LangfuseChatEngine , langfuse_dataset_service : LangfuseDatasetService , ragas_evaluator : RagasEvaluator , run_metadata : dict , ) -> None : \"\"\"Initialize the Langfuse evaluator with required components. Args: chat_engine: The chat engine that will generate responses langfuse_dataset_service: Service to retrieve evaluation datasets ragas_evaluator: Component to calculate quality metrics run_metadata: Dictionary containing metadata about the evaluation run \"\"\" self . chat_engine = chat_engine self . ragas_evaluator = ragas_evaluator self . langfuse_dataset_service = langfuse_dataset_service self . run_name = run_metadata [ \"build_name\" ] self . run_metadata = run_metadata def evaluate ( self , dataset_name : str ) -> None : \"\"\"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Args: dataset_name: Identifier of the dataset to evaluate Note: Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). \"\"\" langfuse_dataset = self . langfuse_dataset_service . get_dataset ( dataset_name ) for item in langfuse_dataset . items : if item . status == DatasetStatus . ARCHIVED : continue response = self . chat_engine . chat ( message = item . input [ \"query_str\" ], chat_history = [], chainlit_message_id = None , source_process = SourceProcess . DEPLOYMENT_EVALUATION , ) scores = self . ragas_evaluator . evaluate ( response = response , item = item ) trace = self . chat_engine . get_current_langfuse_trace () trace . update ( output = response . response ) item . link ( trace_or_observation = trace , run_name = self . run_name , run_description = \"Deployment evaluation\" , run_metadata = self . run_metadata , ) # TODO: How to handle NaNs? if not isnan ( scores [ \"answer_relevancy\" ]): trace . score ( name = \"Answer Relevancy\" , value = scores [ \"answer_relevancy\" ] ) if not isnan ( scores [ \"context_recall\" ]): trace . score ( name = \"Context Recall\" , value = scores [ \"context_recall\" ] ) if not isnan ( scores [ \"faithfulness\" ]): trace . score ( name = \"Faithfulness\" , value = scores [ \"faithfulness\" ]) if not isnan ( scores [ \"harmfulness\" ]): trace . score ( name = \"Harmfulness\" , value = scores [ \"harmfulness\" ]) __init__ ( chat_engine , langfuse_dataset_service , ragas_evaluator , run_metadata ) Initialize the Langfuse evaluator with required components. Parameters: chat_engine ( LangfuseChatEngine ) \u2013 The chat engine that will generate responses langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service to retrieve evaluation datasets ragas_evaluator ( RagasEvaluator ) \u2013 Component to calculate quality metrics run_metadata ( dict ) \u2013 Dictionary containing metadata about the evaluation run Source code in src/evaluation/evaluators/langfuse.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , chat_engine : LangfuseChatEngine , langfuse_dataset_service : LangfuseDatasetService , ragas_evaluator : RagasEvaluator , run_metadata : dict , ) -> None : \"\"\"Initialize the Langfuse evaluator with required components. Args: chat_engine: The chat engine that will generate responses langfuse_dataset_service: Service to retrieve evaluation datasets ragas_evaluator: Component to calculate quality metrics run_metadata: Dictionary containing metadata about the evaluation run \"\"\" self . chat_engine = chat_engine self . ragas_evaluator = ragas_evaluator self . langfuse_dataset_service = langfuse_dataset_service self . run_name = run_metadata [ \"build_name\" ] self . run_metadata = run_metadata evaluate ( dataset_name ) Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Parameters: dataset_name ( str ) \u2013 Identifier of the dataset to evaluate Note Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). Source code in src/evaluation/evaluators/langfuse.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def evaluate ( self , dataset_name : str ) -> None : \"\"\"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Args: dataset_name: Identifier of the dataset to evaluate Note: Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). \"\"\" langfuse_dataset = self . langfuse_dataset_service . get_dataset ( dataset_name ) for item in langfuse_dataset . items : if item . status == DatasetStatus . ARCHIVED : continue response = self . chat_engine . chat ( message = item . input [ \"query_str\" ], chat_history = [], chainlit_message_id = None , source_process = SourceProcess . DEPLOYMENT_EVALUATION , ) scores = self . ragas_evaluator . evaluate ( response = response , item = item ) trace = self . chat_engine . get_current_langfuse_trace () trace . update ( output = response . response ) item . link ( trace_or_observation = trace , run_name = self . run_name , run_description = \"Deployment evaluation\" , run_metadata = self . run_metadata , ) # TODO: How to handle NaNs? if not isnan ( scores [ \"answer_relevancy\" ]): trace . score ( name = \"Answer Relevancy\" , value = scores [ \"answer_relevancy\" ] ) if not isnan ( scores [ \"context_recall\" ]): trace . score ( name = \"Context Recall\" , value = scores [ \"context_recall\" ] ) if not isnan ( scores [ \"faithfulness\" ]): trace . score ( name = \"Faithfulness\" , value = scores [ \"faithfulness\" ]) if not isnan ( scores [ \"harmfulness\" ]): trace . score ( name = \"Harmfulness\" , value = scores [ \"harmfulness\" ]) LangfuseEvaluatorFactory Bases: Factory Factory for creating LangfuseEvaluator instances. Creates properly configured evaluators based on the provided configuration. Source code in src/evaluation/evaluators/langfuse.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class LangfuseEvaluatorFactory ( Factory ): \"\"\"Factory for creating LangfuseEvaluator instances. Creates properly configured evaluators based on the provided configuration. \"\"\" _configuration_class : Type = EvaluationConfiguration @classmethod def _create_instance ( cls , configuration : EvaluationConfiguration ) -> LangfuseEvaluator : \"\"\"Create a new LangfuseEvaluator instance. Args: configuration: Complete evaluation configuration containing settings for the chat engine, datasets, and metrics Returns: A fully configured LangfuseEvaluator instance ready for evaluation \"\"\" chat_engine = ChatEngineRegistry . get ( configuration . augmentation . chat_engine . name ) . create ( configuration ) langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . augmentation . langfuse ) ragas_evaluator = RagasEvaluatorFactory . create ( configuration . evaluation ) return LangfuseEvaluator ( chat_engine = chat_engine , langfuse_dataset_service = langfuse_dataset_service , ragas_evaluator = ragas_evaluator , run_metadata = { \"build_name\" : configuration . metadata . build_name , \"llm_configuration\" : configuration . augmentation . chat_engine . llm . name , \"judge_llm_configuration\" : configuration . evaluation . judge_llm . name , }, )","title":"Langfuse"},{"location":"src/evaluation/evaluators/langfuse/#langfuse","text":"This module contains functionality related to the the langfuse module for evaluation.evaluators .","title":"Langfuse"},{"location":"src/evaluation/evaluators/langfuse/#langfuse_1","text":"","title":"Langfuse"},{"location":"src/evaluation/evaluators/langfuse/#src.evaluation.evaluators.langfuse.LangfuseEvaluator","text":"Evaluator that tracks RAG performance metrics in Langfuse. Integrates chat engine execution with RAGAS evaluation and publishes quality metrics to Langfuse for monitoring and analysis. Source code in src/evaluation/evaluators/langfuse.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 class LangfuseEvaluator : \"\"\"Evaluator that tracks RAG performance metrics in Langfuse. Integrates chat engine execution with RAGAS evaluation and publishes quality metrics to Langfuse for monitoring and analysis. \"\"\" def __init__ ( self , chat_engine : LangfuseChatEngine , langfuse_dataset_service : LangfuseDatasetService , ragas_evaluator : RagasEvaluator , run_metadata : dict , ) -> None : \"\"\"Initialize the Langfuse evaluator with required components. Args: chat_engine: The chat engine that will generate responses langfuse_dataset_service: Service to retrieve evaluation datasets ragas_evaluator: Component to calculate quality metrics run_metadata: Dictionary containing metadata about the evaluation run \"\"\" self . chat_engine = chat_engine self . ragas_evaluator = ragas_evaluator self . langfuse_dataset_service = langfuse_dataset_service self . run_name = run_metadata [ \"build_name\" ] self . run_metadata = run_metadata def evaluate ( self , dataset_name : str ) -> None : \"\"\"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Args: dataset_name: Identifier of the dataset to evaluate Note: Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). \"\"\" langfuse_dataset = self . langfuse_dataset_service . get_dataset ( dataset_name ) for item in langfuse_dataset . items : if item . status == DatasetStatus . ARCHIVED : continue response = self . chat_engine . chat ( message = item . input [ \"query_str\" ], chat_history = [], chainlit_message_id = None , source_process = SourceProcess . DEPLOYMENT_EVALUATION , ) scores = self . ragas_evaluator . evaluate ( response = response , item = item ) trace = self . chat_engine . get_current_langfuse_trace () trace . update ( output = response . response ) item . link ( trace_or_observation = trace , run_name = self . run_name , run_description = \"Deployment evaluation\" , run_metadata = self . run_metadata , ) # TODO: How to handle NaNs? if not isnan ( scores [ \"answer_relevancy\" ]): trace . score ( name = \"Answer Relevancy\" , value = scores [ \"answer_relevancy\" ] ) if not isnan ( scores [ \"context_recall\" ]): trace . score ( name = \"Context Recall\" , value = scores [ \"context_recall\" ] ) if not isnan ( scores [ \"faithfulness\" ]): trace . score ( name = \"Faithfulness\" , value = scores [ \"faithfulness\" ]) if not isnan ( scores [ \"harmfulness\" ]): trace . score ( name = \"Harmfulness\" , value = scores [ \"harmfulness\" ])","title":"LangfuseEvaluator"},{"location":"src/evaluation/evaluators/langfuse/#src.evaluation.evaluators.langfuse.LangfuseEvaluator.__init__","text":"Initialize the Langfuse evaluator with required components. Parameters: chat_engine ( LangfuseChatEngine ) \u2013 The chat engine that will generate responses langfuse_dataset_service ( LangfuseDatasetService ) \u2013 Service to retrieve evaluation datasets ragas_evaluator ( RagasEvaluator ) \u2013 Component to calculate quality metrics run_metadata ( dict ) \u2013 Dictionary containing metadata about the evaluation run Source code in src/evaluation/evaluators/langfuse.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , chat_engine : LangfuseChatEngine , langfuse_dataset_service : LangfuseDatasetService , ragas_evaluator : RagasEvaluator , run_metadata : dict , ) -> None : \"\"\"Initialize the Langfuse evaluator with required components. Args: chat_engine: The chat engine that will generate responses langfuse_dataset_service: Service to retrieve evaluation datasets ragas_evaluator: Component to calculate quality metrics run_metadata: Dictionary containing metadata about the evaluation run \"\"\" self . chat_engine = chat_engine self . ragas_evaluator = ragas_evaluator self . langfuse_dataset_service = langfuse_dataset_service self . run_name = run_metadata [ \"build_name\" ] self . run_metadata = run_metadata","title":"__init__"},{"location":"src/evaluation/evaluators/langfuse/#src.evaluation.evaluators.langfuse.LangfuseEvaluator.evaluate","text":"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Parameters: dataset_name ( str ) \u2013 Identifier of the dataset to evaluate Note Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). Source code in src/evaluation/evaluators/langfuse.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def evaluate ( self , dataset_name : str ) -> None : \"\"\"Run evaluation on a dataset and record results in Langfuse. Processes each item in the dataset, generates responses using the chat engine, calculates evaluation metrics, and uploads all results to Langfuse for monitoring. Args: dataset_name: Identifier of the dataset to evaluate Note: Records scores for answer relevancy, context recall, faithfulness, and harmfulness metrics when they are available (not NaN values). \"\"\" langfuse_dataset = self . langfuse_dataset_service . get_dataset ( dataset_name ) for item in langfuse_dataset . items : if item . status == DatasetStatus . ARCHIVED : continue response = self . chat_engine . chat ( message = item . input [ \"query_str\" ], chat_history = [], chainlit_message_id = None , source_process = SourceProcess . DEPLOYMENT_EVALUATION , ) scores = self . ragas_evaluator . evaluate ( response = response , item = item ) trace = self . chat_engine . get_current_langfuse_trace () trace . update ( output = response . response ) item . link ( trace_or_observation = trace , run_name = self . run_name , run_description = \"Deployment evaluation\" , run_metadata = self . run_metadata , ) # TODO: How to handle NaNs? if not isnan ( scores [ \"answer_relevancy\" ]): trace . score ( name = \"Answer Relevancy\" , value = scores [ \"answer_relevancy\" ] ) if not isnan ( scores [ \"context_recall\" ]): trace . score ( name = \"Context Recall\" , value = scores [ \"context_recall\" ] ) if not isnan ( scores [ \"faithfulness\" ]): trace . score ( name = \"Faithfulness\" , value = scores [ \"faithfulness\" ]) if not isnan ( scores [ \"harmfulness\" ]): trace . score ( name = \"Harmfulness\" , value = scores [ \"harmfulness\" ])","title":"evaluate"},{"location":"src/evaluation/evaluators/langfuse/#src.evaluation.evaluators.langfuse.LangfuseEvaluatorFactory","text":"Bases: Factory Factory for creating LangfuseEvaluator instances. Creates properly configured evaluators based on the provided configuration. Source code in src/evaluation/evaluators/langfuse.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class LangfuseEvaluatorFactory ( Factory ): \"\"\"Factory for creating LangfuseEvaluator instances. Creates properly configured evaluators based on the provided configuration. \"\"\" _configuration_class : Type = EvaluationConfiguration @classmethod def _create_instance ( cls , configuration : EvaluationConfiguration ) -> LangfuseEvaluator : \"\"\"Create a new LangfuseEvaluator instance. Args: configuration: Complete evaluation configuration containing settings for the chat engine, datasets, and metrics Returns: A fully configured LangfuseEvaluator instance ready for evaluation \"\"\" chat_engine = ChatEngineRegistry . get ( configuration . augmentation . chat_engine . name ) . create ( configuration ) langfuse_dataset_service = LangfuseDatasetServiceFactory . create ( configuration . augmentation . langfuse ) ragas_evaluator = RagasEvaluatorFactory . create ( configuration . evaluation ) return LangfuseEvaluator ( chat_engine = chat_engine , langfuse_dataset_service = langfuse_dataset_service , ragas_evaluator = ragas_evaluator , run_metadata = { \"build_name\" : configuration . metadata . build_name , \"llm_configuration\" : configuration . augmentation . chat_engine . llm . name , \"judge_llm_configuration\" : configuration . evaluation . judge_llm . name , }, )","title":"LangfuseEvaluatorFactory"},{"location":"src/evaluation/evaluators/ragas/","text":"Ragas This module contains functionality related to the the ragas module for evaluation.evaluators . Ragas RagasEvaluator Evaluator for RAG system quality using RAGAS framework. Provides automatic evaluation of RAG pipeline quality using multiple metrics from the RAGAS evaluation framework. Evaluates answer relevancy, factual consistency (faithfulness), harmfulness, and source context recall. Source code in src/evaluation/evaluators/ragas.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class RagasEvaluator : \"\"\"Evaluator for RAG system quality using RAGAS framework. Provides automatic evaluation of RAG pipeline quality using multiple metrics from the RAGAS evaluation framework. Evaluates answer relevancy, factual consistency (faithfulness), harmfulness, and source context recall. \"\"\" def __init__ ( self , judge_llm : BaseLLM , judge_embedding_model : BaseEmbedding , evaluator_function : Callable = ragas_evaluate , ) -> None : \"\"\"Initialize RAGAS evaluator with required models and configuration. Args: judge_llm: LlamaIndex LLM used to evaluate response quality judge_embedding_model: Embedding model for semantic comparisons evaluator_function: Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function \"\"\" self . judge_llm = LlamaIndexLLMWrapper ( judge_llm ) self . judge_embedding_model = LlamaIndexEmbeddingsWrapper ( judge_embedding_model ) self . evaluator_function = evaluator_function self . metrics = [ answer_relevancy , faithfulness , harmfulness , context_recall , ] def evaluate ( self , response : Response , item : DatasetItemClient ) -> Series : \"\"\"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Args: response: LlamaIndex response object containing the generated answer and source nodes used for retrieval item: Langfuse dataset item containing the original query and expected ground truth answer Returns: Series: Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) \"\"\" dataset = Dataset . from_dict ( { \"question\" : [ item . input [ \"query_str\" ]], \"contexts\" : [[ n . node . text for n in response . source_nodes ]], \"answer\" : [ response . response ], \"ground_truth\" : [ item . expected_output [ \"result\" ]], } ) return ( self . evaluator_function ( metrics = self . metrics , dataset = dataset , llm = self . judge_llm , embeddings = self . judge_embedding_model , ) . to_pandas () . iloc [ 0 ] ) __init__ ( judge_llm , judge_embedding_model , evaluator_function = ragas_evaluate ) Initialize RAGAS evaluator with required models and configuration. Parameters: judge_llm ( BaseLLM ) \u2013 LlamaIndex LLM used to evaluate response quality judge_embedding_model ( BaseEmbedding ) \u2013 Embedding model for semantic comparisons evaluator_function ( Callable , default: evaluate ) \u2013 Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function Source code in src/evaluation/evaluators/ragas.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , judge_llm : BaseLLM , judge_embedding_model : BaseEmbedding , evaluator_function : Callable = ragas_evaluate , ) -> None : \"\"\"Initialize RAGAS evaluator with required models and configuration. Args: judge_llm: LlamaIndex LLM used to evaluate response quality judge_embedding_model: Embedding model for semantic comparisons evaluator_function: Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function \"\"\" self . judge_llm = LlamaIndexLLMWrapper ( judge_llm ) self . judge_embedding_model = LlamaIndexEmbeddingsWrapper ( judge_embedding_model ) self . evaluator_function = evaluator_function self . metrics = [ answer_relevancy , faithfulness , harmfulness , context_recall , ] evaluate ( response , item ) Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Parameters: response ( Response ) \u2013 LlamaIndex response object containing the generated answer and source nodes used for retrieval item ( DatasetItemClient ) \u2013 Langfuse dataset item containing the original query and expected ground truth answer Returns: Series ( Series ) \u2013 Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) Source code in src/evaluation/evaluators/ragas.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def evaluate ( self , response : Response , item : DatasetItemClient ) -> Series : \"\"\"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Args: response: LlamaIndex response object containing the generated answer and source nodes used for retrieval item: Langfuse dataset item containing the original query and expected ground truth answer Returns: Series: Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) \"\"\" dataset = Dataset . from_dict ( { \"question\" : [ item . input [ \"query_str\" ]], \"contexts\" : [[ n . node . text for n in response . source_nodes ]], \"answer\" : [ response . response ], \"ground_truth\" : [ item . expected_output [ \"result\" ]], } ) return ( self . evaluator_function ( metrics = self . metrics , dataset = dataset , llm = self . judge_llm , embeddings = self . judge_embedding_model , ) . to_pandas () . iloc [ 0 ] ) RagasEvaluatorFactory Bases: Factory Factory for creating configured RagasEvaluator instances. Creates RagasEvaluator objects based on provided configuration, handling the initialization of required LLM and embedding models. Attributes: _configuration_class ( Type ) \u2013 Configuration class type used for validation Source code in src/evaluation/evaluators/ragas.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class RagasEvaluatorFactory ( Factory ): \"\"\"Factory for creating configured RagasEvaluator instances. Creates RagasEvaluator objects based on provided configuration, handling the initialization of required LLM and embedding models. Attributes: _configuration_class: Configuration class type used for validation \"\"\" _configuration_class : Type = _EvaluationConfiguration @classmethod def _create_instance ( cls , configuration : _EvaluationConfiguration ) -> RagasEvaluator : \"\"\"Create and initialize a RagasEvaluator with configured models. Initializes judge LLM and embedding models according to configuration specifications, then creates a RagasEvaluator with those models. Args: configuration: Evaluation configuration object containing model specifications for the judge LLM and embedding model Returns: RagasEvaluator: Fully initialized evaluator ready to assess RAG responses \"\"\" judge_llm = LLMRegistry . get ( configuration . judge_llm . provider ) . create ( configuration . judge_llm ) judge_embedding_model = EmbeddingModelRegistry . get ( configuration . judge_embedding_model . provider ) . create ( configuration . judge_embedding_model ) return RagasEvaluator ( judge_llm = judge_llm , judge_embedding_model = judge_embedding_model )","title":"Ragas"},{"location":"src/evaluation/evaluators/ragas/#ragas","text":"This module contains functionality related to the the ragas module for evaluation.evaluators .","title":"Ragas"},{"location":"src/evaluation/evaluators/ragas/#ragas_1","text":"","title":"Ragas"},{"location":"src/evaluation/evaluators/ragas/#src.evaluation.evaluators.ragas.RagasEvaluator","text":"Evaluator for RAG system quality using RAGAS framework. Provides automatic evaluation of RAG pipeline quality using multiple metrics from the RAGAS evaluation framework. Evaluates answer relevancy, factual consistency (faithfulness), harmfulness, and source context recall. Source code in src/evaluation/evaluators/ragas.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class RagasEvaluator : \"\"\"Evaluator for RAG system quality using RAGAS framework. Provides automatic evaluation of RAG pipeline quality using multiple metrics from the RAGAS evaluation framework. Evaluates answer relevancy, factual consistency (faithfulness), harmfulness, and source context recall. \"\"\" def __init__ ( self , judge_llm : BaseLLM , judge_embedding_model : BaseEmbedding , evaluator_function : Callable = ragas_evaluate , ) -> None : \"\"\"Initialize RAGAS evaluator with required models and configuration. Args: judge_llm: LlamaIndex LLM used to evaluate response quality judge_embedding_model: Embedding model for semantic comparisons evaluator_function: Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function \"\"\" self . judge_llm = LlamaIndexLLMWrapper ( judge_llm ) self . judge_embedding_model = LlamaIndexEmbeddingsWrapper ( judge_embedding_model ) self . evaluator_function = evaluator_function self . metrics = [ answer_relevancy , faithfulness , harmfulness , context_recall , ] def evaluate ( self , response : Response , item : DatasetItemClient ) -> Series : \"\"\"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Args: response: LlamaIndex response object containing the generated answer and source nodes used for retrieval item: Langfuse dataset item containing the original query and expected ground truth answer Returns: Series: Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) \"\"\" dataset = Dataset . from_dict ( { \"question\" : [ item . input [ \"query_str\" ]], \"contexts\" : [[ n . node . text for n in response . source_nodes ]], \"answer\" : [ response . response ], \"ground_truth\" : [ item . expected_output [ \"result\" ]], } ) return ( self . evaluator_function ( metrics = self . metrics , dataset = dataset , llm = self . judge_llm , embeddings = self . judge_embedding_model , ) . to_pandas () . iloc [ 0 ] )","title":"RagasEvaluator"},{"location":"src/evaluation/evaluators/ragas/#src.evaluation.evaluators.ragas.RagasEvaluator.__init__","text":"Initialize RAGAS evaluator with required models and configuration. Parameters: judge_llm ( BaseLLM ) \u2013 LlamaIndex LLM used to evaluate response quality judge_embedding_model ( BaseEmbedding ) \u2013 Embedding model for semantic comparisons evaluator_function ( Callable , default: evaluate ) \u2013 Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function Source code in src/evaluation/evaluators/ragas.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , judge_llm : BaseLLM , judge_embedding_model : BaseEmbedding , evaluator_function : Callable = ragas_evaluate , ) -> None : \"\"\"Initialize RAGAS evaluator with required models and configuration. Args: judge_llm: LlamaIndex LLM used to evaluate response quality judge_embedding_model: Embedding model for semantic comparisons evaluator_function: Function that runs the evaluation pipeline, defaults to the standard RAGAS evaluate function \"\"\" self . judge_llm = LlamaIndexLLMWrapper ( judge_llm ) self . judge_embedding_model = LlamaIndexEmbeddingsWrapper ( judge_embedding_model ) self . evaluator_function = evaluator_function self . metrics = [ answer_relevancy , faithfulness , harmfulness , context_recall , ]","title":"__init__"},{"location":"src/evaluation/evaluators/ragas/#src.evaluation.evaluators.ragas.RagasEvaluator.evaluate","text":"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Parameters: response ( Response ) \u2013 LlamaIndex response object containing the generated answer and source nodes used for retrieval item ( DatasetItemClient ) \u2013 Langfuse dataset item containing the original query and expected ground truth answer Returns: Series ( Series ) \u2013 Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) Source code in src/evaluation/evaluators/ragas.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def evaluate ( self , response : Response , item : DatasetItemClient ) -> Series : \"\"\"Evaluate a RAG response against multiple quality metrics. Calculates RAGAS evaluation metrics comparing the response to ground truth and source contexts. Creates a temporary dataset structure needed for RAGAS evaluation framework. Args: response: LlamaIndex response object containing the generated answer and source nodes used for retrieval item: Langfuse dataset item containing the original query and expected ground truth answer Returns: Series: Pandas Series containing individual scores for each metric (answer relevancy, faithfulness, harmfulness, context recall) \"\"\" dataset = Dataset . from_dict ( { \"question\" : [ item . input [ \"query_str\" ]], \"contexts\" : [[ n . node . text for n in response . source_nodes ]], \"answer\" : [ response . response ], \"ground_truth\" : [ item . expected_output [ \"result\" ]], } ) return ( self . evaluator_function ( metrics = self . metrics , dataset = dataset , llm = self . judge_llm , embeddings = self . judge_embedding_model , ) . to_pandas () . iloc [ 0 ] )","title":"evaluate"},{"location":"src/evaluation/evaluators/ragas/#src.evaluation.evaluators.ragas.RagasEvaluatorFactory","text":"Bases: Factory Factory for creating configured RagasEvaluator instances. Creates RagasEvaluator objects based on provided configuration, handling the initialization of required LLM and embedding models. Attributes: _configuration_class ( Type ) \u2013 Configuration class type used for validation Source code in src/evaluation/evaluators/ragas.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class RagasEvaluatorFactory ( Factory ): \"\"\"Factory for creating configured RagasEvaluator instances. Creates RagasEvaluator objects based on provided configuration, handling the initialization of required LLM and embedding models. Attributes: _configuration_class: Configuration class type used for validation \"\"\" _configuration_class : Type = _EvaluationConfiguration @classmethod def _create_instance ( cls , configuration : _EvaluationConfiguration ) -> RagasEvaluator : \"\"\"Create and initialize a RagasEvaluator with configured models. Initializes judge LLM and embedding models according to configuration specifications, then creates a RagasEvaluator with those models. Args: configuration: Evaluation configuration object containing model specifications for the judge LLM and embedding model Returns: RagasEvaluator: Fully initialized evaluator ready to assess RAG responses \"\"\" judge_llm = LLMRegistry . get ( configuration . judge_llm . provider ) . create ( configuration . judge_llm ) judge_embedding_model = EmbeddingModelRegistry . get ( configuration . judge_embedding_model . provider ) . create ( configuration . judge_embedding_model ) return RagasEvaluator ( judge_llm = judge_llm , judge_embedding_model = judge_embedding_model )","title":"RagasEvaluatorFactory"},{"location":"src/extraction/bootstrap/initializer/","text":"Initializer This module contains functionality related to the the initializer module for extraction.bootstrap . Initializer ExtractionInitializer Bases: BasicInitializer Initializer for the extraction module. Handles the initialization of extraction components including configuration and required packages. Source code in src/extraction/bootstrap/initializer.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class ExtractionInitializer ( BasicInitializer ): \"\"\" Initializer for the extraction module. Handles the initialization of extraction components including configuration and required packages. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = ExtractionConfiguration , package_loader : BasePackageLoader = ExtractionPackageLoader (), ): \"\"\" Initialize the extraction module. Args: configuration_class: Configuration class to use for extraction package_loader: Loader for required extraction packages \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) __init__ ( configuration_class = ExtractionConfiguration , package_loader = ExtractionPackageLoader ()) Initialize the extraction module. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: ExtractionConfiguration ) \u2013 Configuration class to use for extraction package_loader ( BasePackageLoader , default: ExtractionPackageLoader () ) \u2013 Loader for required extraction packages Source code in src/extraction/bootstrap/initializer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = ExtractionConfiguration , package_loader : BasePackageLoader = ExtractionPackageLoader (), ): \"\"\" Initialize the extraction module. Args: configuration_class: Configuration class to use for extraction package_loader: Loader for required extraction packages \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , ) ExtractionPackageLoader Bases: BasePackageLoader Loader for extraction-related packages. Handles loading of extraction datasources and orchestrators packages. Source code in src/extraction/bootstrap/initializer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ExtractionPackageLoader ( BasePackageLoader ): \"\"\" Loader for extraction-related packages. Handles loading of extraction datasources and orchestrators packages. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initialize the extraction package loader. Args: logger: Logger instance for logging information \"\"\" super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\" Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.extraction.datasources\" , \"src.extraction.orchestrators\" ] ) __init__ ( logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the extraction package loader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information Source code in src/extraction/bootstrap/initializer.py 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initialize the extraction package loader. Args: logger: Logger instance for logging information \"\"\" super () . __init__ ( logger ) load_packages () Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. Source code in src/extraction/bootstrap/initializer.py 31 32 33 34 35 36 37 38 39 40 def load_packages ( self ) -> None : \"\"\" Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.extraction.datasources\" , \"src.extraction.orchestrators\" ] )","title":"Initializer"},{"location":"src/extraction/bootstrap/initializer/#initializer","text":"This module contains functionality related to the the initializer module for extraction.bootstrap .","title":"Initializer"},{"location":"src/extraction/bootstrap/initializer/#initializer_1","text":"","title":"Initializer"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionInitializer","text":"Bases: BasicInitializer Initializer for the extraction module. Handles the initialization of extraction components including configuration and required packages. Source code in src/extraction/bootstrap/initializer.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class ExtractionInitializer ( BasicInitializer ): \"\"\" Initializer for the extraction module. Handles the initialization of extraction components including configuration and required packages. \"\"\" def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = ExtractionConfiguration , package_loader : BasePackageLoader = ExtractionPackageLoader (), ): \"\"\" Initialize the extraction module. Args: configuration_class: Configuration class to use for extraction package_loader: Loader for required extraction packages \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , )","title":"ExtractionInitializer"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionInitializer.__init__","text":"Initialize the extraction module. Parameters: configuration_class ( Type [ BaseConfiguration ] , default: ExtractionConfiguration ) \u2013 Configuration class to use for extraction package_loader ( BasePackageLoader , default: ExtractionPackageLoader () ) \u2013 Loader for required extraction packages Source code in src/extraction/bootstrap/initializer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __init__ ( self , configuration_class : Type [ BaseConfiguration ] = ExtractionConfiguration , package_loader : BasePackageLoader = ExtractionPackageLoader (), ): \"\"\" Initialize the extraction module. Args: configuration_class: Configuration class to use for extraction package_loader: Loader for required extraction packages \"\"\" super () . __init__ ( configuration_class = configuration_class , package_loader = package_loader , )","title":"__init__"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionPackageLoader","text":"Bases: BasePackageLoader Loader for extraction-related packages. Handles loading of extraction datasources and orchestrators packages. Source code in src/extraction/bootstrap/initializer.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class ExtractionPackageLoader ( BasePackageLoader ): \"\"\" Loader for extraction-related packages. Handles loading of extraction datasources and orchestrators packages. \"\"\" def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initialize the extraction package loader. Args: logger: Logger instance for logging information \"\"\" super () . __init__ ( logger ) def load_packages ( self ) -> None : \"\"\" Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.extraction.datasources\" , \"src.extraction.orchestrators\" ] )","title":"ExtractionPackageLoader"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionPackageLoader.__init__","text":"Initialize the extraction package loader. Parameters: logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging information Source code in src/extraction/bootstrap/initializer.py 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ) ): \"\"\" Initialize the extraction package loader. Args: logger: Logger instance for logging information \"\"\" super () . __init__ ( logger )","title":"__init__"},{"location":"src/extraction/bootstrap/initializer/#src.extraction.bootstrap.initializer.ExtractionPackageLoader.load_packages","text":"Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. Source code in src/extraction/bootstrap/initializer.py 31 32 33 34 35 36 37 38 39 40 def load_packages ( self ) -> None : \"\"\" Load all required extraction packages. Loads datasources and orchestrators packages needed for extraction operations. \"\"\" super () . load_packages () self . _load_packages ( [ \"src.extraction.datasources\" , \"src.extraction.orchestrators\" ] )","title":"load_packages"},{"location":"src/extraction/bootstrap/configuration/configuration/","text":"Configuration This module contains functionality related to the the configuration module for extraction.bootstrap.configuration . Configuration ExtractionConfiguration Bases: BasicConfiguration Main configuration class for the extraction system. Extends the BasicConfiguration with extraction-specific settings. Source code in src/extraction/bootstrap/configuration/configuration.py 56 57 58 59 60 61 62 63 64 class ExtractionConfiguration ( BasicConfiguration ): \"\"\" Main configuration class for the extraction system. Extends the BasicConfiguration with extraction-specific settings. \"\"\" extraction : _ExtractionConfiguration = Field ( ... , description = \"Extraction configuration.\" ) OrchestratorName Bases: str , Enum Enum representing the available orchestrators for extraction. Source code in src/extraction/bootstrap/configuration/configuration.py 12 13 14 15 16 17 class OrchestratorName ( str , Enum ): \"\"\" Enum representing the available orchestrators for extraction. \"\"\" BASIC = \"basic\"","title":"Configuration"},{"location":"src/extraction/bootstrap/configuration/configuration/#configuration","text":"This module contains functionality related to the the configuration module for extraction.bootstrap.configuration .","title":"Configuration"},{"location":"src/extraction/bootstrap/configuration/configuration/#configuration_1","text":"","title":"Configuration"},{"location":"src/extraction/bootstrap/configuration/configuration/#src.extraction.bootstrap.configuration.configuration.ExtractionConfiguration","text":"Bases: BasicConfiguration Main configuration class for the extraction system. Extends the BasicConfiguration with extraction-specific settings. Source code in src/extraction/bootstrap/configuration/configuration.py 56 57 58 59 60 61 62 63 64 class ExtractionConfiguration ( BasicConfiguration ): \"\"\" Main configuration class for the extraction system. Extends the BasicConfiguration with extraction-specific settings. \"\"\" extraction : _ExtractionConfiguration = Field ( ... , description = \"Extraction configuration.\" )","title":"ExtractionConfiguration"},{"location":"src/extraction/bootstrap/configuration/configuration/#src.extraction.bootstrap.configuration.configuration.OrchestratorName","text":"Bases: str , Enum Enum representing the available orchestrators for extraction. Source code in src/extraction/bootstrap/configuration/configuration.py 12 13 14 15 16 17 class OrchestratorName ( str , Enum ): \"\"\" Enum representing the available orchestrators for extraction. \"\"\" BASIC = \"basic\"","title":"OrchestratorName"},{"location":"src/extraction/bootstrap/configuration/datasources/","text":"Datasources This module contains functionality related to the the datasources module for extraction.bootstrap.configuration . Datasources DatasourceConfiguration Bases: BaseConfigurationWithSecrets , ABC Abstract base class for all data source configurations. This class serves as the foundation for specific data source implementations, providing common configuration parameters. All concrete datasource configurations should inherit from this class. Source code in src/extraction/bootstrap/configuration/datasources.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class DatasourceConfiguration ( BaseConfigurationWithSecrets , ABC ): \"\"\" Abstract base class for all data source configurations. This class serves as the foundation for specific data source implementations, providing common configuration parameters. All concrete datasource configurations should inherit from this class. \"\"\" name : DatasourceName = Field ( ... , description = \"The name of the data source.\" ) export_limit : Optional [ int ] = Field ( None , description = \"The export limit for the data source.\" ) DatasourceConfigurationRegistry Bases: ConfigurationRegistry Registry for datasource configurations. This registry manages all available datasource configurations and provides methods to access them based on their type. Source code in src/extraction/bootstrap/configuration/datasources.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class DatasourceConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for datasource configurations. This registry manages all available datasource configurations and provides methods to access them based on their type. \"\"\" _key_class = DatasourceName @classmethod def get_union_type ( self ) -> List [ DatasourceConfiguration ]: \"\"\" Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List[DatasourceConfiguration]: A type representing a list of all registered datasource configurations. \"\"\" return List [ super () . get_union_type ()] get_union_type () classmethod Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List [ DatasourceConfiguration ] \u2013 List[DatasourceConfiguration]: A type representing a list of all List [ DatasourceConfiguration ] \u2013 registered datasource configurations. Source code in src/extraction/bootstrap/configuration/datasources.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @classmethod def get_union_type ( self ) -> List [ DatasourceConfiguration ]: \"\"\" Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List[DatasourceConfiguration]: A type representing a list of all registered datasource configurations. \"\"\" return List [ super () . get_union_type ()] DatasourceName Bases: str , Enum List of all available datasources. Defines the supported data sources that can be used for extraction: - CONFLUENCE: Atlassian Confluence wiki pages and spaces - NOTION: Notion databases, pages, and blocks - PDF: PDF documents from file system or URLs Source code in src/extraction/bootstrap/configuration/datasources.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class DatasourceName ( str , Enum ): \"\"\" List of all available datasources. Defines the supported data sources that can be used for extraction: - CONFLUENCE: Atlassian Confluence wiki pages and spaces - NOTION: Notion databases, pages, and blocks - PDF: PDF documents from file system or URLs \"\"\" CONFLUENCE = \"confluence\" NOTION = \"notion\" PDF = \"pdf\" BUNDESTAG = \"bundestag\"","title":"Datasources"},{"location":"src/extraction/bootstrap/configuration/datasources/#datasources","text":"This module contains functionality related to the the datasources module for extraction.bootstrap.configuration .","title":"Datasources"},{"location":"src/extraction/bootstrap/configuration/datasources/#datasources_1","text":"","title":"Datasources"},{"location":"src/extraction/bootstrap/configuration/datasources/#src.extraction.bootstrap.configuration.datasources.DatasourceConfiguration","text":"Bases: BaseConfigurationWithSecrets , ABC Abstract base class for all data source configurations. This class serves as the foundation for specific data source implementations, providing common configuration parameters. All concrete datasource configurations should inherit from this class. Source code in src/extraction/bootstrap/configuration/datasources.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class DatasourceConfiguration ( BaseConfigurationWithSecrets , ABC ): \"\"\" Abstract base class for all data source configurations. This class serves as the foundation for specific data source implementations, providing common configuration parameters. All concrete datasource configurations should inherit from this class. \"\"\" name : DatasourceName = Field ( ... , description = \"The name of the data source.\" ) export_limit : Optional [ int ] = Field ( None , description = \"The export limit for the data source.\" )","title":"DatasourceConfiguration"},{"location":"src/extraction/bootstrap/configuration/datasources/#src.extraction.bootstrap.configuration.datasources.DatasourceConfigurationRegistry","text":"Bases: ConfigurationRegistry Registry for datasource configurations. This registry manages all available datasource configurations and provides methods to access them based on their type. Source code in src/extraction/bootstrap/configuration/datasources.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class DatasourceConfigurationRegistry ( ConfigurationRegistry ): \"\"\" Registry for datasource configurations. This registry manages all available datasource configurations and provides methods to access them based on their type. \"\"\" _key_class = DatasourceName @classmethod def get_union_type ( self ) -> List [ DatasourceConfiguration ]: \"\"\" Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List[DatasourceConfiguration]: A type representing a list of all registered datasource configurations. \"\"\" return List [ super () . get_union_type ()]","title":"DatasourceConfigurationRegistry"},{"location":"src/extraction/bootstrap/configuration/datasources/#src.extraction.bootstrap.configuration.datasources.DatasourceConfigurationRegistry.get_union_type","text":"Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List [ DatasourceConfiguration ] \u2013 List[DatasourceConfiguration]: A type representing a list of all List [ DatasourceConfiguration ] \u2013 registered datasource configurations. Source code in src/extraction/bootstrap/configuration/datasources.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @classmethod def get_union_type ( self ) -> List [ DatasourceConfiguration ]: \"\"\" Returns the union type of all available datasources. This method provides a type hint representing a list of all possible datasource configurations, which can be used for validation or type checking when working with collections of datasources. Returns: List[DatasourceConfiguration]: A type representing a list of all registered datasource configurations. \"\"\" return List [ super () . get_union_type ()]","title":"get_union_type"},{"location":"src/extraction/bootstrap/configuration/datasources/#src.extraction.bootstrap.configuration.datasources.DatasourceName","text":"Bases: str , Enum List of all available datasources. Defines the supported data sources that can be used for extraction: - CONFLUENCE: Atlassian Confluence wiki pages and spaces - NOTION: Notion databases, pages, and blocks - PDF: PDF documents from file system or URLs Source code in src/extraction/bootstrap/configuration/datasources.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class DatasourceName ( str , Enum ): \"\"\" List of all available datasources. Defines the supported data sources that can be used for extraction: - CONFLUENCE: Atlassian Confluence wiki pages and spaces - NOTION: Notion databases, pages, and blocks - PDF: PDF documents from file system or URLs \"\"\" CONFLUENCE = \"confluence\" NOTION = \"notion\" PDF = \"pdf\" BUNDESTAG = \"bundestag\"","title":"DatasourceName"},{"location":"src/extraction/datasources/bundestag/","text":"Bundestag Datasource This module contains functionality related to the Bundestag datasource. Client BundestagMineClient Bases: APIClient API Client for the bundestag-mine.de API. Source code in src/extraction/datasources/bundestag/client.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 class BundestagMineClient ( APIClient ): \"\"\" API Client for the bundestag-mine.de API. \"\"\" BASE_URL = \"https://bundestag-mine.de/api/DashboardController\" logger = LoggerConfiguration . get_logger ( __name__ ) def safe_get ( self , path : str ) -> Optional [ Any ]: \"\"\" Perform a GET request, raise for HTTP errors, parse JSON, check API status. Args: path: endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/<protocol_id>\" Returns: Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError: if HTTP status is not OK or unexpected JSON structure. \"\"\" url = f \" { self . BASE_URL } / { path . lstrip ( '/' ) } \" try : resp = self . get ( url ) except Exception as e : self . logger . warning ( f \"Failed to fetch speeches for { url } : { e } \" ) return None try : resp . raise_for_status () except Exception as e : self . logger . error ( f \"HTTP error for { url } : { e } \" ) return None data = resp . json () if not isinstance ( data , dict ) or data . get ( \"status\" ) != \"200\" : self . logger . error ( f \"Unexpected response for { url } : { data } \" ) return None result = data . get ( \"result\" ) if result is None : self . logger . debug ( f \"No result found for { url } \" ) return None return result def get_protocols ( self ) -> Iterator [ Protocol ]: \"\"\" Fetches the list of all protocols. Returns: Iterator[Protocol]: An iterator of valid protocols as Pydantic models. \"\"\" result = self . safe_get ( \"GetProtocols\" ) if not isinstance ( result , list ): raise ResponseParseError ( f \"Expected list of protocols, got: { result } \" ) for protocol_data in result : try : yield Protocol . model_validate ( protocol_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate protocol: { protocol_data } . Error: { e } \" ) def get_agenda_items ( self , protocol_id : str ) -> Iterator [ AgendaItem ]: \"\"\" Fetches agenda items for a specific protocol ID. Args: protocol_id (str): The ID of the protocol. Returns: Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. \"\"\" result = self . safe_get ( f \"GetAgendaItemsOfProtocol/ { protocol_id } \" ) if result is None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return items = result . get ( \"agendaItems\" ) if \"items\" == None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return if not isinstance ( items , list ): self . logger . error ( f \"Expected list of agendaItems for { protocol_id } , got: { items } \" ) return for item_data in items : try : yield AgendaItem . model_validate ( item_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate agenda item: { item_data } . Error: { e } \" ) def get_speaker_data ( self , speaker_id : str ) -> Optional [ Speaker ]: \"\"\" Fetches speaker data for a specific speaker ID. Args: speaker_id (str): The ID of the speaker. Returns: Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. \"\"\" result = self . safe_get ( f \"GetSpeakerById/ { speaker_id } \" ) if not isinstance ( result , dict ): self . logger . error ( f \"Expected speaker data for { speaker_id } , got: { result } \" ) return None try : return Speaker . model_validate ( result ) except ValidationError as e : self . logger . warning ( f \"Failed to validate speaker data for { speaker_id } : { result } . Error: { e } \" ) return None @retry_request def get_speeches ( self , protocol : Protocol , agenda_item : AgendaItem , ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches speeches for a specific agenda item within a protocol. Args: legislature_period (int): The legislature period. protocol_number (int): The protocol number. agenda_item_number (str): The agenda item number. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" raw = f \" { protocol . legislaturePeriod } , { protocol . number } , { agenda_item . agendaItemNumber } \" encoded = quote ( raw , safe = \"\" ) result = self . safe_get ( f \"GetSpeechesOfAgendaItem/ { encoded } \" ) if result is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return speeches = result . get ( \"speeches\" ) if speeches is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return if not isinstance ( speeches , list ): self . logger . warning ( f \"Expected list of speeches for { raw } , got: { speeches } \" ) return for speech in speeches : try : speech = BundestagSpeech . model_validate ( speech ) speech . protocol = protocol speech . agendaItem = agenda_item yield speech except ValidationError as e : self . logger . warning ( f \"Failed to validate speech: { speech } . Error: { e } \" ) def fetch_all_speeches ( self ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" for protocol in self . get_protocols (): self . logger . info ( f \"Processing protocol { protocol . id } \" ) for agenda_item in self . get_agenda_items ( protocol . id ): for speech in self . get_speeches ( protocol = protocol , agenda_item = agenda_item , ): speaker = self . get_speaker_data ( speech . speakerId ) if speaker : speech . speaker = speaker yield speech fetch_all_speeches () Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator [ BundestagSpeech ] \u2013 Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def fetch_all_speeches ( self ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" for protocol in self . get_protocols (): self . logger . info ( f \"Processing protocol { protocol . id } \" ) for agenda_item in self . get_agenda_items ( protocol . id ): for speech in self . get_speeches ( protocol = protocol , agenda_item = agenda_item , ): speaker = self . get_speaker_data ( speech . speakerId ) if speaker : speech . speaker = speaker yield speech get_agenda_items ( protocol_id ) Fetches agenda items for a specific protocol ID. Parameters: protocol_id ( str ) \u2013 The ID of the protocol. Returns: Iterator [ AgendaItem ] \u2013 Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def get_agenda_items ( self , protocol_id : str ) -> Iterator [ AgendaItem ]: \"\"\" Fetches agenda items for a specific protocol ID. Args: protocol_id (str): The ID of the protocol. Returns: Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. \"\"\" result = self . safe_get ( f \"GetAgendaItemsOfProtocol/ { protocol_id } \" ) if result is None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return items = result . get ( \"agendaItems\" ) if \"items\" == None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return if not isinstance ( items , list ): self . logger . error ( f \"Expected list of agendaItems for { protocol_id } , got: { items } \" ) return for item_data in items : try : yield AgendaItem . model_validate ( item_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate agenda item: { item_data } . Error: { e } \" ) get_protocols () Fetches the list of all protocols. Returns: Iterator [ Protocol ] \u2013 Iterator[Protocol]: An iterator of valid protocols as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def get_protocols ( self ) -> Iterator [ Protocol ]: \"\"\" Fetches the list of all protocols. Returns: Iterator[Protocol]: An iterator of valid protocols as Pydantic models. \"\"\" result = self . safe_get ( \"GetProtocols\" ) if not isinstance ( result , list ): raise ResponseParseError ( f \"Expected list of protocols, got: { result } \" ) for protocol_data in result : try : yield Protocol . model_validate ( protocol_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate protocol: { protocol_data } . Error: { e } \" ) get_speaker_data ( speaker_id ) Fetches speaker data for a specific speaker ID. Parameters: speaker_id ( str ) \u2013 The ID of the speaker. Returns: Optional [ Speaker ] \u2013 Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. Source code in src/extraction/datasources/bundestag/client.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def get_speaker_data ( self , speaker_id : str ) -> Optional [ Speaker ]: \"\"\" Fetches speaker data for a specific speaker ID. Args: speaker_id (str): The ID of the speaker. Returns: Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. \"\"\" result = self . safe_get ( f \"GetSpeakerById/ { speaker_id } \" ) if not isinstance ( result , dict ): self . logger . error ( f \"Expected speaker data for { speaker_id } , got: { result } \" ) return None try : return Speaker . model_validate ( result ) except ValidationError as e : self . logger . warning ( f \"Failed to validate speaker data for { speaker_id } : { result } . Error: { e } \" ) return None get_speeches ( protocol , agenda_item ) Fetches speeches for a specific agenda item within a protocol. Parameters: legislature_period ( int ) \u2013 The legislature period. protocol_number ( int ) \u2013 The protocol number. agenda_item_number ( str ) \u2013 The agenda item number. Returns: Iterator [ BundestagSpeech ] \u2013 Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @retry_request def get_speeches ( self , protocol : Protocol , agenda_item : AgendaItem , ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches speeches for a specific agenda item within a protocol. Args: legislature_period (int): The legislature period. protocol_number (int): The protocol number. agenda_item_number (str): The agenda item number. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" raw = f \" { protocol . legislaturePeriod } , { protocol . number } , { agenda_item . agendaItemNumber } \" encoded = quote ( raw , safe = \"\" ) result = self . safe_get ( f \"GetSpeechesOfAgendaItem/ { encoded } \" ) if result is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return speeches = result . get ( \"speeches\" ) if speeches is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return if not isinstance ( speeches , list ): self . logger . warning ( f \"Expected list of speeches for { raw } , got: { speeches } \" ) return for speech in speeches : try : speech = BundestagSpeech . model_validate ( speech ) speech . protocol = protocol speech . agendaItem = agenda_item yield speech except ValidationError as e : self . logger . warning ( f \"Failed to validate speech: { speech } . Error: { e } \" ) safe_get ( path ) Perform a GET request, raise for HTTP errors, parse JSON, check API status. Parameters: path ( str ) \u2013 endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/ \" Returns: Optional [ Any ] \u2013 Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError \u2013 if HTTP status is not OK or unexpected JSON structure. Source code in src/extraction/datasources/bundestag/client.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def safe_get ( self , path : str ) -> Optional [ Any ]: \"\"\" Perform a GET request, raise for HTTP errors, parse JSON, check API status. Args: path: endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/<protocol_id>\" Returns: Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError: if HTTP status is not OK or unexpected JSON structure. \"\"\" url = f \" { self . BASE_URL } / { path . lstrip ( '/' ) } \" try : resp = self . get ( url ) except Exception as e : self . logger . warning ( f \"Failed to fetch speeches for { url } : { e } \" ) return None try : resp . raise_for_status () except Exception as e : self . logger . error ( f \"HTTP error for { url } : { e } \" ) return None data = resp . json () if not isinstance ( data , dict ) or data . get ( \"status\" ) != \"200\" : self . logger . error ( f \"Unexpected response for { url } : { data } \" ) return None result = data . get ( \"result\" ) if result is None : self . logger . debug ( f \"No result found for { url } \" ) return None return result BundestagMineClientFactory Bases: SingletonFactory Factory for creating and managing Bundestag client instances. This factory ensures only one Bundestag client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. Source code in src/extraction/datasources/bundestag/client.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 class BundestagMineClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Bundestag client instances. This factory ensures only one Bundestag client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineClient : \"\"\" Creates a new BundestagMine client instance using the provided configuration. Args: configuration: Configuration object containing BundestagMine details Returns: A configured BundestagMine client instance ready for API interactions. \"\"\" return BundestagMineClient () BundestagSpeech Bases: BaseModel Represents a speech from BundestagMine API. Source code in src/extraction/datasources/bundestag/client.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class BundestagSpeech ( BaseModel ): \"\"\"Represents a speech from BundestagMine API.\"\"\" id : str speakerId : str text : str speaker : Optional [ Speaker ] = None protocol : Optional [ Protocol ] = None agendaItem : Optional [ AgendaItem ] = None @model_validator ( mode = \"after\" ) def validate_text_not_empty ( self ) -> \"BundestagSpeech\" : if not self . text or self . text . strip () == \"\" : raise ValueError ( \"BundestagSpeech text cannot be empty\" ) return self Configuration Document BundestagMineDocument Bases: BaseDocument Represents a document from the Bundestag datasource. Inherits from BaseDocument and includes additional metadata specific to Bundestag documents. Source code in src/extraction/datasources/bundestag/document.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class BundestagMineDocument ( BaseDocument ): \"\"\" Represents a document from the Bundestag datasource. Inherits from BaseDocument and includes additional metadata specific to Bundestag documents. \"\"\" included_embed_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , \"speaker_party\" , \"speaker\" , \"protocol_number\" , \"legislature_period\" , ] included_llm_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , \"speaker_party\" , \"speaker\" , \"protocol_number\" , \"legislature_period\" , ] Manager BundestagMineDatasourceManagerFactory Bases: Factory Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object Source code in src/extraction/datasources/bundestag/manager.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class BundestagMineDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class: Type of configuration object \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create an instance of the BundestagMine datasource manager. This method constructs a BasicDatasourceManager by creating the appropriate reader and parser based on the provided configuration. Args: configuration: Configuration specifying how to set up the BundestagMine datasource manager, reader, and parser. Returns: A configured BasicDatasourceManager instance for handling BundestagMine documents. \"\"\" reader = BundestagMineDatasourceReaderFactory . create ( configuration ) parser = BundestagMineDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration = configuration , reader = reader , parser = parser , ) Parser BundestagMineDatasourceParser Bases: BaseParser [ BundestagMineDocument ] Source code in src/extraction/datasources/bundestag/parser.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class BundestagMineDatasourceParser ( BaseParser [ BundestagMineDocument ]): logger = LoggerConfiguration . get_logger ( __name__ ) def parse ( self , speech : BundestagSpeech ) -> BundestagMineDocument : \"\"\" Parse content into a BundestagMineDocument object. Args: content: Raw response dict to be parsed Returns: Parsed document of type BundestagMineDocument \"\"\" metadata = self . _extract_metadata ( speech ) return BundestagMineDocument ( text = speech . text , metadata = metadata ) def _extract_metadata ( self , speech : BundestagSpeech ) -> dict : \"\"\" Extract metadata from the response. Args: response: Raw response string Returns: Dictionary containing extracted metadata \"\"\" legislature_period = speech . protocol . legislaturePeriod protocol_number = speech . protocol . number agenda_item_number = speech . agendaItem . agendaItemNumber url = f \"https://dserver.bundestag.de/btp/ { legislature_period } / { legislature_period }{ protocol_number } .pdf\" title = f \"Protocol/Legislature/AgendaItem { protocol_number } / { legislature_period } / { agenda_item_number } \" speaker_name = f \" { speech . speaker . firstName } { speech . speaker . lastName } \" return { \"datasource\" : \"bundestag\" , \"language\" : \"de\" , \"url\" : url , \"title\" : title , \"format\" : \"md\" , \"created_time\" : speech . protocol . date , \"last_edited_time\" : speech . protocol . date , \"speaker_party\" : speech . speaker . party , \"speaker\" : speaker_name , \"agenda_item_number\" : agenda_item_number , \"protocol_number\" : protocol_number , \"legislature_period\" : legislature_period , } parse ( speech ) Parse content into a BundestagMineDocument object. Parameters: content \u2013 Raw response dict to be parsed Returns: BundestagMineDocument \u2013 Parsed document of type BundestagMineDocument Source code in src/extraction/datasources/bundestag/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 def parse ( self , speech : BundestagSpeech ) -> BundestagMineDocument : \"\"\" Parse content into a BundestagMineDocument object. Args: content: Raw response dict to be parsed Returns: Parsed document of type BundestagMineDocument \"\"\" metadata = self . _extract_metadata ( speech ) return BundestagMineDocument ( text = speech . text , metadata = metadata ) BundestagMineDatasourceParserFactory Bases: Factory Factory for creating instances of BundestagMineDatasourceParser. Creates and configures BundestagMineDatasourceParser objects according to the provided configuration. Source code in src/extraction/datasources/bundestag/parser.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class BundestagMineDatasourceParserFactory ( Factory ): \"\"\" Factory for creating instances of BundestagMineDatasourceParser. Creates and configures BundestagMineDatasourceParser objects according to the provided configuration. \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineDatasourceParser : \"\"\" Create an instance of BundestagMineDatasourceParser. Args: configuration: Configuration for the parser (not used in this implementation) Returns: An instance of BundestagMineDatasourceParser \"\"\" return BundestagMineDatasourceParser () Reader BundestagMineDatasourceReader Bases: BaseReader Reader for extracting speeches from the BundestagMine API. Implements document extraction from the Bundestag speeches. Source code in src/extraction/datasources/bundestag/reader.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class BundestagMineDatasourceReader ( BaseReader ): \"\"\"Reader for extracting speeches from the BundestagMine API. Implements document extraction from the Bundestag speeches. \"\"\" def __init__ ( self , configuration : BundestagMineDatasourceConfiguration , client : BundestagMineClient , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the BundestagMine reader. Args: configuration: Settings for BundestagMine access and export limits client: Client for BundestagMine API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger async def read_all_async ( self , ) -> AsyncIterator [ dict ]: \"\"\"Asynchronously fetch all speeches from BundestagMine. Yields each speech as a dictionary containing its content and metadata. Returns: AsyncIterator[dict]: An async iterator of page dictionaries containing content and metadata such as text, speaker data, and last update information \"\"\" self . logger . info ( f \"Reading speeches from BundestagMine with limit { self . export_limit } \" ) speech_iterator = self . client . fetch_all_speeches () yield_counter = 0 for speech in speech_iterator : if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Bundestag speech { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield speech __init__ ( configuration , client , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the BundestagMine reader. Parameters: configuration ( BundestagMineDatasourceConfiguration ) \u2013 Settings for BundestagMine access and export limits client ( BundestagMineClient ) \u2013 Client for BundestagMine API interactions logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operation information Source code in src/extraction/datasources/bundestag/reader.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , configuration : BundestagMineDatasourceConfiguration , client : BundestagMineClient , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the BundestagMine reader. Args: configuration: Settings for BundestagMine access and export limits client: Client for BundestagMine API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger read_all_async () async Asynchronously fetch all speeches from BundestagMine. Yields each speech as a dictionary containing its content and metadata. Returns: AsyncIterator [ dict ] \u2013 AsyncIterator[dict]: An async iterator of page dictionaries containing AsyncIterator [ dict ] \u2013 content and metadata such as text, speaker data, and last update information Source code in src/extraction/datasources/bundestag/reader.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 async def read_all_async ( self , ) -> AsyncIterator [ dict ]: \"\"\"Asynchronously fetch all speeches from BundestagMine. Yields each speech as a dictionary containing its content and metadata. Returns: AsyncIterator[dict]: An async iterator of page dictionaries containing content and metadata such as text, speaker data, and last update information \"\"\" self . logger . info ( f \"Reading speeches from BundestagMine with limit { self . export_limit } \" ) speech_iterator = self . client . fetch_all_speeches () yield_counter = 0 for speech in speech_iterator : if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Bundestag speech { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield speech BundestagMineDatasourceReaderFactory Bases: Factory Factory for creating BundestagMine reader instances. Creates and configures BundestagMineDatasourceReader objects with appropriate clients based on the provided configuration. Source code in src/extraction/datasources/bundestag/reader.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class BundestagMineDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating BundestagMine reader instances. Creates and configures BundestagMineDatasourceReader objects with appropriate clients based on the provided configuration. \"\"\" _configuration_class = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineDatasourceReader : \"\"\"Creates a configured BundestagMine reader instance. Initializes the BundestagMine client and reader with the given configuration settings for credentials, URL, and export limits. Args: configuration: BundestagMine connection and access settings Returns: BundestagMineDatasourceReader: Fully configured reader instance \"\"\" client = BundestagMineClientFactory . create ( configuration ) return BundestagMineDatasourceReader ( configuration = configuration , client = client , )","title":"Bundestag"},{"location":"src/extraction/datasources/bundestag/#bundestag-datasource","text":"This module contains functionality related to the Bundestag datasource.","title":"Bundestag Datasource"},{"location":"src/extraction/datasources/bundestag/#client","text":"","title":"Client"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient","text":"Bases: APIClient API Client for the bundestag-mine.de API. Source code in src/extraction/datasources/bundestag/client.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 class BundestagMineClient ( APIClient ): \"\"\" API Client for the bundestag-mine.de API. \"\"\" BASE_URL = \"https://bundestag-mine.de/api/DashboardController\" logger = LoggerConfiguration . get_logger ( __name__ ) def safe_get ( self , path : str ) -> Optional [ Any ]: \"\"\" Perform a GET request, raise for HTTP errors, parse JSON, check API status. Args: path: endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/<protocol_id>\" Returns: Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError: if HTTP status is not OK or unexpected JSON structure. \"\"\" url = f \" { self . BASE_URL } / { path . lstrip ( '/' ) } \" try : resp = self . get ( url ) except Exception as e : self . logger . warning ( f \"Failed to fetch speeches for { url } : { e } \" ) return None try : resp . raise_for_status () except Exception as e : self . logger . error ( f \"HTTP error for { url } : { e } \" ) return None data = resp . json () if not isinstance ( data , dict ) or data . get ( \"status\" ) != \"200\" : self . logger . error ( f \"Unexpected response for { url } : { data } \" ) return None result = data . get ( \"result\" ) if result is None : self . logger . debug ( f \"No result found for { url } \" ) return None return result def get_protocols ( self ) -> Iterator [ Protocol ]: \"\"\" Fetches the list of all protocols. Returns: Iterator[Protocol]: An iterator of valid protocols as Pydantic models. \"\"\" result = self . safe_get ( \"GetProtocols\" ) if not isinstance ( result , list ): raise ResponseParseError ( f \"Expected list of protocols, got: { result } \" ) for protocol_data in result : try : yield Protocol . model_validate ( protocol_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate protocol: { protocol_data } . Error: { e } \" ) def get_agenda_items ( self , protocol_id : str ) -> Iterator [ AgendaItem ]: \"\"\" Fetches agenda items for a specific protocol ID. Args: protocol_id (str): The ID of the protocol. Returns: Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. \"\"\" result = self . safe_get ( f \"GetAgendaItemsOfProtocol/ { protocol_id } \" ) if result is None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return items = result . get ( \"agendaItems\" ) if \"items\" == None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return if not isinstance ( items , list ): self . logger . error ( f \"Expected list of agendaItems for { protocol_id } , got: { items } \" ) return for item_data in items : try : yield AgendaItem . model_validate ( item_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate agenda item: { item_data } . Error: { e } \" ) def get_speaker_data ( self , speaker_id : str ) -> Optional [ Speaker ]: \"\"\" Fetches speaker data for a specific speaker ID. Args: speaker_id (str): The ID of the speaker. Returns: Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. \"\"\" result = self . safe_get ( f \"GetSpeakerById/ { speaker_id } \" ) if not isinstance ( result , dict ): self . logger . error ( f \"Expected speaker data for { speaker_id } , got: { result } \" ) return None try : return Speaker . model_validate ( result ) except ValidationError as e : self . logger . warning ( f \"Failed to validate speaker data for { speaker_id } : { result } . Error: { e } \" ) return None @retry_request def get_speeches ( self , protocol : Protocol , agenda_item : AgendaItem , ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches speeches for a specific agenda item within a protocol. Args: legislature_period (int): The legislature period. protocol_number (int): The protocol number. agenda_item_number (str): The agenda item number. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" raw = f \" { protocol . legislaturePeriod } , { protocol . number } , { agenda_item . agendaItemNumber } \" encoded = quote ( raw , safe = \"\" ) result = self . safe_get ( f \"GetSpeechesOfAgendaItem/ { encoded } \" ) if result is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return speeches = result . get ( \"speeches\" ) if speeches is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return if not isinstance ( speeches , list ): self . logger . warning ( f \"Expected list of speeches for { raw } , got: { speeches } \" ) return for speech in speeches : try : speech = BundestagSpeech . model_validate ( speech ) speech . protocol = protocol speech . agendaItem = agenda_item yield speech except ValidationError as e : self . logger . warning ( f \"Failed to validate speech: { speech } . Error: { e } \" ) def fetch_all_speeches ( self ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" for protocol in self . get_protocols (): self . logger . info ( f \"Processing protocol { protocol . id } \" ) for agenda_item in self . get_agenda_items ( protocol . id ): for speech in self . get_speeches ( protocol = protocol , agenda_item = agenda_item , ): speaker = self . get_speaker_data ( speech . speakerId ) if speaker : speech . speaker = speaker yield speech","title":"BundestagMineClient"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.fetch_all_speeches","text":"Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator [ BundestagSpeech ] \u2013 Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def fetch_all_speeches ( self ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches all speeches by iterating through protocols and their agenda items. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" for protocol in self . get_protocols (): self . logger . info ( f \"Processing protocol { protocol . id } \" ) for agenda_item in self . get_agenda_items ( protocol . id ): for speech in self . get_speeches ( protocol = protocol , agenda_item = agenda_item , ): speaker = self . get_speaker_data ( speech . speakerId ) if speaker : speech . speaker = speaker yield speech","title":"fetch_all_speeches"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.get_agenda_items","text":"Fetches agenda items for a specific protocol ID. Parameters: protocol_id ( str ) \u2013 The ID of the protocol. Returns: Iterator [ AgendaItem ] \u2013 Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def get_agenda_items ( self , protocol_id : str ) -> Iterator [ AgendaItem ]: \"\"\" Fetches agenda items for a specific protocol ID. Args: protocol_id (str): The ID of the protocol. Returns: Iterator[AgendaItem]: An iterator of valid agenda items as Pydantic models. \"\"\" result = self . safe_get ( f \"GetAgendaItemsOfProtocol/ { protocol_id } \" ) if result is None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return items = result . get ( \"agendaItems\" ) if \"items\" == None : self . logger . debug ( f \"No agenda items found for { protocol_id } \" ) return if not isinstance ( items , list ): self . logger . error ( f \"Expected list of agendaItems for { protocol_id } , got: { items } \" ) return for item_data in items : try : yield AgendaItem . model_validate ( item_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate agenda item: { item_data } . Error: { e } \" )","title":"get_agenda_items"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.get_protocols","text":"Fetches the list of all protocols. Returns: Iterator [ Protocol ] \u2013 Iterator[Protocol]: An iterator of valid protocols as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def get_protocols ( self ) -> Iterator [ Protocol ]: \"\"\" Fetches the list of all protocols. Returns: Iterator[Protocol]: An iterator of valid protocols as Pydantic models. \"\"\" result = self . safe_get ( \"GetProtocols\" ) if not isinstance ( result , list ): raise ResponseParseError ( f \"Expected list of protocols, got: { result } \" ) for protocol_data in result : try : yield Protocol . model_validate ( protocol_data ) except ValidationError as e : self . logger . warning ( f \"Failed to validate protocol: { protocol_data } . Error: { e } \" )","title":"get_protocols"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.get_speaker_data","text":"Fetches speaker data for a specific speaker ID. Parameters: speaker_id ( str ) \u2013 The ID of the speaker. Returns: Optional [ Speaker ] \u2013 Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. Source code in src/extraction/datasources/bundestag/client.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def get_speaker_data ( self , speaker_id : str ) -> Optional [ Speaker ]: \"\"\" Fetches speaker data for a specific speaker ID. Args: speaker_id (str): The ID of the speaker. Returns: Optional[Speaker]: Speaker data as a Pydantic model, or None if validation fails. \"\"\" result = self . safe_get ( f \"GetSpeakerById/ { speaker_id } \" ) if not isinstance ( result , dict ): self . logger . error ( f \"Expected speaker data for { speaker_id } , got: { result } \" ) return None try : return Speaker . model_validate ( result ) except ValidationError as e : self . logger . warning ( f \"Failed to validate speaker data for { speaker_id } : { result } . Error: { e } \" ) return None","title":"get_speaker_data"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.get_speeches","text":"Fetches speeches for a specific agenda item within a protocol. Parameters: legislature_period ( int ) \u2013 The legislature period. protocol_number ( int ) \u2013 The protocol number. agenda_item_number ( str ) \u2013 The agenda item number. Returns: Iterator [ BundestagSpeech ] \u2013 Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. Source code in src/extraction/datasources/bundestag/client.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @retry_request def get_speeches ( self , protocol : Protocol , agenda_item : AgendaItem , ) -> Iterator [ BundestagSpeech ]: \"\"\" Fetches speeches for a specific agenda item within a protocol. Args: legislature_period (int): The legislature period. protocol_number (int): The protocol number. agenda_item_number (str): The agenda item number. Returns: Iterator[BundestagSpeech]: An iterator of valid speeches as Pydantic models. \"\"\" raw = f \" { protocol . legislaturePeriod } , { protocol . number } , { agenda_item . agendaItemNumber } \" encoded = quote ( raw , safe = \"\" ) result = self . safe_get ( f \"GetSpeechesOfAgendaItem/ { encoded } \" ) if result is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return speeches = result . get ( \"speeches\" ) if speeches is None : self . logger . debug ( f \"No speeches found for { raw } \" ) return if not isinstance ( speeches , list ): self . logger . warning ( f \"Expected list of speeches for { raw } , got: { speeches } \" ) return for speech in speeches : try : speech = BundestagSpeech . model_validate ( speech ) speech . protocol = protocol speech . agendaItem = agenda_item yield speech except ValidationError as e : self . logger . warning ( f \"Failed to validate speech: { speech } . Error: { e } \" )","title":"get_speeches"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClient.safe_get","text":"Perform a GET request, raise for HTTP errors, parse JSON, check API status. Parameters: path ( str ) \u2013 endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/ \" Returns: Optional [ Any ] \u2013 Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError \u2013 if HTTP status is not OK or unexpected JSON structure. Source code in src/extraction/datasources/bundestag/client.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def safe_get ( self , path : str ) -> Optional [ Any ]: \"\"\" Perform a GET request, raise for HTTP errors, parse JSON, check API status. Args: path: endpoint path under BASE_URL, e.g. \"GetProtocols\" or \"GetAgendaItemsOfProtocol/<protocol_id>\" Returns: Dict[str, Any]: The 'result' field of the API response as a dict. Raises: ResponseParseError: if HTTP status is not OK or unexpected JSON structure. \"\"\" url = f \" { self . BASE_URL } / { path . lstrip ( '/' ) } \" try : resp = self . get ( url ) except Exception as e : self . logger . warning ( f \"Failed to fetch speeches for { url } : { e } \" ) return None try : resp . raise_for_status () except Exception as e : self . logger . error ( f \"HTTP error for { url } : { e } \" ) return None data = resp . json () if not isinstance ( data , dict ) or data . get ( \"status\" ) != \"200\" : self . logger . error ( f \"Unexpected response for { url } : { data } \" ) return None result = data . get ( \"result\" ) if result is None : self . logger . debug ( f \"No result found for { url } \" ) return None return result","title":"safe_get"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagMineClientFactory","text":"Bases: SingletonFactory Factory for creating and managing Bundestag client instances. This factory ensures only one Bundestag client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. Source code in src/extraction/datasources/bundestag/client.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 class BundestagMineClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Bundestag client instances. This factory ensures only one Bundestag client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineClient : \"\"\" Creates a new BundestagMine client instance using the provided configuration. Args: configuration: Configuration object containing BundestagMine details Returns: A configured BundestagMine client instance ready for API interactions. \"\"\" return BundestagMineClient ()","title":"BundestagMineClientFactory"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.client.BundestagSpeech","text":"Bases: BaseModel Represents a speech from BundestagMine API. Source code in src/extraction/datasources/bundestag/client.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class BundestagSpeech ( BaseModel ): \"\"\"Represents a speech from BundestagMine API.\"\"\" id : str speakerId : str text : str speaker : Optional [ Speaker ] = None protocol : Optional [ Protocol ] = None agendaItem : Optional [ AgendaItem ] = None @model_validator ( mode = \"after\" ) def validate_text_not_empty ( self ) -> \"BundestagSpeech\" : if not self . text or self . text . strip () == \"\" : raise ValueError ( \"BundestagSpeech text cannot be empty\" ) return self","title":"BundestagSpeech"},{"location":"src/extraction/datasources/bundestag/#configuration","text":"","title":"Configuration"},{"location":"src/extraction/datasources/bundestag/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.document.BundestagMineDocument","text":"Bases: BaseDocument Represents a document from the Bundestag datasource. Inherits from BaseDocument and includes additional metadata specific to Bundestag documents. Source code in src/extraction/datasources/bundestag/document.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class BundestagMineDocument ( BaseDocument ): \"\"\" Represents a document from the Bundestag datasource. Inherits from BaseDocument and includes additional metadata specific to Bundestag documents. \"\"\" included_embed_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , \"speaker_party\" , \"speaker\" , \"protocol_number\" , \"legislature_period\" , ] included_llm_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , \"speaker_party\" , \"speaker\" , \"protocol_number\" , \"legislature_period\" , ]","title":"BundestagMineDocument"},{"location":"src/extraction/datasources/bundestag/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.manager.BundestagMineDatasourceManagerFactory","text":"Bases: Factory Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object Source code in src/extraction/datasources/bundestag/manager.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class BundestagMineDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class: Type of configuration object \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create an instance of the BundestagMine datasource manager. This method constructs a BasicDatasourceManager by creating the appropriate reader and parser based on the provided configuration. Args: configuration: Configuration specifying how to set up the BundestagMine datasource manager, reader, and parser. Returns: A configured BasicDatasourceManager instance for handling BundestagMine documents. \"\"\" reader = BundestagMineDatasourceReaderFactory . create ( configuration ) parser = BundestagMineDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration = configuration , reader = reader , parser = parser , )","title":"BundestagMineDatasourceManagerFactory"},{"location":"src/extraction/datasources/bundestag/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.parser.BundestagMineDatasourceParser","text":"Bases: BaseParser [ BundestagMineDocument ] Source code in src/extraction/datasources/bundestag/parser.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class BundestagMineDatasourceParser ( BaseParser [ BundestagMineDocument ]): logger = LoggerConfiguration . get_logger ( __name__ ) def parse ( self , speech : BundestagSpeech ) -> BundestagMineDocument : \"\"\" Parse content into a BundestagMineDocument object. Args: content: Raw response dict to be parsed Returns: Parsed document of type BundestagMineDocument \"\"\" metadata = self . _extract_metadata ( speech ) return BundestagMineDocument ( text = speech . text , metadata = metadata ) def _extract_metadata ( self , speech : BundestagSpeech ) -> dict : \"\"\" Extract metadata from the response. Args: response: Raw response string Returns: Dictionary containing extracted metadata \"\"\" legislature_period = speech . protocol . legislaturePeriod protocol_number = speech . protocol . number agenda_item_number = speech . agendaItem . agendaItemNumber url = f \"https://dserver.bundestag.de/btp/ { legislature_period } / { legislature_period }{ protocol_number } .pdf\" title = f \"Protocol/Legislature/AgendaItem { protocol_number } / { legislature_period } / { agenda_item_number } \" speaker_name = f \" { speech . speaker . firstName } { speech . speaker . lastName } \" return { \"datasource\" : \"bundestag\" , \"language\" : \"de\" , \"url\" : url , \"title\" : title , \"format\" : \"md\" , \"created_time\" : speech . protocol . date , \"last_edited_time\" : speech . protocol . date , \"speaker_party\" : speech . speaker . party , \"speaker\" : speaker_name , \"agenda_item_number\" : agenda_item_number , \"protocol_number\" : protocol_number , \"legislature_period\" : legislature_period , }","title":"BundestagMineDatasourceParser"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.parser.BundestagMineDatasourceParser.parse","text":"Parse content into a BundestagMineDocument object. Parameters: content \u2013 Raw response dict to be parsed Returns: BundestagMineDocument \u2013 Parsed document of type BundestagMineDocument Source code in src/extraction/datasources/bundestag/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 def parse ( self , speech : BundestagSpeech ) -> BundestagMineDocument : \"\"\" Parse content into a BundestagMineDocument object. Args: content: Raw response dict to be parsed Returns: Parsed document of type BundestagMineDocument \"\"\" metadata = self . _extract_metadata ( speech ) return BundestagMineDocument ( text = speech . text , metadata = metadata )","title":"parse"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.parser.BundestagMineDatasourceParserFactory","text":"Bases: Factory Factory for creating instances of BundestagMineDatasourceParser. Creates and configures BundestagMineDatasourceParser objects according to the provided configuration. Source code in src/extraction/datasources/bundestag/parser.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class BundestagMineDatasourceParserFactory ( Factory ): \"\"\" Factory for creating instances of BundestagMineDatasourceParser. Creates and configures BundestagMineDatasourceParser objects according to the provided configuration. \"\"\" _configuration_class : Type = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineDatasourceParser : \"\"\" Create an instance of BundestagMineDatasourceParser. Args: configuration: Configuration for the parser (not used in this implementation) Returns: An instance of BundestagMineDatasourceParser \"\"\" return BundestagMineDatasourceParser ()","title":"BundestagMineDatasourceParserFactory"},{"location":"src/extraction/datasources/bundestag/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.reader.BundestagMineDatasourceReader","text":"Bases: BaseReader Reader for extracting speeches from the BundestagMine API. Implements document extraction from the Bundestag speeches. Source code in src/extraction/datasources/bundestag/reader.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class BundestagMineDatasourceReader ( BaseReader ): \"\"\"Reader for extracting speeches from the BundestagMine API. Implements document extraction from the Bundestag speeches. \"\"\" def __init__ ( self , configuration : BundestagMineDatasourceConfiguration , client : BundestagMineClient , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the BundestagMine reader. Args: configuration: Settings for BundestagMine access and export limits client: Client for BundestagMine API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger async def read_all_async ( self , ) -> AsyncIterator [ dict ]: \"\"\"Asynchronously fetch all speeches from BundestagMine. Yields each speech as a dictionary containing its content and metadata. Returns: AsyncIterator[dict]: An async iterator of page dictionaries containing content and metadata such as text, speaker data, and last update information \"\"\" self . logger . info ( f \"Reading speeches from BundestagMine with limit { self . export_limit } \" ) speech_iterator = self . client . fetch_all_speeches () yield_counter = 0 for speech in speech_iterator : if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Bundestag speech { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield speech","title":"BundestagMineDatasourceReader"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.reader.BundestagMineDatasourceReader.__init__","text":"Initialize the BundestagMine reader. Parameters: configuration ( BundestagMineDatasourceConfiguration ) \u2013 Settings for BundestagMine access and export limits client ( BundestagMineClient ) \u2013 Client for BundestagMine API interactions logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operation information Source code in src/extraction/datasources/bundestag/reader.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , configuration : BundestagMineDatasourceConfiguration , client : BundestagMineClient , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the BundestagMine reader. Args: configuration: Settings for BundestagMine access and export limits client: Client for BundestagMine API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger","title":"__init__"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.reader.BundestagMineDatasourceReader.read_all_async","text":"Asynchronously fetch all speeches from BundestagMine. Yields each speech as a dictionary containing its content and metadata. Returns: AsyncIterator [ dict ] \u2013 AsyncIterator[dict]: An async iterator of page dictionaries containing AsyncIterator [ dict ] \u2013 content and metadata such as text, speaker data, and last update information Source code in src/extraction/datasources/bundestag/reader.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 async def read_all_async ( self , ) -> AsyncIterator [ dict ]: \"\"\"Asynchronously fetch all speeches from BundestagMine. Yields each speech as a dictionary containing its content and metadata. Returns: AsyncIterator[dict]: An async iterator of page dictionaries containing content and metadata such as text, speaker data, and last update information \"\"\" self . logger . info ( f \"Reading speeches from BundestagMine with limit { self . export_limit } \" ) speech_iterator = self . client . fetch_all_speeches () yield_counter = 0 for speech in speech_iterator : if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Bundestag speech { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield speech","title":"read_all_async"},{"location":"src/extraction/datasources/bundestag/#src.extraction.datasources.bundestag.reader.BundestagMineDatasourceReaderFactory","text":"Bases: Factory Factory for creating BundestagMine reader instances. Creates and configures BundestagMineDatasourceReader objects with appropriate clients based on the provided configuration. Source code in src/extraction/datasources/bundestag/reader.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 class BundestagMineDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating BundestagMine reader instances. Creates and configures BundestagMineDatasourceReader objects with appropriate clients based on the provided configuration. \"\"\" _configuration_class = BundestagMineDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : BundestagMineDatasourceConfiguration ) -> BundestagMineDatasourceReader : \"\"\"Creates a configured BundestagMine reader instance. Initializes the BundestagMine client and reader with the given configuration settings for credentials, URL, and export limits. Args: configuration: BundestagMine connection and access settings Returns: BundestagMineDatasourceReader: Fully configured reader instance \"\"\" client = BundestagMineClientFactory . create ( configuration ) return BundestagMineDatasourceReader ( configuration = configuration , client = client , )","title":"BundestagMineDatasourceReaderFactory"},{"location":"src/extraction/datasources/confluence/","text":"Confluence Datasource This module contains functionality related to the Confluence datasource. Client ConfluenceClientFactory Bases: SingletonFactory Factory for creating and managing Confluence client instances. This factory ensures only one Confluence client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. Source code in src/extraction/datasources/confluence/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class ConfluenceClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Confluence client instances. This factory ensures only one Confluence client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. \"\"\" _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> Confluence : \"\"\" Creates a new Confluence client instance using the provided configuration. Args: configuration: Configuration object containing Confluence connection details including base URL, username, and password. Returns: A configured Confluence client instance ready for API interactions. \"\"\" return Confluence ( url = configuration . base_url , username = configuration . secrets . username . get_secret_value (), password = configuration . secrets . password . get_secret_value (), ) Configuration ConfluenceDatasourceConfiguration Bases: DatasourceConfiguration Source code in src/extraction/datasources/confluence/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ConfluenceDatasourceConfiguration ( DatasourceConfiguration ): class Secrets ( BaseSecrets ): model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__DATASOURCES__CONFLUENCE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username credential used to authenticate with the Confluence instance\" , ) password : SecretStr = Field ( ... , description = \"Password credential used to authenticate with the Confluence instance\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Confluence server instance\" , ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"Communication protocol used to connect to the Confluence server\" , ) name : Literal [ DatasourceName . CONFLUENCE ] = Field ( ... , description = \"Identifier specifying this configuration is for a Confluence datasource\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials required to access the Confluence instance\" , ) @property def base_url ( self ) -> str : \"\"\" Constructs the complete base URL for the Confluence API from the protocol and host. Returns: str: The fully formed base URL to the Confluence instance \"\"\" return f \" { self . protocol } :// { self . host } \" base_url property Constructs the complete base URL for the Confluence API from the protocol and host. Returns: str ( str ) \u2013 The fully formed base URL to the Confluence instance Document ConfluenceDocument Bases: BaseDocument Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. Source code in src/extraction/datasources/confluence/document.py 4 5 6 7 8 9 10 11 class ConfluenceDocument ( BaseDocument ): \"\"\"Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. \"\"\" pass Manager ConfluenceDatasourceManagerFactory Bases: Factory Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class ( Type ) \u2013 Configuration class used for validating and processing Confluence-specific settings. Source code in src/extraction/datasources/confluence/manager.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class ConfluenceDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class: Configuration class used for validating and processing Confluence-specific settings. \"\"\" _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create a configured Confluence datasource manager. Sets up the necessary reader and parser components based on the provided configuration and assembles them into a functional manager. Args: configuration: Configuration object containing Confluence-specific parameters including authentication details, spaces to extract, and other extraction options. Returns: A fully initialized datasource manager that can extract and process data from Confluence. \"\"\" reader = ConfluenceDatasourceReaderFactory . create ( configuration ) parser = ConfluenceDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration , reader , parser ) Parser ConfluenceDatasourceParser Bases: BaseParser [ ConfluenceDocument ] Source code in src/extraction/datasources/confluence/parser.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class ConfluenceDatasourceParser ( BaseParser [ ConfluenceDocument ]): def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , parser : MarkItDown = MarkItDown (), ): \"\"\"Initialize the Confluence parser with the provided configuration. Args: configuration: Configuration object containing Confluence connection details parser: MarkItDown instance for converting HTML to markdown \"\"\" self . configuration = configuration self . parser = parser def parse ( self , page : ConfluencePage ) -> ConfluenceDocument : \"\"\"Parse a Confluence page into a document. Args: page: Confluence page details Returns: ConfluenceDocument: Parsed document with extracted text and metadata \"\"\" markdown = self . _get_page_markdown ( page ) metadata = self . _extract_metadata ( page , self . configuration . base_url ) return ConfluenceDocument ( text = markdown , metadata = metadata ) def _get_page_markdown ( self , page : ConfluencePage ) -> str : \"\"\"Extract markdown content from a Confluence page. Because of MarkItDown, we need to write the HTML content to a temporary file and then convert it to markdown. Args: page: Confluence page details Returns: str: Markdown content of the page \"\"\" html_content = page . body . view . value if not html_content : return \"\" with tempfile . NamedTemporaryFile ( mode = \"w\" , suffix = \".html\" ) as temp_file : temp_file . write ( html_content ) temp_file . flush () return self . parser . convert ( temp_file . name , file_extension = \".html\" ) . text_content @staticmethod def _extract_metadata ( page : ConfluencePage , base_url : str ) -> dict : \"\"\"Extract and format page metadata. Args: page: Confluence page details base_url: Base URL of the Confluence instance Returns: dict: Structured metadata including dates, IDs, and URLs \"\"\" return { \"created_time\" : page . history . createdDate , \"created_date\" : page . history . createdDate . split ( \"T\" )[ 0 ], \"datasource\" : \"confluence\" , \"format\" : \"md\" , \"last_edited_date\" : page . history . lastUpdated . when , \"last_edited_time\" : page . history . lastUpdated . when . split ( \"T\" )[ 0 ], \"page_id\" : page . id , \"space\" : page . expandable [ \"space\" ] . split ( \"/\" )[ - 1 ], \"title\" : page . title , \"type\" : \"page\" , \"url\" : base_url + page . links . webui , } __init__ ( configuration , parser = MarkItDown ()) Initialize the Confluence parser with the provided configuration. Parameters: configuration ( ConfluenceDatasourceConfiguration ) \u2013 Configuration object containing Confluence connection details parser ( MarkItDown , default: MarkItDown () ) \u2013 MarkItDown instance for converting HTML to markdown Source code in src/extraction/datasources/confluence/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , parser : MarkItDown = MarkItDown (), ): \"\"\"Initialize the Confluence parser with the provided configuration. Args: configuration: Configuration object containing Confluence connection details parser: MarkItDown instance for converting HTML to markdown \"\"\" self . configuration = configuration self . parser = parser parse ( page ) Parse a Confluence page into a document. Parameters: page ( ConfluencePage ) \u2013 Confluence page details Returns: ConfluenceDocument ( ConfluenceDocument ) \u2013 Parsed document with extracted text and metadata Source code in src/extraction/datasources/confluence/parser.py 31 32 33 34 35 36 37 38 39 40 41 42 def parse ( self , page : ConfluencePage ) -> ConfluenceDocument : \"\"\"Parse a Confluence page into a document. Args: page: Confluence page details Returns: ConfluenceDocument: Parsed document with extracted text and metadata \"\"\" markdown = self . _get_page_markdown ( page ) metadata = self . _extract_metadata ( page , self . configuration . base_url ) return ConfluenceDocument ( text = markdown , metadata = metadata ) ConfluenceDatasourceParserFactory Bases: Factory Source code in src/extraction/datasources/confluence/parser.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class ConfluenceDatasourceParserFactory ( Factory ): _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceParser : \"\"\"Creates a Confluence parser instance. Args: configuration: Configuration object containing Confluence connection details Returns: ConfluenceDatasourceParser: Configured Confluence parser instance \"\"\" return ConfluenceDatasourceParser ( configuration ) Reader ConfluenceDatasourceReader Bases: BaseReader Reader for extracting documents from Confluence spaces. Implements document extraction from Confluence spaces, handling pagination and export limits. Source code in src/extraction/datasources/confluence/reader.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class ConfluenceDatasourceReader ( BaseReader ): \"\"\"Reader for extracting documents from Confluence spaces. Implements document extraction from Confluence spaces, handling pagination and export limits. \"\"\" def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , client : Confluence , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Confluence reader. Args: configuration: Settings for Confluence access and export limits client: Client for Confluence API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger async def read_all_async ( self , ) -> AsyncIterator [ ConfluencePage ]: \"\"\"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. \"\"\" self . logger . info ( f \"Reading pages from Confluence with limit { self . export_limit } \" ) response = self . client . get_all_spaces ( space_type = \"global\" ) spaces = [ Space . model_validate ( space ) for space in response [ \"results\" ]] yield_counter = 0 for space in spaces : for page in self . _get_all_pages ( space . key ): if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Confluence page { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield page def _get_all_pages ( self , space : str ) -> Iterator [ ConfluencePage ]: \"\"\"Fetch all pages from a specific Confluence space. Handles pagination internally to retrieve all pages from the specified space, up to the optional limit. Pages include body content and update history. Args: space: Space key to fetch pages from limit: Maximum number of pages to fetch (None for unlimited) Returns: Iterator[ConfluencePage]: Iterator of Confluence pages with content and metadata \"\"\" start = 0 params = { \"space\" : space , \"start\" : start , \"status\" : None , \"expand\" : \"body.view,history.lastUpdated\" , } try : while True : pages_raw = self . client . get_all_pages_from_space ( ** params ) pages = [ ConfluencePage . model_validate ( page ) for page in pages_raw ] if not pages : return for page in pages : yield page start += len ( pages ) params [ \"start\" ] = start except HTTPError as e : self . logger . warning ( f \"Error while fetching Confluence pages from { space } : { e } \" ) __init__ ( configuration , client , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize the Confluence reader. Parameters: configuration ( ConfluenceDatasourceConfiguration ) \u2013 Settings for Confluence access and export limits client ( Confluence ) \u2013 Client for Confluence API interactions logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operation information Source code in src/extraction/datasources/confluence/reader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , client : Confluence , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Confluence reader. Args: configuration: Settings for Confluence access and export limits client: Client for Confluence API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger read_all_async () async Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator [ ConfluencePage ] \u2013 AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. Source code in src/extraction/datasources/confluence/reader.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 async def read_all_async ( self , ) -> AsyncIterator [ ConfluencePage ]: \"\"\"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. \"\"\" self . logger . info ( f \"Reading pages from Confluence with limit { self . export_limit } \" ) response = self . client . get_all_spaces ( space_type = \"global\" ) spaces = [ Space . model_validate ( space ) for space in response [ \"results\" ]] yield_counter = 0 for space in spaces : for page in self . _get_all_pages ( space . key ): if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Confluence page { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield page ConfluenceDatasourceReaderFactory Bases: Factory Factory for creating Confluence reader instances. Creates and configures ConfluenceDatasourceReader objects with appropriate clients based on the provided configuration. Source code in src/extraction/datasources/confluence/reader.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 class ConfluenceDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating Confluence reader instances. Creates and configures ConfluenceDatasourceReader objects with appropriate clients based on the provided configuration. \"\"\" _configuration_class = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceReader : \"\"\"Creates a configured Confluence reader instance. Initializes the Confluence client and reader with the given configuration settings for credentials, URL, and export limits. Args: configuration: Confluence connection and access settings Returns: ConfluenceDatasourceReader: Fully configured reader instance \"\"\" client = ConfluenceClientFactory . create ( configuration ) return ConfluenceDatasourceReader ( configuration = configuration , client = client , ) ConfluencePage Bases: BaseModel Model representing a Confluence page from the API. Source code in src/extraction/datasources/confluence/reader.py 42 43 44 45 46 47 48 49 50 class ConfluencePage ( BaseModel ): \"\"\"Model representing a Confluence page from the API.\"\"\" id : str title : str body : Body history : History links : Links = Field ( alias = \"_links\" ) expandable : Dict [ str , str ] = Field ( alias = \"_expandable\" )","title":"Confluence"},{"location":"src/extraction/datasources/confluence/#confluence-datasource","text":"This module contains functionality related to the Confluence datasource.","title":"Confluence Datasource"},{"location":"src/extraction/datasources/confluence/#client","text":"","title":"Client"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.client.ConfluenceClientFactory","text":"Bases: SingletonFactory Factory for creating and managing Confluence client instances. This factory ensures only one Confluence client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. Source code in src/extraction/datasources/confluence/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class ConfluenceClientFactory ( SingletonFactory ): \"\"\" Factory for creating and managing Confluence client instances. This factory ensures only one Confluence client is created per configuration, following the singleton pattern provided by the parent SingletonFactory class. \"\"\" _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> Confluence : \"\"\" Creates a new Confluence client instance using the provided configuration. Args: configuration: Configuration object containing Confluence connection details including base URL, username, and password. Returns: A configured Confluence client instance ready for API interactions. \"\"\" return Confluence ( url = configuration . base_url , username = configuration . secrets . username . get_secret_value (), password = configuration . secrets . password . get_secret_value (), )","title":"ConfluenceClientFactory"},{"location":"src/extraction/datasources/confluence/#configuration","text":"","title":"Configuration"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.configuration.ConfluenceDatasourceConfiguration","text":"Bases: DatasourceConfiguration Source code in src/extraction/datasources/confluence/configuration.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class ConfluenceDatasourceConfiguration ( DatasourceConfiguration ): class Secrets ( BaseSecrets ): model_config = ConfigDict ( env_file_encoding = \"utf-8\" , env_prefix = \"RAG__DATASOURCES__CONFLUENCE__\" , env_nested_delimiter = \"__\" , extra = \"ignore\" , ) username : SecretStr = Field ( ... , description = \"Username credential used to authenticate with the Confluence instance\" , ) password : SecretStr = Field ( ... , description = \"Password credential used to authenticate with the Confluence instance\" , ) host : str = Field ( \"127.0.0.1\" , description = \"Hostname or IP address of the Confluence server instance\" , ) protocol : Union [ Literal [ \"http\" ], Literal [ \"https\" ]] = Field ( \"http\" , description = \"Communication protocol used to connect to the Confluence server\" , ) name : Literal [ DatasourceName . CONFLUENCE ] = Field ( ... , description = \"Identifier specifying this configuration is for a Confluence datasource\" , ) secrets : Secrets = Field ( None , description = \"Authentication credentials required to access the Confluence instance\" , ) @property def base_url ( self ) -> str : \"\"\" Constructs the complete base URL for the Confluence API from the protocol and host. Returns: str: The fully formed base URL to the Confluence instance \"\"\" return f \" { self . protocol } :// { self . host } \"","title":"ConfluenceDatasourceConfiguration"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.configuration.ConfluenceDatasourceConfiguration.base_url","text":"Constructs the complete base URL for the Confluence API from the protocol and host. Returns: str ( str ) \u2013 The fully formed base URL to the Confluence instance","title":"base_url"},{"location":"src/extraction/datasources/confluence/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.document.ConfluenceDocument","text":"Bases: BaseDocument Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. Source code in src/extraction/datasources/confluence/document.py 4 5 6 7 8 9 10 11 class ConfluenceDocument ( BaseDocument ): \"\"\"Document representation for Confluence page content. Extends BaseDocument to handle Confluence-specific document processing including content extraction, metadata handling, and exclusion configuration. \"\"\" pass","title":"ConfluenceDocument"},{"location":"src/extraction/datasources/confluence/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.manager.ConfluenceDatasourceManagerFactory","text":"Bases: Factory Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class ( Type ) \u2013 Configuration class used for validating and processing Confluence-specific settings. Source code in src/extraction/datasources/confluence/manager.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class ConfluenceDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating Confluence datasource managers. This factory generates managers that handle the extraction of content from Confluence instances. It ensures proper configuration, reading, and parsing of Confluence content. Attributes: _configuration_class: Configuration class used for validating and processing Confluence-specific settings. \"\"\" _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create a configured Confluence datasource manager. Sets up the necessary reader and parser components based on the provided configuration and assembles them into a functional manager. Args: configuration: Configuration object containing Confluence-specific parameters including authentication details, spaces to extract, and other extraction options. Returns: A fully initialized datasource manager that can extract and process data from Confluence. \"\"\" reader = ConfluenceDatasourceReaderFactory . create ( configuration ) parser = ConfluenceDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration , reader , parser )","title":"ConfluenceDatasourceManagerFactory"},{"location":"src/extraction/datasources/confluence/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.parser.ConfluenceDatasourceParser","text":"Bases: BaseParser [ ConfluenceDocument ] Source code in src/extraction/datasources/confluence/parser.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class ConfluenceDatasourceParser ( BaseParser [ ConfluenceDocument ]): def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , parser : MarkItDown = MarkItDown (), ): \"\"\"Initialize the Confluence parser with the provided configuration. Args: configuration: Configuration object containing Confluence connection details parser: MarkItDown instance for converting HTML to markdown \"\"\" self . configuration = configuration self . parser = parser def parse ( self , page : ConfluencePage ) -> ConfluenceDocument : \"\"\"Parse a Confluence page into a document. Args: page: Confluence page details Returns: ConfluenceDocument: Parsed document with extracted text and metadata \"\"\" markdown = self . _get_page_markdown ( page ) metadata = self . _extract_metadata ( page , self . configuration . base_url ) return ConfluenceDocument ( text = markdown , metadata = metadata ) def _get_page_markdown ( self , page : ConfluencePage ) -> str : \"\"\"Extract markdown content from a Confluence page. Because of MarkItDown, we need to write the HTML content to a temporary file and then convert it to markdown. Args: page: Confluence page details Returns: str: Markdown content of the page \"\"\" html_content = page . body . view . value if not html_content : return \"\" with tempfile . NamedTemporaryFile ( mode = \"w\" , suffix = \".html\" ) as temp_file : temp_file . write ( html_content ) temp_file . flush () return self . parser . convert ( temp_file . name , file_extension = \".html\" ) . text_content @staticmethod def _extract_metadata ( page : ConfluencePage , base_url : str ) -> dict : \"\"\"Extract and format page metadata. Args: page: Confluence page details base_url: Base URL of the Confluence instance Returns: dict: Structured metadata including dates, IDs, and URLs \"\"\" return { \"created_time\" : page . history . createdDate , \"created_date\" : page . history . createdDate . split ( \"T\" )[ 0 ], \"datasource\" : \"confluence\" , \"format\" : \"md\" , \"last_edited_date\" : page . history . lastUpdated . when , \"last_edited_time\" : page . history . lastUpdated . when . split ( \"T\" )[ 0 ], \"page_id\" : page . id , \"space\" : page . expandable [ \"space\" ] . split ( \"/\" )[ - 1 ], \"title\" : page . title , \"type\" : \"page\" , \"url\" : base_url + page . links . webui , }","title":"ConfluenceDatasourceParser"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.parser.ConfluenceDatasourceParser.__init__","text":"Initialize the Confluence parser with the provided configuration. Parameters: configuration ( ConfluenceDatasourceConfiguration ) \u2013 Configuration object containing Confluence connection details parser ( MarkItDown , default: MarkItDown () ) \u2013 MarkItDown instance for converting HTML to markdown Source code in src/extraction/datasources/confluence/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , parser : MarkItDown = MarkItDown (), ): \"\"\"Initialize the Confluence parser with the provided configuration. Args: configuration: Configuration object containing Confluence connection details parser: MarkItDown instance for converting HTML to markdown \"\"\" self . configuration = configuration self . parser = parser","title":"__init__"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.parser.ConfluenceDatasourceParser.parse","text":"Parse a Confluence page into a document. Parameters: page ( ConfluencePage ) \u2013 Confluence page details Returns: ConfluenceDocument ( ConfluenceDocument ) \u2013 Parsed document with extracted text and metadata Source code in src/extraction/datasources/confluence/parser.py 31 32 33 34 35 36 37 38 39 40 41 42 def parse ( self , page : ConfluencePage ) -> ConfluenceDocument : \"\"\"Parse a Confluence page into a document. Args: page: Confluence page details Returns: ConfluenceDocument: Parsed document with extracted text and metadata \"\"\" markdown = self . _get_page_markdown ( page ) metadata = self . _extract_metadata ( page , self . configuration . base_url ) return ConfluenceDocument ( text = markdown , metadata = metadata )","title":"parse"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.parser.ConfluenceDatasourceParserFactory","text":"Bases: Factory Source code in src/extraction/datasources/confluence/parser.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class ConfluenceDatasourceParserFactory ( Factory ): _configuration_class : Type = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceParser : \"\"\"Creates a Confluence parser instance. Args: configuration: Configuration object containing Confluence connection details Returns: ConfluenceDatasourceParser: Configured Confluence parser instance \"\"\" return ConfluenceDatasourceParser ( configuration )","title":"ConfluenceDatasourceParserFactory"},{"location":"src/extraction/datasources/confluence/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluenceDatasourceReader","text":"Bases: BaseReader Reader for extracting documents from Confluence spaces. Implements document extraction from Confluence spaces, handling pagination and export limits. Source code in src/extraction/datasources/confluence/reader.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class ConfluenceDatasourceReader ( BaseReader ): \"\"\"Reader for extracting documents from Confluence spaces. Implements document extraction from Confluence spaces, handling pagination and export limits. \"\"\" def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , client : Confluence , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Confluence reader. Args: configuration: Settings for Confluence access and export limits client: Client for Confluence API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger async def read_all_async ( self , ) -> AsyncIterator [ ConfluencePage ]: \"\"\"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. \"\"\" self . logger . info ( f \"Reading pages from Confluence with limit { self . export_limit } \" ) response = self . client . get_all_spaces ( space_type = \"global\" ) spaces = [ Space . model_validate ( space ) for space in response [ \"results\" ]] yield_counter = 0 for space in spaces : for page in self . _get_all_pages ( space . key ): if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Confluence page { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield page def _get_all_pages ( self , space : str ) -> Iterator [ ConfluencePage ]: \"\"\"Fetch all pages from a specific Confluence space. Handles pagination internally to retrieve all pages from the specified space, up to the optional limit. Pages include body content and update history. Args: space: Space key to fetch pages from limit: Maximum number of pages to fetch (None for unlimited) Returns: Iterator[ConfluencePage]: Iterator of Confluence pages with content and metadata \"\"\" start = 0 params = { \"space\" : space , \"start\" : start , \"status\" : None , \"expand\" : \"body.view,history.lastUpdated\" , } try : while True : pages_raw = self . client . get_all_pages_from_space ( ** params ) pages = [ ConfluencePage . model_validate ( page ) for page in pages_raw ] if not pages : return for page in pages : yield page start += len ( pages ) params [ \"start\" ] = start except HTTPError as e : self . logger . warning ( f \"Error while fetching Confluence pages from { space } : { e } \" )","title":"ConfluenceDatasourceReader"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluenceDatasourceReader.__init__","text":"Initialize the Confluence reader. Parameters: configuration ( ConfluenceDatasourceConfiguration ) \u2013 Settings for Confluence access and export limits client ( Confluence ) \u2013 Client for Confluence API interactions logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for recording operation information Source code in src/extraction/datasources/confluence/reader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , configuration : ConfluenceDatasourceConfiguration , client : Confluence , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize the Confluence reader. Args: configuration: Settings for Confluence access and export limits client: Client for Confluence API interactions logger: Logger instance for recording operation information \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . client = client self . logger = logger","title":"__init__"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluenceDatasourceReader.read_all_async","text":"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator [ ConfluencePage ] \u2013 AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. Source code in src/extraction/datasources/confluence/reader.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 async def read_all_async ( self , ) -> AsyncIterator [ ConfluencePage ]: \"\"\"Asynchronously fetch all documents from Confluence. Retrieves pages from all global spaces in Confluence, respecting the export limit. Yields each page as a ConfluencePage containing its content and metadata. Returns: AsyncIterator[ConfluencePage]: An async iterator of Confluence pages. \"\"\" self . logger . info ( f \"Reading pages from Confluence with limit { self . export_limit } \" ) response = self . client . get_all_spaces ( space_type = \"global\" ) spaces = [ Space . model_validate ( space ) for space in response [ \"results\" ]] yield_counter = 0 for space in spaces : for page in self . _get_all_pages ( space . key ): if self . _limit_reached ( yield_counter , self . export_limit ): return self . logger . info ( f \"Fetched Confluence page { yield_counter } / { self . export_limit } .\" ) yield_counter += 1 yield page","title":"read_all_async"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluenceDatasourceReaderFactory","text":"Bases: Factory Factory for creating Confluence reader instances. Creates and configures ConfluenceDatasourceReader objects with appropriate clients based on the provided configuration. Source code in src/extraction/datasources/confluence/reader.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 class ConfluenceDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating Confluence reader instances. Creates and configures ConfluenceDatasourceReader objects with appropriate clients based on the provided configuration. \"\"\" _configuration_class = ConfluenceDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : ConfluenceDatasourceConfiguration ) -> ConfluenceDatasourceReader : \"\"\"Creates a configured Confluence reader instance. Initializes the Confluence client and reader with the given configuration settings for credentials, URL, and export limits. Args: configuration: Confluence connection and access settings Returns: ConfluenceDatasourceReader: Fully configured reader instance \"\"\" client = ConfluenceClientFactory . create ( configuration ) return ConfluenceDatasourceReader ( configuration = configuration , client = client , )","title":"ConfluenceDatasourceReaderFactory"},{"location":"src/extraction/datasources/confluence/#src.extraction.datasources.confluence.reader.ConfluencePage","text":"Bases: BaseModel Model representing a Confluence page from the API. Source code in src/extraction/datasources/confluence/reader.py 42 43 44 45 46 47 48 49 50 class ConfluencePage ( BaseModel ): \"\"\"Model representing a Confluence page from the API.\"\"\" id : str title : str body : Body history : History links : Links = Field ( alias = \"_links\" ) expandable : Dict [ str , str ] = Field ( alias = \"_expandable\" )","title":"ConfluencePage"},{"location":"src/extraction/datasources/core/","text":"Core Datasource This module contains functionality related to the Core datasource. Cleaner BaseCleaner Bases: ABC , Generic [ DocType ] Abstract base class for document cleaning operations. Defines the interface for document cleaners with generic type support to ensure type safety across different document implementations. Source code in src/extraction/datasources/core/cleaner.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BaseCleaner ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document cleaning operations. Defines the interface for document cleaners with generic type support to ensure type safety across different document implementations. \"\"\" @abstractmethod def clean ( self , document : DocType ) -> DocType : \"\"\"Clean a single document. Args: document: The document to be cleaned Returns: The cleaned document or None if document should be filtered out \"\"\" pass clean ( document ) abstractmethod Clean a single document. Parameters: document ( DocType ) \u2013 The document to be cleaned Returns: DocType \u2013 The cleaned document or None if document should be filtered out Source code in src/extraction/datasources/core/cleaner.py 14 15 16 17 18 19 20 21 22 23 24 @abstractmethod def clean ( self , document : DocType ) -> DocType : \"\"\"Clean a single document. Args: document: The document to be cleaned Returns: The cleaned document or None if document should be filtered out \"\"\" pass BasicMarkdownCleaner Bases: BaseCleaner , Generic [ DocType ] Document cleaner for basic content validation. Checks for empty content in documents and filters them out. Works with any document type that has a text attribute. Source code in src/extraction/datasources/core/cleaner.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class BasicMarkdownCleaner ( BaseCleaner , Generic [ DocType ]): \"\"\"Document cleaner for basic content validation. Checks for empty content in documents and filters them out. Works with any document type that has a text attribute. \"\"\" def clean ( self , document : DocType ) -> DocType : \"\"\"Remove document if it contains empty content. Args: document: The document to validate Returns: The original document if content is not empty, None otherwise \"\"\" if not self . _has_empty_content ( document ): return document return None @staticmethod def _has_empty_content ( document : DocType ) -> bool : \"\"\"Check if document content is empty. Args: document: Document to check (must have a text attribute) Returns: True if document's text is empty or contains only whitespace \"\"\" return not document . text . strip () clean ( document ) Remove document if it contains empty content. Parameters: document ( DocType ) \u2013 The document to validate Returns: DocType \u2013 The original document if content is not empty, None otherwise Source code in src/extraction/datasources/core/cleaner.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def clean ( self , document : DocType ) -> DocType : \"\"\"Remove document if it contains empty content. Args: document: The document to validate Returns: The original document if content is not empty, None otherwise \"\"\" if not self . _has_empty_content ( document ): return document return None Document BaseDocument Bases: Document Base document class for structured content storage. Extends LlamaIndex Document to add support for attachments and metadata filtering for embedding and LLM contexts. Attributes: attachments ( Optional [ Dict [ str , str ]] ) \u2013 Dictionary mapping placeholder keys to attachment content included_embed_metadata_keys ( List [ str ] ) \u2013 Metadata fields to include in embeddings included_llm_metadata_keys ( List [ str ] ) \u2013 Metadata fields to include in LLM context Note DocType TypeVar ensures type safety when implementing document types. Default metadata includes title and timestamp information. Source code in src/extraction/datasources/core/document.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseDocument ( Document ): \"\"\"Base document class for structured content storage. Extends LlamaIndex Document to add support for attachments and metadata filtering for embedding and LLM contexts. Attributes: attachments: Dictionary mapping placeholder keys to attachment content included_embed_metadata_keys: Metadata fields to include in embeddings included_llm_metadata_keys: Metadata fields to include in LLM context Note: DocType TypeVar ensures type safety when implementing document types. Default metadata includes title and timestamp information. \"\"\" attachments : Optional [ Dict [ str , str ]] = Field ( description = \"Document attachments with placeholders as keys and content as values\" , default = None , ) included_embed_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , ] included_llm_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , ] def __init__ ( self , text : str , metadata : dict , attachments : dict = None ): \"\"\"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. \"\"\" super () . __init__ ( text = text , metadata = metadata ) self . attachments = attachments or {} self . excluded_embed_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_embed_metadata_keys ) self . excluded_llm_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_llm_metadata_keys ) @staticmethod def _set_excluded_metadata_keys ( metadata : dict , included_keys : List [ str ] ) -> List [ str ]: \"\"\"Identify metadata keys to exclude from processing. Returns all keys from metadata that aren't in the included_keys list. \"\"\" return [ key for key in metadata . keys () if key not in included_keys ] __init__ ( text , metadata , attachments = None ) Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. Source code in src/extraction/datasources/core/document.py 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , text : str , metadata : dict , attachments : dict = None ): \"\"\"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. \"\"\" super () . __init__ ( text = text , metadata = metadata ) self . attachments = attachments or {} self . excluded_embed_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_embed_metadata_keys ) self . excluded_llm_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_llm_metadata_keys ) Manager BaseDatasourceManager Bases: ABC , Generic [ DocType ] Abstract base class for datasource management. Defines the interface for managing the extraction, parsing, cleaning, and splitting of documents from a data source. This class serves as a template for implementing specific datasource managers, ensuring a consistent interface and behavior across different implementations. Source code in src/extraction/datasources/core/manager.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class BaseDatasourceManager ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for datasource management. Defines the interface for managing the extraction, parsing, cleaning, and splitting of documents from a data source. This class serves as a template for implementing specific datasource managers, ensuring a consistent interface and behavior across different implementations. \"\"\" def __init__ ( self , configuration : ExtractionConfiguration , reader : BaseReader , parser : BaseParser = BasicMarkdownParser (), cleaner : BaseCleaner = BasicMarkdownCleaner (), splitter : BaseSplitter = BasicMarkdownSplitter (), ): \"\"\"Initialize datasource manager. Args: configuration: Embedding and processing settings reader: Content extraction component cleaner: Content cleaning component splitter: Content splitting component \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner self . splitter = splitter @abstractmethod async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Extract and process all content from the datasource. Returns: An async iterator yielding processed document chunks of type DocType \"\"\" pass @abstractmethod def incremental_sync ( self ): \"\"\"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. \"\"\" pass __init__ ( configuration , reader , parser = BasicMarkdownParser (), cleaner = BasicMarkdownCleaner (), splitter = BasicMarkdownSplitter ()) Initialize datasource manager. Parameters: configuration ( ExtractionConfiguration ) \u2013 Embedding and processing settings reader ( BaseReader ) \u2013 Content extraction component cleaner ( BaseCleaner , default: BasicMarkdownCleaner () ) \u2013 Content cleaning component splitter ( BaseSplitter , default: BasicMarkdownSplitter () ) \u2013 Content splitting component Source code in src/extraction/datasources/core/manager.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , configuration : ExtractionConfiguration , reader : BaseReader , parser : BaseParser = BasicMarkdownParser (), cleaner : BaseCleaner = BasicMarkdownCleaner (), splitter : BaseSplitter = BasicMarkdownSplitter (), ): \"\"\"Initialize datasource manager. Args: configuration: Embedding and processing settings reader: Content extraction component cleaner: Content cleaning component splitter: Content splitting component \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner self . splitter = splitter full_refresh_sync () abstractmethod async Extract and process all content from the datasource. Returns: AsyncIterator [ DocType ] \u2013 An async iterator yielding processed document chunks of type DocType Source code in src/extraction/datasources/core/manager.py 52 53 54 55 56 57 58 59 60 61 @abstractmethod async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Extract and process all content from the datasource. Returns: An async iterator yielding processed document chunks of type DocType \"\"\" pass incremental_sync () abstractmethod Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. Source code in src/extraction/datasources/core/manager.py 63 64 65 66 67 68 69 70 71 @abstractmethod def incremental_sync ( self ): \"\"\"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. \"\"\" pass BasicDatasourceManager Bases: BaseDatasourceManager , Generic [ DocType ] Standard implementation of datasource content processing pipeline. Handles the extraction, parsing, cleaning, and splitting of documents from a data source. Processes documents using the provided components in a sequential pipeline to prepare them for embedding and storage. Source code in src/extraction/datasources/core/manager.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class BasicDatasourceManager ( BaseDatasourceManager , Generic [ DocType ]): \"\"\"Standard implementation of datasource content processing pipeline. Handles the extraction, parsing, cleaning, and splitting of documents from a data source. Processes documents using the provided components in a sequential pipeline to prepare them for embedding and storage. \"\"\" async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: An async iterator yielding processed document chunks of type DocType \"\"\" objects = self . reader . read_all_async () async for object in objects : md_document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( md_document ) if not cleaned_document : continue for split_document in self . splitter . split ( cleaned_document ): yield split_document def incremental_sync ( self ): \"\"\"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError: This feature is not yet implemented \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) full_refresh_sync () async Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: AsyncIterator [ DocType ] \u2013 An async iterator yielding processed document chunks of type DocType Source code in src/extraction/datasources/core/manager.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: An async iterator yielding processed document chunks of type DocType \"\"\" objects = self . reader . read_all_async () async for object in objects : md_document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( md_document ) if not cleaned_document : continue for split_document in self . splitter . split ( cleaned_document ): yield split_document incremental_sync () Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError \u2013 This feature is not yet implemented Source code in src/extraction/datasources/core/manager.py 106 107 108 109 110 111 112 113 114 115 def incremental_sync ( self ): \"\"\"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError: This feature is not yet implemented \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) Parser BaseParser Bases: ABC , Generic [ DocType ] Abstract base class for document parsers. Defines the interface for parsing content into documents of specified type (DocType). Source code in src/extraction/datasources/core/parser.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class BaseParser ( ABC , Generic [ DocType ]): \"\"\" Abstract base class for document parsers. Defines the interface for parsing content into documents of specified type (DocType). \"\"\" @abstractmethod def parse ( self , content : str ) -> DocType : \"\"\" Parse content into a document of type DocType. Args: content: Raw content string to be parsed Returns: Parsed document of type DocType \"\"\" pass parse ( content ) abstractmethod Parse content into a document of type DocType. Parameters: content ( str ) \u2013 Raw content string to be parsed Returns: DocType \u2013 Parsed document of type DocType Source code in src/extraction/datasources/core/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 @abstractmethod def parse ( self , content : str ) -> DocType : \"\"\" Parse content into a document of type DocType. Args: content: Raw content string to be parsed Returns: Parsed document of type DocType \"\"\" pass BasicMarkdownParser Bases: BaseParser [ Document ] Markdown parser that converts markdown text into Document objects. Implements the BaseParser interface for basic markdown content. Source code in src/extraction/datasources/core/parser.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class BasicMarkdownParser ( BaseParser [ Document ]): \"\"\" Markdown parser that converts markdown text into Document objects. Implements the BaseParser interface for basic markdown content. \"\"\" def parse ( self , markdown : str ) -> Document : \"\"\" Parse markdown content into a Document object. Args: markdown: Markdown content to be parsed Returns: Document object containing the markdown text \"\"\" return Document ( text = markdown , metadata = {}) parse ( markdown ) Parse markdown content into a Document object. Parameters: markdown ( str ) \u2013 Markdown content to be parsed Returns: Document \u2013 Document object containing the markdown text Source code in src/extraction/datasources/core/parser.py 38 39 40 41 42 43 44 45 46 47 48 def parse ( self , markdown : str ) -> Document : \"\"\" Parse markdown content into a Document object. Args: markdown: Markdown content to be parsed Returns: Document object containing the markdown text \"\"\" return Document ( text = markdown , metadata = {}) Reader BaseReader Bases: ABC Abstract base class for document source readers. This class defines a standard interface for extracting documents from various data sources. Concrete implementations should inherit from this class and implement the required methods to handle specific data source types. The generic typing allows for flexibility in the document types returned by different implementations. Source code in src/extraction/datasources/core/reader.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class BaseReader ( ABC ): \"\"\"Abstract base class for document source readers. This class defines a standard interface for extracting documents from various data sources. Concrete implementations should inherit from this class and implement the required methods to handle specific data source types. The generic typing allows for flexibility in the document types returned by different implementations. \"\"\" @abstractmethod async def read_all_async ( self ) -> AsyncIterator [ Any ]: \"\"\"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError: This abstract method must be implemented by subclasses. \"\"\" pass @staticmethod def _limit_reached ( yield_count : int , limit : Optional [ int ]) -> bool : \"\"\"Check if the object retrieval limit has been reached. Determines whether the number of fetched objects has reached or exceeded the specified limit. Args: yield_count: Current number of objects fetched limit: Maximum number of objects to retrieve (None for unlimited) Returns: bool: True if limit reached or exceeded, False otherwise \"\"\" return limit is not None and yield_count >= limit read_all_async () abstractmethod async Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator [ Any ] \u2013 AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError \u2013 This abstract method must be implemented by subclasses. Source code in src/extraction/datasources/core/reader.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @abstractmethod async def read_all_async ( self ) -> AsyncIterator [ Any ]: \"\"\"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError: This abstract method must be implemented by subclasses. \"\"\" pass Splitter BaseSplitter Bases: ABC , Generic [ DocType ] Abstract base class for document splitters. This class defines the interface for splitting documents into smaller chunks. All splitter implementations should inherit from this class. Source code in src/extraction/datasources/core/splitter.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BaseSplitter ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document splitters. This class defines the interface for splitting documents into smaller chunks. All splitter implementations should inherit from this class. \"\"\" @abstractmethod def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Split a document into multiple smaller documents. Args: document: The document to be split. Returns: A list of document chunks. \"\"\" pass split ( document ) abstractmethod Split a document into multiple smaller documents. Parameters: document ( DocType ) \u2013 The document to be split. Returns: List [ DocType ] \u2013 A list of document chunks. Source code in src/extraction/datasources/core/splitter.py 14 15 16 17 18 19 20 21 22 23 24 @abstractmethod def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Split a document into multiple smaller documents. Args: document: The document to be split. Returns: A list of document chunks. \"\"\" pass BasicMarkdownSplitter Bases: BaseSplitter , Generic [ DocType ] A simple splitter implementation that returns the document as-is. This splitter does not perform any actual splitting and is primarily used as a pass-through when splitting is not required. Source code in src/extraction/datasources/core/splitter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class BasicMarkdownSplitter ( BaseSplitter , Generic [ DocType ]): \"\"\"A simple splitter implementation that returns the document as-is. This splitter does not perform any actual splitting and is primarily used as a pass-through when splitting is not required. \"\"\" def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Return the document as a single-item list without splitting. Args: document: The document to be processed. Returns: A list containing the original document as the only element. \"\"\" return [ document ] split ( document ) Return the document as a single-item list without splitting. Parameters: document ( DocType ) \u2013 The document to be processed. Returns: List [ DocType ] \u2013 A list containing the original document as the only element. Source code in src/extraction/datasources/core/splitter.py 34 35 36 37 38 39 40 41 42 43 def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Return the document as a single-item list without splitting. Args: document: The document to be processed. Returns: A list containing the original document as the only element. \"\"\" return [ document ]","title":"Core"},{"location":"src/extraction/datasources/core/#core-datasource","text":"This module contains functionality related to the Core datasource.","title":"Core Datasource"},{"location":"src/extraction/datasources/core/#cleaner","text":"","title":"Cleaner"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.cleaner.BaseCleaner","text":"Bases: ABC , Generic [ DocType ] Abstract base class for document cleaning operations. Defines the interface for document cleaners with generic type support to ensure type safety across different document implementations. Source code in src/extraction/datasources/core/cleaner.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BaseCleaner ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document cleaning operations. Defines the interface for document cleaners with generic type support to ensure type safety across different document implementations. \"\"\" @abstractmethod def clean ( self , document : DocType ) -> DocType : \"\"\"Clean a single document. Args: document: The document to be cleaned Returns: The cleaned document or None if document should be filtered out \"\"\" pass","title":"BaseCleaner"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.cleaner.BaseCleaner.clean","text":"Clean a single document. Parameters: document ( DocType ) \u2013 The document to be cleaned Returns: DocType \u2013 The cleaned document or None if document should be filtered out Source code in src/extraction/datasources/core/cleaner.py 14 15 16 17 18 19 20 21 22 23 24 @abstractmethod def clean ( self , document : DocType ) -> DocType : \"\"\"Clean a single document. Args: document: The document to be cleaned Returns: The cleaned document or None if document should be filtered out \"\"\" pass","title":"clean"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.cleaner.BasicMarkdownCleaner","text":"Bases: BaseCleaner , Generic [ DocType ] Document cleaner for basic content validation. Checks for empty content in documents and filters them out. Works with any document type that has a text attribute. Source code in src/extraction/datasources/core/cleaner.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class BasicMarkdownCleaner ( BaseCleaner , Generic [ DocType ]): \"\"\"Document cleaner for basic content validation. Checks for empty content in documents and filters them out. Works with any document type that has a text attribute. \"\"\" def clean ( self , document : DocType ) -> DocType : \"\"\"Remove document if it contains empty content. Args: document: The document to validate Returns: The original document if content is not empty, None otherwise \"\"\" if not self . _has_empty_content ( document ): return document return None @staticmethod def _has_empty_content ( document : DocType ) -> bool : \"\"\"Check if document content is empty. Args: document: Document to check (must have a text attribute) Returns: True if document's text is empty or contains only whitespace \"\"\" return not document . text . strip ()","title":"BasicMarkdownCleaner"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.cleaner.BasicMarkdownCleaner.clean","text":"Remove document if it contains empty content. Parameters: document ( DocType ) \u2013 The document to validate Returns: DocType \u2013 The original document if content is not empty, None otherwise Source code in src/extraction/datasources/core/cleaner.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def clean ( self , document : DocType ) -> DocType : \"\"\"Remove document if it contains empty content. Args: document: The document to validate Returns: The original document if content is not empty, None otherwise \"\"\" if not self . _has_empty_content ( document ): return document return None","title":"clean"},{"location":"src/extraction/datasources/core/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.document.BaseDocument","text":"Bases: Document Base document class for structured content storage. Extends LlamaIndex Document to add support for attachments and metadata filtering for embedding and LLM contexts. Attributes: attachments ( Optional [ Dict [ str , str ]] ) \u2013 Dictionary mapping placeholder keys to attachment content included_embed_metadata_keys ( List [ str ] ) \u2013 Metadata fields to include in embeddings included_llm_metadata_keys ( List [ str ] ) \u2013 Metadata fields to include in LLM context Note DocType TypeVar ensures type safety when implementing document types. Default metadata includes title and timestamp information. Source code in src/extraction/datasources/core/document.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseDocument ( Document ): \"\"\"Base document class for structured content storage. Extends LlamaIndex Document to add support for attachments and metadata filtering for embedding and LLM contexts. Attributes: attachments: Dictionary mapping placeholder keys to attachment content included_embed_metadata_keys: Metadata fields to include in embeddings included_llm_metadata_keys: Metadata fields to include in LLM context Note: DocType TypeVar ensures type safety when implementing document types. Default metadata includes title and timestamp information. \"\"\" attachments : Optional [ Dict [ str , str ]] = Field ( description = \"Document attachments with placeholders as keys and content as values\" , default = None , ) included_embed_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , ] included_llm_metadata_keys : List [ str ] = [ \"title\" , \"created_time\" , \"last_edited_time\" , ] def __init__ ( self , text : str , metadata : dict , attachments : dict = None ): \"\"\"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. \"\"\" super () . __init__ ( text = text , metadata = metadata ) self . attachments = attachments or {} self . excluded_embed_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_embed_metadata_keys ) self . excluded_llm_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_llm_metadata_keys ) @staticmethod def _set_excluded_metadata_keys ( metadata : dict , included_keys : List [ str ] ) -> List [ str ]: \"\"\"Identify metadata keys to exclude from processing. Returns all keys from metadata that aren't in the included_keys list. \"\"\" return [ key for key in metadata . keys () if key not in included_keys ]","title":"BaseDocument"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.document.BaseDocument.__init__","text":"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. Source code in src/extraction/datasources/core/document.py 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , text : str , metadata : dict , attachments : dict = None ): \"\"\"Initialize a document with text, metadata, and optional attachments. Sets up excluded metadata keys for embedding and LLM contexts. \"\"\" super () . __init__ ( text = text , metadata = metadata ) self . attachments = attachments or {} self . excluded_embed_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_embed_metadata_keys ) self . excluded_llm_metadata_keys = self . _set_excluded_metadata_keys ( self . metadata , self . included_llm_metadata_keys )","title":"__init__"},{"location":"src/extraction/datasources/core/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BaseDatasourceManager","text":"Bases: ABC , Generic [ DocType ] Abstract base class for datasource management. Defines the interface for managing the extraction, parsing, cleaning, and splitting of documents from a data source. This class serves as a template for implementing specific datasource managers, ensuring a consistent interface and behavior across different implementations. Source code in src/extraction/datasources/core/manager.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class BaseDatasourceManager ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for datasource management. Defines the interface for managing the extraction, parsing, cleaning, and splitting of documents from a data source. This class serves as a template for implementing specific datasource managers, ensuring a consistent interface and behavior across different implementations. \"\"\" def __init__ ( self , configuration : ExtractionConfiguration , reader : BaseReader , parser : BaseParser = BasicMarkdownParser (), cleaner : BaseCleaner = BasicMarkdownCleaner (), splitter : BaseSplitter = BasicMarkdownSplitter (), ): \"\"\"Initialize datasource manager. Args: configuration: Embedding and processing settings reader: Content extraction component cleaner: Content cleaning component splitter: Content splitting component \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner self . splitter = splitter @abstractmethod async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Extract and process all content from the datasource. Returns: An async iterator yielding processed document chunks of type DocType \"\"\" pass @abstractmethod def incremental_sync ( self ): \"\"\"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. \"\"\" pass","title":"BaseDatasourceManager"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BaseDatasourceManager.__init__","text":"Initialize datasource manager. Parameters: configuration ( ExtractionConfiguration ) \u2013 Embedding and processing settings reader ( BaseReader ) \u2013 Content extraction component cleaner ( BaseCleaner , default: BasicMarkdownCleaner () ) \u2013 Content cleaning component splitter ( BaseSplitter , default: BasicMarkdownSplitter () ) \u2013 Content splitting component Source code in src/extraction/datasources/core/manager.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def __init__ ( self , configuration : ExtractionConfiguration , reader : BaseReader , parser : BaseParser = BasicMarkdownParser (), cleaner : BaseCleaner = BasicMarkdownCleaner (), splitter : BaseSplitter = BasicMarkdownSplitter (), ): \"\"\"Initialize datasource manager. Args: configuration: Embedding and processing settings reader: Content extraction component cleaner: Content cleaning component splitter: Content splitting component \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner self . splitter = splitter","title":"__init__"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BaseDatasourceManager.full_refresh_sync","text":"Extract and process all content from the datasource. Returns: AsyncIterator [ DocType ] \u2013 An async iterator yielding processed document chunks of type DocType Source code in src/extraction/datasources/core/manager.py 52 53 54 55 56 57 58 59 60 61 @abstractmethod async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Extract and process all content from the datasource. Returns: An async iterator yielding processed document chunks of type DocType \"\"\" pass","title":"full_refresh_sync"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BaseDatasourceManager.incremental_sync","text":"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. Source code in src/extraction/datasources/core/manager.py 63 64 65 66 67 68 69 70 71 @abstractmethod def incremental_sync ( self ): \"\"\"Process only new or changed content from the datasource. This method should handle differential updates to avoid reprocessing all content when only portions have changed. Implementations should update the vector storage accordingly. \"\"\" pass","title":"incremental_sync"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BasicDatasourceManager","text":"Bases: BaseDatasourceManager , Generic [ DocType ] Standard implementation of datasource content processing pipeline. Handles the extraction, parsing, cleaning, and splitting of documents from a data source. Processes documents using the provided components in a sequential pipeline to prepare them for embedding and storage. Source code in src/extraction/datasources/core/manager.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class BasicDatasourceManager ( BaseDatasourceManager , Generic [ DocType ]): \"\"\"Standard implementation of datasource content processing pipeline. Handles the extraction, parsing, cleaning, and splitting of documents from a data source. Processes documents using the provided components in a sequential pipeline to prepare them for embedding and storage. \"\"\" async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: An async iterator yielding processed document chunks of type DocType \"\"\" objects = self . reader . read_all_async () async for object in objects : md_document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( md_document ) if not cleaned_document : continue for split_document in self . splitter . split ( cleaned_document ): yield split_document def incremental_sync ( self ): \"\"\"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError: This feature is not yet implemented \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" )","title":"BasicDatasourceManager"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BasicDatasourceManager.full_refresh_sync","text":"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: AsyncIterator [ DocType ] \u2013 An async iterator yielding processed document chunks of type DocType Source code in src/extraction/datasources/core/manager.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 async def full_refresh_sync ( self , ) -> AsyncIterator [ DocType ]: \"\"\"Process all content from the datasource from scratch. Executes the complete pipeline: 1. Reads source objects asynchronously 2. Parses each object into a document 3. Cleans the content 4. Splits into appropriate chunks Returns: An async iterator yielding processed document chunks of type DocType \"\"\" objects = self . reader . read_all_async () async for object in objects : md_document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( md_document ) if not cleaned_document : continue for split_document in self . splitter . split ( cleaned_document ): yield split_document","title":"full_refresh_sync"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.manager.BasicDatasourceManager.incremental_sync","text":"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError \u2013 This feature is not yet implemented Source code in src/extraction/datasources/core/manager.py 106 107 108 109 110 111 112 113 114 115 def incremental_sync ( self ): \"\"\"Process only new or changed content since the last sync. Should be implemented by subclasses to provide efficient updates when only a portion of the datasource has changed. Raises: NotImplementedError: This feature is not yet implemented \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" )","title":"incremental_sync"},{"location":"src/extraction/datasources/core/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.parser.BaseParser","text":"Bases: ABC , Generic [ DocType ] Abstract base class for document parsers. Defines the interface for parsing content into documents of specified type (DocType). Source code in src/extraction/datasources/core/parser.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class BaseParser ( ABC , Generic [ DocType ]): \"\"\" Abstract base class for document parsers. Defines the interface for parsing content into documents of specified type (DocType). \"\"\" @abstractmethod def parse ( self , content : str ) -> DocType : \"\"\" Parse content into a document of type DocType. Args: content: Raw content string to be parsed Returns: Parsed document of type DocType \"\"\" pass","title":"BaseParser"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.parser.BaseParser.parse","text":"Parse content into a document of type DocType. Parameters: content ( str ) \u2013 Raw content string to be parsed Returns: DocType \u2013 Parsed document of type DocType Source code in src/extraction/datasources/core/parser.py 17 18 19 20 21 22 23 24 25 26 27 28 @abstractmethod def parse ( self , content : str ) -> DocType : \"\"\" Parse content into a document of type DocType. Args: content: Raw content string to be parsed Returns: Parsed document of type DocType \"\"\" pass","title":"parse"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.parser.BasicMarkdownParser","text":"Bases: BaseParser [ Document ] Markdown parser that converts markdown text into Document objects. Implements the BaseParser interface for basic markdown content. Source code in src/extraction/datasources/core/parser.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class BasicMarkdownParser ( BaseParser [ Document ]): \"\"\" Markdown parser that converts markdown text into Document objects. Implements the BaseParser interface for basic markdown content. \"\"\" def parse ( self , markdown : str ) -> Document : \"\"\" Parse markdown content into a Document object. Args: markdown: Markdown content to be parsed Returns: Document object containing the markdown text \"\"\" return Document ( text = markdown , metadata = {})","title":"BasicMarkdownParser"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.parser.BasicMarkdownParser.parse","text":"Parse markdown content into a Document object. Parameters: markdown ( str ) \u2013 Markdown content to be parsed Returns: Document \u2013 Document object containing the markdown text Source code in src/extraction/datasources/core/parser.py 38 39 40 41 42 43 44 45 46 47 48 def parse ( self , markdown : str ) -> Document : \"\"\" Parse markdown content into a Document object. Args: markdown: Markdown content to be parsed Returns: Document object containing the markdown text \"\"\" return Document ( text = markdown , metadata = {})","title":"parse"},{"location":"src/extraction/datasources/core/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.reader.BaseReader","text":"Bases: ABC Abstract base class for document source readers. This class defines a standard interface for extracting documents from various data sources. Concrete implementations should inherit from this class and implement the required methods to handle specific data source types. The generic typing allows for flexibility in the document types returned by different implementations. Source code in src/extraction/datasources/core/reader.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class BaseReader ( ABC ): \"\"\"Abstract base class for document source readers. This class defines a standard interface for extracting documents from various data sources. Concrete implementations should inherit from this class and implement the required methods to handle specific data source types. The generic typing allows for flexibility in the document types returned by different implementations. \"\"\" @abstractmethod async def read_all_async ( self ) -> AsyncIterator [ Any ]: \"\"\"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError: This abstract method must be implemented by subclasses. \"\"\" pass @staticmethod def _limit_reached ( yield_count : int , limit : Optional [ int ]) -> bool : \"\"\"Check if the object retrieval limit has been reached. Determines whether the number of fetched objects has reached or exceeded the specified limit. Args: yield_count: Current number of objects fetched limit: Maximum number of objects to retrieve (None for unlimited) Returns: bool: True if limit reached or exceeded, False otherwise \"\"\" return limit is not None and yield_count >= limit","title":"BaseReader"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.reader.BaseReader.read_all_async","text":"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator [ Any ] \u2013 AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError \u2013 This abstract method must be implemented by subclasses. Source code in src/extraction/datasources/core/reader.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @abstractmethod async def read_all_async ( self ) -> AsyncIterator [ Any ]: \"\"\"Asynchronously retrieve documents from the source. Implementations should use async iteration to efficiently stream documents from the source without loading all content into memory at once. Returns: AsyncIterator[Any]: An async iterator that yields documents as they're extracted from the source. Raises: NotImplementedError: This abstract method must be implemented by subclasses. \"\"\" pass","title":"read_all_async"},{"location":"src/extraction/datasources/core/#splitter","text":"","title":"Splitter"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.splitter.BaseSplitter","text":"Bases: ABC , Generic [ DocType ] Abstract base class for document splitters. This class defines the interface for splitting documents into smaller chunks. All splitter implementations should inherit from this class. Source code in src/extraction/datasources/core/splitter.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class BaseSplitter ( ABC , Generic [ DocType ]): \"\"\"Abstract base class for document splitters. This class defines the interface for splitting documents into smaller chunks. All splitter implementations should inherit from this class. \"\"\" @abstractmethod def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Split a document into multiple smaller documents. Args: document: The document to be split. Returns: A list of document chunks. \"\"\" pass","title":"BaseSplitter"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.splitter.BaseSplitter.split","text":"Split a document into multiple smaller documents. Parameters: document ( DocType ) \u2013 The document to be split. Returns: List [ DocType ] \u2013 A list of document chunks. Source code in src/extraction/datasources/core/splitter.py 14 15 16 17 18 19 20 21 22 23 24 @abstractmethod def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Split a document into multiple smaller documents. Args: document: The document to be split. Returns: A list of document chunks. \"\"\" pass","title":"split"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.splitter.BasicMarkdownSplitter","text":"Bases: BaseSplitter , Generic [ DocType ] A simple splitter implementation that returns the document as-is. This splitter does not perform any actual splitting and is primarily used as a pass-through when splitting is not required. Source code in src/extraction/datasources/core/splitter.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class BasicMarkdownSplitter ( BaseSplitter , Generic [ DocType ]): \"\"\"A simple splitter implementation that returns the document as-is. This splitter does not perform any actual splitting and is primarily used as a pass-through when splitting is not required. \"\"\" def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Return the document as a single-item list without splitting. Args: document: The document to be processed. Returns: A list containing the original document as the only element. \"\"\" return [ document ]","title":"BasicMarkdownSplitter"},{"location":"src/extraction/datasources/core/#src.extraction.datasources.core.splitter.BasicMarkdownSplitter.split","text":"Return the document as a single-item list without splitting. Parameters: document ( DocType ) \u2013 The document to be processed. Returns: List [ DocType ] \u2013 A list containing the original document as the only element. Source code in src/extraction/datasources/core/splitter.py 34 35 36 37 38 39 40 41 42 43 def split ( self , document : DocType ) -> List [ DocType ]: \"\"\"Return the document as a single-item list without splitting. Args: document: The document to be processed. Returns: A list containing the original document as the only element. \"\"\" return [ document ]","title":"split"},{"location":"src/extraction/datasources/notion/","text":"Notion Datasource This module contains functionality related to the Notion datasource. Cleaner NotionDatasourceCleaner Bases: BasicMarkdownCleaner [ NotionDocument ] Cleaner for Notion document content. Implements cleaning logic for Notion databases and pages, removing HTML tags and comments while preserving meaningful content. Note Expects documents to be in markdown format. Source code in src/extraction/datasources/notion/cleaner.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class NotionDatasourceCleaner ( BasicMarkdownCleaner [ NotionDocument ]): \"\"\"Cleaner for Notion document content. Implements cleaning logic for Notion databases and pages, removing HTML tags and comments while preserving meaningful content. Note: Expects documents to be in markdown format. \"\"\" def clean ( self , document : NotionDocument ) -> NotionDocument : \"\"\"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Args: document: Notion document to clean Returns: NotionDocument: Cleaned document, or None if content is empty after cleaning \"\"\" if document . metadata [ \"type\" ] == \"database\" : cleaned_text = self . _clean_database ( document ) document . set_content ( cleaned_text ) if document . metadata [ \"type\" ] == \"page\" : cleaned_text = self . _clean_page ( document ) document . set_content ( cleaned_text ) if self . _has_empty_content ( document ): return None return document def _clean_database ( self , document : NotionDocument ) -> str : \"\"\"Clean Notion database content. Extracts and cleans the text content from a Notion database document, processing any embedded HTML elements. Args: document: Database document to clean Returns: str: Cleaned database content as markdown text \"\"\" return NotionDatasourceCleaner . _parse_html_in_markdown ( document . text ) def _clean_page ( self , document : NotionDocument ) -> str : \"\"\"Clean Notion page content. Extracts and cleans the text content from a Notion page document, processing any embedded HTML elements. Args: document: Page document to clean Returns: str: Cleaned page content as markdown text \"\"\" return NotionDatasourceCleaner . _parse_html_in_markdown ( document . text ) @staticmethod def _parse_html_in_markdown ( md_text : str ) -> str : \"\"\"Process HTML elements within markdown content. Performs two main cleaning operations: 1. Removes HTML comments completely 2. Converts HTML tags to markdown format 3. Removes elements that don't contain alphanumeric characters Args: md_text: Text containing markdown and HTML Returns: str: Cleaned markdown text with HTML properly converted or removed Note: Uses BeautifulSoup for HTML parsing and markdownify for HTML-to-markdown conversion \"\"\" def replace_html ( match ): html_content = match . group ( 0 ) soup = BeautifulSoup ( html_content , \"html.parser\" ) markdown = md ( str ( soup )) if not re . search ( r \"[a-zA-Z0-9]\" , markdown ): return \"\" return markdown md_text = re . sub ( r \"<!--.*?-->\" , \"\" , md_text , flags = re . DOTALL ) html_block_re = re . compile ( r \"<.*?>\" , re . DOTALL ) return re . sub ( html_block_re , replace_html , md_text ) clean ( document ) Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Parameters: document ( NotionDocument ) \u2013 Notion document to clean Returns: NotionDocument ( NotionDocument ) \u2013 Cleaned document, or None if content is empty after cleaning Source code in src/extraction/datasources/notion/cleaner.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def clean ( self , document : NotionDocument ) -> NotionDocument : \"\"\"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Args: document: Notion document to clean Returns: NotionDocument: Cleaned document, or None if content is empty after cleaning \"\"\" if document . metadata [ \"type\" ] == \"database\" : cleaned_text = self . _clean_database ( document ) document . set_content ( cleaned_text ) if document . metadata [ \"type\" ] == \"page\" : cleaned_text = self . _clean_page ( document ) document . set_content ( cleaned_text ) if self . _has_empty_content ( document ): return None return document NotionDatasourceCleanerFactory Bases: Factory Factory for creating NotionDatasourceCleaner instances. This factory is responsible for creating instances of NotionDatasourceCleaner with the appropriate configuration. Source code in src/extraction/datasources/notion/cleaner.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class NotionDatasourceCleanerFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceCleaner instances. This factory is responsible for creating instances of NotionDatasourceCleaner with the appropriate configuration. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , _ : NotionDatasourceConfiguration ) -> NotionDatasourceCleaner : \"\"\"Create a new instance of NotionDatasourceCleaner. Args: configuration: Configuration for the cleaner Returns: NotionDatasourceCleaner: Instance of NotionDatasourceCleaner \"\"\" return NotionDatasourceCleaner () Client NotionClientFactory Bases: SingletonFactory Factory for creating and managing Notion API client instances. This singleton factory ensures that only one Notion client instance is created for a specific configuration, promoting resource efficiency and consistency. Client instances are created using the Notion API authentication token from the provided configuration. The factory follows the singleton pattern to prevent multiple instantiations of clients with identical configurations. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object used to create the client Source code in src/extraction/datasources/notion/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class NotionClientFactory ( SingletonFactory ): \"\"\"Factory for creating and managing Notion API client instances. This singleton factory ensures that only one Notion client instance is created for a specific configuration, promoting resource efficiency and consistency. Client instances are created using the Notion API authentication token from the provided configuration. The factory follows the singleton pattern to prevent multiple instantiations of clients with identical configurations. Attributes: _configuration_class: Type of configuration object used to create the client \"\"\" _configuration_class : Type = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration ) -> Client : \"\"\"Create a new instance of the Notion API client. This method extracts the API token from the provided configuration's secrets and uses it to authenticate a new Notion client. Args: configuration: Configuration object containing Notion API credentials and other settings. Returns: A configured Notion API client instance ready for making API calls. \"\"\" return Client ( auth = configuration . secrets . api_token . get_secret_value ()) Configuration Document NotionDocument Bases: BaseDocument Document representation for Notion page content. Extends BaseDocument to handle Notion-specific document processing including metadata handling and filtering for embeddings and LLM contexts. Source code in src/extraction/datasources/notion/document.py 4 5 6 7 8 9 10 11 class NotionDocument ( BaseDocument ): \"\"\"Document representation for Notion page content. Extends BaseDocument to handle Notion-specific document processing including metadata handling and filtering for embeddings and LLM contexts. \"\"\" pass Exporter NotionExporter Exporter for converting Notion pages to markdown documents. Provides a high-level interface for extracting Notion content and converting it to structured NotionDocument instances. Source code in src/extraction/datasources/notion/exporter.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 class NotionExporter : \"\"\"Exporter for converting Notion pages to markdown documents. Provides a high-level interface for extracting Notion content and converting it to structured NotionDocument instances. \"\"\" def __init__ ( self , api_token : str , ): \"\"\"Initialize Notion exporter. Args: api_token: Authentication token for Notion API \"\"\" self . notion_exporter = _NotionExporterCore ( notion_token = api_token , export_child_pages = False , extract_page_metadata = True , ) async def run ( self , page_ids : List [ str ] = None , database_ids : List [ str ] = None ) -> List [ NotionDocument ]: \"\"\"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Args: page_ids: List of page IDs to export database_ids: List of database IDs to export Returns: List of NotionDocument objects containing content and metadata Raises: ValueError: If neither page_ids nor database_ids provided \"\"\" extracted_objects = await self . notion_exporter . async_export_pages ( page_ids = page_ids , database_ids = database_ids ) objects = [] for object_id , extracted_data in extracted_objects . items (): objects . append ( { \"metadata\" : extracted_data [ \"metadata\" ], \"markdown\" : extracted_data [ \"content\" ], } ) return objects __init__ ( api_token ) Initialize Notion exporter. Parameters: api_token ( str ) \u2013 Authentication token for Notion API Source code in src/extraction/datasources/notion/exporter.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def __init__ ( self , api_token : str , ): \"\"\"Initialize Notion exporter. Args: api_token: Authentication token for Notion API \"\"\" self . notion_exporter = _NotionExporterCore ( notion_token = api_token , export_child_pages = False , extract_page_metadata = True , ) run ( page_ids = None , database_ids = None ) async Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Parameters: page_ids ( List [ str ] , default: None ) \u2013 List of page IDs to export database_ids ( List [ str ] , default: None ) \u2013 List of database IDs to export Returns: List [ NotionDocument ] \u2013 List of NotionDocument objects containing content and metadata Raises: ValueError \u2013 If neither page_ids nor database_ids provided Source code in src/extraction/datasources/notion/exporter.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 async def run ( self , page_ids : List [ str ] = None , database_ids : List [ str ] = None ) -> List [ NotionDocument ]: \"\"\"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Args: page_ids: List of page IDs to export database_ids: List of database IDs to export Returns: List of NotionDocument objects containing content and metadata Raises: ValueError: If neither page_ids nor database_ids provided \"\"\" extracted_objects = await self . notion_exporter . async_export_pages ( page_ids = page_ids , database_ids = database_ids ) objects = [] for object_id , extracted_data in extracted_objects . items (): objects . append ( { \"metadata\" : extracted_data [ \"metadata\" ], \"markdown\" : extracted_data [ \"content\" ], } ) return objects NotionExporterFactory Bases: SingletonFactory Factory for creating NotionExporter instances. Ensures only one instance of NotionExporter is created and reused throughout the application, following the singleton pattern. Source code in src/extraction/datasources/notion/exporter.py 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 class NotionExporterFactory ( SingletonFactory ): \"\"\"Factory for creating NotionExporter instances. Ensures only one instance of NotionExporter is created and reused throughout the application, following the singleton pattern. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration ) -> NotionExporter : \"\"\"Create a NotionExporter instance with the given configuration. Args: configuration: Configuration containing Notion API token Returns: Configured NotionExporter instance \"\"\" return NotionExporter ( configuration . secrets . api_token . get_secret_value () ) Manager NotionDatasourceManager Bases: BaseDatasourceManager [ NotionDocument ] Manager for handling Notion datasource extraction and processing. This class coordinates the reading, parsing, and cleaning of Notion content to produce structured NotionDocument objects ready for further processing. Source code in src/extraction/datasources/notion/manager.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class NotionDatasourceManager ( BaseDatasourceManager [ NotionDocument ]): \"\"\"Manager for handling Notion datasource extraction and processing. This class coordinates the reading, parsing, and cleaning of Notion content to produce structured NotionDocument objects ready for further processing. \"\"\" def __init__ ( self , configuration : NotionDatasourceConfiguration , reader : NotionDatasourceReader , parser : NotionDatasourceParser , cleaner : NotionDatasourceCleaner , ): \"\"\"Initialize the Notion datasource manager. Args: configuration: Configuration for the Notion datasource reader: Component responsible for fetching data from Notion parser: Component responsible for parsing Notion data cleaner: Component responsible for cleaning parsed Notion documents \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner def incremental_sync ( self ): \"\"\" Not implemented. \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) async def full_refresh_sync ( self , ) -> AsyncIterator [ NotionDocument ]: \"\"\"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: An async iterator of cleaned NotionDocument objects \"\"\" objects = await self . reader . read_all_async () for object in objects : document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( document ) if cleaned_document : yield cleaned_document __init__ ( configuration , reader , parser , cleaner ) Initialize the Notion datasource manager. Parameters: configuration ( NotionDatasourceConfiguration ) \u2013 Configuration for the Notion datasource reader ( NotionDatasourceReader ) \u2013 Component responsible for fetching data from Notion parser ( NotionDatasourceParser ) \u2013 Component responsible for parsing Notion data cleaner ( NotionDatasourceCleaner ) \u2013 Component responsible for cleaning parsed Notion documents Source code in src/extraction/datasources/notion/manager.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , configuration : NotionDatasourceConfiguration , reader : NotionDatasourceReader , parser : NotionDatasourceParser , cleaner : NotionDatasourceCleaner , ): \"\"\"Initialize the Notion datasource manager. Args: configuration: Configuration for the Notion datasource reader: Component responsible for fetching data from Notion parser: Component responsible for parsing Notion data cleaner: Component responsible for cleaning parsed Notion documents \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner full_refresh_sync () async Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: AsyncIterator [ NotionDocument ] \u2013 An async iterator of cleaned NotionDocument objects Source code in src/extraction/datasources/notion/manager.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 async def full_refresh_sync ( self , ) -> AsyncIterator [ NotionDocument ]: \"\"\"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: An async iterator of cleaned NotionDocument objects \"\"\" objects = await self . reader . read_all_async () for object in objects : document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( document ) if cleaned_document : yield cleaned_document incremental_sync () Not implemented. Source code in src/extraction/datasources/notion/manager.py 50 51 52 53 54 def incremental_sync ( self ): \"\"\" Not implemented. \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) NotionDatasourceManagerFactory Bases: Factory Factory for creating NotionDatasourceManager instances. This factory is responsible for creating instances of the NotionDatasourceManager class, which manages the extraction and processing of content from Notion databases and pages. Attributes: _configuration_class \u2013 Type of configuration object for Notion datasource Source code in src/extraction/datasources/notion/manager.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class NotionDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceManager instances. This factory is responsible for creating instances of the NotionDatasourceManager class, which manages the extraction and processing of content from Notion databases and pages. Attributes: _configuration_class: Type of configuration object for Notion datasource \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration , ) -> NotionDatasourceManager : \"\"\"Create a new instance of NotionDatasourceManager. This method creates all necessary components (reader, parser, cleaner) and assembles them into a NotionDatasourceManager instance. Args: configuration: Configuration object for the Notion datasource Returns: A fully configured NotionDatasourceManager instance \"\"\" reader = NotionDatasourceReaderFactory . create ( configuration ) parser = NotionDatasourceParserFactory . create ( configuration ) cleaner = NotionDatasourceCleanerFactory . create ( configuration ) return NotionDatasourceManager ( configuration = configuration , reader = reader , parser = parser , cleaner = cleaner , ) Parser NotionDatasourceParser Bases: BaseParser [ NotionDocument ] Parser for Notion content. Transforms raw Notion page data into structured NotionDocument objects. Source code in src/extraction/datasources/notion/parser.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class NotionDatasourceParser ( BaseParser [ NotionDocument ]): \"\"\"Parser for Notion content. Transforms raw Notion page data into structured NotionDocument objects. \"\"\" def __init__ ( self ): \"\"\"Initialize the Notion parser.\"\"\" pass def parse ( self , object : str ) -> NotionDocument : \"\"\"Parse Notion page data into a structured document. Args: object: Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: A NotionDocument containing the parsed content and enhanced metadata. \"\"\" markdown = object [ \"markdown\" ] metadata = self . _extract_metadata ( object [ \"metadata\" ]) return NotionDocument ( text = markdown , metadata = metadata ) @staticmethod def _extract_metadata ( metadata : dict ) -> dict : \"\"\"Process and enhance page metadata. Args: metadata: Raw page metadata dictionary Returns: dict: Enhanced metadata including source and formatted dates \"\"\" metadata [ \"datasource\" ] = \"notion\" metadata [ \"created_date\" ] = metadata [ \"created_time\" ] . split ( \"T\" )[ 0 ] metadata [ \"last_edited_date\" ] = metadata [ \"last_edited_time\" ] . split ( \"T\" )[ 0 ] return metadata __init__ () Initialize the Notion parser. Source code in src/extraction/datasources/notion/parser.py 15 16 17 def __init__ ( self ): \"\"\"Initialize the Notion parser.\"\"\" pass parse ( object ) Parse Notion page data into a structured document. Parameters: object ( str ) \u2013 Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: NotionDocument \u2013 A NotionDocument containing the parsed content and enhanced metadata. Source code in src/extraction/datasources/notion/parser.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def parse ( self , object : str ) -> NotionDocument : \"\"\"Parse Notion page data into a structured document. Args: object: Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: A NotionDocument containing the parsed content and enhanced metadata. \"\"\" markdown = object [ \"markdown\" ] metadata = self . _extract_metadata ( object [ \"metadata\" ]) return NotionDocument ( text = markdown , metadata = metadata ) NotionDatasourceParserFactory Bases: Factory Factory for creating NotionDatasourceParser instances. Creates and configures parser instances for Notion content. Source code in src/extraction/datasources/notion/parser.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class NotionDatasourceParserFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceParser instances. Creates and configures parser instances for Notion content. \"\"\" _configuration_class : NotionDatasourceConfiguration = ( NotionDatasourceConfiguration ) @classmethod def _create_instance ( cls , _ : NotionDatasourceConfiguration ) -> NotionDatasourceParser : \"\"\" Create a NotionDatasourceParser instance. Returns: NotionDatasourceParser: Instance of NotionDatasourceParser. \"\"\" return NotionDatasourceParser () Reader NotionDatasourceReader Bases: BaseReader Reader for extracting documents from Notion workspace. Implements document extraction from Notion pages and databases with support for batched async operations and export limits. Source code in src/extraction/datasources/notion/reader.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 class NotionDatasourceReader ( BaseReader ): \"\"\"Reader for extracting documents from Notion workspace. Implements document extraction from Notion pages and databases with support for batched async operations and export limits. \"\"\" def __init__ ( self , configuration : NotionDatasourceConfiguration , client : Client , exporter : NotionExporter , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize Notion reader. Args: configuration: Settings for Notion access and limits client: Client for Notion API interaction exporter: Component for content export and conversion logger: Logger for logging messages and errors \"\"\" super () . __init__ () self . client = client self . export_batch_size = configuration . export_batch_size self . export_limit = configuration . export_limit self . exporter = exporter self . home_page_database_id = configuration . home_page_database_id self . logger = logger async def read_all_async ( self ) -> List [ NotionDocument ]: \"\"\"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List[NotionDocument]: Collection of processed documents \"\"\" if self . home_page_database_id is None : database_ids = [] page_ids = [] else : database_ids , page_ids = self . _get_ids_from_home_page () database_ids . extend ( self . _get_all_ids ( NotionObjectType . DATABASE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) page_ids . extend ( self . _get_all_ids ( NotionObjectType . PAGE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) # Process IDs database_ids = set ( database_ids ) database_ids . discard ( self . home_page_database_id ) page_ids = set ( page_ids ) # Batch and export chunked_database_ids = list ( chunked ( database_ids , self . export_batch_size ) ) chunked_page_ids = list ( chunked ( page_ids , self . export_batch_size )) databases , databases_failed = await self . _export_documents ( chunked_database_ids , NotionObjectType . DATABASE ) pages , pages_failed = await self . _export_documents ( chunked_page_ids , NotionObjectType . PAGE ) # Log failures if databases_failed : self . logger . warning ( f \"Failed to export { len ( databases_failed ) } databases: { databases_failed } \" ) if pages_failed : self . logger . warning ( f \"Failed to export { len ( pages_failed ) } pages: { pages_failed } \" ) # Apply limit if needed objects = databases + pages return ( objects if self . export_limit is None else objects [: self . export_limit ] ) async def _export_documents ( self , chunked_ids : List [ List [ str ]], objects_type : NotionObjectType ) -> Tuple [ List [ NotionDocument ], List [ str ]]: \"\"\"Export Notion documents in batches with progress tracking. Processes batches of Notion object IDs, exporting them through the exporter component. Handles errors gracefully by tracking failed exports and continuing with the next batch. Args: chunked_ids: List of ID batches, where each batch is a list of IDs to be processed together objects_type: Type of Notion objects to export (PAGE or DATABASE) Returns: Tuple containing: - List of successfully exported NotionDocument objects - List of IDs that failed during export Raises: ValueError: If objects_type is not a valid NotionObjectType \"\"\" all_objects = [] failed_exports = [] number_of_chunks = len ( chunked_ids ) for i , chunk_ids in enumerate ( chunked_ids ): self . logger . info ( f \"[ { i } / { number_of_chunks } ] Reading chunk of Notion { objects_type . name } s.\" ) try : objects = await self . exporter . run ( page_ids = ( chunk_ids if objects_type == NotionObjectType . PAGE else None ), database_ids = ( chunk_ids if objects_type == NotionObjectType . DATABASE else None ), ) all_objects . extend ( objects ) self . logger . debug ( f \"Added { len ( objects ) } { objects_type . name } s\" ) except Exception as e : self . logger . error ( f \"Export failed for { objects_type . name } : { chunk_ids } . { e } \" ) failed_exports . extend ( chunk_ids ) if failed_exports : self . logger . warning ( f \"Failed to export { len ( failed_exports ) } { objects_type . name } s\" ) return all_objects , failed_exports def _get_ids_from_home_page ( self ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Extract database and page IDs from home page database. Queries the configured home page database and extracts IDs for both databases and pages. Returns: Tuple containing: - List of database IDs - List of page IDs \"\"\" self . logger . info ( f \"Reading all object ids from Notion's home page with limit { self . export_limit } ...\" ) response = self . _collect_paginated_api ( function = self . client . databases . query , limit = self . export_limit , database_id = self . home_page_database_id , ) database_ids = [ entry [ \"id\" ] for entry in response if entry [ \"object\" ] == \"database\" ] page_ids = [ entry [ \"id\" ] for entry in response if entry [ \"object\" ] == \"page\" ] self . logger . info ( f \"Found { len ( database_ids ) } database ids and { len ( page_ids ) } page ids in Notion.\" ) return database_ids , page_ids def _get_all_ids ( self , objects_type : NotionObjectType , limit : int = None ) -> List [ str ]: \"\"\"Fetch all IDs for specified Notion object type. Args: objects_type: Type of Notion objects to fetch limit: Maximum number of IDs to fetch (None for unlimited) Returns: List[str]: Collection of object IDs Note: Returns empty list if limit is 0 or negative \"\"\" if limit is not None and limit <= 0 : return [] self . logger . info ( f \"Reading all ids of { objects_type . name } objects from Notion API with limit { limit } ...\" ) params = { \"filter\" : { \"value\" : objects_type . name . lower (), \"property\" : \"object\" , }, } results = NotionDatasourceReader . _collect_paginated_api ( self . client . search , limit , ** params ) object_ids = [ object [ \"id\" ] for object in results ] object_ids = object_ids [: limit ] if limit is not None else object_ids self . logger . info ( f \"Found { len ( object_ids ) } ids of { objects_type . name } objects in Notion.\" ) return object_ids def _get_current_limit ( self , database_ids : List [ str ], page_ids : List [ str ] ) -> int : \"\"\"Calculate remaining object limit based on existing IDs. Args: database_ids: Currently collected database IDs page_ids: Currently collected page IDs Returns: int: Remaining limit (None if no limit configured) Note: Subtracts total of existing IDs from configured export limit \"\"\" return ( self . export_limit - len ( database_ids ) - len ( page_ids ) if self . export_limit else None ) @staticmethod def _collect_paginated_api ( function : Callable [ ... , Any ], limit : int , ** kwargs : Any ) -> List [ Any ]: \"\"\"Collect all results from paginated Notion API endpoint. Args: function: API function to call limit: Maximum number of results to collect **kwargs: Additional arguments for API function Returns: List[Any]: Collected API results \"\"\" next_cursor = kwargs . pop ( \"start_cursor\" , None ) result = [] while True : response = function ( ** kwargs , start_cursor = next_cursor ) result . extend ( response . get ( \"results\" )) if NotionDatasourceReader . _limit_reached ( result , limit ): return result [: limit ] if not NotionDatasourceReader . _has_more_pages ( response ): return result [: limit ] if limit else result next_cursor = response . get ( \"next_cursor\" ) @staticmethod def _limit_reached ( result : List [ dict ], limit : int ) -> bool : \"\"\"Check if result count has reached limit. Args: result: Current results limit: Maximum allowed results Returns: bool: True if limit reached \"\"\" return limit is not None and len ( result ) >= limit @staticmethod def _has_more_pages ( response : dict ) -> bool : \"\"\"Check if more pages are available. Args: response: API response dictionary Returns: bool: True if more pages available \"\"\" return response . get ( \"has_more\" ) and response . get ( \"next_cursor\" ) __init__ ( configuration , client , exporter , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize Notion reader. Parameters: configuration ( NotionDatasourceConfiguration ) \u2013 Settings for Notion access and limits client ( Client ) \u2013 Client for Notion API interaction exporter ( NotionExporter ) \u2013 Component for content export and conversion logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger for logging messages and errors Source code in src/extraction/datasources/notion/reader.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , configuration : NotionDatasourceConfiguration , client : Client , exporter : NotionExporter , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize Notion reader. Args: configuration: Settings for Notion access and limits client: Client for Notion API interaction exporter: Component for content export and conversion logger: Logger for logging messages and errors \"\"\" super () . __init__ () self . client = client self . export_batch_size = configuration . export_batch_size self . export_limit = configuration . export_limit self . exporter = exporter self . home_page_database_id = configuration . home_page_database_id self . logger = logger read_all_async () async Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List [ NotionDocument ] \u2013 List[NotionDocument]: Collection of processed documents Source code in src/extraction/datasources/notion/reader.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 async def read_all_async ( self ) -> List [ NotionDocument ]: \"\"\"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List[NotionDocument]: Collection of processed documents \"\"\" if self . home_page_database_id is None : database_ids = [] page_ids = [] else : database_ids , page_ids = self . _get_ids_from_home_page () database_ids . extend ( self . _get_all_ids ( NotionObjectType . DATABASE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) page_ids . extend ( self . _get_all_ids ( NotionObjectType . PAGE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) # Process IDs database_ids = set ( database_ids ) database_ids . discard ( self . home_page_database_id ) page_ids = set ( page_ids ) # Batch and export chunked_database_ids = list ( chunked ( database_ids , self . export_batch_size ) ) chunked_page_ids = list ( chunked ( page_ids , self . export_batch_size )) databases , databases_failed = await self . _export_documents ( chunked_database_ids , NotionObjectType . DATABASE ) pages , pages_failed = await self . _export_documents ( chunked_page_ids , NotionObjectType . PAGE ) # Log failures if databases_failed : self . logger . warning ( f \"Failed to export { len ( databases_failed ) } databases: { databases_failed } \" ) if pages_failed : self . logger . warning ( f \"Failed to export { len ( pages_failed ) } pages: { pages_failed } \" ) # Apply limit if needed objects = databases + pages return ( objects if self . export_limit is None else objects [: self . export_limit ] ) NotionDatasourceReaderFactory Bases: Factory Factory for creating NotionDatasourceReader instances. This class is responsible for creating instances of NotionDatasourceReader with the provided configuration and Notion client. Attributes: _configuration_class \u2013 The configuration class used to create the NotionDatasourceReader instance. Source code in src/extraction/datasources/notion/reader.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 class NotionDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceReader instances. This class is responsible for creating instances of NotionDatasourceReader with the provided configuration and Notion client. Attributes: _configuration_class: The configuration class used to create the NotionDatasourceReader instance. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration , ) -> NotionDatasourceReader : client = NotionClientFactory . create ( configuration ) exporter = NotionExporterFactory . create ( configuration ) return NotionDatasourceReader ( configuration = configuration , client = client , exporter = exporter , ) NotionObjectType Bases: Enum Enum representing Notion object types. This enum is used to specify the type of Notion object being processed, such as a page or a database. Source code in src/extraction/datasources/notion/reader.py 22 23 24 25 26 27 28 29 30 class NotionObjectType ( Enum ): \"\"\" Enum representing Notion object types. This enum is used to specify the type of Notion object being processed, such as a page or a database. \"\"\" PAGE = \"page\" DATABASE = \"database\"","title":"Notion"},{"location":"src/extraction/datasources/notion/#notion-datasource","text":"This module contains functionality related to the Notion datasource.","title":"Notion Datasource"},{"location":"src/extraction/datasources/notion/#cleaner","text":"","title":"Cleaner"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.cleaner.NotionDatasourceCleaner","text":"Bases: BasicMarkdownCleaner [ NotionDocument ] Cleaner for Notion document content. Implements cleaning logic for Notion databases and pages, removing HTML tags and comments while preserving meaningful content. Note Expects documents to be in markdown format. Source code in src/extraction/datasources/notion/cleaner.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class NotionDatasourceCleaner ( BasicMarkdownCleaner [ NotionDocument ]): \"\"\"Cleaner for Notion document content. Implements cleaning logic for Notion databases and pages, removing HTML tags and comments while preserving meaningful content. Note: Expects documents to be in markdown format. \"\"\" def clean ( self , document : NotionDocument ) -> NotionDocument : \"\"\"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Args: document: Notion document to clean Returns: NotionDocument: Cleaned document, or None if content is empty after cleaning \"\"\" if document . metadata [ \"type\" ] == \"database\" : cleaned_text = self . _clean_database ( document ) document . set_content ( cleaned_text ) if document . metadata [ \"type\" ] == \"page\" : cleaned_text = self . _clean_page ( document ) document . set_content ( cleaned_text ) if self . _has_empty_content ( document ): return None return document def _clean_database ( self , document : NotionDocument ) -> str : \"\"\"Clean Notion database content. Extracts and cleans the text content from a Notion database document, processing any embedded HTML elements. Args: document: Database document to clean Returns: str: Cleaned database content as markdown text \"\"\" return NotionDatasourceCleaner . _parse_html_in_markdown ( document . text ) def _clean_page ( self , document : NotionDocument ) -> str : \"\"\"Clean Notion page content. Extracts and cleans the text content from a Notion page document, processing any embedded HTML elements. Args: document: Page document to clean Returns: str: Cleaned page content as markdown text \"\"\" return NotionDatasourceCleaner . _parse_html_in_markdown ( document . text ) @staticmethod def _parse_html_in_markdown ( md_text : str ) -> str : \"\"\"Process HTML elements within markdown content. Performs two main cleaning operations: 1. Removes HTML comments completely 2. Converts HTML tags to markdown format 3. Removes elements that don't contain alphanumeric characters Args: md_text: Text containing markdown and HTML Returns: str: Cleaned markdown text with HTML properly converted or removed Note: Uses BeautifulSoup for HTML parsing and markdownify for HTML-to-markdown conversion \"\"\" def replace_html ( match ): html_content = match . group ( 0 ) soup = BeautifulSoup ( html_content , \"html.parser\" ) markdown = md ( str ( soup )) if not re . search ( r \"[a-zA-Z0-9]\" , markdown ): return \"\" return markdown md_text = re . sub ( r \"<!--.*?-->\" , \"\" , md_text , flags = re . DOTALL ) html_block_re = re . compile ( r \"<.*?>\" , re . DOTALL ) return re . sub ( html_block_re , replace_html , md_text )","title":"NotionDatasourceCleaner"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.cleaner.NotionDatasourceCleaner.clean","text":"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Parameters: document ( NotionDocument ) \u2013 Notion document to clean Returns: NotionDocument ( NotionDocument ) \u2013 Cleaned document, or None if content is empty after cleaning Source code in src/extraction/datasources/notion/cleaner.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def clean ( self , document : NotionDocument ) -> NotionDocument : \"\"\"Clean a single Notion document. Processes the document based on its type (database or page), removing HTML artifacts and cleaning the content. Args: document: Notion document to clean Returns: NotionDocument: Cleaned document, or None if content is empty after cleaning \"\"\" if document . metadata [ \"type\" ] == \"database\" : cleaned_text = self . _clean_database ( document ) document . set_content ( cleaned_text ) if document . metadata [ \"type\" ] == \"page\" : cleaned_text = self . _clean_page ( document ) document . set_content ( cleaned_text ) if self . _has_empty_content ( document ): return None return document","title":"clean"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.cleaner.NotionDatasourceCleanerFactory","text":"Bases: Factory Factory for creating NotionDatasourceCleaner instances. This factory is responsible for creating instances of NotionDatasourceCleaner with the appropriate configuration. Source code in src/extraction/datasources/notion/cleaner.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class NotionDatasourceCleanerFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceCleaner instances. This factory is responsible for creating instances of NotionDatasourceCleaner with the appropriate configuration. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , _ : NotionDatasourceConfiguration ) -> NotionDatasourceCleaner : \"\"\"Create a new instance of NotionDatasourceCleaner. Args: configuration: Configuration for the cleaner Returns: NotionDatasourceCleaner: Instance of NotionDatasourceCleaner \"\"\" return NotionDatasourceCleaner ()","title":"NotionDatasourceCleanerFactory"},{"location":"src/extraction/datasources/notion/#client","text":"","title":"Client"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.client.NotionClientFactory","text":"Bases: SingletonFactory Factory for creating and managing Notion API client instances. This singleton factory ensures that only one Notion client instance is created for a specific configuration, promoting resource efficiency and consistency. Client instances are created using the Notion API authentication token from the provided configuration. The factory follows the singleton pattern to prevent multiple instantiations of clients with identical configurations. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object used to create the client Source code in src/extraction/datasources/notion/client.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class NotionClientFactory ( SingletonFactory ): \"\"\"Factory for creating and managing Notion API client instances. This singleton factory ensures that only one Notion client instance is created for a specific configuration, promoting resource efficiency and consistency. Client instances are created using the Notion API authentication token from the provided configuration. The factory follows the singleton pattern to prevent multiple instantiations of clients with identical configurations. Attributes: _configuration_class: Type of configuration object used to create the client \"\"\" _configuration_class : Type = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration ) -> Client : \"\"\"Create a new instance of the Notion API client. This method extracts the API token from the provided configuration's secrets and uses it to authenticate a new Notion client. Args: configuration: Configuration object containing Notion API credentials and other settings. Returns: A configured Notion API client instance ready for making API calls. \"\"\" return Client ( auth = configuration . secrets . api_token . get_secret_value ())","title":"NotionClientFactory"},{"location":"src/extraction/datasources/notion/#configuration","text":"","title":"Configuration"},{"location":"src/extraction/datasources/notion/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.document.NotionDocument","text":"Bases: BaseDocument Document representation for Notion page content. Extends BaseDocument to handle Notion-specific document processing including metadata handling and filtering for embeddings and LLM contexts. Source code in src/extraction/datasources/notion/document.py 4 5 6 7 8 9 10 11 class NotionDocument ( BaseDocument ): \"\"\"Document representation for Notion page content. Extends BaseDocument to handle Notion-specific document processing including metadata handling and filtering for embeddings and LLM contexts. \"\"\" pass","title":"NotionDocument"},{"location":"src/extraction/datasources/notion/#exporter","text":"","title":"Exporter"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.exporter.NotionExporter","text":"Exporter for converting Notion pages to markdown documents. Provides a high-level interface for extracting Notion content and converting it to structured NotionDocument instances. Source code in src/extraction/datasources/notion/exporter.py 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 class NotionExporter : \"\"\"Exporter for converting Notion pages to markdown documents. Provides a high-level interface for extracting Notion content and converting it to structured NotionDocument instances. \"\"\" def __init__ ( self , api_token : str , ): \"\"\"Initialize Notion exporter. Args: api_token: Authentication token for Notion API \"\"\" self . notion_exporter = _NotionExporterCore ( notion_token = api_token , export_child_pages = False , extract_page_metadata = True , ) async def run ( self , page_ids : List [ str ] = None , database_ids : List [ str ] = None ) -> List [ NotionDocument ]: \"\"\"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Args: page_ids: List of page IDs to export database_ids: List of database IDs to export Returns: List of NotionDocument objects containing content and metadata Raises: ValueError: If neither page_ids nor database_ids provided \"\"\" extracted_objects = await self . notion_exporter . async_export_pages ( page_ids = page_ids , database_ids = database_ids ) objects = [] for object_id , extracted_data in extracted_objects . items (): objects . append ( { \"metadata\" : extracted_data [ \"metadata\" ], \"markdown\" : extracted_data [ \"content\" ], } ) return objects","title":"NotionExporter"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.exporter.NotionExporter.__init__","text":"Initialize Notion exporter. Parameters: api_token ( str ) \u2013 Authentication token for Notion API Source code in src/extraction/datasources/notion/exporter.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def __init__ ( self , api_token : str , ): \"\"\"Initialize Notion exporter. Args: api_token: Authentication token for Notion API \"\"\" self . notion_exporter = _NotionExporterCore ( notion_token = api_token , export_child_pages = False , extract_page_metadata = True , )","title":"__init__"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.exporter.NotionExporter.run","text":"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Parameters: page_ids ( List [ str ] , default: None ) \u2013 List of page IDs to export database_ids ( List [ str ] , default: None ) \u2013 List of database IDs to export Returns: List [ NotionDocument ] \u2013 List of NotionDocument objects containing content and metadata Raises: ValueError \u2013 If neither page_ids nor database_ids provided Source code in src/extraction/datasources/notion/exporter.py 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 async def run ( self , page_ids : List [ str ] = None , database_ids : List [ str ] = None ) -> List [ NotionDocument ]: \"\"\"Export Notion content to document collection. Extracts content from specified pages and databases and converts them to structured document objects. Args: page_ids: List of page IDs to export database_ids: List of database IDs to export Returns: List of NotionDocument objects containing content and metadata Raises: ValueError: If neither page_ids nor database_ids provided \"\"\" extracted_objects = await self . notion_exporter . async_export_pages ( page_ids = page_ids , database_ids = database_ids ) objects = [] for object_id , extracted_data in extracted_objects . items (): objects . append ( { \"metadata\" : extracted_data [ \"metadata\" ], \"markdown\" : extracted_data [ \"content\" ], } ) return objects","title":"run"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.exporter.NotionExporterFactory","text":"Bases: SingletonFactory Factory for creating NotionExporter instances. Ensures only one instance of NotionExporter is created and reused throughout the application, following the singleton pattern. Source code in src/extraction/datasources/notion/exporter.py 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 class NotionExporterFactory ( SingletonFactory ): \"\"\"Factory for creating NotionExporter instances. Ensures only one instance of NotionExporter is created and reused throughout the application, following the singleton pattern. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration ) -> NotionExporter : \"\"\"Create a NotionExporter instance with the given configuration. Args: configuration: Configuration containing Notion API token Returns: Configured NotionExporter instance \"\"\" return NotionExporter ( configuration . secrets . api_token . get_secret_value () )","title":"NotionExporterFactory"},{"location":"src/extraction/datasources/notion/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManager","text":"Bases: BaseDatasourceManager [ NotionDocument ] Manager for handling Notion datasource extraction and processing. This class coordinates the reading, parsing, and cleaning of Notion content to produce structured NotionDocument objects ready for further processing. Source code in src/extraction/datasources/notion/manager.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class NotionDatasourceManager ( BaseDatasourceManager [ NotionDocument ]): \"\"\"Manager for handling Notion datasource extraction and processing. This class coordinates the reading, parsing, and cleaning of Notion content to produce structured NotionDocument objects ready for further processing. \"\"\" def __init__ ( self , configuration : NotionDatasourceConfiguration , reader : NotionDatasourceReader , parser : NotionDatasourceParser , cleaner : NotionDatasourceCleaner , ): \"\"\"Initialize the Notion datasource manager. Args: configuration: Configuration for the Notion datasource reader: Component responsible for fetching data from Notion parser: Component responsible for parsing Notion data cleaner: Component responsible for cleaning parsed Notion documents \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner def incremental_sync ( self ): \"\"\" Not implemented. \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" ) async def full_refresh_sync ( self , ) -> AsyncIterator [ NotionDocument ]: \"\"\"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: An async iterator of cleaned NotionDocument objects \"\"\" objects = await self . reader . read_all_async () for object in objects : document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( document ) if cleaned_document : yield cleaned_document","title":"NotionDatasourceManager"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManager.__init__","text":"Initialize the Notion datasource manager. Parameters: configuration ( NotionDatasourceConfiguration ) \u2013 Configuration for the Notion datasource reader ( NotionDatasourceReader ) \u2013 Component responsible for fetching data from Notion parser ( NotionDatasourceParser ) \u2013 Component responsible for parsing Notion data cleaner ( NotionDatasourceCleaner ) \u2013 Component responsible for cleaning parsed Notion documents Source code in src/extraction/datasources/notion/manager.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def __init__ ( self , configuration : NotionDatasourceConfiguration , reader : NotionDatasourceReader , parser : NotionDatasourceParser , cleaner : NotionDatasourceCleaner , ): \"\"\"Initialize the Notion datasource manager. Args: configuration: Configuration for the Notion datasource reader: Component responsible for fetching data from Notion parser: Component responsible for parsing Notion data cleaner: Component responsible for cleaning parsed Notion documents \"\"\" self . configuration = configuration self . reader = reader self . parser = parser self . cleaner = cleaner","title":"__init__"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManager.full_refresh_sync","text":"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: AsyncIterator [ NotionDocument ] \u2013 An async iterator of cleaned NotionDocument objects Source code in src/extraction/datasources/notion/manager.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 async def full_refresh_sync ( self , ) -> AsyncIterator [ NotionDocument ]: \"\"\"Perform a full refresh of all documents from the Notion datasource. This method reads all objects from the Notion datasource, parses them into documents, cleans them, and yields the cleaned documents. Returns: An async iterator of cleaned NotionDocument objects \"\"\" objects = await self . reader . read_all_async () for object in objects : document = self . parser . parse ( object ) cleaned_document = self . cleaner . clean ( document ) if cleaned_document : yield cleaned_document","title":"full_refresh_sync"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManager.incremental_sync","text":"Not implemented. Source code in src/extraction/datasources/notion/manager.py 50 51 52 53 54 def incremental_sync ( self ): \"\"\" Not implemented. \"\"\" raise NotImplementedError ( \"Currently unsupported feature.\" )","title":"incremental_sync"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.manager.NotionDatasourceManagerFactory","text":"Bases: Factory Factory for creating NotionDatasourceManager instances. This factory is responsible for creating instances of the NotionDatasourceManager class, which manages the extraction and processing of content from Notion databases and pages. Attributes: _configuration_class \u2013 Type of configuration object for Notion datasource Source code in src/extraction/datasources/notion/manager.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class NotionDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceManager instances. This factory is responsible for creating instances of the NotionDatasourceManager class, which manages the extraction and processing of content from Notion databases and pages. Attributes: _configuration_class: Type of configuration object for Notion datasource \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration , ) -> NotionDatasourceManager : \"\"\"Create a new instance of NotionDatasourceManager. This method creates all necessary components (reader, parser, cleaner) and assembles them into a NotionDatasourceManager instance. Args: configuration: Configuration object for the Notion datasource Returns: A fully configured NotionDatasourceManager instance \"\"\" reader = NotionDatasourceReaderFactory . create ( configuration ) parser = NotionDatasourceParserFactory . create ( configuration ) cleaner = NotionDatasourceCleanerFactory . create ( configuration ) return NotionDatasourceManager ( configuration = configuration , reader = reader , parser = parser , cleaner = cleaner , )","title":"NotionDatasourceManagerFactory"},{"location":"src/extraction/datasources/notion/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.parser.NotionDatasourceParser","text":"Bases: BaseParser [ NotionDocument ] Parser for Notion content. Transforms raw Notion page data into structured NotionDocument objects. Source code in src/extraction/datasources/notion/parser.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class NotionDatasourceParser ( BaseParser [ NotionDocument ]): \"\"\"Parser for Notion content. Transforms raw Notion page data into structured NotionDocument objects. \"\"\" def __init__ ( self ): \"\"\"Initialize the Notion parser.\"\"\" pass def parse ( self , object : str ) -> NotionDocument : \"\"\"Parse Notion page data into a structured document. Args: object: Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: A NotionDocument containing the parsed content and enhanced metadata. \"\"\" markdown = object [ \"markdown\" ] metadata = self . _extract_metadata ( object [ \"metadata\" ]) return NotionDocument ( text = markdown , metadata = metadata ) @staticmethod def _extract_metadata ( metadata : dict ) -> dict : \"\"\"Process and enhance page metadata. Args: metadata: Raw page metadata dictionary Returns: dict: Enhanced metadata including source and formatted dates \"\"\" metadata [ \"datasource\" ] = \"notion\" metadata [ \"created_date\" ] = metadata [ \"created_time\" ] . split ( \"T\" )[ 0 ] metadata [ \"last_edited_date\" ] = metadata [ \"last_edited_time\" ] . split ( \"T\" )[ 0 ] return metadata","title":"NotionDatasourceParser"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.parser.NotionDatasourceParser.__init__","text":"Initialize the Notion parser. Source code in src/extraction/datasources/notion/parser.py 15 16 17 def __init__ ( self ): \"\"\"Initialize the Notion parser.\"\"\" pass","title":"__init__"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.parser.NotionDatasourceParser.parse","text":"Parse Notion page data into a structured document. Parameters: object ( str ) \u2013 Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: NotionDocument \u2013 A NotionDocument containing the parsed content and enhanced metadata. Source code in src/extraction/datasources/notion/parser.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def parse ( self , object : str ) -> NotionDocument : \"\"\"Parse Notion page data into a structured document. Args: object: Dictionary containing Notion page content with 'markdown' text and 'metadata' information. Returns: A NotionDocument containing the parsed content and enhanced metadata. \"\"\" markdown = object [ \"markdown\" ] metadata = self . _extract_metadata ( object [ \"metadata\" ]) return NotionDocument ( text = markdown , metadata = metadata )","title":"parse"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.parser.NotionDatasourceParserFactory","text":"Bases: Factory Factory for creating NotionDatasourceParser instances. Creates and configures parser instances for Notion content. Source code in src/extraction/datasources/notion/parser.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class NotionDatasourceParserFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceParser instances. Creates and configures parser instances for Notion content. \"\"\" _configuration_class : NotionDatasourceConfiguration = ( NotionDatasourceConfiguration ) @classmethod def _create_instance ( cls , _ : NotionDatasourceConfiguration ) -> NotionDatasourceParser : \"\"\" Create a NotionDatasourceParser instance. Returns: NotionDatasourceParser: Instance of NotionDatasourceParser. \"\"\" return NotionDatasourceParser ()","title":"NotionDatasourceParserFactory"},{"location":"src/extraction/datasources/notion/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionDatasourceReader","text":"Bases: BaseReader Reader for extracting documents from Notion workspace. Implements document extraction from Notion pages and databases with support for batched async operations and export limits. Source code in src/extraction/datasources/notion/reader.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 class NotionDatasourceReader ( BaseReader ): \"\"\"Reader for extracting documents from Notion workspace. Implements document extraction from Notion pages and databases with support for batched async operations and export limits. \"\"\" def __init__ ( self , configuration : NotionDatasourceConfiguration , client : Client , exporter : NotionExporter , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize Notion reader. Args: configuration: Settings for Notion access and limits client: Client for Notion API interaction exporter: Component for content export and conversion logger: Logger for logging messages and errors \"\"\" super () . __init__ () self . client = client self . export_batch_size = configuration . export_batch_size self . export_limit = configuration . export_limit self . exporter = exporter self . home_page_database_id = configuration . home_page_database_id self . logger = logger async def read_all_async ( self ) -> List [ NotionDocument ]: \"\"\"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List[NotionDocument]: Collection of processed documents \"\"\" if self . home_page_database_id is None : database_ids = [] page_ids = [] else : database_ids , page_ids = self . _get_ids_from_home_page () database_ids . extend ( self . _get_all_ids ( NotionObjectType . DATABASE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) page_ids . extend ( self . _get_all_ids ( NotionObjectType . PAGE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) # Process IDs database_ids = set ( database_ids ) database_ids . discard ( self . home_page_database_id ) page_ids = set ( page_ids ) # Batch and export chunked_database_ids = list ( chunked ( database_ids , self . export_batch_size ) ) chunked_page_ids = list ( chunked ( page_ids , self . export_batch_size )) databases , databases_failed = await self . _export_documents ( chunked_database_ids , NotionObjectType . DATABASE ) pages , pages_failed = await self . _export_documents ( chunked_page_ids , NotionObjectType . PAGE ) # Log failures if databases_failed : self . logger . warning ( f \"Failed to export { len ( databases_failed ) } databases: { databases_failed } \" ) if pages_failed : self . logger . warning ( f \"Failed to export { len ( pages_failed ) } pages: { pages_failed } \" ) # Apply limit if needed objects = databases + pages return ( objects if self . export_limit is None else objects [: self . export_limit ] ) async def _export_documents ( self , chunked_ids : List [ List [ str ]], objects_type : NotionObjectType ) -> Tuple [ List [ NotionDocument ], List [ str ]]: \"\"\"Export Notion documents in batches with progress tracking. Processes batches of Notion object IDs, exporting them through the exporter component. Handles errors gracefully by tracking failed exports and continuing with the next batch. Args: chunked_ids: List of ID batches, where each batch is a list of IDs to be processed together objects_type: Type of Notion objects to export (PAGE or DATABASE) Returns: Tuple containing: - List of successfully exported NotionDocument objects - List of IDs that failed during export Raises: ValueError: If objects_type is not a valid NotionObjectType \"\"\" all_objects = [] failed_exports = [] number_of_chunks = len ( chunked_ids ) for i , chunk_ids in enumerate ( chunked_ids ): self . logger . info ( f \"[ { i } / { number_of_chunks } ] Reading chunk of Notion { objects_type . name } s.\" ) try : objects = await self . exporter . run ( page_ids = ( chunk_ids if objects_type == NotionObjectType . PAGE else None ), database_ids = ( chunk_ids if objects_type == NotionObjectType . DATABASE else None ), ) all_objects . extend ( objects ) self . logger . debug ( f \"Added { len ( objects ) } { objects_type . name } s\" ) except Exception as e : self . logger . error ( f \"Export failed for { objects_type . name } : { chunk_ids } . { e } \" ) failed_exports . extend ( chunk_ids ) if failed_exports : self . logger . warning ( f \"Failed to export { len ( failed_exports ) } { objects_type . name } s\" ) return all_objects , failed_exports def _get_ids_from_home_page ( self ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Extract database and page IDs from home page database. Queries the configured home page database and extracts IDs for both databases and pages. Returns: Tuple containing: - List of database IDs - List of page IDs \"\"\" self . logger . info ( f \"Reading all object ids from Notion's home page with limit { self . export_limit } ...\" ) response = self . _collect_paginated_api ( function = self . client . databases . query , limit = self . export_limit , database_id = self . home_page_database_id , ) database_ids = [ entry [ \"id\" ] for entry in response if entry [ \"object\" ] == \"database\" ] page_ids = [ entry [ \"id\" ] for entry in response if entry [ \"object\" ] == \"page\" ] self . logger . info ( f \"Found { len ( database_ids ) } database ids and { len ( page_ids ) } page ids in Notion.\" ) return database_ids , page_ids def _get_all_ids ( self , objects_type : NotionObjectType , limit : int = None ) -> List [ str ]: \"\"\"Fetch all IDs for specified Notion object type. Args: objects_type: Type of Notion objects to fetch limit: Maximum number of IDs to fetch (None for unlimited) Returns: List[str]: Collection of object IDs Note: Returns empty list if limit is 0 or negative \"\"\" if limit is not None and limit <= 0 : return [] self . logger . info ( f \"Reading all ids of { objects_type . name } objects from Notion API with limit { limit } ...\" ) params = { \"filter\" : { \"value\" : objects_type . name . lower (), \"property\" : \"object\" , }, } results = NotionDatasourceReader . _collect_paginated_api ( self . client . search , limit , ** params ) object_ids = [ object [ \"id\" ] for object in results ] object_ids = object_ids [: limit ] if limit is not None else object_ids self . logger . info ( f \"Found { len ( object_ids ) } ids of { objects_type . name } objects in Notion.\" ) return object_ids def _get_current_limit ( self , database_ids : List [ str ], page_ids : List [ str ] ) -> int : \"\"\"Calculate remaining object limit based on existing IDs. Args: database_ids: Currently collected database IDs page_ids: Currently collected page IDs Returns: int: Remaining limit (None if no limit configured) Note: Subtracts total of existing IDs from configured export limit \"\"\" return ( self . export_limit - len ( database_ids ) - len ( page_ids ) if self . export_limit else None ) @staticmethod def _collect_paginated_api ( function : Callable [ ... , Any ], limit : int , ** kwargs : Any ) -> List [ Any ]: \"\"\"Collect all results from paginated Notion API endpoint. Args: function: API function to call limit: Maximum number of results to collect **kwargs: Additional arguments for API function Returns: List[Any]: Collected API results \"\"\" next_cursor = kwargs . pop ( \"start_cursor\" , None ) result = [] while True : response = function ( ** kwargs , start_cursor = next_cursor ) result . extend ( response . get ( \"results\" )) if NotionDatasourceReader . _limit_reached ( result , limit ): return result [: limit ] if not NotionDatasourceReader . _has_more_pages ( response ): return result [: limit ] if limit else result next_cursor = response . get ( \"next_cursor\" ) @staticmethod def _limit_reached ( result : List [ dict ], limit : int ) -> bool : \"\"\"Check if result count has reached limit. Args: result: Current results limit: Maximum allowed results Returns: bool: True if limit reached \"\"\" return limit is not None and len ( result ) >= limit @staticmethod def _has_more_pages ( response : dict ) -> bool : \"\"\"Check if more pages are available. Args: response: API response dictionary Returns: bool: True if more pages available \"\"\" return response . get ( \"has_more\" ) and response . get ( \"next_cursor\" )","title":"NotionDatasourceReader"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionDatasourceReader.__init__","text":"Initialize Notion reader. Parameters: configuration ( NotionDatasourceConfiguration ) \u2013 Settings for Notion access and limits client ( Client ) \u2013 Client for Notion API interaction exporter ( NotionExporter ) \u2013 Component for content export and conversion logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger for logging messages and errors Source code in src/extraction/datasources/notion/reader.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , configuration : NotionDatasourceConfiguration , client : Client , exporter : NotionExporter , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize Notion reader. Args: configuration: Settings for Notion access and limits client: Client for Notion API interaction exporter: Component for content export and conversion logger: Logger for logging messages and errors \"\"\" super () . __init__ () self . client = client self . export_batch_size = configuration . export_batch_size self . export_limit = configuration . export_limit self . exporter = exporter self . home_page_database_id = configuration . home_page_database_id self . logger = logger","title":"__init__"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionDatasourceReader.read_all_async","text":"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List [ NotionDocument ] \u2013 List[NotionDocument]: Collection of processed documents Source code in src/extraction/datasources/notion/reader.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 async def read_all_async ( self ) -> List [ NotionDocument ]: \"\"\"Asynchronously retrieve all documents from Notion. Fetches pages and databases in batches, respecting export limits and batch sizes. Returns: List[NotionDocument]: Collection of processed documents \"\"\" if self . home_page_database_id is None : database_ids = [] page_ids = [] else : database_ids , page_ids = self . _get_ids_from_home_page () database_ids . extend ( self . _get_all_ids ( NotionObjectType . DATABASE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) page_ids . extend ( self . _get_all_ids ( NotionObjectType . PAGE , limit = self . _get_current_limit ( database_ids , page_ids ), ) ) # Process IDs database_ids = set ( database_ids ) database_ids . discard ( self . home_page_database_id ) page_ids = set ( page_ids ) # Batch and export chunked_database_ids = list ( chunked ( database_ids , self . export_batch_size ) ) chunked_page_ids = list ( chunked ( page_ids , self . export_batch_size )) databases , databases_failed = await self . _export_documents ( chunked_database_ids , NotionObjectType . DATABASE ) pages , pages_failed = await self . _export_documents ( chunked_page_ids , NotionObjectType . PAGE ) # Log failures if databases_failed : self . logger . warning ( f \"Failed to export { len ( databases_failed ) } databases: { databases_failed } \" ) if pages_failed : self . logger . warning ( f \"Failed to export { len ( pages_failed ) } pages: { pages_failed } \" ) # Apply limit if needed objects = databases + pages return ( objects if self . export_limit is None else objects [: self . export_limit ] )","title":"read_all_async"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionDatasourceReaderFactory","text":"Bases: Factory Factory for creating NotionDatasourceReader instances. This class is responsible for creating instances of NotionDatasourceReader with the provided configuration and Notion client. Attributes: _configuration_class \u2013 The configuration class used to create the NotionDatasourceReader instance. Source code in src/extraction/datasources/notion/reader.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 class NotionDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating NotionDatasourceReader instances. This class is responsible for creating instances of NotionDatasourceReader with the provided configuration and Notion client. Attributes: _configuration_class: The configuration class used to create the NotionDatasourceReader instance. \"\"\" _configuration_class = NotionDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : NotionDatasourceConfiguration , ) -> NotionDatasourceReader : client = NotionClientFactory . create ( configuration ) exporter = NotionExporterFactory . create ( configuration ) return NotionDatasourceReader ( configuration = configuration , client = client , exporter = exporter , )","title":"NotionDatasourceReaderFactory"},{"location":"src/extraction/datasources/notion/#src.extraction.datasources.notion.reader.NotionObjectType","text":"Bases: Enum Enum representing Notion object types. This enum is used to specify the type of Notion object being processed, such as a page or a database. Source code in src/extraction/datasources/notion/reader.py 22 23 24 25 26 27 28 29 30 class NotionObjectType ( Enum ): \"\"\" Enum representing Notion object types. This enum is used to specify the type of Notion object being processed, such as a page or a database. \"\"\" PAGE = \"page\" DATABASE = \"database\"","title":"NotionObjectType"},{"location":"src/extraction/datasources/pdf/","text":"Pdf Datasource This module contains functionality related to the Pdf datasource. Configuration PDFDatasourceConfiguration Bases: DatasourceConfiguration Configuration for PDF data source. This class defines the configuration parameters required for extracting data from PDF files. It inherits from the base DatasourceConfiguration class. Source code in src/extraction/datasources/pdf/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 class PDFDatasourceConfiguration ( DatasourceConfiguration ): \"\"\"Configuration for PDF data source. This class defines the configuration parameters required for extracting data from PDF files. It inherits from the base DatasourceConfiguration class. \"\"\" name : Literal [ DatasourceName . PDF ] = Field ( ... , description = \"The name of the data source.\" ) base_path : str = Field ( ... , description = \"Base path to the directory containing PDF files\" ) Document PDFDocument Bases: BaseDocument Document representation for PDF file content. Extends BaseDocument to handle PDF-specific document processing including metadata filtering for embeddings and LLM contexts. Source code in src/extraction/datasources/pdf/document.py 4 5 6 7 8 9 10 11 class PDFDocument ( BaseDocument ): \"\"\"Document representation for PDF file content. Extends BaseDocument to handle PDF-specific document processing including metadata filtering for embeddings and LLM contexts. \"\"\" pass Manager PDFDatasourceManagerFactory Bases: Factory Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object Source code in src/extraction/datasources/pdf/manager.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class PDFDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class: Type of configuration object \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : PDFDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create an instance of the PDF datasource manager. This method constructs a BasicDatasourceManager by creating the appropriate reader and parser based on the provided configuration. Args: configuration: Configuration specifying how to set up the PDF datasource manager, reader, and parser. Returns: A configured BasicDatasourceManager instance for handling PDF documents. \"\"\" reader = PDFDatasourceReaderFactory . create ( configuration ) parser = PDFDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration = configuration , reader = reader , parser = parser , ) Parser PDFDatasourceParser Bases: BaseParser [ PDFDocument ] Parser for PDF documents that converts them to structured PDFDocument objects. Uses MarkItDown to convert PDF files to markdown format for easier processing. Source code in src/extraction/datasources/pdf/parser.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class PDFDatasourceParser ( BaseParser [ PDFDocument ]): \"\"\" Parser for PDF documents that converts them to structured PDFDocument objects. Uses MarkItDown to convert PDF files to markdown format for easier processing. \"\"\" def __init__ ( self , parser : MarkItDown = MarkItDown ()): \"\"\" Initialize the PDF parser. Attributes: parser: MarkItDown parser instance for PDF to markdown conversion \"\"\" self . parser = parser def parse ( self , file_path : str ) -> PDFDocument : \"\"\" Parses the given PDF file into a structured document. Args: file_path: Path to the PDF file Returns: PDFDocument object containing the parsed content and metadata \"\"\" markdown = self . parser . convert ( file_path , file_extension = \".pdf\" ) . text_content metadata = self . _extract_metadata ( file_path ) return PDFDocument ( text = markdown , metadata = metadata ) def _extract_metadata ( self , file_path : str ) -> dict : \"\"\" Extract and process PDF metadata from the file. Args: file_path: Path to the PDF file Returns: Processed metadata dictionary with standardized fields \"\"\" metadata = default_file_metadata_func ( file_path ) metadata . update ( { \"datasource\" : \"pdf\" , \"format\" : \"pdf\" , \"url\" : None , \"title\" : os . path . basename ( file_path ), \"last_edited_date\" : metadata [ \"last_modified_date\" ], \"created_date\" : metadata [ \"creation_date\" ], } ) del metadata [ \"last_modified_date\" ] del metadata [ \"creation_date\" ] return metadata __init__ ( parser = MarkItDown ()) Initialize the PDF parser. Attributes: parser \u2013 MarkItDown parser instance for PDF to markdown conversion Source code in src/extraction/datasources/pdf/parser.py 20 21 22 23 24 25 26 27 def __init__ ( self , parser : MarkItDown = MarkItDown ()): \"\"\" Initialize the PDF parser. Attributes: parser: MarkItDown parser instance for PDF to markdown conversion \"\"\" self . parser = parser parse ( file_path ) Parses the given PDF file into a structured document. Parameters: file_path ( str ) \u2013 Path to the PDF file Returns: PDFDocument \u2013 PDFDocument object containing the parsed content and metadata Source code in src/extraction/datasources/pdf/parser.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def parse ( self , file_path : str ) -> PDFDocument : \"\"\" Parses the given PDF file into a structured document. Args: file_path: Path to the PDF file Returns: PDFDocument object containing the parsed content and metadata \"\"\" markdown = self . parser . convert ( file_path , file_extension = \".pdf\" ) . text_content metadata = self . _extract_metadata ( file_path ) return PDFDocument ( text = markdown , metadata = metadata ) PDFDatasourceParserFactory Bases: Factory Factory for creating PDF parser instances. Creates and configures PDFDatasourceParser objects according to the provided configuration. Source code in src/extraction/datasources/pdf/parser.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class PDFDatasourceParserFactory ( Factory ): \"\"\" Factory for creating PDF parser instances. Creates and configures PDFDatasourceParser objects according to the provided configuration. \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , _ : PDFDatasourceConfiguration ) -> PDFDatasourceParser : \"\"\" Creates a new instance of the PDF parser. Args: _: Configuration object for the parser (not used in this implementation) Returns: PDFDatasourceParser: Configured parser instance \"\"\" return PDFDatasourceParser () Reader PDFDatasourceReader Bases: BaseReader Source code in src/extraction/datasources/pdf/reader.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class PDFDatasourceReader ( BaseReader ): def __init__ ( self , configuration : PDFDatasourceConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize PDF reader. Args: configuration: Settings for PDF processing logger: Logger instance for logging messages \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . base_path = configuration . base_path self . logger = logger async def read_all_async ( self ) -> AsyncIterator [ str ]: \"\"\"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator[str]: An asynchronous iterator of PDF file paths \"\"\" self . logger . info ( f \"Reading PDF files from ' { self . base_path } ' with limit { self . export_limit } \" ) pdf_files = [ f for f in os . listdir ( self . base_path ) if f . endswith ( \".pdf\" ) ] files_to_load = ( pdf_files if self . export_limit is None else pdf_files [: self . export_limit ] ) for i , file_name in enumerate ( files_to_load ): self . logger . info ( f \"[ { i } / { self . export_limit } ] Reading PDF file ' { file_name } '\" ) file_path = os . path . join ( self . base_path , file_name ) if os . path . isfile ( file_path ): yield file_path __init__ ( configuration , logger = LoggerConfiguration . get_logger ( __name__ )) Initialize PDF reader. Parameters: configuration ( PDFDatasourceConfiguration ) \u2013 Settings for PDF processing logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/extraction/datasources/pdf/reader.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , configuration : PDFDatasourceConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize PDF reader. Args: configuration: Settings for PDF processing logger: Logger instance for logging messages \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . base_path = configuration . base_path self . logger = logger read_all_async () async Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator [ str ] \u2013 AsyncIterator[str]: An asynchronous iterator of PDF file paths Source code in src/extraction/datasources/pdf/reader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 async def read_all_async ( self ) -> AsyncIterator [ str ]: \"\"\"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator[str]: An asynchronous iterator of PDF file paths \"\"\" self . logger . info ( f \"Reading PDF files from ' { self . base_path } ' with limit { self . export_limit } \" ) pdf_files = [ f for f in os . listdir ( self . base_path ) if f . endswith ( \".pdf\" ) ] files_to_load = ( pdf_files if self . export_limit is None else pdf_files [: self . export_limit ] ) for i , file_name in enumerate ( files_to_load ): self . logger . info ( f \"[ { i } / { self . export_limit } ] Reading PDF file ' { file_name } '\" ) file_path = os . path . join ( self . base_path , file_name ) if os . path . isfile ( file_path ): yield file_path PDFDatasourceReaderFactory Bases: Factory Factory for creating PDF reader instances. Implements the factory pattern to produce configured PDFDatasourceReader objects based on the provided configuration settings. Source code in src/extraction/datasources/pdf/reader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class PDFDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating PDF reader instances. Implements the factory pattern to produce configured PDFDatasourceReader objects based on the provided configuration settings. \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : PDFDatasourceConfiguration ) -> PDFDatasourceReader : \"\"\"Create a new PDFDatasourceReader with the specified configuration. Args: configuration: Settings that control PDF processing behavior including base path and export limits Returns: PDFDatasourceReader: A fully configured reader instance ready for use \"\"\" return PDFDatasourceReader ( configuration = configuration )","title":"Pdf"},{"location":"src/extraction/datasources/pdf/#pdf-datasource","text":"This module contains functionality related to the Pdf datasource.","title":"Pdf Datasource"},{"location":"src/extraction/datasources/pdf/#configuration","text":"","title":"Configuration"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.configuration.PDFDatasourceConfiguration","text":"Bases: DatasourceConfiguration Configuration for PDF data source. This class defines the configuration parameters required for extracting data from PDF files. It inherits from the base DatasourceConfiguration class. Source code in src/extraction/datasources/pdf/configuration.py 11 12 13 14 15 16 17 18 19 20 21 22 23 class PDFDatasourceConfiguration ( DatasourceConfiguration ): \"\"\"Configuration for PDF data source. This class defines the configuration parameters required for extracting data from PDF files. It inherits from the base DatasourceConfiguration class. \"\"\" name : Literal [ DatasourceName . PDF ] = Field ( ... , description = \"The name of the data source.\" ) base_path : str = Field ( ... , description = \"Base path to the directory containing PDF files\" )","title":"PDFDatasourceConfiguration"},{"location":"src/extraction/datasources/pdf/#document","text":"","title":"Document"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.document.PDFDocument","text":"Bases: BaseDocument Document representation for PDF file content. Extends BaseDocument to handle PDF-specific document processing including metadata filtering for embeddings and LLM contexts. Source code in src/extraction/datasources/pdf/document.py 4 5 6 7 8 9 10 11 class PDFDocument ( BaseDocument ): \"\"\"Document representation for PDF file content. Extends BaseDocument to handle PDF-specific document processing including metadata filtering for embeddings and LLM contexts. \"\"\" pass","title":"PDFDocument"},{"location":"src/extraction/datasources/pdf/#manager","text":"","title":"Manager"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.manager.PDFDatasourceManagerFactory","text":"Bases: Factory Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class ( Type ) \u2013 Type of configuration object Source code in src/extraction/datasources/pdf/manager.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class PDFDatasourceManagerFactory ( Factory ): \"\"\"Factory for creating datasource managers. Provides type-safe creation of datasource managers based on configuration. Attributes: _configuration_class: Type of configuration object \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : PDFDatasourceConfiguration ) -> BasicDatasourceManager : \"\"\"Create an instance of the PDF datasource manager. This method constructs a BasicDatasourceManager by creating the appropriate reader and parser based on the provided configuration. Args: configuration: Configuration specifying how to set up the PDF datasource manager, reader, and parser. Returns: A configured BasicDatasourceManager instance for handling PDF documents. \"\"\" reader = PDFDatasourceReaderFactory . create ( configuration ) parser = PDFDatasourceParserFactory . create ( configuration ) return BasicDatasourceManager ( configuration = configuration , reader = reader , parser = parser , )","title":"PDFDatasourceManagerFactory"},{"location":"src/extraction/datasources/pdf/#parser","text":"","title":"Parser"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.parser.PDFDatasourceParser","text":"Bases: BaseParser [ PDFDocument ] Parser for PDF documents that converts them to structured PDFDocument objects. Uses MarkItDown to convert PDF files to markdown format for easier processing. Source code in src/extraction/datasources/pdf/parser.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class PDFDatasourceParser ( BaseParser [ PDFDocument ]): \"\"\" Parser for PDF documents that converts them to structured PDFDocument objects. Uses MarkItDown to convert PDF files to markdown format for easier processing. \"\"\" def __init__ ( self , parser : MarkItDown = MarkItDown ()): \"\"\" Initialize the PDF parser. Attributes: parser: MarkItDown parser instance for PDF to markdown conversion \"\"\" self . parser = parser def parse ( self , file_path : str ) -> PDFDocument : \"\"\" Parses the given PDF file into a structured document. Args: file_path: Path to the PDF file Returns: PDFDocument object containing the parsed content and metadata \"\"\" markdown = self . parser . convert ( file_path , file_extension = \".pdf\" ) . text_content metadata = self . _extract_metadata ( file_path ) return PDFDocument ( text = markdown , metadata = metadata ) def _extract_metadata ( self , file_path : str ) -> dict : \"\"\" Extract and process PDF metadata from the file. Args: file_path: Path to the PDF file Returns: Processed metadata dictionary with standardized fields \"\"\" metadata = default_file_metadata_func ( file_path ) metadata . update ( { \"datasource\" : \"pdf\" , \"format\" : \"pdf\" , \"url\" : None , \"title\" : os . path . basename ( file_path ), \"last_edited_date\" : metadata [ \"last_modified_date\" ], \"created_date\" : metadata [ \"creation_date\" ], } ) del metadata [ \"last_modified_date\" ] del metadata [ \"creation_date\" ] return metadata","title":"PDFDatasourceParser"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.parser.PDFDatasourceParser.__init__","text":"Initialize the PDF parser. Attributes: parser \u2013 MarkItDown parser instance for PDF to markdown conversion Source code in src/extraction/datasources/pdf/parser.py 20 21 22 23 24 25 26 27 def __init__ ( self , parser : MarkItDown = MarkItDown ()): \"\"\" Initialize the PDF parser. Attributes: parser: MarkItDown parser instance for PDF to markdown conversion \"\"\" self . parser = parser","title":"__init__"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.parser.PDFDatasourceParser.parse","text":"Parses the given PDF file into a structured document. Parameters: file_path ( str ) \u2013 Path to the PDF file Returns: PDFDocument \u2013 PDFDocument object containing the parsed content and metadata Source code in src/extraction/datasources/pdf/parser.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def parse ( self , file_path : str ) -> PDFDocument : \"\"\" Parses the given PDF file into a structured document. Args: file_path: Path to the PDF file Returns: PDFDocument object containing the parsed content and metadata \"\"\" markdown = self . parser . convert ( file_path , file_extension = \".pdf\" ) . text_content metadata = self . _extract_metadata ( file_path ) return PDFDocument ( text = markdown , metadata = metadata )","title":"parse"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.parser.PDFDatasourceParserFactory","text":"Bases: Factory Factory for creating PDF parser instances. Creates and configures PDFDatasourceParser objects according to the provided configuration. Source code in src/extraction/datasources/pdf/parser.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 class PDFDatasourceParserFactory ( Factory ): \"\"\" Factory for creating PDF parser instances. Creates and configures PDFDatasourceParser objects according to the provided configuration. \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , _ : PDFDatasourceConfiguration ) -> PDFDatasourceParser : \"\"\" Creates a new instance of the PDF parser. Args: _: Configuration object for the parser (not used in this implementation) Returns: PDFDatasourceParser: Configured parser instance \"\"\" return PDFDatasourceParser ()","title":"PDFDatasourceParserFactory"},{"location":"src/extraction/datasources/pdf/#reader","text":"","title":"Reader"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.reader.PDFDatasourceReader","text":"Bases: BaseReader Source code in src/extraction/datasources/pdf/reader.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class PDFDatasourceReader ( BaseReader ): def __init__ ( self , configuration : PDFDatasourceConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize PDF reader. Args: configuration: Settings for PDF processing logger: Logger instance for logging messages \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . base_path = configuration . base_path self . logger = logger async def read_all_async ( self ) -> AsyncIterator [ str ]: \"\"\"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator[str]: An asynchronous iterator of PDF file paths \"\"\" self . logger . info ( f \"Reading PDF files from ' { self . base_path } ' with limit { self . export_limit } \" ) pdf_files = [ f for f in os . listdir ( self . base_path ) if f . endswith ( \".pdf\" ) ] files_to_load = ( pdf_files if self . export_limit is None else pdf_files [: self . export_limit ] ) for i , file_name in enumerate ( files_to_load ): self . logger . info ( f \"[ { i } / { self . export_limit } ] Reading PDF file ' { file_name } '\" ) file_path = os . path . join ( self . base_path , file_name ) if os . path . isfile ( file_path ): yield file_path","title":"PDFDatasourceReader"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.reader.PDFDatasourceReader.__init__","text":"Initialize PDF reader. Parameters: configuration ( PDFDatasourceConfiguration ) \u2013 Settings for PDF processing logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages Source code in src/extraction/datasources/pdf/reader.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , configuration : PDFDatasourceConfiguration , logger : logging . Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\"Initialize PDF reader. Args: configuration: Settings for PDF processing logger: Logger instance for logging messages \"\"\" super () . __init__ () self . export_limit = configuration . export_limit self . base_path = configuration . base_path self . logger = logger","title":"__init__"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.reader.PDFDatasourceReader.read_all_async","text":"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator [ str ] \u2013 AsyncIterator[str]: An asynchronous iterator of PDF file paths Source code in src/extraction/datasources/pdf/reader.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 async def read_all_async ( self ) -> AsyncIterator [ str ]: \"\"\"Asynchronously yield PDF file paths from the configured directory. Retrieves a list of PDF files from the base path, applies any configured export limit, and yields each file path individually. Returns: AsyncIterator[str]: An asynchronous iterator of PDF file paths \"\"\" self . logger . info ( f \"Reading PDF files from ' { self . base_path } ' with limit { self . export_limit } \" ) pdf_files = [ f for f in os . listdir ( self . base_path ) if f . endswith ( \".pdf\" ) ] files_to_load = ( pdf_files if self . export_limit is None else pdf_files [: self . export_limit ] ) for i , file_name in enumerate ( files_to_load ): self . logger . info ( f \"[ { i } / { self . export_limit } ] Reading PDF file ' { file_name } '\" ) file_path = os . path . join ( self . base_path , file_name ) if os . path . isfile ( file_path ): yield file_path","title":"read_all_async"},{"location":"src/extraction/datasources/pdf/#src.extraction.datasources.pdf.reader.PDFDatasourceReaderFactory","text":"Bases: Factory Factory for creating PDF reader instances. Implements the factory pattern to produce configured PDFDatasourceReader objects based on the provided configuration settings. Source code in src/extraction/datasources/pdf/reader.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 class PDFDatasourceReaderFactory ( Factory ): \"\"\"Factory for creating PDF reader instances. Implements the factory pattern to produce configured PDFDatasourceReader objects based on the provided configuration settings. \"\"\" _configuration_class : Type = PDFDatasourceConfiguration @classmethod def _create_instance ( cls , configuration : PDFDatasourceConfiguration ) -> PDFDatasourceReader : \"\"\"Create a new PDFDatasourceReader with the specified configuration. Args: configuration: Settings that control PDF processing behavior including base path and export limits Returns: PDFDatasourceReader: A fully configured reader instance ready for use \"\"\" return PDFDatasourceReader ( configuration = configuration )","title":"PDFDatasourceReaderFactory"},{"location":"src/extraction/datasources/registry/","text":"Registry This module contains functionality related to the the registry module for extraction.datasources . Registry DatasourceManagerRegistry Bases: Registry Registry for datasource managers. This registry maps DatasourceName enums to their corresponding manager implementations. It provides a centralized way to register and retrieve datasource managers. Attributes: _key_class ( Type ) \u2013 Type of the key used in the registry (DatasourceName). Source code in src/extraction/datasources/registry.py 7 8 9 10 11 12 13 14 15 16 17 class DatasourceManagerRegistry ( Registry ): \"\"\"Registry for datasource managers. This registry maps DatasourceName enums to their corresponding manager implementations. It provides a centralized way to register and retrieve datasource managers. Attributes: _key_class: Type of the key used in the registry (DatasourceName). \"\"\" _key_class : Type = DatasourceName","title":"Registry"},{"location":"src/extraction/datasources/registry/#registry","text":"This module contains functionality related to the the registry module for extraction.datasources .","title":"Registry"},{"location":"src/extraction/datasources/registry/#registry_1","text":"","title":"Registry"},{"location":"src/extraction/datasources/registry/#src.extraction.datasources.registry.DatasourceManagerRegistry","text":"Bases: Registry Registry for datasource managers. This registry maps DatasourceName enums to their corresponding manager implementations. It provides a centralized way to register and retrieve datasource managers. Attributes: _key_class ( Type ) \u2013 Type of the key used in the registry (DatasourceName). Source code in src/extraction/datasources/registry.py 7 8 9 10 11 12 13 14 15 16 17 class DatasourceManagerRegistry ( Registry ): \"\"\"Registry for datasource managers. This registry maps DatasourceName enums to their corresponding manager implementations. It provides a centralized way to register and retrieve datasource managers. Attributes: _key_class: Type of the key used in the registry (DatasourceName). \"\"\" _key_class : Type = DatasourceName","title":"DatasourceManagerRegistry"},{"location":"src/extraction/orchestrators/base_orchestator/","text":"Base_orchestator This module contains functionality related to the the base_orchestator module for extraction.orchestrators . Base_orchestator BaseDatasourceOrchestrator Bases: ABC Abstract base class for datasource orchestration. Defines interface for managing content extraction, embedding generation, and vector storage operations across datasources. This class serves as a coordinator for multiple datasource managers, providing unified methods for extracting and processing documents from various data sources in both full and incremental sync modes. Note All implementing classes must provide concrete implementations of extract, embed, save and update methods. Source code in src/extraction/orchestrators/base_orchestator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseDatasourceOrchestrator ( ABC ): \"\"\"Abstract base class for datasource orchestration. Defines interface for managing content extraction, embedding generation, and vector storage operations across datasources. This class serves as a coordinator for multiple datasource managers, providing unified methods for extracting and processing documents from various data sources in both full and incremental sync modes. Note: All implementing classes must provide concrete implementations of extract, embed, save and update methods. \"\"\" def __init__ ( self , datasource_managers : List [ BaseDatasourceManager ], ): \"\"\"Initialize the orchestrator with datasource managers. Args: datasource_managers: A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. \"\"\" self . datasource_managers = datasource_managers @abstractmethod async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: An asynchronous iterator yielding BaseDocument objects representing the extracted content from all datasources. \"\"\" pass @abstractmethod async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: An asynchronous iterator yielding BaseDocument objects representing newly added or modified content from all datasources. \"\"\" pass __init__ ( datasource_managers ) Initialize the orchestrator with datasource managers. Parameters: datasource_managers ( List [ BaseDatasourceManager ] ) \u2013 A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. Source code in src/extraction/orchestrators/base_orchestator.py 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , datasource_managers : List [ BaseDatasourceManager ], ): \"\"\"Initialize the orchestrator with datasource managers. Args: datasource_managers: A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. \"\"\" self . datasource_managers = datasource_managers full_refresh_sync () abstractmethod async Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: AsyncIterator [ BaseDocument ] \u2013 An asynchronous iterator yielding BaseDocument objects representing AsyncIterator [ BaseDocument ] \u2013 the extracted content from all datasources. Source code in src/extraction/orchestrators/base_orchestator.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @abstractmethod async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: An asynchronous iterator yielding BaseDocument objects representing the extracted content from all datasources. \"\"\" pass incremental_sync () abstractmethod async Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: AsyncIterator [ BaseDocument ] \u2013 An asynchronous iterator yielding BaseDocument objects representing AsyncIterator [ BaseDocument ] \u2013 newly added or modified content from all datasources. Source code in src/extraction/orchestrators/base_orchestator.py 52 53 54 55 56 57 58 59 60 61 62 63 64 @abstractmethod async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: An asynchronous iterator yielding BaseDocument objects representing newly added or modified content from all datasources. \"\"\" pass","title":"Base_orchestator"},{"location":"src/extraction/orchestrators/base_orchestator/#base_orchestator","text":"This module contains functionality related to the the base_orchestator module for extraction.orchestrators .","title":"Base_orchestator"},{"location":"src/extraction/orchestrators/base_orchestator/#base_orchestator_1","text":"","title":"Base_orchestator"},{"location":"src/extraction/orchestrators/base_orchestator/#src.extraction.orchestrators.base_orchestator.BaseDatasourceOrchestrator","text":"Bases: ABC Abstract base class for datasource orchestration. Defines interface for managing content extraction, embedding generation, and vector storage operations across datasources. This class serves as a coordinator for multiple datasource managers, providing unified methods for extracting and processing documents from various data sources in both full and incremental sync modes. Note All implementing classes must provide concrete implementations of extract, embed, save and update methods. Source code in src/extraction/orchestrators/base_orchestator.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class BaseDatasourceOrchestrator ( ABC ): \"\"\"Abstract base class for datasource orchestration. Defines interface for managing content extraction, embedding generation, and vector storage operations across datasources. This class serves as a coordinator for multiple datasource managers, providing unified methods for extracting and processing documents from various data sources in both full and incremental sync modes. Note: All implementing classes must provide concrete implementations of extract, embed, save and update methods. \"\"\" def __init__ ( self , datasource_managers : List [ BaseDatasourceManager ], ): \"\"\"Initialize the orchestrator with datasource managers. Args: datasource_managers: A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. \"\"\" self . datasource_managers = datasource_managers @abstractmethod async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: An asynchronous iterator yielding BaseDocument objects representing the extracted content from all datasources. \"\"\" pass @abstractmethod async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: An asynchronous iterator yielding BaseDocument objects representing newly added or modified content from all datasources. \"\"\" pass","title":"BaseDatasourceOrchestrator"},{"location":"src/extraction/orchestrators/base_orchestator/#src.extraction.orchestrators.base_orchestator.BaseDatasourceOrchestrator.__init__","text":"Initialize the orchestrator with datasource managers. Parameters: datasource_managers ( List [ BaseDatasourceManager ] ) \u2013 A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. Source code in src/extraction/orchestrators/base_orchestator.py 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , datasource_managers : List [ BaseDatasourceManager ], ): \"\"\"Initialize the orchestrator with datasource managers. Args: datasource_managers: A list of datasource manager instances that implement the BaseDatasourceManager interface. These managers handle the actual data extraction from specific datasource types. \"\"\" self . datasource_managers = datasource_managers","title":"__init__"},{"location":"src/extraction/orchestrators/base_orchestator/#src.extraction.orchestrators.base_orchestator.BaseDatasourceOrchestrator.full_refresh_sync","text":"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: AsyncIterator [ BaseDocument ] \u2013 An asynchronous iterator yielding BaseDocument objects representing AsyncIterator [ BaseDocument ] \u2013 the extracted content from all datasources. Source code in src/extraction/orchestrators/base_orchestator.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @abstractmethod async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract content from configured datasources. Performs asynchronous content extraction from all configured datasource implementations. This method should perform a complete refresh of all available content from the datasources, regardless of previous sync state. Returns: An asynchronous iterator yielding BaseDocument objects representing the extracted content from all datasources. \"\"\" pass","title":"full_refresh_sync"},{"location":"src/extraction/orchestrators/base_orchestator/#src.extraction.orchestrators.base_orchestator.BaseDatasourceOrchestrator.incremental_sync","text":"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: AsyncIterator [ BaseDocument ] \u2013 An asynchronous iterator yielding BaseDocument objects representing AsyncIterator [ BaseDocument ] \u2013 newly added or modified content from all datasources. Source code in src/extraction/orchestrators/base_orchestator.py 52 53 54 55 56 57 58 59 60 61 62 63 64 @abstractmethod async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Perform an incremental sync from configured datasources. Extracts only new or modified content since the last sync operation. This method is designed for efficient regular updates without re-processing unchanged content. Returns: An asynchronous iterator yielding BaseDocument objects representing newly added or modified content from all datasources. \"\"\" pass","title":"incremental_sync"},{"location":"src/extraction/orchestrators/registry/","text":"Registry This module contains functionality related to the the registry module for extraction.orchestrators . Registry DatasourceOrchestratorRegistry Bases: Registry Registry for datasource orchestrators. A specialized registry implementation that maps OrchestratorName enum values to their corresponding orchestrator factories. Attributes: _key_class \u2013 The class type used as registry keys, specifically OrchestratorName. Source code in src/extraction/orchestrators/registry.py 5 6 7 8 9 10 11 12 13 14 15 16 class DatasourceOrchestratorRegistry ( Registry ): \"\"\" Registry for datasource orchestrators. A specialized registry implementation that maps OrchestratorName enum values to their corresponding orchestrator factories. Attributes: _key_class: The class type used as registry keys, specifically OrchestratorName. \"\"\" _key_class = OrchestratorName","title":"Registry"},{"location":"src/extraction/orchestrators/registry/#registry","text":"This module contains functionality related to the the registry module for extraction.orchestrators .","title":"Registry"},{"location":"src/extraction/orchestrators/registry/#registry_1","text":"","title":"Registry"},{"location":"src/extraction/orchestrators/registry/#src.extraction.orchestrators.registry.DatasourceOrchestratorRegistry","text":"Bases: Registry Registry for datasource orchestrators. A specialized registry implementation that maps OrchestratorName enum values to their corresponding orchestrator factories. Attributes: _key_class \u2013 The class type used as registry keys, specifically OrchestratorName. Source code in src/extraction/orchestrators/registry.py 5 6 7 8 9 10 11 12 13 14 15 16 class DatasourceOrchestratorRegistry ( Registry ): \"\"\" Registry for datasource orchestrators. A specialized registry implementation that maps OrchestratorName enum values to their corresponding orchestrator factories. Attributes: _key_class: The class type used as registry keys, specifically OrchestratorName. \"\"\" _key_class = OrchestratorName","title":"DatasourceOrchestratorRegistry"},{"location":"src/extraction/orchestrators/basic/orchestrator/","text":"Orchestrator This module contains functionality related to the the orchestrator module for extraction.orchestrators.basic . Orchestrator BasicDatasourceOrchestrator Bases: BaseDatasourceOrchestrator Orchestrator for multi-datasource content processing. Source code in src/extraction/orchestrators/basic/orchestrator.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class BasicDatasourceOrchestrator ( BaseDatasourceOrchestrator ): \"\"\" Orchestrator for multi-datasource content processing. \"\"\" async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources \"\"\" for datasource_manager in self . datasource_managers : async for document in datasource_manager . full_refresh_sync (): yield document async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\" Not implemented yet. \"\"\" raise NotImplementedError ( \"Incremental sync is not supported yet.\" ) full_refresh_sync () async Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator [ BaseDocument ] \u2013 AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources Source code in src/extraction/orchestrators/basic/orchestrator.py 17 18 19 20 21 22 23 24 25 26 27 async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources \"\"\" for datasource_manager in self . datasource_managers : async for document in datasource_manager . full_refresh_sync (): yield document incremental_sync () async Not implemented yet. Source code in src/extraction/orchestrators/basic/orchestrator.py 29 30 31 32 33 async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\" Not implemented yet. \"\"\" raise NotImplementedError ( \"Incremental sync is not supported yet.\" ) BasicDatasourceOrchestratorFactory Bases: Factory Factory for creating BasicDatasourceOrchestrator instances. Creates orchestrator instances configured with appropriate datasource managers based on the provided extraction configuration. Source code in src/extraction/orchestrators/basic/orchestrator.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class BasicDatasourceOrchestratorFactory ( Factory ): \"\"\"Factory for creating BasicDatasourceOrchestrator instances. Creates orchestrator instances configured with appropriate datasource managers based on the provided extraction configuration. \"\"\" _configuration_class : Type = ExtractionConfiguration @classmethod def _create_instance ( cls , configuration : ExtractionConfiguration ) -> BasicDatasourceOrchestrator : \"\"\"Creates a configured BasicDatasourceOrchestrator. Initializes datasource managers for each configured datasource and creates an orchestrator instance with those managers. Args: configuration: Settings for extraction process configuration Returns: BasicDatasourceOrchestrator: Configured orchestrator instance \"\"\" datasource_managers = [ DatasourceManagerRegistry . get ( datasource_configuration . name ) . create ( datasource_configuration ) for datasource_configuration in configuration . extraction . datasources ] return BasicDatasourceOrchestrator ( datasource_managers = datasource_managers )","title":"Orchestrator"},{"location":"src/extraction/orchestrators/basic/orchestrator/#orchestrator","text":"This module contains functionality related to the the orchestrator module for extraction.orchestrators.basic .","title":"Orchestrator"},{"location":"src/extraction/orchestrators/basic/orchestrator/#orchestrator_1","text":"","title":"Orchestrator"},{"location":"src/extraction/orchestrators/basic/orchestrator/#src.extraction.orchestrators.basic.orchestrator.BasicDatasourceOrchestrator","text":"Bases: BaseDatasourceOrchestrator Orchestrator for multi-datasource content processing. Source code in src/extraction/orchestrators/basic/orchestrator.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class BasicDatasourceOrchestrator ( BaseDatasourceOrchestrator ): \"\"\" Orchestrator for multi-datasource content processing. \"\"\" async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources \"\"\" for datasource_manager in self . datasource_managers : async for document in datasource_manager . full_refresh_sync (): yield document async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\" Not implemented yet. \"\"\" raise NotImplementedError ( \"Incremental sync is not supported yet.\" )","title":"BasicDatasourceOrchestrator"},{"location":"src/extraction/orchestrators/basic/orchestrator/#src.extraction.orchestrators.basic.orchestrator.BasicDatasourceOrchestrator.full_refresh_sync","text":"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator [ BaseDocument ] \u2013 AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources Source code in src/extraction/orchestrators/basic/orchestrator.py 17 18 19 20 21 22 23 24 25 26 27 async def full_refresh_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\"Extract and process content from all datasources. Processes each configured datasource to extract documents and clean content. Returns: AsyncIterator[BaseDocument]: Stream of documents extracted from all datasources \"\"\" for datasource_manager in self . datasource_managers : async for document in datasource_manager . full_refresh_sync (): yield document","title":"full_refresh_sync"},{"location":"src/extraction/orchestrators/basic/orchestrator/#src.extraction.orchestrators.basic.orchestrator.BasicDatasourceOrchestrator.incremental_sync","text":"Not implemented yet. Source code in src/extraction/orchestrators/basic/orchestrator.py 29 30 31 32 33 async def incremental_sync ( self ) -> AsyncIterator [ BaseDocument ]: \"\"\" Not implemented yet. \"\"\" raise NotImplementedError ( \"Incremental sync is not supported yet.\" )","title":"incremental_sync"},{"location":"src/extraction/orchestrators/basic/orchestrator/#src.extraction.orchestrators.basic.orchestrator.BasicDatasourceOrchestratorFactory","text":"Bases: Factory Factory for creating BasicDatasourceOrchestrator instances. Creates orchestrator instances configured with appropriate datasource managers based on the provided extraction configuration. Source code in src/extraction/orchestrators/basic/orchestrator.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class BasicDatasourceOrchestratorFactory ( Factory ): \"\"\"Factory for creating BasicDatasourceOrchestrator instances. Creates orchestrator instances configured with appropriate datasource managers based on the provided extraction configuration. \"\"\" _configuration_class : Type = ExtractionConfiguration @classmethod def _create_instance ( cls , configuration : ExtractionConfiguration ) -> BasicDatasourceOrchestrator : \"\"\"Creates a configured BasicDatasourceOrchestrator. Initializes datasource managers for each configured datasource and creates an orchestrator instance with those managers. Args: configuration: Settings for extraction process configuration Returns: BasicDatasourceOrchestrator: Configured orchestrator instance \"\"\" datasource_managers = [ DatasourceManagerRegistry . get ( datasource_configuration . name ) . create ( datasource_configuration ) for datasource_configuration in configuration . extraction . datasources ] return BasicDatasourceOrchestrator ( datasource_managers = datasource_managers )","title":"BasicDatasourceOrchestratorFactory"},{"location":"src/jobs/queries_retention/","text":"Queries_retention This module contains functionality related to the the queries_retention module for jobs . Queries_retention LangfuseRenetionJob Langfuse V2 API does not support trace deletion, therefore we need to delete traces manually. Source code in src/jobs/queries_retention.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class LangfuseRenetionJob : \"\"\"Langfuse V2 API does not support trace deletion, therefore we need to delete traces manually.\"\"\" def __init__ ( self , configuration : LangfuseConfiguration , client : Langfuse , logger : Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: langfuse_client (Langfuse): The Langfuse client instance to interact with the Langfuse API. logger (Logger): Logger instance for logging messages. \"\"\" self . configuration = configuration self . client = client self . logger = logger def run ( self ): retention_days = self . configuration . retention_job . retention_days cutoff_date = datetime . now () - timedelta ( days = retention_days ) ids = list ( self . _get_traces ( cutoff_date )) self . logger . info ( f \"Found { len ( ids ) } traces to delete older than { cutoff_date } .\" ) self . delete_traces ( ids ) def _get_traces ( self , cutoff_date : datetime ) -> Iterator [ int ]: \"\"\" Fetches traces from Langfuse that are older than the specified cutoff date. Args: cuttoff_date (datetime): The date before which traces will be fetched. Returns: Iterator: An iterator over the fetched traces. \"\"\" limit = 100 response = self . client . fetch_traces ( to_timestamp = cutoff_date , limit = limit ) total_pages = response . meta . total_pages for trace in response . data : yield trace . id for page in range ( 2 , total_pages + 1 ): response = self . client . fetch_traces ( to_timestamp = cutoff_date , page = page , limit = limit ) for trace in response . data : yield trace . id def delete_traces ( self , ids : List [ int ]) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" if not ids : self . logger . info ( \"No traces to delete.\" ) return connnection = self . _get_pg_connection () try : self . _delete_related_dataset_run_items ( trace_ids = ids , connection = connnection ) self . _delete_related_dataset_items ( trace_ids = ids , connection = connnection ) self . _delete_related_scores ( trace_ids = ids , connection = connnection ) self . _delete_related_observations ( trace_ids = ids , connection = connnection ) self . _delete_traces ( trace_ids = ids , connection = connnection ) self . logger . info ( f \"Deleted { len ( ids ) } traces.\" ) except psycopg2 . Error as e : self . logger . error ( f \"Failed to delete traces. Database error: { e } \" ) connnection . rollback () finally : connnection . close () def _delete_related_dataset_items ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes datasets related to the specified trace IDs. Args: ids (List[int]): A list of trace IDs for which related datasets will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM dataset_items WHERE source_trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } datasets related to traces.\" ) def _delete_related_dataset_run_items ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes dataset runs related to the specified trace IDs. Args: ids (List[int]): A list of trace IDs for which related dataset runs will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM dataset_run_items WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } dataset runs related to traces.\" ) def _delete_related_scores ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes scores related to the specified trace IDs. Args: trace_ids (List[int]): A list of trace IDs for which related scores will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM scores WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } scores related to traces.\" ) def _delete_related_observations ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes observations (spans, generations, events) related to the specified trace IDs. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM observations WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } observations related to traces.\" ) def _delete_traces ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM traces WHERE id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { len ( trace_ids ) } traces.\" ) def _get_pg_connection ( self ): \"\"\" Retrieves a PostgreSQL connection using the configuration. Returns: psycopg2.extensions.connection: A PostgreSQL connection object. \"\"\" return psycopg2 . connect ( host = self . configuration . database . host , port = self . configuration . database . port , database = self . configuration . database . db , user = self . configuration . database . secrets . user . get_secret_value (), password = self . configuration . database . secrets . password . get_secret_value (), ) __init__ ( configuration , client , logger = LoggerConfiguration . get_logger ( __name__ )) Parameters: langfuse_client ( Langfuse ) \u2013 The Langfuse client instance to interact with the Langfuse API. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages. Source code in src/jobs/queries_retention.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , configuration : LangfuseConfiguration , client : Langfuse , logger : Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: langfuse_client (Langfuse): The Langfuse client instance to interact with the Langfuse API. logger (Logger): Logger instance for logging messages. \"\"\" self . configuration = configuration self . client = client self . logger = logger delete_traces ( ids ) Deletes traces with the specified IDs from Langfuse. Parameters: trace_ids ( List [ int ] ) \u2013 A list of trace IDs to delete. Source code in src/jobs/queries_retention.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def delete_traces ( self , ids : List [ int ]) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" if not ids : self . logger . info ( \"No traces to delete.\" ) return connnection = self . _get_pg_connection () try : self . _delete_related_dataset_run_items ( trace_ids = ids , connection = connnection ) self . _delete_related_dataset_items ( trace_ids = ids , connection = connnection ) self . _delete_related_scores ( trace_ids = ids , connection = connnection ) self . _delete_related_observations ( trace_ids = ids , connection = connnection ) self . _delete_traces ( trace_ids = ids , connection = connnection ) self . logger . info ( f \"Deleted { len ( ids ) } traces.\" ) except psycopg2 . Error as e : self . logger . error ( f \"Failed to delete traces. Database error: { e } \" ) connnection . rollback () finally : connnection . close () LangfuseRenetionJobFactory Bases: Factory Source code in src/jobs/queries_retention.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class LangfuseRenetionJobFactory ( Factory ): _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> LangfuseRenetionJob : \"\"\" Create an instance of QueriesRetentionService using the provided configuration. Args: configuration (LangfuseConfiguration): The configuration object used to create the instance. Returns: QueriesRetentionService: A new instance of QueriesRetentionService. \"\"\" langfuse_client = LangfuseClientFactory . create ( configuration = configuration ) return LangfuseRenetionJob ( configuration = configuration , client = langfuse_client )","title":"Queries_retention"},{"location":"src/jobs/queries_retention/#queries_retention","text":"This module contains functionality related to the the queries_retention module for jobs .","title":"Queries_retention"},{"location":"src/jobs/queries_retention/#queries_retention_1","text":"","title":"Queries_retention"},{"location":"src/jobs/queries_retention/#src.jobs.queries_retention.LangfuseRenetionJob","text":"Langfuse V2 API does not support trace deletion, therefore we need to delete traces manually. Source code in src/jobs/queries_retention.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 class LangfuseRenetionJob : \"\"\"Langfuse V2 API does not support trace deletion, therefore we need to delete traces manually.\"\"\" def __init__ ( self , configuration : LangfuseConfiguration , client : Langfuse , logger : Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: langfuse_client (Langfuse): The Langfuse client instance to interact with the Langfuse API. logger (Logger): Logger instance for logging messages. \"\"\" self . configuration = configuration self . client = client self . logger = logger def run ( self ): retention_days = self . configuration . retention_job . retention_days cutoff_date = datetime . now () - timedelta ( days = retention_days ) ids = list ( self . _get_traces ( cutoff_date )) self . logger . info ( f \"Found { len ( ids ) } traces to delete older than { cutoff_date } .\" ) self . delete_traces ( ids ) def _get_traces ( self , cutoff_date : datetime ) -> Iterator [ int ]: \"\"\" Fetches traces from Langfuse that are older than the specified cutoff date. Args: cuttoff_date (datetime): The date before which traces will be fetched. Returns: Iterator: An iterator over the fetched traces. \"\"\" limit = 100 response = self . client . fetch_traces ( to_timestamp = cutoff_date , limit = limit ) total_pages = response . meta . total_pages for trace in response . data : yield trace . id for page in range ( 2 , total_pages + 1 ): response = self . client . fetch_traces ( to_timestamp = cutoff_date , page = page , limit = limit ) for trace in response . data : yield trace . id def delete_traces ( self , ids : List [ int ]) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" if not ids : self . logger . info ( \"No traces to delete.\" ) return connnection = self . _get_pg_connection () try : self . _delete_related_dataset_run_items ( trace_ids = ids , connection = connnection ) self . _delete_related_dataset_items ( trace_ids = ids , connection = connnection ) self . _delete_related_scores ( trace_ids = ids , connection = connnection ) self . _delete_related_observations ( trace_ids = ids , connection = connnection ) self . _delete_traces ( trace_ids = ids , connection = connnection ) self . logger . info ( f \"Deleted { len ( ids ) } traces.\" ) except psycopg2 . Error as e : self . logger . error ( f \"Failed to delete traces. Database error: { e } \" ) connnection . rollback () finally : connnection . close () def _delete_related_dataset_items ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes datasets related to the specified trace IDs. Args: ids (List[int]): A list of trace IDs for which related datasets will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM dataset_items WHERE source_trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } datasets related to traces.\" ) def _delete_related_dataset_run_items ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes dataset runs related to the specified trace IDs. Args: ids (List[int]): A list of trace IDs for which related dataset runs will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM dataset_run_items WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } dataset runs related to traces.\" ) def _delete_related_scores ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes scores related to the specified trace IDs. Args: trace_ids (List[int]): A list of trace IDs for which related scores will be deleted. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM scores WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } scores related to traces.\" ) def _delete_related_observations ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes observations (spans, generations, events) related to the specified trace IDs. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM observations WHERE trace_id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { cursor . rowcount } observations related to traces.\" ) def _delete_traces ( self , trace_ids : List [ int ], connection ) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" cursor = connection . cursor () query = sql . SQL ( \"DELETE FROM traces WHERE id IN %s \" ) cursor . execute ( query , ( tuple ( trace_ids ),)) connection . commit () self . logger . debug ( f \"Deleted { len ( trace_ids ) } traces.\" ) def _get_pg_connection ( self ): \"\"\" Retrieves a PostgreSQL connection using the configuration. Returns: psycopg2.extensions.connection: A PostgreSQL connection object. \"\"\" return psycopg2 . connect ( host = self . configuration . database . host , port = self . configuration . database . port , database = self . configuration . database . db , user = self . configuration . database . secrets . user . get_secret_value (), password = self . configuration . database . secrets . password . get_secret_value (), )","title":"LangfuseRenetionJob"},{"location":"src/jobs/queries_retention/#src.jobs.queries_retention.LangfuseRenetionJob.__init__","text":"Parameters: langfuse_client ( Langfuse ) \u2013 The Langfuse client instance to interact with the Langfuse API. logger ( Logger , default: get_logger ( __name__ ) ) \u2013 Logger instance for logging messages. Source code in src/jobs/queries_retention.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , configuration : LangfuseConfiguration , client : Langfuse , logger : Logger = LoggerConfiguration . get_logger ( __name__ ), ): \"\"\" Args: langfuse_client (Langfuse): The Langfuse client instance to interact with the Langfuse API. logger (Logger): Logger instance for logging messages. \"\"\" self . configuration = configuration self . client = client self . logger = logger","title":"__init__"},{"location":"src/jobs/queries_retention/#src.jobs.queries_retention.LangfuseRenetionJob.delete_traces","text":"Deletes traces with the specified IDs from Langfuse. Parameters: trace_ids ( List [ int ] ) \u2013 A list of trace IDs to delete. Source code in src/jobs/queries_retention.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def delete_traces ( self , ids : List [ int ]) -> None : \"\"\" Deletes traces with the specified IDs from Langfuse. Args: trace_ids (List[int]): A list of trace IDs to delete. \"\"\" if not ids : self . logger . info ( \"No traces to delete.\" ) return connnection = self . _get_pg_connection () try : self . _delete_related_dataset_run_items ( trace_ids = ids , connection = connnection ) self . _delete_related_dataset_items ( trace_ids = ids , connection = connnection ) self . _delete_related_scores ( trace_ids = ids , connection = connnection ) self . _delete_related_observations ( trace_ids = ids , connection = connnection ) self . _delete_traces ( trace_ids = ids , connection = connnection ) self . logger . info ( f \"Deleted { len ( ids ) } traces.\" ) except psycopg2 . Error as e : self . logger . error ( f \"Failed to delete traces. Database error: { e } \" ) connnection . rollback () finally : connnection . close ()","title":"delete_traces"},{"location":"src/jobs/queries_retention/#src.jobs.queries_retention.LangfuseRenetionJobFactory","text":"Bases: Factory Source code in src/jobs/queries_retention.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class LangfuseRenetionJobFactory ( Factory ): _configuration_class : Type = LangfuseConfiguration @classmethod def _create_instance ( cls , configuration : LangfuseConfiguration ) -> LangfuseRenetionJob : \"\"\" Create an instance of QueriesRetentionService using the provided configuration. Args: configuration (LangfuseConfiguration): The configuration object used to create the instance. Returns: QueriesRetentionService: A new instance of QueriesRetentionService. \"\"\" langfuse_client = LangfuseClientFactory . create ( configuration = configuration ) return LangfuseRenetionJob ( configuration = configuration , client = langfuse_client )","title":"LangfuseRenetionJobFactory"}]}